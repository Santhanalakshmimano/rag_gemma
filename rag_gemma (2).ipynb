{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c6e0974f15e40f98a76df63f0f44b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b76429f0e8994def883df711e3155333",
              "IPY_MODEL_73891815cad54a16832a8f9a83757722",
              "IPY_MODEL_c04c653037394709841dfd68278b3310"
            ],
            "layout": "IPY_MODEL_b67a5f0fa6b74e88b818e9ad310eb639"
          }
        },
        "b76429f0e8994def883df711e3155333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_079b743ffb23450a85852c1fef638467",
            "placeholder": "​",
            "style": "IPY_MODEL_cd9acdff4537409997d6f8cd986d4296",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "73891815cad54a16832a8f9a83757722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0768abfbc36c4cc3a609e8f36b07629e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21dd17f24f1a49acb1f17c9c50d606b1",
            "value": 2
          }
        },
        "c04c653037394709841dfd68278b3310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c8f86e5ed0b451bb85ebb74aad0346c",
            "placeholder": "​",
            "style": "IPY_MODEL_03c003bd3aee4a8da1a3d1ba78c2c0ec",
            "value": " 2/2 [00:25&lt;00:00, 10.54s/it]"
          }
        },
        "b67a5f0fa6b74e88b818e9ad310eb639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "079b743ffb23450a85852c1fef638467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd9acdff4537409997d6f8cd986d4296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0768abfbc36c4cc3a609e8f36b07629e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21dd17f24f1a49acb1f17c9c50d606b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c8f86e5ed0b451bb85ebb74aad0346c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03c003bd3aee4a8da1a3d1ba78c2c0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Colab Notebooks"
      ],
      "metadata": {
        "id": "RPmm17Z6XqIS",
        "outputId": "5c3030fe-72be-49b0-9fc9-9e4fb7e7b773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain\n",
        "!pip install langchain==0.1.6\n",
        "!pip install langchain-cli==0.0.21\n",
        "!pip install langchain-openai==0.0.6\n",
        "!pip install huggingface_hub==0.21.4\n",
        "!pip install python-dotenv==1.0.0\n",
        "!pip install pydantic==1.10.13\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers\n",
        "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E7hzb77Cn3s",
        "outputId": "c51c94d7-4b4e-4757-8d88-e15d1f611a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.1.6 in /usr/local/lib/python3.10/dist-packages (0.1.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (0.0.20)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (0.1.23)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (0.0.87)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.6) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.6) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain==0.1.6) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain==0.1.6) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.6) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.6) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.6) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.6) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain==0.1.6) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain==0.1.6) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (1.0.0)\n",
            "Requirement already satisfied: langchain-cli==0.0.21 in /usr/local/lib/python3.10/dist-packages (0.0.21)\n",
            "Requirement already satisfied: gitpython<4.0.0,>=3.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-cli==0.0.21) (3.1.43)\n",
            "Requirement already satisfied: langserve[all]>=0.0.16 in /usr/local/lib/python3.10/dist-packages (from langchain-cli==0.0.21) (0.0.51)\n",
            "Requirement already satisfied: tomlkit<0.13.0,>=0.12.2 in /usr/local/lib/python3.10/dist-packages (from langchain-cli==0.0.21) (0.12.4)\n",
            "Requirement already satisfied: typer[all]<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cli==0.0.21) (0.9.4)\n",
            "Requirement already satisfied: uvicorn<0.24.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-cli==0.0.21) (0.23.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4.0.0,>=3.1.40->langchain-cli==0.0.21) (4.0.11)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langserve[all]>=0.0.16->langchain-cli==0.0.21) (0.27.0)\n",
            "Requirement already satisfied: langchain>=0.0.333 in /usr/local/lib/python3.10/dist-packages (from langserve[all]>=0.0.16->langchain-cli==0.0.21) (0.1.6)\n",
            "Requirement already satisfied: orjson>=2 in /usr/local/lib/python3.10/dist-packages (from langserve[all]>=0.0.16->langchain-cli==0.0.21) (3.10.0)\n",
            "Requirement already satisfied: pydantic>=1 in /usr/local/lib/python3.10/dist-packages (from langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.10.13)\n",
            "Requirement already satisfied: fastapi<1,>=0.90.1 in /usr/local/lib/python3.10/dist-packages (from langserve[all]>=0.0.16->langchain-cli==0.0.21) (0.110.1)\n",
            "Requirement already satisfied: httpx-sse>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from langserve[all]>=0.0.16->langchain-cli==0.0.21) (0.4.0)\n",
            "Requirement already satisfied: sse-starlette<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.8.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli==0.0.21) (8.1.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli==0.0.21) (4.11.0)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli==0.0.21) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli==0.0.21) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli==0.0.21) (13.7.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn<0.24.0,>=0.23.2->langchain-cli==0.0.21) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.90.1->langserve[all]>=0.0.16->langchain-cli==0.0.21) (0.37.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.40->langchain-cli==0.0.21) (5.0.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->langserve[all]>=0.0.16->langchain-cli==0.0.21) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->langserve[all]>=0.0.16->langchain-cli==0.0.21) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->langserve[all]>=0.0.16->langchain-cli==0.0.21) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (0.0.20)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (0.1.23)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (0.0.87)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (8.2.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli==0.0.21) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli==0.0.21) (2.16.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (23.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli==0.0.21) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli==0.0.21) (1.0.0)\n",
            "Requirement already satisfied: langchain-openai==0.0.6 in /usr/local/lib/python3.10/dist-packages (0.0.6)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.0.6) (0.1.23)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.0.6) (1.25.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.0.6) (1.17.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.0.6) (0.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (1.33)\n",
            "Requirement already satisfied: langsmith<0.0.88,>=0.0.87 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (0.0.87)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (8.2.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (4.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.0.6) (2023.12.25)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.6) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.6) (2.0.7)\n",
            "Requirement already satisfied: huggingface_hub==0.21.4 in /usr/local/lib/python3.10/dist-packages (0.21.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.21.4) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.21.4) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.21.4) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.21.4) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.21.4) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.21.4) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.21.4) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.21.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.21.4) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.21.4) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.21.4) (2024.2.2)\n",
            "Requirement already satisfied: python-dotenv==1.0.0 in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pydantic==1.10.13 in /usr/local/lib/python3.10/dist-packages (1.10.13)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.13) (4.11.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GemmaForCausalLM\n",
        "from transformers import GemmaConfig, GemmaModel\n",
        "from langchain_community.llms import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "VK6gAj3jC35Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, we have to provide knowledge base to the LLM model and here Knowledge Base is a PDF file. And the file contains ~700 pages. The bot that we are going to develop should answer queries from this PDF file only"
      ],
      "metadata": {
        "id": "0TYhT0slDSuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read the PDF file\n",
        "#we are going to load the file using Langchain framework\n",
        "\n",
        "file_name = \"Hands on Machine Learning with Scikit Learn and Tensorflow.pdf\"\n",
        "loader = PyPDFLoader(file_name)\n",
        "pdf_content = loader.load()\n",
        "print(pdf_content)"
      ],
      "metadata": {
        "id": "y2B0J0SCEBe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b8ab3e-27d8-4320-aa86-98fa5dba36bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 0}), Document(page_content='Hands-On\\tMachine\\tLearning\\twith\\tScikit-Learn\\nand\\tTensorFlow\\nConcepts,\\tTools,\\tand\\tTechniques\\tto\\t\\nBuild\\tIntelligent\\tSystems\\nAurélien\\tGéron\\n', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 1}), Document(page_content='Hands-On\\tMachine\\tLearning\\twith\\tScikit-Learn\\tand\\tTensorFlow\\nby\\t\\nAurélien\\t\\nGéron\\nCopyright\\t©\\t2017\\tAurélien\\tGéron.\\tAll\\trights\\treserved.\\nPrinted\\tin\\tthe\\tUnited\\tStates\\tof\\tAmerica.\\nPublished\\tby\\t\\nO’Reilly\\tMedia,\\tInc.\\n,\\t1005\\tGravenstein\\tHighway\\tNorth,\\tSebastopol,\\tCA\\t95472.\\nO’Reilly\\tbooks\\tmay\\tbe\\tpurchased\\tfor\\teducational,\\tbusiness,\\tor\\tsales\\tpromotional\\tuse.\\tOnline\\teditions\\tare\\nalso\\tavailable\\tfor\\tmost\\ttitles\\t(\\nhttp://oreilly.com/safari\\n).\\tFor\\tmore\\tinformation,\\tcontact\\tour\\ncorporate/institutional\\tsales\\tdepartment:\\t800-998-9938\\tor\\t\\ncorporate@oreilly.com\\n.\\nEditor:\\n\\tNicole\\tTache\\nProduction\\tEditor:\\n\\tNicholas\\tAdams\\nCopyeditor:\\n\\tRachel\\tMonaghan\\nProofreader:\\n\\tCharles\\tRoumeliotis\\nIndexer:\\n\\tWendy\\tCatalano\\nInterior\\tDesigner:\\n\\tDavid\\tFutato\\nCover\\tDesigner:\\n\\tRandy\\tComer\\nIllustrator:\\n\\tRebecca\\tDemarest\\nMarch\\t2017:\\n\\tFirst\\tEdition', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 2}), Document(page_content='Revision\\tHistory\\tfor\\tthe\\tFirst\\tEdition\\n2017-03-10:\\n\\tFirst\\tRelease\\n2017-06-09:\\n\\tSecond\\tRelease\\nSee\\t\\nhttp://oreilly.com/catalog/errata.csp?isbn=9781491962299\\n\\tfor\\trelease\\tdetails.\\nThe\\tO’Reilly\\tlogo\\tis\\ta\\tregistered\\ttrademark\\tof\\tO’Reilly\\tMedia,\\tInc.\\t\\nHands-On\\tMachine\\tLearning\\twith\\nScikit-Learn\\tand\\tTensorFlow\\n,\\tthe\\tcover\\timage,\\tand\\trelated\\ttrade\\tdress\\tare\\ttrademarks\\tof\\tO’Reilly\\nMedia,\\tInc.\\nWhile\\tthe\\tpublisher\\tand\\tthe\\tauthor\\thave\\tused\\tgood\\tfaith\\tefforts\\tto\\tensure\\tthat\\tthe\\tinformation\\tand\\ninstructions\\tcontained\\tin\\tthis\\twork\\tare\\taccurate,\\tthe\\tpublisher\\tand\\tthe\\tauthor\\tdisclaim\\tall\\tresponsibility\\nfor\\terrors\\tor\\tomissions,\\tincluding\\twithout\\tlimitation\\tresponsibility\\tfor\\tdamages\\tresulting\\tfrom\\tthe\\tuse\\tof\\nor\\treliance\\ton\\tthis\\twork.\\tUse\\tof\\tthe\\tinformation\\tand\\tinstructions\\tcontained\\tin\\tthis\\twork\\tis\\tat\\tyour\\town\\nrisk.\\tIf\\tany\\tcode\\tsamples\\tor\\tother\\ttechnology\\tthis\\twork\\tcontains\\tor\\tdescribes\\tis\\tsubject\\tto\\topen\\tsource\\nlicenses\\tor\\tthe\\tintellectual\\tproperty\\trights\\tof\\tothers,\\tit\\tis\\tyour\\tresponsibility\\tto\\tensure\\tthat\\tyour\\tuse\\tthereof\\ncomplies\\twith\\tsuch\\tlicenses\\tand/or\\trights.\\n978-1-491-96229-9\\n[LSI]', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 3}), Document(page_content='Preface', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 4}), Document(page_content='The\\tMachine\\tLearning\\tTsunami\\nIn\\t2006,\\tGeoffrey\\tHinton\\tet\\tal.\\tpublished\\ta\\tpaper\\n1\\n\\tshowing\\thow\\tto\\ttrain\\ta\\tdeep\\tneural\\tnetwork\\tcapable\\tof\\nrecognizing\\thandwritten\\tdigits\\twith\\tstate-of-the-art\\tprecision\\t(>98%).\\tThey\\tbranded\\tthis\\ttechnique\\t“Deep\\nLearning.”\\t\\nTraining\\ta\\tdeep\\tneural\\tnet\\twas\\twidely\\tconsidered\\timpossible\\tat\\tthe\\ttime,\\n2\\n\\tand\\tmost\\nresearchers\\thad\\tabandoned\\tthe\\tidea\\tsince\\tthe\\t1990s.\\tThis\\tpaper\\trevived\\tthe\\tinterest\\tof\\tthe\\tscientific\\ncommunity\\tand\\tbefore\\tlong\\tmany\\tnew\\tpapers\\tdemonstrated\\tthat\\tDeep\\tLearning\\twas\\tnot\\tonly\\tpossible,\\tbut\\ncapable\\tof\\tmind-blowing\\tachievements\\tthat\\tno\\tother\\tMachine\\tLearning\\t(ML)\\ttechnique\\tcould\\thope\\tto\\nmatch\\t(with\\tthe\\thelp\\tof\\ttremendous\\tcomputing\\tpower\\tand\\tgreat\\tamounts\\tof\\tdata).\\tThis\\tenthusiasm\\tsoon\\nextended\\tto\\tmany\\tother\\tareas\\tof\\tMachine\\tLearning.\\nFast-forward\\t10\\tyears\\tand\\tMachine\\tLearning\\thas\\tconquered\\tthe\\tindustry:\\tit\\tis\\tnow\\tat\\tthe\\theart\\tof\\tmuch\\tof\\nthe\\tmagic\\tin\\ttoday’s\\thigh-tech\\tproducts,\\tranking\\tyour\\tweb\\tsearch\\tresults,\\tpowering\\tyour\\tsmartphone’s\\nspeech\\trecognition,\\tand\\trecommending\\tvideos,\\tbeating\\tthe\\tworld\\tchampion\\tat\\tthe\\tgame\\tof\\tGo.\\tBefore\\tyou\\nknow\\tit,\\tit\\twill\\tbe\\tdriving\\tyour\\tcar.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 5}), Document(page_content='Machine\\tLearning\\tin\\tYour\\tProjects\\nSo\\t\\nnaturally\\tyou\\tare\\texcited\\tabout\\tMachine\\tLearning\\tand\\tyou\\twould\\tlove\\tto\\tjoin\\tthe\\tparty!\\nPerhaps\\tyou\\twould\\tlike\\tto\\tgive\\tyour\\thomemade\\trobot\\ta\\tbrain\\tof\\tits\\town?\\tMake\\tit\\trecognize\\tfaces?\\tOr\\nlearn\\tto\\twalk\\taround?\\nOr\\tmaybe\\tyour\\tcompany\\thas\\ttons\\tof\\tdata\\t(user\\tlogs,\\tfinancial\\tdata,\\tproduction\\tdata,\\tmachine\\tsensor\\tdata,\\nhotline\\tstats,\\tHR\\treports,\\tetc.),\\tand\\tmore\\tthan\\tlikely\\tyou\\tcould\\tunearth\\tsome\\thidden\\tgems\\tif\\tyou\\tjust\\tknew\\nwhere\\tto\\tlook;\\tfor\\texample:\\nSegment\\tcustomers\\tand\\tfind\\tthe\\tbest\\tmarketing\\tstrategy\\tfor\\teach\\tgroup\\nRecommend\\tproducts\\tfor\\teach\\tclient\\tbased\\ton\\twhat\\tsimilar\\tclients\\tbought\\nDetect\\twhich\\ttransactions\\tare\\tlikely\\tto\\tbe\\tfraudulent\\nPredict\\tnext\\tyear’s\\trevenue\\nAnd\\tmore\\nWhatever\\tthe\\treason,\\tyou\\thave\\tdecided\\tto\\tlearn\\tMachine\\tLearning\\tand\\timplement\\tit\\tin\\tyour\\tprojects.\\nGreat\\tidea!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 6}), Document(page_content='Objective\\tand\\tApproach\\nThis\\tbook\\tassumes\\tthat\\tyou\\tknow\\tclose\\tto\\tnothing\\tabout\\tMachine\\tLearning.\\tIts\\tgoal\\tis\\tto\\tgive\\tyou\\tthe\\nconcepts,\\tthe\\tintuitions,\\tand\\tthe\\ttools\\tyou\\tneed\\tto\\tactually\\timplement\\tprograms\\tcapable\\tof\\t\\nlearning\\tfrom\\ndata\\n.\\nWe\\twill\\tcover\\ta\\tlarge\\tnumber\\tof\\ttechniques,\\tfrom\\tthe\\tsimplest\\tand\\tmost\\tcommonly\\tused\\t(such\\tas\\tlinear\\nregression)\\tto\\tsome\\tof\\tthe\\tDeep\\tLearning\\ttechniques\\tthat\\tregularly\\twin\\tcompetitions.\\nRather\\tthan\\timplementing\\tour\\town\\ttoy\\tversions\\tof\\teach\\talgorithm,\\twe\\twill\\tbe\\tusing\\tactual\\tproduction-\\nready\\tPython\\tframeworks:\\nScikit-Learn\\n\\tis\\t\\nvery\\teasy\\tto\\tuse,\\tyet\\tit\\timplements\\tmany\\tMachine\\tLearning\\talgorithms\\tefficiently,\\tso\\nit\\tmakes\\tfor\\ta\\tgreat\\tentry\\tpoint\\tto\\tlearn\\tMachine\\tLearning.\\nTensorFlow\\n\\tis\\t\\na\\tmore\\tcomplex\\tlibrary\\tfor\\tdistributed\\tnumerical\\tcomputation\\tusing\\tdata\\tflow\\tgraphs.\\nIt\\tmakes\\tit\\tpossible\\tto\\ttrain\\tand\\trun\\tvery\\tlarge\\tneural\\tnetworks\\tefficiently\\tby\\tdistributing\\tthe\\ncomputations\\tacross\\tpotentially\\tthousands\\tof\\tmulti-GPU\\tservers.\\tTensorFlow\\twas\\tcreated\\tat\\tGoogle\\nand\\tsupports\\tmany\\tof\\ttheir\\tlarge-scale\\tMachine\\tLearning\\tapplications.\\tIt\\twas\\topen-sourced\\tin\\nNovember\\t2015.\\nThe\\tbook\\tfavors\\ta\\thands-on\\tapproach,\\tgrowing\\tan\\tintuitive\\tunderstanding\\tof\\tMachine\\tLearning\\tthrough\\nconcrete\\tworking\\texamples\\tand\\tjust\\ta\\tlittle\\tbit\\tof\\ttheory.\\tWhile\\tyou\\tcan\\tread\\tthis\\tbook\\twithout\\tpicking\\tup\\nyour\\tlaptop,\\twe\\thighly\\trecommend\\tyou\\texperiment\\twith\\tthe\\tcode\\texamples\\tavailable\\tonline\\tas\\tJupyter\\nnotebooks\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 7}), Document(page_content='Prerequisites\\nThis\\tbook\\tassumes\\tthat\\tyou\\thave\\tsome\\tPython\\tprogramming\\texperience\\tand\\tthat\\tyou\\tare\\tfamiliar\\twith\\nPython’s\\tmain\\tscientific\\tlibraries,\\tin\\tparticular\\t\\nNumPy\\n,\\t\\nPandas\\n,\\tand\\t\\nMatplotlib\\n.\\nAlso,\\tif\\tyou\\tcare\\tabout\\twhat’s\\tunder\\tthe\\thood\\tyou\\tshould\\thave\\ta\\treasonable\\tunderstanding\\tof\\tcollege-\\nlevel\\tmath\\tas\\twell\\t(calculus,\\tlinear\\talgebra,\\tprobabilities,\\tand\\tstatistics).\\nIf\\tyou\\tdon’t\\tknow\\tPython\\tyet,\\t\\nhttp://learnpython.org/\\n\\tis\\ta\\tgreat\\tplace\\tto\\tstart.\\tThe\\tofficial\\ttutorial\\ton\\npython.org\\n\\tis\\talso\\tquite\\tgood.\\nIf\\tyou\\thave\\tnever\\tused\\tJupyter,\\t\\nChapter\\t2\\n\\twill\\tguide\\tyou\\tthrough\\tinstallation\\tand\\tthe\\tbasics:\\tit\\tis\\ta\\tgreat\\ntool\\tto\\thave\\tin\\tyour\\ttoolbox.\\nIf\\tyou\\tare\\tnot\\tfamiliar\\twith\\tPython’s\\tscientific\\tlibraries,\\tthe\\tprovided\\tJupyter\\tnotebooks\\tinclude\\ta\\tfew\\ntutorials.\\tThere\\tis\\talso\\ta\\tquick\\tmath\\ttutorial\\tfor\\tlinear\\talgebra.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 8}), Document(page_content='Roadmap\\nThis\\tbook\\tis\\torganized\\tin\\ttwo\\tparts.\\t\\nPart\\tI,\\t\\nThe\\tFundamentals\\tof\\t\\nMachine\\tLearning\\n,\\tcovers\\tthe\\nfollowing\\ttopics:\\nWhat\\tis\\tMachine\\tLearning?\\tWhat\\tproblems\\tdoes\\tit\\ttry\\tto\\tsolve?\\tWhat\\tare\\tthe\\tmain\\tcategories\\tand\\nfundamental\\tconcepts\\tof\\tMachine\\tLearning\\tsystems?\\nThe\\tmain\\tsteps\\tin\\ta\\ttypical\\tMachine\\tLearning\\tproject.\\nLearning\\tby\\tfitting\\ta\\tmodel\\tto\\tdata.\\nOptimizing\\ta\\tcost\\tfunction.\\nHandling,\\tcleaning,\\tand\\tpreparing\\tdata.\\nSelecting\\tand\\tengineering\\tfeatures.\\nSelecting\\ta\\tmodel\\tand\\ttuning\\thyperparameters\\tusing\\tcross-validation.\\nThe\\tmain\\tchallenges\\tof\\tMachine\\tLearning,\\tin\\tparticular\\tunderfitting\\tand\\toverfitting\\t(the\\nbias/variance\\ttradeoff).\\nReducing\\tthe\\tdimensionality\\tof\\tthe\\ttraining\\tdata\\tto\\tfight\\tthe\\tcurse\\tof\\tdimensionality.\\nThe\\tmost\\tcommon\\tlearning\\talgorithms:\\tLinear\\tand\\tPolynomial\\tRegression,\\tLogistic\\tRegression,\\tk-\\nNearest\\tNeighbors,\\tSupport\\tVector\\tMachines,\\tDecision\\tTrees,\\tRandom\\tForests,\\tand\\tEnsemble\\nmethods.\\nPart\\tII,\\t\\nNeural\\tNetworks\\tand\\tDeep\\tLearning\\n,\\tcovers\\tthe\\tfollowing\\ttopics:\\nWhat\\tare\\tneural\\tnets?\\tWhat\\tare\\tthey\\tgood\\tfor?\\nBuilding\\tand\\ttraining\\tneural\\tnets\\tusing\\tTensorFlow.\\nThe\\tmost\\timportant\\tneural\\tnet\\tarchitectures:\\tfeedforward\\tneural\\tnets,\\tconvolutional\\tnets,\\trecurrent\\nnets,\\tlong\\tshort-term\\tmemory\\t(LSTM)\\tnets,\\tand\\tautoencoders.\\nTechniques\\tfor\\ttraining\\tdeep\\tneural\\tnets.\\nScaling\\tneural\\tnetworks\\tfor\\thuge\\tdatasets.\\nReinforcement\\tlearning.\\nThe\\tfirst\\tpart\\tis\\tbased\\tmostly\\ton\\tScikit-Learn\\twhile\\tthe\\tsecond\\tpart\\tuses\\tTensorFlow.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 9}), Document(page_content='CAUTION\\nDon’t\\t\\njump\\tinto\\tdeep\\twaters\\ttoo\\thastily:\\twhile\\tDeep\\tLearning\\tis\\tno\\tdoubt\\tone\\tof\\tthe\\tmost\\texciting\\tareas\\tin\\tMachine\\tLearning,\\nyou\\tshould\\tmaster\\tthe\\tfundamentals\\tfirst.\\tMoreover,\\tmost\\tproblems\\tcan\\tbe\\tsolved\\tquite\\twell\\tusing\\tsimpler\\ttechniques\\tsuch\\tas\\nRandom\\tForests\\tand\\tEnsemble\\tmethods\\t(discussed\\tin\\t\\nPart\\tI\\n).\\tDeep\\tLearning\\tis\\tbest\\tsuited\\tfor\\tcomplex\\tproblems\\tsuch\\tas\\timage\\nrecognition,\\tspeech\\trecognition,\\tor\\tnatural\\tlanguage\\tprocessing,\\tprovided\\tyou\\thave\\tenough\\tdata,\\tcomputing\\tpower,\\tand\\tpatience.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 10}), Document(page_content='Other\\tResources\\nMany\\t\\nresources\\tare\\tavailable\\tto\\tlearn\\tabout\\tMachine\\tLearning.\\tAndrew\\tNg’s\\t\\nML\\tcourse\\ton\\tCoursera\\n\\tand\\nGeoffrey\\tHinton’s\\t\\ncourse\\ton\\tneural\\tnetworks\\tand\\tDeep\\tLearning\\n\\tare\\tamazing,\\talthough\\tthey\\tboth\\trequire\\ta\\nsignificant\\ttime\\tinvestment\\t(think\\tmonths).\\nThere\\tare\\talso\\tmany\\tinteresting\\twebsites\\tabout\\tMachine\\tLearning,\\tincluding\\tof\\tcourse\\tScikit-Learn’s\\nexceptional\\t\\nUser\\tGuide\\n.\\tYou\\tmay\\talso\\tenjoy\\t\\nDataquest\\n,\\t\\nwhich\\tprovides\\tvery\\tnice\\tinteractive\\ttutorials,\\nand\\tML\\tblogs\\tsuch\\tas\\tthose\\tlisted\\ton\\t\\nQuora\\n.\\tFinally,\\tthe\\t\\nDeep\\tLearning\\twebsite\\n\\thas\\ta\\tgood\\tlist\\tof\\nresources\\tto\\tlearn\\tmore.\\nOf\\tcourse\\tthere\\tare\\talso\\tmany\\tother\\tintroductory\\tbooks\\tabout\\tMachine\\tLearning,\\tin\\tparticular:\\nJoel\\tGrus,\\t\\nData\\tScience\\tfrom\\tScratch\\n\\t(O’Reilly).\\tThis\\tbook\\tpresents\\tthe\\tfundamentals\\tof\\tMachine\\nLearning,\\tand\\timplements\\tsome\\tof\\tthe\\tmain\\talgorithms\\tin\\tpure\\tPython\\t(from\\tscratch,\\tas\\tthe\\tname\\nsuggests).\\nStephen\\tMarsland,\\t\\nMachine\\tLearning:\\tAn\\tAlgorithmic\\tPerspective\\n\\t(Chapman\\tand\\tHall).\\tThis\\tbook\\nis\\ta\\tgreat\\tintroduction\\tto\\tMachine\\tLearning,\\tcovering\\ta\\twide\\trange\\tof\\ttopics\\tin\\tdepth,\\twith\\tcode\\nexamples\\tin\\tPython\\t(also\\tfrom\\tscratch,\\tbut\\tusing\\tNumPy).\\nSebastian\\tRaschka,\\t\\nPython\\tMachine\\tLearning\\n\\t(Packt\\tPublishing).\\tAlso\\ta\\tgreat\\tintroduction\\tto\\nMachine\\tLearning,\\tthis\\tbook\\tleverages\\tPython\\topen\\tsource\\tlibraries\\t(Pylearn\\t2\\tand\\tTheano).\\nYaser\\tS.\\tAbu-Mostafa,\\tMalik\\tMagdon-Ismail,\\tand\\tHsuan-Tien\\tLin,\\t\\nLearning\\tfrom\\tData\\n(AMLBook).\\tA\\trather\\ttheoretical\\tapproach\\tto\\tML,\\tthis\\tbook\\tprovides\\tdeep\\tinsights,\\tin\\tparticular\\ton\\nthe\\tbias/variance\\ttradeoff\\t(see\\t\\nChapter\\t4\\n).\\nStuart\\tRussell\\tand\\tPeter\\tNorvig,\\t\\nArtificial\\tIntelligence:\\tA\\tModern\\tApproach,\\t3rd\\tEdition\\n(Pearson).\\tThis\\tis\\ta\\tgreat\\t(and\\thuge)\\tbook\\tcovering\\tan\\tincredible\\tamount\\tof\\ttopics,\\tincluding\\nMachine\\tLearning.\\tIt\\thelps\\tput\\tML\\tinto\\tperspective.\\nFinally,\\ta\\tgreat\\tway\\tto\\tlearn\\tis\\tto\\tjoin\\tML\\tcompetition\\twebsites\\tsuch\\tas\\t\\nKaggle.com\\n\\tthis\\twill\\tallow\\tyou\\nto\\tpractice\\tyour\\tskills\\ton\\treal-world\\tproblems,\\twith\\thelp\\tand\\tinsights\\tfrom\\tsome\\tof\\tthe\\tbest\\tML\\nprofessionals\\tout\\t\\nthere.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 11}), Document(page_content='Conventions\\tUsed\\tin\\tThis\\tBook\\nThe\\tfollowing\\ttypographical\\tconventions\\tare\\tused\\tin\\tthis\\tbook:\\nItalic\\nIndicates\\tnew\\tterms,\\tURLs,\\temail\\taddresses,\\tfilenames,\\tand\\tfile\\textensions.\\nConstant\\twidth\\nUsed\\tfor\\tprogram\\tlistings,\\tas\\twell\\tas\\twithin\\tparagraphs\\tto\\trefer\\tto\\tprogram\\telements\\tsuch\\tas\\nvariable\\tor\\tfunction\\tnames,\\tdatabases,\\tdata\\ttypes,\\tenvironment\\tvariables,\\tstatements\\tand\\tkeywords.\\nConstant\\twidth\\tbold\\nShows\\tcommands\\tor\\tother\\ttext\\tthat\\tshould\\tbe\\ttyped\\tliterally\\tby\\tthe\\tuser.\\nConstant\\twidth\\titalic\\nShows\\ttext\\tthat\\tshould\\tbe\\treplaced\\twith\\tuser-supplied\\tvalues\\tor\\tby\\tvalues\\tdetermined\\tby\\tcontext.\\nTIP\\nThis\\telement\\tsignifies\\ta\\ttip\\tor\\tsuggestion.\\nNOTE\\nThis\\telement\\tsignifies\\ta\\tgeneral\\tnote.\\nWARNING\\nThis\\telement\\tindicates\\ta\\twarning\\tor\\tcaution.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 12}), Document(page_content='Using\\tCode\\tExamples\\nSupplemental\\tmaterial\\t(code\\texamples,\\texercises,\\tetc.)\\tis\\tavailable\\tfor\\tdownload\\tat\\nhttps://github.com/ageron/handson-ml\\n.\\nThis\\tbook\\tis\\there\\tto\\thelp\\tyou\\tget\\tyour\\tjob\\tdone.\\tIn\\tgeneral,\\tif\\texample\\tcode\\tis\\toffered\\twith\\tthis\\tbook,\\tyou\\nmay\\tuse\\tit\\tin\\tyour\\tprograms\\tand\\tdocumentation.\\tYou\\tdo\\tnot\\tneed\\tto\\tcontact\\tus\\tfor\\tpermission\\tunless\\nyou’re\\treproducing\\ta\\tsignificant\\tportion\\tof\\tthe\\tcode.\\tFor\\texample,\\twriting\\ta\\tprogram\\tthat\\tuses\\tseveral\\nchunks\\tof\\tcode\\tfrom\\tthis\\tbook\\tdoes\\tnot\\trequire\\tpermission.\\tSelling\\tor\\tdistributing\\ta\\tCD-ROM\\tof\\nexamples\\tfrom\\tO’Reilly\\tbooks\\tdoes\\trequire\\tpermission.\\tAnswering\\ta\\tquestion\\tby\\tciting\\tthis\\tbook\\tand\\nquoting\\texample\\tcode\\tdoes\\tnot\\trequire\\tpermission.\\tIncorporating\\ta\\tsignificant\\tamount\\tof\\texample\\tcode\\nfrom\\tthis\\tbook\\tinto\\tyour\\tproduct’s\\tdocumentation\\tdoes\\trequire\\tpermission.\\nWe\\tappreciate,\\tbut\\tdo\\tnot\\trequire,\\tattribution.\\tAn\\tattribution\\tusually\\tincludes\\tthe\\ttitle,\\tauthor,\\tpublisher,\\nand\\tISBN.\\tFor\\texample:\\t“\\nHands-On\\tMachine\\tLearning\\twith\\tScikit-Learn\\tand\\tTensorFlow\\n\\tby\\tAurélien\\nGéron\\t(O’Reilly).\\tCopyright\\t2017\\tAurélien\\tGéron,\\t978-1-491-96229-9.”\\nIf\\tyou\\tfeel\\tyour\\tuse\\tof\\tcode\\texamples\\tfalls\\toutside\\tfair\\tuse\\tor\\tthe\\tpermission\\tgiven\\tabove,\\tfeel\\tfree\\tto\\ncontact\\tus\\tat\\t\\npermissions@oreilly.com\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 13}), Document(page_content='O’Reilly\\tSafari\\nNOTE\\nSafari\\n\\t(formerly\\tSafari\\tBooks\\tOnline)\\tis\\ta\\tmembership-based\\ttraining\\tand\\treference\\tplatform\\tfor\\nenterprise,\\tgovernment,\\teducators,\\tand\\tindividuals.\\nMembers\\thave\\taccess\\tto\\tthousands\\tof\\tbooks,\\ttraining\\tvideos,\\tLearning\\tPaths,\\tinteractive\\ttutorials,\\tand\\ncurated\\tplaylists\\tfrom\\tover\\t250\\tpublishers,\\tincluding\\tO’Reilly\\tMedia,\\tHarvard\\tBusiness\\tReview,\\nPrentice\\tHall\\tProfessional,\\tAddison-Wesley\\tProfessional,\\tMicrosoft\\tPress,\\tSams,\\tQue,\\tPeachpit\\tPress,\\nAdobe,\\tFocal\\tPress,\\tCisco\\tPress,\\tJohn\\tWiley\\t&\\tSons,\\tSyngress,\\tMorgan\\tKaufmann,\\tIBM\\tRedbooks,\\nPackt,\\tAdobe\\tPress,\\tFT\\tPress,\\tApress,\\tManning,\\tNew\\tRiders,\\tMcGraw-Hill,\\tJones\\t&\\tBartlett,\\tand\\nCourse\\tTechnology,\\tamong\\tothers.\\nFor\\tmore\\tinformation,\\tplease\\tvisit\\t\\nhttp://oreilly.com/safari\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 14}), Document(page_content='How\\tto\\tContact\\tUs\\nPlease\\taddress\\tcomments\\tand\\tquestions\\tconcerning\\tthis\\tbook\\tto\\tthe\\tpublisher:\\nO’Reilly\\tMedia,\\tInc.\\n1005\\tGravenstein\\tHighway\\tNorth\\nSebastopol,\\tCA\\t95472\\n800-998-9938\\t(in\\tthe\\tUnited\\tStates\\tor\\tCanada)\\n707-829-0515\\t(international\\tor\\tlocal)\\n707-829-0104\\t(fax)\\nWe\\thave\\ta\\tweb\\tpage\\tfor\\tthis\\tbook,\\twhere\\twe\\tlist\\terrata,\\texamples,\\tand\\tany\\tadditional\\tinformation.\\tYou\\ncan\\taccess\\tthis\\tpage\\tat\\t\\nhttp://bit.ly/hands-on-machine-learning-with-scikit-learn-and-tensorflow\\n.\\nTo\\tcomment\\tor\\task\\ttechnical\\tquestions\\tabout\\tthis\\tbook,\\tsend\\temail\\tto\\t\\nbookquestions@oreilly.com\\n.\\nFor\\tmore\\tinformation\\tabout\\tour\\tbooks,\\tcourses,\\tconferences,\\tand\\tnews,\\tsee\\tour\\twebsite\\tat\\nhttp://www.oreilly.com\\n.\\nFind\\tus\\ton\\tFacebook:\\t\\nhttp://facebook.com/oreilly\\nFollow\\tus\\ton\\tTwitter:\\t\\nhttp://twitter.com/oreillymedia\\nWatch\\tus\\ton\\tYouTube:\\t\\nhttp://www.youtube.com/oreillymedia', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 15}), Document(page_content='Acknowledgments\\nI\\twould\\tlike\\tto\\tthank\\tmy\\tGoogle\\tcolleagues,\\tin\\tparticular\\tthe\\tYouTube\\tvideo\\tclassification\\tteam,\\tfor\\nteaching\\tme\\tso\\tmuch\\tabout\\tMachine\\tLearning.\\tI\\tcould\\tnever\\thave\\tstarted\\tthis\\tproject\\twithout\\tthem.\\nSpecial\\tthanks\\tto\\tmy\\tpersonal\\tML\\tgurus:\\tClément\\tCourbet,\\tJulien\\tDubois,\\tMathias\\tKende,\\tDaniel\\nKitachewsky,\\tJames\\tPack,\\tAlexander\\tPak,\\tAnosh\\tRaj,\\tVitor\\tSessak,\\tWiktor\\tTomczak,\\tIngrid\\tvon\\tGlehn,\\nRich\\tWashington,\\tand\\teveryone\\tat\\tYouTube\\tParis.\\nI\\tam\\tincredibly\\tgrateful\\tto\\tall\\tthe\\tamazing\\tpeople\\twho\\ttook\\ttime\\tout\\tof\\ttheir\\tbusy\\tlives\\tto\\treview\\tmy\\tbook\\nin\\tso\\tmuch\\tdetail.\\tThanks\\tto\\tPete\\tWarden\\tfor\\tanswering\\tall\\tmy\\tTensorFlow\\tquestions,\\treviewing\\t\\nPart\\tII\\n,\\nproviding\\tmany\\tinteresting\\tinsights,\\tand\\tof\\tcourse\\tfor\\tbeing\\tpart\\tof\\tthe\\tcore\\tTensorFlow\\tteam.\\tYou\\tshould\\ndefinitely\\tcheck\\tout\\t\\nhis\\tblog\\n!\\tMany\\tthanks\\tto\\tLukas\\tBiewald\\tfor\\this\\tvery\\tthorough\\treview\\tof\\t\\nPart\\tII\\n:\\the\\nleft\\tno\\tstone\\tunturned,\\ttested\\tall\\tthe\\tcode\\t(and\\tcaught\\ta\\tfew\\terrors),\\tmade\\tmany\\tgreat\\tsuggestions,\\tand\\this\\nenthusiasm\\twas\\tcontagious.\\tYou\\tshould\\tcheck\\tout\\t\\nhis\\tblog\\n\\tand\\this\\t\\ncool\\trobots\\n!\\tThanks\\tto\\tJustin\\tFrancis,\\nwho\\talso\\treviewed\\t\\nPart\\tII\\n\\tvery\\tthoroughly,\\tcatching\\terrors\\tand\\tproviding\\tgreat\\tinsights,\\tin\\tparticular\\tin\\nChapter\\t16\\n.\\tCheck\\tout\\t\\nhis\\tposts\\n\\ton\\tTensorFlow!\\nHuge\\tthanks\\tas\\twell\\tto\\tDavid\\tAndrzejewski,\\twho\\treviewed\\t\\nPart\\tI\\n\\tand\\tprovided\\tincredibly\\tuseful\\nfeedback,\\tidentifying\\tunclear\\tsections\\tand\\tsuggesting\\thow\\tto\\timprove\\tthem.\\tCheck\\tout\\t\\nhis\\twebsite\\n!\\nThanks\\tto\\tGrégoire\\tMesnil,\\twho\\treviewed\\t\\nPart\\tII\\n\\tand\\tcontributed\\tvery\\tinteresting\\tpractical\\tadvice\\ton\\ntraining\\tneural\\tnetworks.\\tThanks\\tas\\twell\\tto\\tEddy\\tHung,\\tSalim\\tSémaoune,\\tKarim\\tMatrah,\\tIngrid\\tvon\\nGlehn,\\tIain\\tSmears,\\tand\\tVincent\\tGuilbeau\\tfor\\treviewing\\t\\nPart\\tI\\n\\tand\\tmaking\\tmany\\tuseful\\tsuggestions.\\tAnd\\tI\\nalso\\twish\\tto\\tthank\\tmy\\tfather-in-law,\\tMichel\\tTessier,\\tformer\\tmathematics\\tteacher\\tand\\tnow\\ta\\tgreat\\ntranslator\\tof\\tAnton\\tChekhov,\\tfor\\thelping\\tme\\tiron\\tout\\tsome\\tof\\tthe\\tmathematics\\tand\\tnotations\\tin\\tthis\\tbook\\nand\\treviewing\\tthe\\tlinear\\talgebra\\tJupyter\\tnotebook.\\nAnd\\tof\\tcourse,\\ta\\tgigantic\\t“thank\\tyou”\\tto\\tmy\\tdear\\tbrother\\tSylvain,\\twho\\treviewed\\tevery\\tsingle\\tchapter,\\ntested\\tevery\\tline\\tof\\tcode,\\tprovided\\tfeedback\\ton\\tvirtually\\tevery\\tsection,\\tand\\tencouraged\\tme\\tfrom\\tthe\\tfirst\\nline\\tto\\tthe\\tlast.\\tLove\\tyou,\\tbro!\\nMany\\tthanks\\tas\\twell\\tto\\tO’Reilly’s\\tfantastic\\tstaff,\\tin\\tparticular\\tNicole\\tTache,\\twho\\tgave\\tme\\tinsightful\\nfeedback,\\talways\\tcheerful,\\tencouraging,\\tand\\thelpful.\\tThanks\\tas\\twell\\tto\\tMarie\\tBeaugureau,\\tBen\\tLorica,\\nMike\\tLoukides,\\tand\\tLaurel\\tRuma\\tfor\\tbelieving\\tin\\tthis\\tproject\\tand\\thelping\\tme\\tdefine\\tits\\tscope.\\tThanks\\tto\\nMatt\\tHacker\\tand\\tall\\tof\\tthe\\tAtlas\\tteam\\tfor\\tanswering\\tall\\tmy\\ttechnical\\tquestions\\tregarding\\tformatting,\\nasciidoc,\\tand\\tLaTeX,\\tand\\tthanks\\tto\\tRachel\\tMonaghan,\\tNick\\tAdams,\\tand\\tall\\tof\\tthe\\tproduction\\tteam\\tfor\\ntheir\\tfinal\\treview\\tand\\ttheir\\thundreds\\tof\\tcorrections.\\nLast\\tbut\\tnot\\tleast,\\tI\\tam\\tinfinitely\\tgrateful\\tto\\tmy\\tbeloved\\twife,\\tEmmanuelle,\\tand\\tto\\tour\\tthree\\twonderful\\nkids,\\tAlexandre,\\tRémi,\\tand\\tGabrielle,\\tfor\\tencouraging\\tme\\tto\\twork\\thard\\ton\\tthis\\tbook,\\tasking\\tmany\\nquestions\\t(who\\tsaid\\tyou\\tcan’t\\tteach\\tneural\\tnetworks\\tto\\ta\\tseven-year-old?),\\tand\\teven\\tbringing\\tme\\tcookies\\nand\\tcoffee.\\tWhat\\tmore\\tcan\\tone\\tdream\\tof?\\nAvailable\\ton\\tHinton’s\\thome\\tpage\\tat\\t\\nhttp://www.cs.toronto.edu/~hinton/\\n.\\nDespite\\tthe\\tfact\\tthat\\tYann\\tLecun’s\\tdeep\\tconvolutional\\tneural\\tnetworks\\thad\\tworked\\twell\\tfor\\timage\\trecognition\\tsince\\tthe\\t1990s,\\talthough\\nthey\\twere\\tnot\\tas\\tgeneral\\tpurpose.\\n1\\n2', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 16}), Document(page_content='Part\\tI.\\t\\nThe\\tFundamentals\\tof\\t\\nMachine\\tLearning', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 17}), Document(page_content='Chapter\\t1.\\t\\nThe\\tMachine\\tLearning\\tLandscape\\nWhen\\t\\nmost\\tpeople\\thear\\t“Machine\\tLearning,”\\tthey\\tpicture\\ta\\trobot:\\ta\\tdependable\\tbutler\\tor\\ta\\tdeadly\\nTerminator\\tdepending\\ton\\twho\\tyou\\task.\\tBut\\tMachine\\tLearning\\tis\\tnot\\tjust\\ta\\tfuturistic\\tfantasy,\\tit’s\\talready\\nhere.\\tIn\\tfact,\\tit\\thas\\tbeen\\taround\\tfor\\tdecades\\tin\\tsome\\tspecialized\\tapplications,\\tsuch\\tas\\t\\nOptical\\tCharacter\\nRecognition\\n\\t(OCR).\\t\\nBut\\tthe\\tfirst\\tML\\tapplication\\tthat\\treally\\tbecame\\tmainstream,\\timproving\\tthe\\tlives\\tof\\nhundreds\\tof\\tmillions\\tof\\tpeople,\\ttook\\tover\\tthe\\tworld\\tback\\tin\\tthe\\t1990s:\\tit\\twas\\t\\nthe\\t\\nspam\\tfilter\\n.\\tNot\\texactly\\na\\tself-aware\\tSkynet,\\tbut\\tit\\tdoes\\ttechnically\\tqualify\\tas\\tMachine\\tLearning\\t(it\\thas\\tactually\\tlearned\\tso\\twell\\nthat\\tyou\\tseldom\\tneed\\tto\\tflag\\tan\\temail\\tas\\tspam\\tanymore).\\tIt\\twas\\tfollowed\\tby\\thundreds\\tof\\tML\\tapplications\\nthat\\tnow\\tquietly\\tpower\\thundreds\\tof\\tproducts\\tand\\tfeatures\\tthat\\tyou\\tuse\\tregularly,\\tfrom\\tbetter\\nrecommendations\\tto\\tvoice\\tsearch.\\nWhere\\tdoes\\tMachine\\tLearning\\tstart\\tand\\twhere\\tdoes\\tit\\tend?\\tWhat\\texactly\\tdoes\\tit\\tmean\\tfor\\ta\\tmachine\\tto\\nlearn\\n\\tsomething?\\tIf\\tI\\tdownload\\ta\\tcopy\\tof\\tWikipedia,\\thas\\tmy\\tcomputer\\treally\\t“learned”\\tsomething?\\tIs\\tit\\nsuddenly\\tsmarter?\\tIn\\tthis\\tchapter\\twe\\twill\\tstart\\tby\\tclarifying\\twhat\\tMachine\\tLearning\\tis\\tand\\twhy\\tyou\\tmay\\nwant\\tto\\tuse\\tit.\\nThen,\\tbefore\\twe\\tset\\tout\\tto\\texplore\\tthe\\tMachine\\tLearning\\tcontinent,\\twe\\twill\\ttake\\ta\\tlook\\tat\\tthe\\tmap\\tand\\nlearn\\tabout\\tthe\\tmain\\tregions\\tand\\tthe\\tmost\\tnotable\\tlandmarks:\\tsupervised\\tversus\\tunsupervised\\tlearning,\\nonline\\tversus\\tbatch\\tlearning,\\tinstance-based\\tversus\\tmodel-based\\tlearning.\\tThen\\twe\\twill\\tlook\\tat\\tthe\\nworkflow\\tof\\ta\\ttypical\\tML\\tproject,\\tdiscuss\\tthe\\tmain\\tchallenges\\tyou\\tmay\\tface,\\tand\\tcover\\thow\\tto\\tevaluate\\nand\\tfine-tune\\ta\\tMachine\\tLearning\\tsystem.\\nThis\\tchapter\\tintroduces\\ta\\tlot\\tof\\tfundamental\\tconcepts\\t(and\\tjargon)\\tthat\\tevery\\tdata\\tscientist\\tshould\\tknow\\nby\\theart.\\tIt\\twill\\tbe\\ta\\thigh-level\\toverview\\t(the\\tonly\\tchapter\\twithout\\tmuch\\tcode),\\tall\\trather\\tsimple,\\tbut\\tyou\\nshould\\tmake\\tsure\\teverything\\tis\\tcrystal-clear\\tto\\tyou\\tbefore\\tcontinuing\\tto\\tthe\\trest\\tof\\tthe\\tbook.\\tSo\\tgrab\\ta\\ncoffee\\tand\\tlet’s\\tget\\tstarted!\\nTIP\\nIf\\tyou\\talready\\tknow\\tall\\tthe\\tMachine\\tLearning\\tbasics,\\tyou\\tmay\\twant\\tto\\tskip\\tdirectly\\tto\\t\\nChapter\\t2\\n.\\tIf\\tyou\\tare\\tnot\\tsure,\\ttry\\tto\\nanswer\\tall\\tthe\\tquestions\\tlisted\\tat\\tthe\\tend\\tof\\tthe\\tchapter\\tbefore\\tmoving\\ton.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 18}), Document(page_content='What\\tIs\\tMachine\\tLearning?\\nMachine\\tLearning\\t\\nis\\tthe\\tscience\\t(and\\tart)\\tof\\tprogramming\\tcomputers\\tso\\tthey\\tcan\\t\\nlearn\\tfrom\\tdata\\n.\\nHere\\tis\\ta\\tslightly\\tmore\\tgeneral\\tdefinition:\\n[Machine\\tLearning\\tis\\tthe]\\tfield\\tof\\tstudy\\tthat\\tgives\\tcomputers\\tthe\\tability\\tto\\tlearn\\twithout\\tbeing\\nexplicitly\\tprogrammed.\\nArthur\\tSamuel,\\t\\n1959\\nAnd\\ta\\tmore\\tengineering-oriented\\tone:\\nA\\tcomputer\\tprogram\\tis\\tsaid\\tto\\tlearn\\tfrom\\texperience\\tE\\twith\\trespect\\tto\\tsome\\ttask\\tT\\tand\\tsome\\nperformance\\tmeasure\\tP,\\tif\\tits\\tperformance\\ton\\tT,\\tas\\tmeasured\\tby\\tP,\\timproves\\twith\\texperience\\tE.\\nTom\\tMitchell,\\t\\n1997\\nFor\\t\\nexample,\\tyour\\tspam\\tfilter\\tis\\ta\\tMachine\\tLearning\\tprogram\\tthat\\tcan\\tlearn\\tto\\tflag\\tspam\\tgiven\\texamples\\nof\\tspam\\temails\\t(e.g.,\\tflagged\\tby\\tusers)\\tand\\texamples\\tof\\tregular\\t(nonspam,\\talso\\tcalled\\t“ham”)\\temails.\\tThe\\nexamples\\tthat\\tthe\\tsystem\\tuses\\tto\\tlearn\\tare\\tcalled\\tthe\\t\\ntraining\\tset\\n.\\t\\nEach\\ttraining\\texample\\tis\\tcalled\\ta\\ntraining\\tinstance\\n\\t(or\\t\\nsample\\n).\\tIn\\tthis\\tcase,\\tthe\\ttask\\tT\\tis\\tto\\tflag\\tspam\\tfor\\tnew\\temails,\\tthe\\texperience\\tE\\tis\\nthe\\t\\ntraining\\tdata\\n,\\t\\nand\\tthe\\tperformance\\tmeasure\\tP\\tneeds\\tto\\tbe\\tdefined;\\tfor\\texample,\\tyou\\tcan\\tuse\\tthe\\tratio\\nof\\tcorrectly\\tclassified\\temails.\\tThis\\tparticular\\tperformance\\tmeasure\\tis\\tcalled\\t\\naccuracy\\n\\t\\nand\\tit\\tis\\toften\\nused\\tin\\tclassification\\ttasks.\\nIf\\tyou\\tjust\\tdownload\\ta\\tcopy\\tof\\tWikipedia,\\tyour\\tcomputer\\thas\\ta\\tlot\\tmore\\tdata,\\tbut\\tit\\tis\\tnot\\tsuddenly\\tbetter\\nat\\tany\\ttask.\\tThus,\\tit\\tis\\tnot\\tMachine\\tLearning.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 19}), Document(page_content='Why\\tUse\\tMachine\\tLearning?\\nConsider\\t\\nhow\\tyou\\twould\\twrite\\ta\\tspam\\tfilter\\tusing\\ttraditional\\tprogramming\\ttechniques\\t(\\nFigure\\t1-1\\n):\\n1\\n.\\t\\nFirst\\tyou\\twould\\tlook\\tat\\twhat\\tspam\\ttypically\\tlooks\\tlike.\\tYou\\tmight\\tnotice\\tthat\\tsome\\twords\\tor\\tphrases\\n(such\\tas\\t“4U,”\\t“credit\\tcard,”\\t“free,”\\tand\\t“amazing”)\\ttend\\tto\\tcome\\tup\\ta\\tlot\\tin\\tthe\\tsubject.\\tPerhaps\\nyou\\twould\\talso\\tnotice\\ta\\tfew\\tother\\tpatterns\\tin\\tthe\\tsender’s\\tname,\\tthe\\temail’s\\tbody,\\tand\\tso\\ton.\\n2\\n.\\t\\nYou\\twould\\twrite\\ta\\tdetection\\talgorithm\\tfor\\teach\\tof\\tthe\\tpatterns\\tthat\\tyou\\tnoticed,\\tand\\tyour\\tprogram\\nwould\\tflag\\temails\\tas\\tspam\\tif\\ta\\tnumber\\tof\\tthese\\tpatterns\\tare\\tdetected.\\n3\\n.\\t\\nYou\\twould\\ttest\\tyour\\tprogram,\\tand\\trepeat\\tsteps\\t1\\tand\\t2\\tuntil\\tit\\tis\\tgood\\tenough.\\nFigure\\t1-1.\\t\\nThe\\ttraditional\\tapproach\\nSince\\tthe\\tproblem\\tis\\tnot\\ttrivial,\\tyour\\tprogram\\twill\\tlikely\\tbecome\\ta\\tlong\\tlist\\tof\\tcomplex\\trules\\t—\\tpretty\\nhard\\tto\\tmaintain.\\nIn\\tcontrast,\\ta\\tspam\\tfilter\\tbased\\ton\\tMachine\\tLearning\\ttechniques\\tautomatically\\tlearns\\twhich\\twords\\tand\\nphrases\\tare\\tgood\\tpredictors\\tof\\tspam\\tby\\tdetecting\\tunusually\\tfrequent\\tpatterns\\tof\\twords\\tin\\tthe\\tspam\\nexamples\\tcompared\\tto\\tthe\\tham\\texamples\\t(\\nFigure\\t1-2\\n).\\tThe\\tprogram\\tis\\tmuch\\tshorter,\\teasier\\tto\\tmaintain,\\nand\\tmost\\tlikely\\tmore\\taccurate.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 20}), Document(page_content='Figure\\t1-2.\\t\\nMachine\\tLearning\\tapproach\\nMoreover,\\tif\\tspammers\\tnotice\\tthat\\tall\\ttheir\\temails\\tcontaining\\t“4U”\\tare\\tblocked,\\tthey\\tmight\\tstart\\twriting\\n“For\\tU”\\tinstead.\\tA\\tspam\\tfilter\\tusing\\ttraditional\\tprogramming\\ttechniques\\twould\\tneed\\tto\\tbe\\tupdated\\tto\\tflag\\n“For\\tU”\\temails.\\tIf\\tspammers\\tkeep\\tworking\\taround\\tyour\\tspam\\tfilter,\\tyou\\twill\\tneed\\tto\\tkeep\\twriting\\tnew\\nrules\\tforever.\\nIn\\tcontrast,\\ta\\tspam\\tfilter\\tbased\\ton\\tMachine\\tLearning\\ttechniques\\tautomatically\\tnotices\\tthat\\t“For\\tU”\\thas\\nbecome\\tunusually\\tfrequent\\tin\\tspam\\tflagged\\tby\\tusers,\\tand\\tit\\tstarts\\tflagging\\tthem\\twithout\\tyour\\tintervention\\n(\\nFigure\\t1-3\\n).\\nFigure\\t1-3.\\t\\nAutomatically\\tadapting\\tto\\tchange\\nAnother\\t\\narea\\twhere\\tMachine\\tLearning\\tshines\\tis\\tfor\\tproblems\\tthat\\teither\\tare\\ttoo\\tcomplex\\tfor\\ttraditional\\napproaches\\tor\\thave\\tno\\tknown\\talgorithm.\\tFor\\texample,\\tconsider\\t\\nspeech\\trecognition:\\tsay\\tyou\\twant\\tto\\tstart\\nsimple\\tand\\twrite\\ta\\tprogram\\tcapable\\tof\\tdistinguishing\\tthe\\twords\\t“one”\\tand\\t“two.”\\tYou\\tmight\\tnotice\\tthat\\nthe\\tword\\t“two”\\tstarts\\twith\\ta\\thigh-pitch\\tsound\\t(“T”),\\tso\\tyou\\tcould\\thardcode\\tan\\talgorithm\\tthat\\tmeasures\\nhigh-pitch\\tsound\\tintensity\\tand\\tuse\\tthat\\tto\\tdistinguish\\tones\\tand\\ttwos.\\tObviously\\tthis\\ttechnique\\twill\\tnot\\nscale\\tto\\tthousands\\tof\\twords\\tspoken\\tby\\tmillions\\tof\\tvery\\tdifferent\\tpeople\\tin\\tnoisy\\tenvironments\\tand\\tin', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 21}), Document(page_content='dozens\\tof\\tlanguages.\\tThe\\tbest\\tsolution\\t(at\\tleast\\ttoday)\\tis\\tto\\twrite\\tan\\talgorithm\\tthat\\tlearns\\tby\\titself,\\tgiven\\nmany\\texample\\trecordings\\tfor\\teach\\tword.\\nFinally,\\tMachine\\tLearning\\tcan\\thelp\\thumans\\tlearn\\t(\\nFigure\\t1-4\\n):\\tML\\talgorithms\\tcan\\tbe\\tinspected\\tto\\tsee\\nwhat\\tthey\\thave\\tlearned\\t(although\\tfor\\tsome\\talgorithms\\tthis\\tcan\\tbe\\ttricky).\\tFor\\tinstance,\\tonce\\tthe\\tspam\\nfilter\\thas\\tbeen\\ttrained\\ton\\tenough\\tspam,\\tit\\tcan\\teasily\\tbe\\tinspected\\tto\\treveal\\tthe\\tlist\\tof\\twords\\tand\\ncombinations\\tof\\twords\\tthat\\tit\\tbelieves\\tare\\tthe\\tbest\\tpredictors\\tof\\tspam.\\tSometimes\\tthis\\twill\\treveal\\nunsuspected\\tcorrelations\\tor\\tnew\\ttrends,\\tand\\tthereby\\tlead\\tto\\ta\\tbetter\\tunderstanding\\tof\\tthe\\tproblem.\\nApplying\\t\\nML\\ttechniques\\tto\\tdig\\tinto\\tlarge\\tamounts\\tof\\tdata\\tcan\\thelp\\tdiscover\\tpatterns\\tthat\\twere\\tnot\\nimmediately\\tapparent.\\tThis\\tis\\tcalled\\t\\ndata\\tmining\\n.\\nFigure\\t1-4.\\t\\nMachine\\tLearning\\tcan\\thelp\\thumans\\tlearn\\nTo\\tsummarize,\\tMachine\\tLearning\\tis\\tgreat\\tfor:\\nProblems\\tfor\\twhich\\texisting\\tsolutions\\trequire\\ta\\tlot\\tof\\thand-tuning\\tor\\tlong\\tlists\\tof\\trules:\\tone\\tMachine\\nLearning\\talgorithm\\tcan\\toften\\tsimplify\\tcode\\tand\\tperform\\tbetter.\\nComplex\\tproblems\\tfor\\twhich\\tthere\\tis\\tno\\tgood\\tsolution\\tat\\tall\\tusing\\ta\\ttraditional\\tapproach:\\tthe\\tbest\\nMachine\\tLearning\\ttechniques\\tcan\\tfind\\ta\\tsolution.\\nFluctuating\\tenvironments:\\ta\\tMachine\\tLearning\\tsystem\\tcan\\tadapt\\tto\\tnew\\tdata.\\nGetting\\tinsights\\tabout\\tcomplex\\tproblems\\tand\\tlarge\\tamounts\\t\\nof\\tdata.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 22}), Document(page_content='Types\\tof\\tMachine\\tLearning\\tSystems\\nThere\\t\\nare\\tso\\tmany\\tdifferent\\ttypes\\tof\\tMachine\\tLearning\\tsystems\\tthat\\tit\\tis\\tuseful\\tto\\tclassify\\tthem\\tin\\tbroad\\ncategories\\tbased\\ton:\\nWhether\\tor\\tnot\\tthey\\tare\\ttrained\\twith\\thuman\\tsupervision\\t(supervised,\\tunsupervised,\\tsemisupervised,\\nand\\tReinforcement\\tLearning)\\nWhether\\tor\\tnot\\tthey\\tcan\\tlearn\\tincrementally\\ton\\tthe\\tfly\\t(online\\tversus\\tbatch\\tlearning)\\nWhether\\tthey\\twork\\tby\\tsimply\\tcomparing\\tnew\\tdata\\tpoints\\tto\\tknown\\tdata\\tpoints,\\tor\\tinstead\\tdetect\\npatterns\\tin\\tthe\\ttraining\\tdata\\tand\\tbuild\\ta\\tpredictive\\tmodel,\\tmuch\\tlike\\tscientists\\tdo\\t(instance-based\\nversus\\tmodel-based\\tlearning)\\nThese\\tcriteria\\tare\\tnot\\texclusive;\\tyou\\tcan\\tcombine\\tthem\\tin\\tany\\tway\\tyou\\tlike.\\tFor\\texample,\\ta\\tstate-of-the-\\nart\\tspam\\tfilter\\tmay\\tlearn\\ton\\tthe\\tfly\\tusing\\ta\\tdeep\\tneural\\tnetwork\\tmodel\\ttrained\\tusing\\texamples\\tof\\tspam\\tand\\nham;\\tthis\\tmakes\\tit\\tan\\tonline,\\tmodel-based,\\tsupervised\\tlearning\\tsystem.\\nLet’s\\tlook\\tat\\teach\\tof\\tthese\\tcriteria\\ta\\tbit\\tmore\\tclosely.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 23}), Document(page_content='Supervised/Unsupervised\\tLearning\\nMachine\\t\\nLearning\\tsystems\\tcan\\tbe\\tclassified\\taccording\\tto\\tthe\\tamount\\tand\\ttype\\tof\\tsupervision\\tthey\\tget\\nduring\\ttraining.\\tThere\\tare\\tfour\\tmajor\\tcategories:\\tsupervised\\tlearning,\\tunsupervised\\tlearning,\\nsemisupervised\\tlearning,\\tand\\tReinforcement\\tLearning.\\nSupervised\\tlearning\\nIn\\t\\nsupervised\\tlearning\\n,\\tthe\\ttraining\\tdata\\tyou\\tfeed\\tto\\tthe\\talgorithm\\tincludes\\tthe\\tdesired\\tsolutions,\\t\\ncalled\\nlabels\\n\\t(\\nFigure\\t1-5\\n).\\nFigure\\t1-5.\\t\\nA\\tlabeled\\ttraining\\tset\\tfor\\tsupervised\\tlearning\\t(e.g.,\\tspam\\tclassification)\\nA\\ttypical\\tsupervised\\tlearning\\ttask\\t\\nis\\t\\nclassification\\n.\\tThe\\t\\nspam\\tfilter\\tis\\ta\\tgood\\texample\\tof\\tthis:\\tit\\tis\\ttrained\\nwith\\tmany\\texample\\temails\\talong\\twith\\ttheir\\t\\nclass\\n\\t(spam\\tor\\tham),\\tand\\tit\\tmust\\tlearn\\thow\\tto\\tclassify\\tnew\\nemails.\\nAnother\\ttypical\\ttask\\tis\\tto\\tpredict\\ta\\t\\ntarget\\n\\tnumeric\\tvalue,\\tsuch\\tas\\tthe\\tprice\\tof\\ta\\tcar,\\tgiven\\ta\\tset\\tof\\t\\nfeatures\\n(mileage,\\tage,\\tbrand,\\tetc.)\\tcalled\\t\\npredictors\\n.\\t\\nThis\\tsort\\tof\\ttask\\tis\\t\\ncalled\\t\\nregression\\n\\t(\\nFigure\\t1-6\\n).\\n1\\n\\tTo\\ttrain\\nthe\\tsystem,\\tyou\\tneed\\tto\\tgive\\tit\\tmany\\texamples\\tof\\tcars,\\tincluding\\tboth\\ttheir\\tpredictors\\tand\\ttheir\\tlabels\\n(i.e.,\\ttheir\\tprices).\\nNOTE\\nIn\\tMachine\\tLearning\\t\\nan\\t\\nattribute\\n\\tis\\ta\\tdata\\ttype\\t(e.g.,\\t“Mileage”),\\twhile\\ta\\t\\nfeature\\n\\t\\nhas\\tseveral\\tmeanings\\tdepending\\ton\\tthe\\ncontext,\\tbut\\tgenerally\\tmeans\\tan\\tattribute\\tplus\\tits\\tvalue\\t(e.g.,\\t“Mileage\\t=\\t15,000”).\\tMany\\tpeople\\tuse\\tthe\\twords\\t\\nattribute\\n\\tand\\nfeature\\n\\tinterchangeably,\\tthough.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 24}), Document(page_content='Figure\\t1-6.\\t\\nRegression\\nNote\\tthat\\tsome\\tregression\\talgorithms\\tcan\\tbe\\tused\\tfor\\tclassification\\tas\\twell,\\tand\\tvice\\tversa.\\tFor\\texample,\\nLogistic\\tRegression\\n\\t\\nis\\tcommonly\\tused\\tfor\\tclassification,\\tas\\tit\\tcan\\toutput\\ta\\tvalue\\tthat\\tcorresponds\\tto\\tthe\\nprobability\\tof\\tbelonging\\tto\\ta\\tgiven\\tclass\\t(e.g.,\\t20%\\tchance\\tof\\tbeing\\tspam).\\nHere\\tare\\tsome\\tof\\tthe\\tmost\\timportant\\tsupervised\\tlearning\\talgorithms\\t(covered\\tin\\tthis\\tbook):\\nk-Nearest\\tNeighbors\\nLinear\\tRegression\\nLogistic\\tRegression\\nSupport\\tVector\\tMachines\\t(SVMs)\\nDecision\\tTrees\\tand\\tRandom\\tForests\\nNeural\\tnetworks\\n2\\nUnsupervised\\tlearning\\nIn\\t\\nunsupervised\\tlearning\\n,\\t\\nas\\tyou\\tmight\\tguess,\\tthe\\ttraining\\tdata\\tis\\tunlabeled\\t(\\nFigure\\t1-7\\n).\\tThe\\tsystem\\ttries\\nto\\tlearn\\twithout\\ta\\tteacher.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 25}), Document(page_content='Figure\\t1-7.\\t\\nAn\\tunlabeled\\ttraining\\tset\\tfor\\tunsupervised\\tlearning\\nHere\\tare\\tsome\\tof\\tthe\\tmost\\timportant\\tunsupervised\\tlearning\\talgorithms\\t(we\\twill\\tcover\\tdimensionality\\nreduction\\tin\\t\\nChapter\\t8\\n):\\nClustering\\nk-Means\\nHierarchical\\tCluster\\tAnalysis\\t(HCA)\\nExpectation\\tMaximization\\nVisualization\\tand\\tdimensionality\\treduction\\nPrincipal\\tComponent\\tAnalysis\\t(PCA)\\nKernel\\tPCA\\nLocally-Linear\\tEmbedding\\t(LLE)\\nt-distributed\\tStochastic\\tNeighbor\\tEmbedding\\t(t-SNE)\\nAssociation\\trule\\t\\nlearning\\nApriori\\nEclat\\nFor\\texample,\\tsay\\tyou\\thave\\ta\\tlot\\tof\\tdata\\tabout\\tyour\\tblog’s\\tvisitors.\\tYou\\tmay\\twant\\tto\\trun\\ta\\t\\nclustering\\nalgorithm\\t\\nto\\ttry\\tto\\tdetect\\tgroups\\tof\\tsimilar\\tvisitors\\t(\\nFigure\\t1-8\\n).\\tAt\\tno\\tpoint\\tdo\\tyou\\ttell\\tthe\\talgorithm\\nwhich\\tgroup\\ta\\tvisitor\\tbelongs\\tto:\\tit\\tfinds\\tthose\\tconnections\\twithout\\tyour\\thelp.\\tFor\\texample,\\tit\\tmight\\nnotice\\tthat\\t40%\\tof\\tyour\\tvisitors\\tare\\tmales\\twho\\tlove\\tcomic\\tbooks\\tand\\tgenerally\\tread\\tyour\\tblog\\tin\\tthe\\nevening,\\twhile\\t20%\\tare\\tyoung\\tsci-fi\\tlovers\\twho\\tvisit\\tduring\\tthe\\tweekends,\\tand\\tso\\ton.\\tIf\\tyou\\tuse\\t\\na\\nhierarchical\\tclustering\\n\\talgorithm,\\tit\\tmay\\talso\\tsubdivide\\teach\\tgroup\\tinto\\tsmaller\\tgroups.\\tThis\\tmay\\thelp\\nyou\\ttarget\\tyour\\tposts\\tfor\\teach\\tgroup.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 26}), Document(page_content='Figure\\t1-8.\\t\\nClustering\\nVisualization\\n\\talgorithms\\t\\nare\\talso\\tgood\\texamples\\tof\\tunsupervised\\tlearning\\talgorithms:\\tyou\\tfeed\\tthem\\ta\\tlot\\nof\\tcomplex\\tand\\tunlabeled\\tdata,\\tand\\tthey\\toutput\\ta\\t2D\\tor\\t3D\\trepresentation\\tof\\tyour\\tdata\\tthat\\tcan\\teasily\\tbe\\nplotted\\t(\\nFigure\\t1-9\\n).\\tThese\\talgorithms\\ttry\\tto\\tpreserve\\tas\\tmuch\\tstructure\\tas\\tthey\\tcan\\t(e.g.,\\ttrying\\tto\\tkeep\\nseparate\\tclusters\\tin\\tthe\\tinput\\tspace\\tfrom\\toverlapping\\tin\\tthe\\tvisualization),\\tso\\tyou\\tcan\\tunderstand\\thow\\tthe\\ndata\\tis\\torganized\\tand\\tperhaps\\tidentify\\tunsuspected\\tpatterns.\\nFigure\\t1-9.\\t\\nExample\\tof\\ta\\tt-SNE\\tvisualization\\thighlighting\\tsemantic\\tclusters\\n3\\nA\\t\\nrelated\\ttask\\tis\\t\\ndimensionality\\treduction\\n,\\tin\\twhich\\t\\nthe\\tgoal\\tis\\tto\\tsimplify\\tthe\\tdata\\twithout\\tlosing\\ttoo\\nmuch\\tinformation.\\tOne\\tway\\tto\\tdo\\tthis\\tis\\tto\\tmerge\\tseveral\\tcorrelated\\tfeatures\\tinto\\tone.\\tFor\\texample,\\ta\\ncar’s\\tmileage\\tmay\\tbe\\tvery\\tcorrelated\\twith\\tits\\tage,\\tso\\tthe\\tdimensionality\\treduction\\talgorithm\\twill\\tmerge\\nthem\\tinto\\tone\\tfeature\\tthat\\trepresents\\tthe\\tcar’s\\twear\\tand\\ttear.\\tThis\\tis\\t\\ncalled\\t\\nfeature\\textraction\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 27}), Document(page_content='TIP\\nIt\\tis\\toften\\ta\\tgood\\tidea\\tto\\ttry\\tto\\treduce\\tthe\\tdimension\\tof\\tyour\\ttraining\\tdata\\tusing\\ta\\tdimensionality\\treduction\\talgorithm\\tbefore\\tyou\\nfeed\\tit\\tto\\tanother\\tMachine\\tLearning\\talgorithm\\t(such\\tas\\ta\\tsupervised\\tlearning\\talgorithm).\\tIt\\twill\\trun\\tmuch\\tfaster,\\tthe\\tdata\\twill\\ttake\\nup\\tless\\tdisk\\tand\\tmemory\\tspace,\\tand\\tin\\tsome\\tcases\\tit\\tmay\\talso\\tperform\\tbetter.\\nYet\\tanother\\timportant\\t\\nunsupervised\\ttask\\tis\\t\\nanomaly\\tdetection\\n\\t—\\tfor\\texample,\\tdetecting\\tunusual\\tcredit\\ncard\\ttransactions\\tto\\tprevent\\tfraud,\\tcatching\\tmanufacturing\\tdefects,\\tor\\tautomatically\\tremoving\\toutliers\\nfrom\\ta\\tdataset\\tbefore\\tfeeding\\tit\\tto\\tanother\\tlearning\\talgorithm.\\tThe\\tsystem\\tis\\ttrained\\twith\\tnormal\\ninstances,\\tand\\twhen\\tit\\tsees\\ta\\tnew\\tinstance\\tit\\tcan\\ttell\\twhether\\tit\\tlooks\\tlike\\ta\\tnormal\\tone\\tor\\twhether\\tit\\tis\\nlikely\\tan\\tanomaly\\t(see\\t\\nFigure\\t1-10\\n).\\nFigure\\t1-10.\\t\\nAnomaly\\tdetection\\nFinally,\\tanother\\tcommon\\t\\nunsupervised\\ttask\\tis\\t\\nassociation\\trule\\tlearning\\n,\\tin\\twhich\\tthe\\tgoal\\tis\\tto\\tdig\\tinto\\nlarge\\tamounts\\tof\\tdata\\tand\\tdiscover\\tinteresting\\trelations\\tbetween\\tattributes.\\tFor\\texample,\\tsuppose\\tyou\\nown\\ta\\tsupermarket.\\tRunning\\tan\\tassociation\\trule\\ton\\tyour\\tsales\\tlogs\\tmay\\treveal\\tthat\\tpeople\\twho\\tpurchase\\nbarbecue\\tsauce\\tand\\tpotato\\tchips\\talso\\ttend\\tto\\tbuy\\tsteak.\\tThus,\\tyou\\tmay\\twant\\tto\\tplace\\tthese\\titems\\tclose\\tto\\neach\\t\\nother.\\nSemisupervised\\tlearning\\nSome\\t\\nalgorithms\\tcan\\tdeal\\twith\\tpartially\\tlabeled\\ttraining\\tdata,\\tusually\\ta\\tlot\\tof\\tunlabeled\\tdata\\tand\\ta\\tlittle\\nbit\\tof\\tlabeled\\tdata.\\tThis\\tis\\tcalled\\t\\nsemisupervised\\tlearning\\n\\t(\\nFigure\\t1-11\\n).\\nSome\\t\\nphoto-hosting\\tservices,\\tsuch\\tas\\t\\nGoogle\\tPhotos,\\tare\\tgood\\texamples\\tof\\tthis.\\tOnce\\tyou\\tupload\\tall\\tyour\\nfamily\\tphotos\\tto\\tthe\\tservice,\\tit\\tautomatically\\trecognizes\\tthat\\tthe\\tsame\\tperson\\tA\\tshows\\tup\\tin\\tphotos\\t1,\\t5,\\nand\\t11,\\twhile\\tanother\\tperson\\tB\\tshows\\tup\\tin\\tphotos\\t2,\\t5,\\tand\\t7.\\tThis\\tis\\tthe\\tunsupervised\\tpart\\tof\\tthe\\nalgorithm\\t(clustering).\\tNow\\tall\\tthe\\tsystem\\tneeds\\tis\\tfor\\tyou\\tto\\ttell\\tit\\twho\\tthese\\tpeople\\tare.\\tJust\\tone\\tlabel\\nper\\tperson,\\n4\\n\\tand\\tit\\tis\\table\\tto\\tname\\teveryone\\tin\\tevery\\tphoto,\\twhich\\tis\\tuseful\\tfor\\tsearching\\tphotos.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 28}), Document(page_content='Figure\\t1-11.\\t\\nSemisupervised\\tlearning\\nMost\\tsemisupervised\\tlearning\\talgorithms\\tare\\tcombinations\\tof\\tunsupervised\\tand\\tsupervised\\talgorithms.\\nFor\\texample,\\t\\ndeep\\tbelief\\tnetworks\\n\\t\\n(DBNs)\\tare\\tbased\\ton\\tunsupervised\\tcomponents\\tcalled\\t\\nrestricted\\nBoltzmann\\tmachines\\n\\t(RBMs)\\t\\nstacked\\ton\\ttop\\tof\\tone\\tanother.\\tRBMs\\tare\\ttrained\\tsequentially\\tin\\tan\\nunsupervised\\tmanner,\\tand\\tthen\\tthe\\twhole\\tsystem\\tis\\tfine-tuned\\tusing\\tsupervised\\tlearning\\ttechniques.\\nReinforcement\\tLearning\\nReinforcement\\tLearning\\n\\tis\\ta\\t\\nvery\\tdifferent\\tbeast.\\tThe\\tlearning\\tsystem,\\tcalled\\tan\\t\\nagent\\n\\tin\\tthis\\tcontext,\\ncan\\tobserve\\tthe\\tenvironment,\\tselect\\tand\\tperform\\tactions,\\tand\\tget\\t\\nrewards\\n\\tin\\treturn\\t(or\\t\\npenalties\\n\\tin\\tthe\\nform\\tof\\tnegative\\trewards,\\tas\\tin\\t\\nFigure\\t1-12\\n).\\tIt\\tmust\\tthen\\tlearn\\tby\\titself\\twhat\\tis\\tthe\\tbest\\tstrategy,\\tcalled\\ta\\npolicy\\n,\\tto\\tget\\tthe\\tmost\\treward\\tover\\ttime.\\tA\\tpolicy\\tdefines\\twhat\\taction\\tthe\\tagent\\tshould\\tchoose\\twhen\\tit\\tis\\nin\\ta\\tgiven\\tsituation.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 29}), Document(page_content='Figure\\t1-12.\\t\\nReinforcement\\tLearning\\nFor\\texample,\\tmany\\trobots\\timplement\\tReinforcement\\tLearning\\talgorithms\\tto\\tlearn\\thow\\tto\\twalk.\\nDeepMind’s\\tAlphaGo\\t\\nprogram\\tis\\talso\\ta\\tgood\\texample\\tof\\tReinforcement\\tLearning:\\tit\\tmade\\tthe\\theadlines\\nin\\tMarch\\t2016\\twhen\\tit\\tbeat\\tthe\\tworld\\tchampion\\tLee\\tSedol\\tat\\tthe\\tgame\\tof\\t\\nGo\\n.\\tIt\\tlearned\\tits\\twinning\\npolicy\\tby\\tanalyzing\\tmillions\\tof\\tgames,\\tand\\tthen\\tplaying\\tmany\\tgames\\tagainst\\titself.\\tNote\\tthat\\tlearning\\twas\\nturned\\toff\\tduring\\tthe\\tgames\\tagainst\\tthe\\tchampion;\\tAlphaGo\\twas\\tjust\\tapplying\\tthe\\tpolicy\\tit\\thad\\t\\nlearned.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 30}), Document(page_content='Batch\\tand\\tOnline\\tLearning\\nAnother\\t\\ncriterion\\tused\\tto\\tclassify\\tMachine\\tLearning\\tsystems\\tis\\twhether\\tor\\tnot\\tthe\\tsystem\\tcan\\tlearn\\nincrementally\\tfrom\\ta\\tstream\\tof\\tincoming\\tdata.\\nBatch\\tlearning\\nIn\\t\\nbatch\\tlearning\\n,\\tthe\\tsystem\\tis\\tincapable\\tof\\tlearning\\tincrementally:\\tit\\tmust\\tbe\\ttrained\\tusing\\tall\\tthe\\navailable\\tdata.\\tThis\\twill\\tgenerally\\ttake\\ta\\tlot\\tof\\ttime\\tand\\tcomputing\\tresources,\\tso\\tit\\tis\\ttypically\\tdone\\noffline.\\tFirst\\tthe\\tsystem\\tis\\ttrained,\\tand\\tthen\\tit\\tis\\tlaunched\\tinto\\tproduction\\tand\\truns\\twithout\\tlearning\\nanymore;\\tit\\tjust\\tapplies\\twhat\\tit\\thas\\tlearned.\\tThis\\tis\\t\\ncalled\\t\\noffline\\tlearning\\n.\\nIf\\tyou\\twant\\ta\\tbatch\\tlearning\\tsystem\\tto\\tknow\\tabout\\tnew\\tdata\\t(such\\tas\\ta\\tnew\\ttype\\tof\\tspam),\\tyou\\tneed\\tto\\ntrain\\ta\\tnew\\tversion\\tof\\tthe\\tsystem\\tfrom\\tscratch\\ton\\tthe\\tfull\\tdataset\\t(not\\tjust\\tthe\\tnew\\tdata,\\tbut\\talso\\tthe\\told\\ndata),\\tthen\\tstop\\tthe\\told\\tsystem\\tand\\treplace\\tit\\twith\\tthe\\tnew\\tone.\\nFortunately,\\tthe\\twhole\\tprocess\\tof\\ttraining,\\tevaluating,\\tand\\tlaunching\\ta\\tMachine\\tLearning\\tsystem\\tcan\\tbe\\nautomated\\tfairly\\teasily\\t(as\\tshown\\tin\\t\\nFigure\\t1-3\\n),\\tso\\teven\\ta\\tbatch\\tlearning\\tsystem\\tcan\\tadapt\\tto\\tchange.\\nSimply\\tupdate\\tthe\\tdata\\tand\\ttrain\\ta\\tnew\\tversion\\tof\\tthe\\tsystem\\tfrom\\tscratch\\tas\\toften\\tas\\tneeded.\\nThis\\tsolution\\tis\\tsimple\\tand\\toften\\tworks\\tfine,\\tbut\\ttraining\\tusing\\tthe\\tfull\\tset\\tof\\tdata\\tcan\\ttake\\tmany\\thours,\\tso\\nyou\\twould\\ttypically\\ttrain\\ta\\tnew\\tsystem\\tonly\\tevery\\t24\\thours\\tor\\teven\\tjust\\tweekly.\\tIf\\tyour\\tsystem\\tneeds\\tto\\nadapt\\tto\\trapidly\\tchanging\\tdata\\t(e.g.,\\tto\\tpredict\\tstock\\tprices),\\tthen\\tyou\\tneed\\ta\\tmore\\treactive\\tsolution.\\nAlso,\\ttraining\\ton\\tthe\\tfull\\tset\\tof\\tdata\\trequires\\ta\\tlot\\tof\\tcomputing\\tresources\\t(CPU,\\tmemory\\tspace,\\tdisk\\nspace,\\tdisk\\tI/O,\\tnetwork\\tI/O,\\tetc.).\\tIf\\tyou\\thave\\ta\\tlot\\tof\\tdata\\tand\\tyou\\tautomate\\tyour\\tsystem\\tto\\ttrain\\tfrom\\nscratch\\tevery\\tday,\\tit\\twill\\tend\\tup\\tcosting\\tyou\\ta\\tlot\\tof\\tmoney.\\tIf\\tthe\\tamount\\tof\\tdata\\tis\\thuge,\\tit\\tmay\\teven\\tbe\\nimpossible\\tto\\tuse\\ta\\tbatch\\tlearning\\talgorithm.\\nFinally,\\tif\\tyour\\tsystem\\tneeds\\tto\\tbe\\table\\tto\\tlearn\\tautonomously\\tand\\tit\\thas\\tlimited\\tresources\\t(e.g.,\\ta\\nsmartphone\\tapplication\\tor\\ta\\trover\\ton\\tMars),\\tthen\\tcarrying\\taround\\tlarge\\tamounts\\tof\\ttraining\\tdata\\tand\\ntaking\\tup\\ta\\tlot\\tof\\tresources\\tto\\ttrain\\tfor\\thours\\tevery\\tday\\tis\\ta\\tshowstopper.\\nFortunately,\\ta\\tbetter\\toption\\tin\\tall\\tthese\\tcases\\tis\\tto\\tuse\\talgorithms\\tthat\\tare\\tcapable\\tof\\tlearning\\nincrementally.\\nOnline\\tlearning\\nIn\\t\\nonline\\tlearning\\n,\\t\\nyou\\ttrain\\tthe\\tsystem\\tincrementally\\tby\\tfeeding\\tit\\tdata\\tinstances\\tsequentially,\\teither\\nindividually\\tor\\tby\\tsmall\\tgroups\\t\\ncalled\\t\\nmini-batches\\n.\\tEach\\tlearning\\tstep\\tis\\tfast\\tand\\tcheap,\\tso\\tthe\\tsystem\\ncan\\tlearn\\tabout\\tnew\\tdata\\ton\\tthe\\tfly,\\tas\\tit\\tarrives\\t(see\\t\\nFigure\\t1-13\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 31}), Document(page_content='Figure\\t1-13.\\t\\nOnline\\tlearning\\nOnline\\tlearning\\tis\\tgreat\\tfor\\tsystems\\tthat\\treceive\\tdata\\tas\\ta\\tcontinuous\\tflow\\t(e.g.,\\tstock\\tprices)\\tand\\tneed\\tto\\nadapt\\tto\\tchange\\trapidly\\tor\\tautonomously.\\tIt\\tis\\talso\\ta\\tgood\\toption\\tif\\tyou\\thave\\tlimited\\tcomputing\\tresources:\\nonce\\tan\\tonline\\tlearning\\tsystem\\thas\\tlearned\\tabout\\tnew\\tdata\\tinstances,\\tit\\tdoes\\tnot\\tneed\\tthem\\tanymore,\\tso\\nyou\\tcan\\tdiscard\\tthem\\t(unless\\tyou\\twant\\tto\\tbe\\table\\tto\\troll\\tback\\tto\\ta\\tprevious\\tstate\\tand\\t“replay”\\tthe\\tdata).\\nThis\\tcan\\tsave\\ta\\thuge\\tamount\\tof\\tspace.\\nOnline\\tlearning\\talgorithms\\tcan\\talso\\tbe\\tused\\tto\\ttrain\\tsystems\\ton\\thuge\\tdatasets\\tthat\\tcannot\\tfit\\tin\\tone\\nmachine’s\\tmain\\tmemory\\t(this\\tis\\tcalled\\t\\nout-of-core\\n\\t\\nlearning).\\tThe\\talgorithm\\tloads\\tpart\\tof\\tthe\\tdata,\\truns\\ta\\ntraining\\tstep\\ton\\tthat\\tdata,\\tand\\trepeats\\tthe\\tprocess\\tuntil\\tit\\thas\\trun\\ton\\tall\\tof\\tthe\\tdata\\t(see\\t\\nFigure\\t1-14\\n).\\nWARNING\\nThis\\twhole\\tprocess\\tis\\tusually\\tdone\\toffline\\t(i.e.,\\tnot\\ton\\tthe\\tlive\\tsystem),\\tso\\t\\nonline\\tlearning\\n\\tcan\\tbe\\ta\\tconfusing\\tname.\\tThink\\tof\\tit\\nas\\t\\nincremental\\tlearning\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 32}), Document(page_content='Figure\\t1-14.\\t\\nUsing\\tonline\\tlearning\\tto\\thandle\\thuge\\tdatasets\\nOne\\timportant\\tparameter\\tof\\tonline\\tlearning\\tsystems\\tis\\thow\\tfast\\tthey\\tshould\\tadapt\\tto\\tchanging\\tdata:\\tthis\\tis\\ncalled\\t\\nthe\\t\\nlearning\\trate\\n.\\tIf\\tyou\\tset\\ta\\thigh\\tlearning\\trate,\\tthen\\tyour\\tsystem\\twill\\trapidly\\tadapt\\tto\\tnew\\tdata,\\nbut\\tit\\twill\\talso\\ttend\\tto\\tquickly\\tforget\\tthe\\told\\tdata\\t(you\\tdon’t\\twant\\ta\\tspam\\tfilter\\tto\\tflag\\tonly\\tthe\\tlatest\\tkinds\\nof\\tspam\\tit\\twas\\tshown).\\tConversely,\\tif\\tyou\\tset\\ta\\tlow\\tlearning\\trate,\\tthe\\tsystem\\twill\\thave\\tmore\\tinertia;\\tthat\\nis,\\tit\\twill\\tlearn\\tmore\\tslowly,\\tbut\\tit\\twill\\talso\\tbe\\tless\\tsensitive\\tto\\tnoise\\tin\\tthe\\tnew\\tdata\\tor\\tto\\tsequences\\tof\\nnonrepresentative\\tdata\\tpoints.\\nA\\tbig\\tchallenge\\twith\\tonline\\tlearning\\tis\\tthat\\tif\\tbad\\tdata\\tis\\tfed\\tto\\tthe\\tsystem,\\tthe\\tsystem’s\\tperformance\\twill\\ngradually\\tdecline.\\tIf\\twe\\tare\\ttalking\\tabout\\ta\\tlive\\tsystem,\\tyour\\tclients\\twill\\tnotice.\\tFor\\texample,\\tbad\\tdata\\ncould\\tcome\\tfrom\\ta\\tmalfunctioning\\tsensor\\ton\\ta\\trobot,\\tor\\tfrom\\tsomeone\\tspamming\\ta\\tsearch\\tengine\\tto\\ttry\\tto\\nrank\\thigh\\tin\\tsearch\\tresults.\\tTo\\treduce\\tthis\\trisk,\\tyou\\tneed\\tto\\tmonitor\\tyour\\tsystem\\tclosely\\tand\\tpromptly\\nswitch\\tlearning\\toff\\t(and\\tpossibly\\trevert\\tto\\ta\\tpreviously\\tworking\\tstate)\\tif\\tyou\\tdetect\\ta\\tdrop\\tin\\nperformance.\\tYou\\tmay\\talso\\twant\\tto\\tmonitor\\tthe\\tinput\\tdata\\tand\\treact\\tto\\tabnormal\\t\\ndata\\t(e.g.,\\tusing\\tan\\nanomaly\\tdetection\\talgorithm).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 33}), Document(page_content='Instance-Based\\tVersus\\tModel-Based\\tLearning\\nOne\\t\\nmore\\tway\\tto\\tcategorize\\tMachine\\tLearning\\tsystems\\tis\\tby\\thow\\tthey\\t\\ngeneralize\\n.\\tMost\\tMachine\\nLearning\\ttasks\\tare\\tabout\\tmaking\\tpredictions.\\tThis\\tmeans\\tthat\\tgiven\\ta\\tnumber\\tof\\ttraining\\texamples,\\tthe\\nsystem\\tneeds\\tto\\tbe\\table\\tto\\tgeneralize\\tto\\texamples\\tit\\thas\\tnever\\tseen\\tbefore.\\tHaving\\ta\\tgood\\tperformance\\nmeasure\\ton\\tthe\\ttraining\\tdata\\tis\\tgood,\\tbut\\tinsufficient;\\tthe\\ttrue\\tgoal\\tis\\tto\\tperform\\twell\\ton\\tnew\\tinstances.\\nThere\\tare\\ttwo\\tmain\\tapproaches\\tto\\tgeneralization:\\tinstance-based\\tlearning\\tand\\tmodel-based\\tlearning.\\nInstance-based\\tlearning\\nPossibly\\t\\nthe\\tmost\\ttrivial\\tform\\tof\\tlearning\\tis\\tsimply\\tto\\tlearn\\tby\\theart.\\tIf\\tyou\\twere\\tto\\tcreate\\ta\\tspam\\tfilter\\nthis\\tway,\\tit\\twould\\tjust\\tflag\\tall\\temails\\tthat\\tare\\tidentical\\tto\\temails\\tthat\\thave\\talready\\tbeen\\tflagged\\tby\\tusers\\n—\\tnot\\tthe\\tworst\\tsolution,\\tbut\\tcertainly\\tnot\\tthe\\tbest.\\nInstead\\tof\\tjust\\tflagging\\temails\\tthat\\tare\\tidentical\\tto\\tknown\\tspam\\temails,\\tyour\\tspam\\tfilter\\tcould\\tbe\\nprogrammed\\tto\\talso\\tflag\\temails\\tthat\\tare\\tvery\\tsimilar\\tto\\tknown\\tspam\\temails.\\tThis\\trequires\\t\\na\\t\\nmeasure\\tof\\nsimilarity\\n\\tbetween\\ttwo\\temails.\\tA\\t(very\\tbasic)\\tsimilarity\\tmeasure\\tbetween\\ttwo\\temails\\tcould\\tbe\\tto\\tcount\\nthe\\tnumber\\tof\\twords\\tthey\\thave\\tin\\tcommon.\\tThe\\tsystem\\twould\\tflag\\tan\\temail\\tas\\tspam\\tif\\tit\\thas\\tmany\\twords\\nin\\tcommon\\twith\\ta\\tknown\\tspam\\temail.\\nThis\\tis\\tcalled\\t\\ninstance-based\\tlearning\\n:\\tthe\\tsystem\\tlearns\\tthe\\texamples\\tby\\theart,\\tthen\\tgeneralizes\\tto\\tnew\\ncases\\tusing\\ta\\tsimilarity\\tmeasure\\t(\\nFigure\\t1-15\\n).\\nFigure\\t1-15.\\t\\nInstance-based\\tlearning\\nModel-based\\tlearning\\nAnother\\t\\nway\\tto\\tgeneralize\\tfrom\\ta\\tset\\tof\\texamples\\tis\\tto\\tbuild\\ta\\tmodel\\tof\\tthese\\texamples,\\tthen\\tuse\\tthat\\nmodel\\tto\\tmake\\t\\npredictions\\n.\\tThis\\tis\\tcalled\\t\\nmodel-based\\tlearning\\n\\t(\\nFigure\\t1-16\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 34}), Document(page_content='Figure\\t1-16.\\t\\nModel-based\\tlearning\\nFor\\texample,\\t\\nsuppose\\tyou\\twant\\tto\\tknow\\tif\\tmoney\\tmakes\\tpeople\\thappy,\\tso\\tyou\\tdownload\\tthe\\t\\nBetter\\tLife\\nIndex\\n\\tdata\\tfrom\\tthe\\t\\nOECD’s\\twebsite\\n\\tas\\twell\\tas\\tstats\\tabout\\tGDP\\tper\\tcapita\\tfrom\\tthe\\t\\nIMF’s\\twebsite\\n.\\tThen\\nyou\\tjoin\\tthe\\ttables\\tand\\tsort\\tby\\tGDP\\tper\\tcapita.\\t\\nTable\\t1-1\\n\\tshows\\tan\\texcerpt\\tof\\twhat\\tyou\\tget.\\nTable\\t1-1.\\t\\nDoes\\tmoney\\tmake\\tpeople\\nhappier?\\nCountry\\nGDP\\tper\\tcapita\\t(USD)\\nLife\\tsatisfaction\\nHungary\\n12,240\\n4.9\\nKorea\\n27,195\\n5.8\\nFrance\\n37,675\\n6.5\\nAustralia\\n50,962\\n7.3\\nUnited\\tStates\\n55,805\\n7.2\\nLet’s\\tplot\\tthe\\tdata\\tfor\\ta\\tfew\\trandom\\tcountries\\t(\\nFigure\\t1-17\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 35}), Document(page_content='Figure\\t1-17.\\t\\nDo\\tyou\\tsee\\ta\\ttrend\\there?\\nThere\\tdoes\\tseem\\tto\\tbe\\ta\\ttrend\\there!\\tAlthough\\tthe\\tdata\\tis\\t\\nnoisy\\n\\t(i.e.,\\tpartly\\trandom),\\tit\\tlooks\\tlike\\tlife\\nsatisfaction\\tgoes\\tup\\tmore\\tor\\tless\\tlinearly\\tas\\tthe\\tcountry’s\\tGDP\\tper\\tcapita\\tincreases.\\tSo\\tyou\\tdecide\\tto\\nmodel\\tlife\\tsatisfaction\\tas\\ta\\tlinear\\tfunction\\tof\\tGDP\\tper\\tcapita.\\tThis\\tstep\\tis\\tcalled\\t\\nmodel\\tselection\\n:\\t\\nyou\\nselected\\ta\\t\\nlinear\\tmodel\\n\\tof\\tlife\\tsatisfaction\\twith\\tjust\\tone\\tattribute,\\tGDP\\tper\\tcapita\\t(\\nEquation\\t1-1\\n).\\nEquation\\t1-1.\\t\\nA\\tsimple\\tlinear\\tmodel\\nThis\\tmodel\\thas\\t\\ntwo\\t\\nmodel\\tparameters\\n,\\t\\nθ\\n0\\n\\tand\\t\\nθ\\n1\\n.\\n5\\n\\tBy\\ttweaking\\tthese\\tparameters,\\tyou\\tcan\\tmake\\tyour\\nmodel\\trepresent\\tany\\tlinear\\tfunction,\\tas\\tshown\\tin\\t\\nFigure\\t1-18\\n.\\nFigure\\t1-18.\\t\\nA\\tfew\\tpossible\\tlinear\\tmodels\\nBefore\\tyou\\tcan\\tuse\\tyour\\tmodel,\\tyou\\tneed\\tto\\tdefine\\tthe\\tparameter\\tvalues\\t\\nθ\\n0\\n\\tand\\t\\nθ\\n1\\n.\\tHow\\tcan\\tyou\\tknow', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 36}), Document(page_content='which\\tvalues\\twill\\tmake\\tyour\\tmodel\\tperform\\tbest?\\tTo\\tanswer\\tthis\\tquestion,\\tyou\\tneed\\tto\\tspecify\\ta\\nperformance\\tmeasure.\\tYou\\tcan\\teither\\tdefine\\ta\\t\\nutility\\tfunction\\n\\t(or\\t\\nfitness\\tfunction\\n)\\t\\nthat\\t\\nmeasures\\thow\\ngood\\n\\tyour\\tmodel\\tis,\\tor\\tyou\\tcan\\tdefine\\ta\\t\\ncost\\tfunction\\n\\t\\nthat\\tmeasures\\thow\\t\\nbad\\n\\tit\\tis.\\tFor\\tlinear\\tregression\\nproblems,\\tpeople\\ttypically\\tuse\\ta\\tcost\\tfunction\\tthat\\tmeasures\\tthe\\tdistance\\tbetween\\tthe\\tlinear\\tmodel’s\\npredictions\\tand\\tthe\\ttraining\\texamples;\\tthe\\tobjective\\tis\\tto\\tminimize\\tthis\\tdistance.\\nThis\\tis\\twhere\\tthe\\t\\nLinear\\tRegression\\talgorithm\\tcomes\\tin:\\tyou\\tfeed\\tit\\tyour\\ttraining\\texamples\\tand\\tit\\tfinds\\nthe\\tparameters\\tthat\\tmake\\tthe\\tlinear\\tmodel\\tfit\\tbest\\tto\\tyour\\tdata.\\tThis\\tis\\tcalled\\t\\ntraining\\n\\t\\nthe\\tmodel.\\tIn\\tour\\ncase\\tthe\\talgorithm\\tfinds\\tthat\\tthe\\toptimal\\tparameter\\tvalues\\tare\\t\\nθ\\n0\\n\\t=\\t4.85\\tand\\t\\nθ\\n1\\n\\t=\\t4.91\\t×\\t10\\n–5\\n.\\nNow\\tthe\\tmodel\\tfits\\tthe\\ttraining\\tdata\\tas\\tclosely\\tas\\tpossible\\t(for\\ta\\tlinear\\tmodel),\\tas\\tyou\\tcan\\tsee\\tin\\nFigure\\t1-19\\n.\\nFigure\\t1-19.\\t\\nThe\\tlinear\\tmodel\\tthat\\tfits\\tthe\\ttraining\\tdata\\tbest\\nYou\\tare\\tfinally\\tready\\tto\\trun\\tthe\\tmodel\\tto\\tmake\\tpredictions.\\tFor\\texample,\\tsay\\tyou\\twant\\tto\\tknow\\thow\\nhappy\\tCypriots\\tare,\\tand\\tthe\\tOECD\\tdata\\tdoes\\tnot\\thave\\tthe\\tanswer.\\tFortunately,\\tyou\\tcan\\tuse\\tyour\\tmodel\\tto\\nmake\\ta\\tgood\\tprediction:\\tyou\\tlook\\tup\\tCyprus’s\\tGDP\\tper\\tcapita,\\tfind\\t$22,587,\\tand\\tthen\\tapply\\tyour\\tmodel\\nand\\tfind\\tthat\\tlife\\tsatisfaction\\tis\\tlikely\\tto\\tbe\\tsomewhere\\taround\\t4.85\\t+\\t22,587\\t×\\t4.91\\t×\\t10\\n-5\\n\\t=\\t5.96.\\nTo\\twhet\\tyour\\tappetite,\\t\\nExample\\t1-1\\n\\tshows\\tthe\\tPython\\tcode\\tthat\\tloads\\tthe\\tdata,\\tprepares\\tit,\\n6\\n\\tcreates\\ta\\nscatterplot\\tfor\\tvisualization,\\tand\\tthen\\ttrains\\ta\\tlinear\\tmodel\\tand\\tmakes\\ta\\t\\nprediction.\\n7\\nExample\\t1-1.\\t\\nTraining\\tand\\trunning\\ta\\tlinear\\tmodel\\tusing\\tScikit-Learn\\nimport\\n\\t\\nmatplotlib\\nimport\\n\\t\\nmatplotlib.pyplot\\n\\t\\nas\\n\\t\\nplt\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nimport\\n\\t\\npandas\\n\\t\\nas\\n\\t\\npd\\nimport\\n\\t\\nsklearn\\n#\\tLoad\\tthe\\tdata\\noecd_bli\\n\\t\\n=\\n\\t\\npd\\n.\\nread_csv\\n(\\n\"oecd_bli_2015.csv\"\\n,\\n\\t\\nthousands\\n=\\n\\',\\'\\n)\\ngdp_per_capita\\n\\t\\n=\\n\\t\\npd\\n.\\nread_csv\\n(\\n\"gdp_per_capita.csv\"\\n,\\nthousands\\n=\\n\\',\\'\\n,\\ndelimiter\\n=\\n\\'\\n\\\\t\\n\\'\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nencoding\\n=\\n\\'latin1\\'\\n,\\n\\t\\nna_values\\n=\\n\"n/a\"\\n)\\n#\\tPrepare\\tthe\\tdata\\ncountry_stats\\n\\t\\n=\\n\\t\\nprepare_country_stats\\n(\\noecd_bli\\n,\\n\\t\\ngdp_per_capita\\n)\\nX\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\ncountry_stats\\n[\\n\"GDP\\tper\\tcapita\"\\n]]', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 37}), Document(page_content='y\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\ncountry_stats\\n[\\n\"Life\\tsatisfaction\"\\n]]\\n#\\tVisualize\\tthe\\tdata\\ncountry_stats\\n.\\nplot\\n(\\nkind\\n=\\n\\'scatter\\'\\n,\\n\\t\\nx\\n=\\n\"GDP\\tper\\tcapita\"\\n,\\n\\t\\ny\\n=\\n\\'Life\\tsatisfaction\\'\\n)\\nplt\\n.\\nshow\\n()\\n#\\tSelect\\ta\\tlinear\\tmodel\\nmodel\\n\\t\\n=\\n\\t\\nsklearn\\n.\\nlinear_model\\n.\\nLinearRegression\\n()\\n#\\tTrain\\tthe\\tmodel\\nmodel\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\n#\\tMake\\ta\\tprediction\\tfor\\tCyprus\\nX_new\\n\\t\\n=\\n\\t\\n[[\\n22587\\n]]\\n\\t\\t\\n#\\tCyprus\\'\\tGDP\\tper\\tcapita\\nprint\\n(\\nmodel\\n.\\npredict\\n(\\nX_new\\n))\\n\\t\\n#\\toutputs\\t[[\\t5.96242338]]\\nNOTE\\nIf\\tyou\\thad\\tused\\t\\nan\\tinstance-based\\tlearning\\talgorithm\\tinstead,\\tyou\\twould\\thave\\tfound\\tthat\\tSlovenia\\thas\\tthe\\tclosest\\tGDP\\tper\\tcapita\\nto\\tthat\\tof\\tCyprus\\t($20,732),\\tand\\tsince\\tthe\\tOECD\\tdata\\ttells\\tus\\tthat\\tSlovenians’\\tlife\\tsatisfaction\\tis\\t5.7,\\tyou\\twould\\thave\\tpredicted\\ta\\nlife\\tsatisfaction\\tof\\t5.7\\tfor\\tCyprus.\\tIf\\tyou\\tzoom\\tout\\ta\\tbit\\tand\\tlook\\tat\\tthe\\ttwo\\tnext\\tclosest\\tcountries,\\tyou\\twill\\tfind\\tPortugal\\tand\\nSpain\\twith\\tlife\\tsatisfactions\\tof\\t5.1\\tand\\t6.5,\\trespectively.\\tAveraging\\tthese\\tthree\\tvalues,\\tyou\\tget\\t5.77,\\twhich\\tis\\tpretty\\tclose\\tto\\tyour\\nmodel-based\\tprediction.\\tThis\\tsimple\\talgorithm\\tis\\tcalled\\t\\nk-Nearest\\tNeighbors\\n\\t\\nregression\\t(in\\tthis\\texample,\\t\\nk\\n\\t=\\t3).\\nReplacing\\tthe\\tLinear\\tRegression\\tmodel\\twith\\tk-Nearest\\tNeighbors\\tregression\\tin\\tthe\\tprevious\\tcode\\tis\\tas\\tsimple\\tas\\treplacing\\tthis\\nline:\\nmodel\\n\\t\\n=\\n\\t\\nsklearn\\n.\\nlinear_model\\n.\\nLinearRegression\\n()\\nwith\\tthis\\t\\none:\\nmodel\\n\\t\\n=\\n\\t\\nsklearn\\n.\\nneighbors\\n.\\nKNeighborsRegressor\\n(\\nn_neighbors\\n=\\n3\\n)\\nIf\\t\\nall\\twent\\twell,\\tyour\\tmodel\\twill\\tmake\\tgood\\tpredictions.\\tIf\\tnot,\\tyou\\tmay\\tneed\\tto\\tuse\\tmore\\tattributes\\n(employment\\trate,\\thealth,\\tair\\tpollution,\\tetc.),\\tget\\tmore\\tor\\tbetter\\tquality\\ttraining\\tdata,\\tor\\tperhaps\\tselect\\ta\\nmore\\tpowerful\\tmodel\\t\\n(e.g.,\\ta\\tPolynomial\\tRegression\\tmodel).\\nIn\\tsummary:\\nYou\\tstudied\\tthe\\tdata.\\nYou\\tselected\\ta\\tmodel.\\nYou\\ttrained\\tit\\ton\\tthe\\ttraining\\tdata\\t(i.e.,\\tthe\\tlearning\\talgorithm\\tsearched\\tfor\\tthe\\tmodel\\tparameter\\nvalues\\tthat\\tminimize\\ta\\tcost\\tfunction).\\nFinally,\\tyou\\tapplied\\tthe\\tmodel\\tto\\tmake\\tpredictions\\ton\\tnew\\tcases\\t(this\\tis\\tcalled\\t\\ninference\\n),\\t\\nhoping\\nthat\\tthis\\tmodel\\twill\\tgeneralize\\twell.\\nThis\\tis\\twhat\\ta\\ttypical\\tMachine\\tLearning\\tproject\\tlooks\\tlike.\\tIn\\t\\nChapter\\t2\\n\\tyou\\twill\\texperience\\tthis\\tfirst-\\nhand\\tby\\tgoing\\tthrough\\tan\\tend-to-end\\tproject.\\nWe\\thave\\tcovered\\ta\\tlot\\tof\\tground\\tso\\tfar:\\tyou\\tnow\\tknow\\twhat\\tMachine\\tLearning\\tis\\treally\\tabout,\\twhy\\tit\\tis\\nuseful,\\twhat\\tsome\\tof\\tthe\\tmost\\tcommon\\tcategories\\tof\\tML\\tsystems\\tare,\\tand\\twhat\\ta\\ttypical\\tproject\\tworkflow\\nlooks\\tlike.\\tNow\\tlet’s\\tlook\\tat\\twhat\\tcan\\tgo\\twrong\\tin\\tlearning\\tand\\tprevent\\tyou\\tfrom\\tmaking\\taccurate', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 38}), Document(page_content='predictions.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 39}), Document(page_content='Main\\tChallenges\\tof\\tMachine\\tLearning\\nIn\\t\\nshort,\\tsince\\tyour\\tmain\\ttask\\tis\\tto\\tselect\\ta\\tlearning\\talgorithm\\tand\\ttrain\\tit\\ton\\tsome\\tdata,\\tthe\\ttwo\\tthings\\tthat\\ncan\\tgo\\twrong\\tare\\t“bad\\talgorithm”\\tand\\t“bad\\tdata.”\\tLet’s\\tstart\\twith\\texamples\\tof\\tbad\\tdata.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 40}), Document(page_content='Insufficient\\tQuantity\\tof\\tTraining\\tData\\nFor\\t\\na\\ttoddler\\tto\\tlearn\\twhat\\tan\\tapple\\tis,\\tall\\tit\\ttakes\\tis\\tfor\\tyou\\tto\\tpoint\\tto\\tan\\tapple\\tand\\tsay\\t“apple”\\n(possibly\\trepeating\\tthis\\tprocedure\\ta\\tfew\\ttimes).\\tNow\\tthe\\tchild\\tis\\table\\tto\\trecognize\\tapples\\tin\\tall\\tsorts\\tof\\ncolors\\tand\\tshapes.\\tGenius.\\nMachine\\tLearning\\tis\\tnot\\tquite\\tthere\\tyet;\\tit\\ttakes\\ta\\tlot\\tof\\tdata\\tfor\\tmost\\tMachine\\tLearning\\talgorithms\\tto\\nwork\\tproperly.\\tEven\\tfor\\tvery\\tsimple\\tproblems\\tyou\\ttypically\\tneed\\tthousands\\tof\\texamples,\\tand\\tfor\\ncomplex\\tproblems\\tsuch\\tas\\timage\\tor\\tspeech\\trecognition\\tyou\\tmay\\tneed\\tmillions\\tof\\texamples\\t(unless\\tyou\\ncan\\treuse\\tparts\\tof\\tan\\texisting\\tmodel).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 41}), Document(page_content='THE\\tUNREASONABLE\\tEFFECTIVENESS\\tOF\\tDATA\\nIn\\ta\\t\\nfamous\\tpaper\\n\\tpublished\\tin\\t2001,\\tMicrosoft\\tresearchers\\tMichele\\tBanko\\tand\\tEric\\tBrill\\tshowed\\tthat\\tvery\\tdifferent\\tMachine\\tLearning\\nalgorithms,\\tincluding\\tfairly\\tsimple\\tones,\\tperformed\\talmost\\tidentically\\twell\\ton\\ta\\tcomplex\\tproblem\\tof\\tnatural\\tlanguage\\tdisambiguation\\n8\\n\\tonce\\nthey\\twere\\tgiven\\tenough\\tdata\\t(as\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t1-20\\n).\\nFigure\\t1-20.\\t\\nThe\\timportance\\tof\\tdata\\tversus\\talgorithms\\n9\\nAs\\tthe\\tauthors\\tput\\tit:\\t“these\\tresults\\tsuggest\\tthat\\twe\\tmay\\twant\\tto\\treconsider\\tthe\\ttrade-off\\tbetween\\tspending\\ttime\\tand\\tmoney\\ton\\talgorithm\\ndevelopment\\tversus\\tspending\\tit\\ton\\tcorpus\\tdevelopment.”\\nThe\\tidea\\tthat\\tdata\\tmatters\\tmore\\tthan\\talgorithms\\tfor\\tcomplex\\tproblems\\twas\\tfurther\\tpopularized\\tby\\tPeter\\tNorvig\\tet\\tal.\\tin\\ta\\tpaper\\ttitled\\n“The\\tUnreasonable\\tEffectiveness\\tof\\tData”\\n\\tpublished\\tin\\t2009.\\n10\\n\\t\\nIt\\tshould\\tbe\\tnoted,\\thowever,\\tthat\\tsmall-\\tand\\tmedium-sized\\tdatasets\\tare\\nstill\\tvery\\tcommon,\\tand\\tit\\tis\\tnot\\talways\\teasy\\tor\\tcheap\\tto\\tget\\textra\\ttraining\\tdata,\\tso\\tdon’t\\tabandon\\talgorithms\\tjust\\tyet.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 42}), Document(page_content='Nonrepresentative\\tTraining\\tData\\nIn\\t\\norder\\tto\\tgeneralize\\twell,\\tit\\tis\\tcrucial\\tthat\\tyour\\ttraining\\tdata\\tbe\\trepresentative\\tof\\tthe\\tnew\\tcases\\tyou\\nwant\\tto\\tgeneralize\\tto.\\tThis\\tis\\ttrue\\twhether\\tyou\\tuse\\tinstance-based\\tlearning\\tor\\tmodel-based\\tlearning.\\nFor\\texample,\\tthe\\tset\\tof\\tcountries\\twe\\tused\\tearlier\\tfor\\ttraining\\tthe\\tlinear\\tmodel\\twas\\tnot\\tperfectly\\nrepresentative;\\ta\\tfew\\tcountries\\twere\\tmissing.\\t\\nFigure\\t1-21\\n\\tshows\\twhat\\tthe\\tdata\\tlooks\\tlike\\twhen\\tyou\\tadd\\nthe\\tmissing\\tcountries.\\nFigure\\t1-21.\\t\\nA\\tmore\\trepresentative\\ttraining\\tsample\\nIf\\tyou\\ttrain\\ta\\tlinear\\tmodel\\ton\\tthis\\tdata,\\tyou\\tget\\tthe\\tsolid\\tline,\\twhile\\tthe\\told\\tmodel\\tis\\trepresented\\tby\\tthe\\ndotted\\tline.\\tAs\\tyou\\tcan\\tsee,\\tnot\\tonly\\tdoes\\tadding\\ta\\tfew\\tmissing\\tcountries\\tsignificantly\\talter\\tthe\\tmodel,\\tbut\\nit\\tmakes\\tit\\tclear\\tthat\\tsuch\\ta\\tsimple\\tlinear\\tmodel\\tis\\tprobably\\tnever\\tgoing\\tto\\twork\\twell.\\tIt\\tseems\\tthat\\tvery\\nrich\\tcountries\\tare\\tnot\\thappier\\tthan\\tmoderately\\trich\\tcountries\\t(in\\tfact\\tthey\\tseem\\tunhappier),\\tand\\nconversely\\tsome\\tpoor\\tcountries\\tseem\\thappier\\tthan\\tmany\\trich\\tcountries.\\nBy\\tusing\\ta\\tnonrepresentative\\ttraining\\tset,\\twe\\ttrained\\ta\\tmodel\\tthat\\tis\\tunlikely\\tto\\tmake\\taccurate\\npredictions,\\tespecially\\tfor\\tvery\\tpoor\\tand\\tvery\\trich\\tcountries.\\nIt\\tis\\tcrucial\\tto\\tuse\\ta\\ttraining\\tset\\tthat\\tis\\trepresentative\\tof\\tthe\\tcases\\tyou\\twant\\tto\\tgeneralize\\tto.\\tThis\\tis\\toften\\nharder\\tthan\\tit\\tsounds:\\tif\\tthe\\tsample\\tis\\ttoo\\tsmall,\\tyou\\twill\\t\\nhave\\t\\nsampling\\tnoise\\n\\t(i.e.,\\tnonrepresentative\\ndata\\tas\\ta\\tresult\\tof\\tchance),\\tbut\\teven\\tvery\\tlarge\\tsamples\\tcan\\tbe\\tnonrepresentative\\tif\\tthe\\tsampling\\tmethod\\nis\\tflawed.\\tThis\\t\\nis\\tcalled\\t\\nsampling\\tbias\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 43}), Document(page_content='A\\tFAMOUS\\tEXAMPLE\\tOF\\tSAMPLING\\tBIAS\\nPerhaps\\tthe\\tmost\\tfamous\\texample\\tof\\tsampling\\tbias\\thappened\\tduring\\tthe\\tUS\\tpresidential\\telection\\tin\\t1936,\\twhich\\tpitted\\tLandon\\tagainst\\nRoosevelt:\\tthe\\t\\nLiterary\\tDigest\\n\\tconducted\\ta\\tvery\\tlarge\\tpoll,\\tsending\\tmail\\tto\\tabout\\t10\\tmillion\\tpeople.\\tIt\\tgot\\t2.4\\tmillion\\tanswers,\\tand\\npredicted\\twith\\thigh\\tconfidence\\tthat\\tLandon\\twould\\tget\\t57%\\tof\\tthe\\tvotes.\\tInstead,\\tRoosevelt\\twon\\twith\\t62%\\tof\\tthe\\tvotes.\\tThe\\tflaw\\twas\\tin\\nthe\\t\\nLiterary\\tDigest\\n’s\\tsampling\\tmethod:\\nFirst,\\tto\\tobtain\\tthe\\taddresses\\tto\\tsend\\tthe\\tpolls\\tto,\\tthe\\t\\nLiterary\\tDigest\\n\\tused\\ttelephone\\tdirectories,\\tlists\\tof\\tmagazine\\tsubscribers,\\tclub\\nmembership\\tlists,\\tand\\tthe\\tlike.\\tAll\\tof\\tthese\\tlists\\ttend\\tto\\tfavor\\twealthier\\tpeople,\\twho\\tare\\tmore\\tlikely\\tto\\tvote\\tRepublican\\t(hence\\nLandon).\\nSecond,\\tless\\tthan\\t25%\\tof\\tthe\\tpeople\\twho\\treceived\\tthe\\tpoll\\tanswered.\\tAgain,\\tthis\\tintroduces\\ta\\tsampling\\tbias,\\tby\\truling\\tout\\tpeople\\nwho\\tdon’t\\tcare\\tmuch\\tabout\\tpolitics,\\tpeople\\twho\\tdon’t\\tlike\\tthe\\t\\nLiterary\\tDigest\\n,\\tand\\tother\\tkey\\tgroups.\\tThis\\tis\\ta\\tspecial\\ttype\\tof\\nsampling\\tbias\\t\\ncalled\\t\\nnonresponse\\tbias\\n.\\nHere\\tis\\tanother\\texample:\\tsay\\tyou\\twant\\tto\\tbuild\\ta\\tsystem\\tto\\trecognize\\tfunk\\tmusic\\tvideos.\\tOne\\tway\\tto\\tbuild\\tyour\\ttraining\\tset\\tis\\tto\\tsearch\\n“funk\\tmusic”\\ton\\tYouTube\\tand\\tuse\\tthe\\tresulting\\tvideos.\\tBut\\tthis\\tassumes\\tthat\\tYouTube’s\\tsearch\\tengine\\treturns\\ta\\tset\\tof\\tvideos\\tthat\\tare\\nrepresentative\\tof\\tall\\tthe\\tfunk\\tmusic\\tvideos\\ton\\tYouTube.\\tIn\\treality,\\tthe\\tsearch\\tresults\\tare\\tlikely\\tto\\tbe\\tbiased\\ttoward\\tpopular\\tartists\\t(and\\tif\\nyou\\tlive\\tin\\tBrazil\\tyou\\twill\\tget\\ta\\tlot\\tof\\t“funk\\tcarioca”\\tvideos,\\twhich\\tsound\\tnothing\\tlike\\tJames\\tBrown).\\tOn\\tthe\\tother\\thand,\\thow\\telse\\tcan\\nyou\\tget\\ta\\tlarge\\ttraining\\tset?', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 44}), Document(page_content='Poor-Quality\\tData\\nObviously,\\t\\nif\\tyour\\ttraining\\tdata\\tis\\tfull\\tof\\terrors,\\toutliers,\\tand\\tnoise\\t(e.g.,\\tdue\\tto\\tpoor-quality\\nmeasurements),\\tit\\twill\\tmake\\tit\\tharder\\tfor\\tthe\\tsystem\\tto\\tdetect\\tthe\\tunderlying\\tpatterns,\\tso\\tyour\\tsystem\\tis\\nless\\tlikely\\tto\\tperform\\twell.\\tIt\\tis\\toften\\twell\\tworth\\tthe\\teffort\\tto\\tspend\\ttime\\tcleaning\\tup\\tyour\\ttraining\\tdata.\\nThe\\ttruth\\tis,\\tmost\\tdata\\tscientists\\tspend\\ta\\tsignificant\\tpart\\tof\\ttheir\\ttime\\tdoing\\tjust\\tthat.\\tFor\\texample:\\nIf\\tsome\\tinstances\\tare\\tclearly\\toutliers,\\tit\\tmay\\thelp\\tto\\tsimply\\tdiscard\\tthem\\tor\\ttry\\tto\\tfix\\tthe\\terrors\\nmanually.\\nIf\\tsome\\tinstances\\tare\\tmissing\\ta\\tfew\\tfeatures\\t(e.g.,\\t5%\\tof\\tyour\\tcustomers\\tdid\\tnot\\tspecify\\ttheir\\tage),\\nyou\\tmust\\tdecide\\twhether\\tyou\\twant\\tto\\tignore\\tthis\\tattribute\\taltogether,\\tignore\\tthese\\tinstances,\\tfill\\tin\\tthe\\nmissing\\tvalues\\t(e.g.,\\twith\\tthe\\tmedian\\tage),\\tor\\ttrain\\tone\\tmodel\\twith\\tthe\\tfeature\\tand\\tone\\tmodel\\twithout\\nit,\\tand\\tso\\t\\non.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 45}), Document(page_content='Irrelevant\\tFeatures\\nAs\\t\\nthe\\tsaying\\tgoes:\\tgarbage\\tin,\\tgarbage\\tout.\\tYour\\tsystem\\twill\\tonly\\tbe\\tcapable\\tof\\tlearning\\tif\\tthe\\ttraining\\ndata\\tcontains\\tenough\\trelevant\\tfeatures\\tand\\tnot\\ttoo\\tmany\\tirrelevant\\tones.\\tA\\tcritical\\tpart\\tof\\tthe\\tsuccess\\tof\\ta\\nMachine\\tLearning\\tproject\\tis\\tcoming\\tup\\twith\\ta\\tgood\\tset\\tof\\tfeatures\\tto\\ttrain\\ton.\\tThis\\tprocess,\\t\\ncalled\\nfeature\\tengineering\\n,\\tinvolves:\\nFeature\\tselection\\n:\\tselecting\\t\\nthe\\tmost\\tuseful\\tfeatures\\tto\\ttrain\\ton\\tamong\\texisting\\tfeatures.\\nFeature\\textraction\\n:\\tcombining\\texisting\\tfeatures\\tto\\tproduce\\ta\\tmore\\tuseful\\tone\\t(as\\twe\\tsaw\\tearlier,\\ndimensionality\\treduction\\talgorithms\\tcan\\thelp).\\nCreating\\tnew\\tfeatures\\tby\\tgathering\\tnew\\tdata.\\nNow\\tthat\\twe\\thave\\tlooked\\tat\\tmany\\texamples\\tof\\tbad\\tdata,\\tlet’s\\tlook\\tat\\ta\\tcouple\\tof\\texamples\\tof\\tbad\\nalgorithms.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 46}), Document(page_content='Overfitting\\tthe\\tTraining\\tData\\nSay\\t\\nyou\\tare\\tvisiting\\ta\\tforeign\\tcountry\\tand\\tthe\\ttaxi\\tdriver\\trips\\tyou\\toff.\\tYou\\tmight\\tbe\\ttempted\\tto\\tsay\\tthat\\t\\nall\\ntaxi\\tdrivers\\tin\\tthat\\tcountry\\tare\\tthieves.\\tOvergeneralizing\\tis\\tsomething\\tthat\\twe\\thumans\\tdo\\tall\\ttoo\\toften,\\tand\\nunfortunately\\tmachines\\tcan\\tfall\\tinto\\tthe\\tsame\\ttrap\\tif\\twe\\tare\\tnot\\tcareful.\\tIn\\tMachine\\tLearning\\tthis\\tis\\tcalled\\noverfitting\\n:\\tit\\tmeans\\tthat\\tthe\\tmodel\\tperforms\\twell\\ton\\tthe\\ttraining\\tdata,\\tbut\\tit\\tdoes\\tnot\\tgeneralize\\twell.\\nFigure\\t1-22\\n\\tshows\\tan\\texample\\tof\\ta\\thigh-degree\\tpolynomial\\tlife\\tsatisfaction\\tmodel\\tthat\\tstrongly\\toverfits\\nthe\\ttraining\\tdata.\\tEven\\tthough\\tit\\tperforms\\tmuch\\tbetter\\ton\\tthe\\ttraining\\tdata\\tthan\\tthe\\tsimple\\tlinear\\tmodel,\\nwould\\tyou\\treally\\ttrust\\tits\\tpredictions?\\nFigure\\t1-22.\\t\\nOverfitting\\tthe\\ttraining\\tdata\\nComplex\\tmodels\\tsuch\\tas\\tdeep\\tneural\\tnetworks\\tcan\\tdetect\\tsubtle\\tpatterns\\tin\\tthe\\tdata,\\tbut\\tif\\tthe\\ttraining\\tset\\nis\\tnoisy,\\tor\\tif\\tit\\tis\\ttoo\\tsmall\\t(which\\tintroduces\\tsampling\\tnoise),\\tthen\\tthe\\tmodel\\tis\\tlikely\\tto\\tdetect\\tpatterns\\nin\\tthe\\tnoise\\titself.\\tObviously\\tthese\\tpatterns\\twill\\tnot\\tgeneralize\\tto\\tnew\\tinstances.\\tFor\\texample,\\tsay\\tyou\\nfeed\\tyour\\tlife\\tsatisfaction\\tmodel\\tmany\\tmore\\tattributes,\\tincluding\\tuninformative\\tones\\tsuch\\tas\\tthe\\tcountry’s\\nname.\\tIn\\tthat\\tcase,\\ta\\tcomplex\\tmodel\\tmay\\tdetect\\tpatterns\\tlike\\tthe\\tfact\\tthat\\tall\\tcountries\\tin\\tthe\\ttraining\\tdata\\nwith\\ta\\t\\nw\\n\\tin\\ttheir\\tname\\thave\\ta\\tlife\\tsatisfaction\\tgreater\\tthan\\t7:\\tNew\\tZealand\\t(7.3),\\tNorway\\t(7.4),\\tSweden\\n(7.2),\\tand\\tSwitzerland\\t(7.5).\\tHow\\tconfident\\tare\\tyou\\tthat\\tthe\\tW-satisfaction\\trule\\tgeneralizes\\tto\\tRwanda\\tor\\nZimbabwe?\\tObviously\\tthis\\tpattern\\toccurred\\tin\\tthe\\ttraining\\tdata\\tby\\tpure\\tchance,\\tbut\\tthe\\tmodel\\thas\\tno\\tway\\nto\\ttell\\twhether\\ta\\tpattern\\tis\\treal\\tor\\tsimply\\tthe\\tresult\\tof\\tnoise\\tin\\tthe\\tdata.\\nWARNING\\nOverfitting\\thappens\\twhen\\tthe\\tmodel\\tis\\ttoo\\tcomplex\\trelative\\tto\\tthe\\tamount\\tand\\tnoisiness\\tof\\tthe\\ttraining\\tdata.\\tThe\\tpossible\\nsolutions\\tare:\\nTo\\tsimplify\\tthe\\tmodel\\tby\\tselecting\\tone\\twith\\tfewer\\tparameters\\t(e.g.,\\ta\\tlinear\\tmodel\\trather\\tthan\\ta\\thigh-degree\\tpolynomial\\nmodel),\\tby\\treducing\\tthe\\tnumber\\tof\\tattributes\\tin\\tthe\\ttraining\\tdata\\tor\\tby\\tconstraining\\tthe\\tmodel\\nTo\\tgather\\tmore\\ttraining\\tdata\\nTo\\treduce\\tthe\\tnoise\\tin\\tthe\\ttraining\\tdata\\t(e.g.,\\tfix\\tdata\\terrors\\tand\\tremove\\toutliers)\\nConstraining\\ta\\tmodel\\tto\\tmake\\tit\\tsimpler\\tand\\treduce\\tthe\\trisk\\tof\\toverfitting\\tis\\t\\ncalled\\t\\nregularization\\n.\\tFor', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 47}), Document(page_content='example,\\tthe\\tlinear\\tmodel\\twe\\tdefined\\tearlier\\thas\\ttwo\\tparameters,\\t\\nθ\\n0\\n\\tand\\t\\nθ\\n1\\n.\\tThis\\tgives\\tthe\\tlearning\\nalgorithm\\ttwo\\t\\ndegrees\\tof\\tfreedom\\n\\tto\\t\\nadapt\\tthe\\tmodel\\tto\\tthe\\ttraining\\tdata:\\tit\\tcan\\ttweak\\tboth\\tthe\\theight\\t(\\nθ\\n0\\n)\\nand\\tthe\\tslope\\t(\\nθ\\n1\\n)\\tof\\tthe\\tline.\\tIf\\twe\\tforced\\t\\nθ\\n1\\n\\t=\\t0,\\tthe\\talgorithm\\twould\\thave\\tonly\\tone\\tdegree\\tof\\tfreedom\\nand\\twould\\thave\\ta\\tmuch\\tharder\\ttime\\tfitting\\tthe\\tdata\\tproperly:\\tall\\tit\\tcould\\tdo\\tis\\tmove\\tthe\\tline\\tup\\tor\\tdown\\nto\\tget\\tas\\tclose\\tas\\tpossible\\tto\\tthe\\ttraining\\tinstances,\\tso\\tit\\twould\\tend\\tup\\taround\\tthe\\tmean.\\tA\\tvery\\tsimple\\nmodel\\tindeed!\\tIf\\twe\\tallow\\tthe\\talgorithm\\tto\\tmodify\\t\\nθ\\n1\\n\\tbut\\twe\\tforce\\tit\\tto\\tkeep\\tit\\tsmall,\\tthen\\tthe\\tlearning\\nalgorithm\\twill\\teffectively\\thave\\tsomewhere\\tin\\tbetween\\tone\\tand\\ttwo\\tdegrees\\tof\\tfreedom.\\tIt\\twill\\tproduce\\ta\\nsimpler\\tmodel\\tthan\\twith\\ttwo\\tdegrees\\tof\\tfreedom,\\tbut\\tmore\\tcomplex\\tthan\\twith\\tjust\\tone.\\tYou\\twant\\tto\\tfind\\nthe\\tright\\tbalance\\tbetween\\tfitting\\tthe\\tdata\\tperfectly\\tand\\tkeeping\\tthe\\tmodel\\tsimple\\tenough\\tto\\tensure\\tthat\\tit\\nwill\\tgeneralize\\twell.\\nFigure\\t1-23\\n\\tshows\\tthree\\tmodels:\\tthe\\tdotted\\tline\\trepresents\\tthe\\toriginal\\tmodel\\tthat\\twas\\ttrained\\twith\\ta\\tfew\\ncountries\\tmissing,\\tthe\\tdashed\\tline\\tis\\tour\\tsecond\\tmodel\\ttrained\\twith\\tall\\tcountries,\\tand\\tthe\\tsolid\\tline\\tis\\ta\\nlinear\\tmodel\\ttrained\\twith\\tthe\\tsame\\tdata\\tas\\tthe\\tfirst\\tmodel\\tbut\\twith\\ta\\tregularization\\tconstraint.\\tYou\\tcan\\tsee\\nthat\\tregularization\\tforced\\tthe\\tmodel\\tto\\thave\\ta\\tsmaller\\tslope,\\twhich\\tfits\\ta\\tbit\\tless\\tthe\\ttraining\\tdata\\tthat\\tthe\\nmodel\\twas\\ttrained\\ton,\\tbut\\tactually\\tallows\\tit\\tto\\tgeneralize\\tbetter\\tto\\tnew\\texamples.\\nFigure\\t1-23.\\t\\nRegularization\\treduces\\tthe\\trisk\\tof\\toverfitting\\nThe\\tamount\\tof\\tregularization\\tto\\tapply\\tduring\\tlearning\\tcan\\tbe\\tcontrolled\\tby\\ta\\t\\nhyperparameter\\n.\\tA\\nhyperparameter\\tis\\ta\\tparameter\\tof\\ta\\tlearning\\talgorithm\\t(not\\tof\\tthe\\tmodel).\\tAs\\tsuch,\\tit\\tis\\tnot\\taffected\\tby\\tthe\\nlearning\\talgorithm\\titself;\\tit\\tmust\\tbe\\tset\\tprior\\tto\\ttraining\\tand\\tremains\\tconstant\\tduring\\ttraining.\\tIf\\tyou\\tset\\tthe\\nregularization\\thyperparameter\\tto\\ta\\tvery\\tlarge\\tvalue,\\tyou\\twill\\tget\\tan\\talmost\\tflat\\tmodel\\t(a\\tslope\\tclose\\tto\\nzero);\\tthe\\tlearning\\talgorithm\\twill\\talmost\\tcertainly\\tnot\\toverfit\\tthe\\ttraining\\tdata,\\tbut\\tit\\twill\\tbe\\tless\\tlikely\\tto\\nfind\\ta\\tgood\\tsolution.\\tTuning\\thyperparameters\\tis\\tan\\timportant\\tpart\\tof\\tbuilding\\ta\\t\\nMachine\\tLearning\\tsystem\\n(you\\twill\\tsee\\ta\\tdetailed\\texample\\tin\\tthe\\tnext\\tchapter).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 48}), Document(page_content='Underfitting\\tthe\\tTraining\\tData\\nAs\\t\\nyou\\tmight\\tguess,\\t\\nunderfitting\\n\\tis\\tthe\\topposite\\tof\\toverfitting:\\tit\\toccurs\\twhen\\tyour\\tmodel\\tis\\ttoo\\tsimple\\tto\\nlearn\\tthe\\tunderlying\\tstructure\\tof\\tthe\\tdata.\\tFor\\texample,\\ta\\tlinear\\tmodel\\tof\\tlife\\tsatisfaction\\tis\\tprone\\tto\\nunderfit;\\treality\\tis\\tjust\\tmore\\tcomplex\\tthan\\tthe\\tmodel,\\tso\\tits\\tpredictions\\tare\\tbound\\tto\\tbe\\tinaccurate,\\teven\\non\\tthe\\ttraining\\texamples.\\nThe\\tmain\\toptions\\tto\\tfix\\tthis\\tproblem\\tare:\\nSelecting\\ta\\tmore\\tpowerful\\tmodel,\\twith\\tmore\\tparameters\\nFeeding\\tbetter\\tfeatures\\tto\\tthe\\tlearning\\talgorithm\\t(feature\\tengineering)\\nReducing\\tthe\\tconstraints\\ton\\tthe\\tmodel\\t\\n(e.g.,\\treducing\\tthe\\tregularization\\thyperparameter)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 49}), Document(page_content='Stepping\\tBack\\nBy\\t\\nnow\\tyou\\talready\\tknow\\ta\\tlot\\tabout\\tMachine\\tLearning.\\tHowever,\\twe\\twent\\tthrough\\tso\\tmany\\tconcepts\\nthat\\tyou\\tmay\\tbe\\tfeeling\\ta\\tlittle\\tlost,\\tso\\tlet’s\\tstep\\tback\\tand\\tlook\\tat\\tthe\\tbig\\tpicture:\\nMachine\\tLearning\\tis\\tabout\\tmaking\\tmachines\\tget\\tbetter\\tat\\tsome\\ttask\\tby\\tlearning\\tfrom\\tdata,\\tinstead\\tof\\nhaving\\tto\\texplicitly\\tcode\\trules.\\nThere\\tare\\tmany\\tdifferent\\ttypes\\tof\\tML\\tsystems:\\tsupervised\\tor\\tnot,\\tbatch\\tor\\tonline,\\tinstance-based\\tor\\nmodel-based,\\tand\\tso\\ton.\\nIn\\ta\\tML\\tproject\\tyou\\tgather\\tdata\\tin\\ta\\ttraining\\tset,\\tand\\tyou\\tfeed\\tthe\\ttraining\\tset\\tto\\ta\\tlearning\\talgorithm.\\nIf\\tthe\\talgorithm\\tis\\tmodel-based\\tit\\ttunes\\tsome\\tparameters\\tto\\tfit\\tthe\\tmodel\\tto\\tthe\\ttraining\\tset\\t(i.e.,\\tto\\nmake\\tgood\\tpredictions\\ton\\tthe\\ttraining\\tset\\titself),\\tand\\tthen\\thopefully\\tit\\twill\\tbe\\table\\tto\\tmake\\tgood\\npredictions\\ton\\tnew\\tcases\\tas\\twell.\\tIf\\tthe\\talgorithm\\tis\\tinstance-based,\\tit\\tjust\\tlearns\\tthe\\texamples\\tby\\nheart\\tand\\tuses\\ta\\tsimilarity\\tmeasure\\tto\\tgeneralize\\tto\\tnew\\tinstances.\\nThe\\tsystem\\twill\\tnot\\tperform\\twell\\tif\\tyour\\ttraining\\tset\\tis\\ttoo\\tsmall,\\tor\\tif\\tthe\\tdata\\tis\\tnot\\trepresentative,\\nnoisy,\\tor\\tpolluted\\twith\\tirrelevant\\tfeatures\\t(garbage\\tin,\\tgarbage\\tout).\\tLastly,\\tyour\\tmodel\\tneeds\\tto\\tbe\\nneither\\ttoo\\tsimple\\t(in\\twhich\\tcase\\tit\\twill\\tunderfit)\\tnor\\ttoo\\tcomplex\\t(in\\twhich\\tcase\\tit\\twill\\toverfit).\\nThere’s\\tjust\\tone\\tlast\\timportant\\ttopic\\tto\\tcover:\\tonce\\tyou\\thave\\ttrained\\ta\\tmodel,\\tyou\\tdon’t\\twant\\tto\\tjust\\n“hope”\\tit\\tgeneralizes\\tto\\tnew\\tcases.\\tYou\\twant\\tto\\tevaluate\\tit,\\tand\\tfine-tune\\tit\\tif\\tnecessary.\\t\\nLet’s\\tsee\\thow.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 50}), Document(page_content='Testing\\tand\\tValidating\\nThe\\t\\nonly\\tway\\tto\\tknow\\thow\\twell\\ta\\tmodel\\twill\\tgeneralize\\tto\\tnew\\tcases\\tis\\tto\\tactually\\ttry\\tit\\tout\\ton\\tnew\\ncases.\\tOne\\tway\\tto\\tdo\\tthat\\tis\\tto\\tput\\tyour\\tmodel\\tin\\tproduction\\tand\\tmonitor\\thow\\twell\\tit\\tperforms.\\tThis\\nworks\\twell,\\tbut\\tif\\tyour\\tmodel\\tis\\thorribly\\tbad,\\tyour\\tusers\\twill\\tcomplain\\t—\\tnot\\tthe\\tbest\\tidea.\\nA\\tbetter\\toption\\tis\\tto\\tsplit\\tyour\\tdata\\tinto\\ttwo\\tsets:\\t\\nthe\\t\\ntraining\\tset\\n\\tand\\t\\nthe\\t\\ntest\\tset\\n.\\tAs\\tthese\\tnames\\timply,\\nyou\\ttrain\\tyour\\tmodel\\tusing\\tthe\\ttraining\\tset,\\tand\\tyou\\ttest\\tit\\tusing\\tthe\\ttest\\tset.\\tThe\\terror\\trate\\ton\\tnew\\tcases\\tis\\ncalled\\tthe\\t\\ngeneralization\\terror\\n\\t(or\\t\\nout-of-sample\\terror\\n),\\t\\nand\\tby\\tevaluating\\tyour\\tmodel\\ton\\tthe\\ttest\\tset,\\nyou\\tget\\tan\\testimation\\tof\\tthis\\terror.\\tThis\\tvalue\\ttells\\tyou\\thow\\twell\\tyour\\tmodel\\twill\\tperform\\ton\\tinstances\\tit\\nhas\\tnever\\tseen\\tbefore.\\nIf\\tthe\\ttraining\\terror\\tis\\tlow\\t(i.e.,\\tyour\\tmodel\\tmakes\\tfew\\tmistakes\\ton\\tthe\\ttraining\\tset)\\tbut\\tthe\\tgeneralization\\nerror\\tis\\thigh,\\tit\\tmeans\\tthat\\tyour\\tmodel\\tis\\toverfitting\\tthe\\ttraining\\tdata.\\nTIP\\nIt\\tis\\tcommon\\tto\\tuse\\t80%\\tof\\tthe\\tdata\\tfor\\ttraining\\tand\\t\\nhold\\tout\\n\\t20%\\tfor\\ttesting.\\nSo\\tevaluating\\ta\\tmodel\\tis\\tsimple\\tenough:\\tjust\\tuse\\ta\\ttest\\tset.\\tNow\\tsuppose\\tyou\\tare\\thesitating\\tbetween\\ttwo\\nmodels\\t(say\\ta\\tlinear\\tmodel\\tand\\ta\\tpolynomial\\tmodel):\\thow\\tcan\\tyou\\tdecide?\\tOne\\toption\\tis\\tto\\ttrain\\tboth\\nand\\tcompare\\thow\\twell\\tthey\\tgeneralize\\tusing\\tthe\\ttest\\tset.\\nNow\\tsuppose\\tthat\\tthe\\tlinear\\tmodel\\tgeneralizes\\tbetter,\\tbut\\tyou\\twant\\tto\\tapply\\tsome\\t\\nregularization\\tto\\tavoid\\noverfitting.\\tThe\\tquestion\\tis:\\thow\\tdo\\tyou\\tchoose\\tthe\\tvalue\\tof\\tthe\\tregularization\\thyperparameter?\\tOne\\noption\\tis\\tto\\ttrain\\t100\\tdifferent\\tmodels\\tusing\\t100\\tdifferent\\tvalues\\tfor\\tthis\\thyperparameter.\\tSuppose\\tyou\\nfind\\tthe\\tbest\\thyperparameter\\tvalue\\tthat\\tproduces\\ta\\tmodel\\twith\\tthe\\tlowest\\tgeneralization\\terror,\\tsay\\tjust\\n5%\\terror.\\nSo\\tyou\\tlaunch\\tthis\\tmodel\\tinto\\tproduction,\\tbut\\tunfortunately\\tit\\tdoes\\tnot\\tperform\\tas\\twell\\tas\\texpected\\tand\\nproduces\\t15%\\terrors.\\tWhat\\tjust\\thappened?\\nThe\\tproblem\\tis\\tthat\\tyou\\tmeasured\\tthe\\tgeneralization\\terror\\tmultiple\\ttimes\\ton\\tthe\\ttest\\tset,\\tand\\tyou\\tadapted\\nthe\\tmodel\\tand\\thyperparameters\\tto\\tproduce\\tthe\\tbest\\tmodel\\t\\nfor\\tthat\\tset\\n.\\tThis\\tmeans\\tthat\\tthe\\tmodel\\tis\\nunlikely\\tto\\tperform\\tas\\twell\\ton\\tnew\\tdata.\\nA\\tcommon\\tsolution\\tto\\tthis\\tproblem\\tis\\tto\\thave\\ta\\tsecond\\tholdout\\tset\\tcalled\\tthe\\t\\nvalidation\\tset\\n.\\t\\nYou\\ttrain\\nmultiple\\tmodels\\twith\\tvarious\\thyperparameters\\tusing\\tthe\\ttraining\\tset,\\tyou\\tselect\\tthe\\tmodel\\tand\\nhyperparameters\\tthat\\tperform\\tbest\\ton\\tthe\\tvalidation\\tset,\\tand\\twhen\\tyou’re\\thappy\\twith\\tyour\\tmodel\\tyou\\trun\\na\\tsingle\\tfinal\\ttest\\tagainst\\tthe\\ttest\\tset\\tto\\tget\\tan\\testimate\\tof\\tthe\\tgeneralization\\terror.\\nTo\\tavoid\\t“wasting”\\ttoo\\tmuch\\ttraining\\tdata\\tin\\tvalidation\\tsets,\\ta\\tcommon\\ttechnique\\tis\\tto\\tuse\\t\\ncross-\\nvalidation\\n:\\t\\nthe\\ttraining\\tset\\tis\\tsplit\\tinto\\tcomplementary\\tsubsets,\\tand\\teach\\tmodel\\tis\\ttrained\\tagainst\\ta\\ndifferent\\tcombination\\tof\\tthese\\tsubsets\\tand\\tvalidated\\tagainst\\tthe\\tremaining\\tparts.\\tOnce\\tthe\\tmodel\\ttype\\tand\\nhyperparameters\\thave\\tbeen\\tselected,\\ta\\tfinal\\tmodel\\tis\\ttrained\\tusing\\tthese\\thyperparameters\\ton\\tthe\\tfull\\ntraining\\tset,\\tand\\tthe\\tgeneralized\\terror\\tis\\tmeasured\\ton\\tthe\\ttest\\tset.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 51}), Document(page_content='NO\\tFREE\\tLUNCH\\tTHEOREM\\nA\\t\\nmodel\\t\\nis\\ta\\tsimplified\\tversion\\tof\\tthe\\tobservations.\\tThe\\tsimplifications\\tare\\tmeant\\tto\\tdiscard\\tthe\\tsuperfluous\\tdetails\\tthat\\tare\\tunlikely\\tto\\ngeneralize\\tto\\tnew\\tinstances.\\tHowever,\\tto\\tdecide\\twhat\\tdata\\tto\\tdiscard\\tand\\twhat\\tdata\\tto\\tkeep,\\tyou\\tmust\\tmake\\t\\nassumptions\\n.\\tFor\\texample,\\na\\tlinear\\tmodel\\tmakes\\tthe\\tassumption\\tthat\\tthe\\tdata\\tis\\tfundamentally\\tlinear\\tand\\tthat\\tthe\\tdistance\\tbetween\\tthe\\tinstances\\tand\\tthe\\tstraight\\tline\\nis\\tjust\\tnoise,\\twhich\\tcan\\tsafely\\tbe\\tignored.\\nIn\\ta\\t\\nfamous\\t1996\\tpaper\\n,\\n11\\n\\tDavid\\tWolpert\\tdemonstrated\\tthat\\tif\\tyou\\tmake\\tabsolutely\\tno\\tassumption\\tabout\\tthe\\tdata,\\tthen\\tthere\\tis\\tno\\treason\\nto\\tprefer\\tone\\tmodel\\tover\\tany\\tother.\\tThis\\tis\\tcalled\\tthe\\t\\nNo\\tFree\\tLunch\\n\\t(NFL)\\ttheorem.\\tFor\\tsome\\tdatasets\\tthe\\tbest\\tmodel\\tis\\ta\\tlinear\\nmodel,\\twhile\\tfor\\tother\\tdatasets\\tit\\tis\\ta\\tneural\\tnetwork.\\tThere\\tis\\tno\\tmodel\\tthat\\tis\\t\\na\\tpriori\\n\\tguaranteed\\tto\\twork\\tbetter\\t(hence\\tthe\\tname\\tof\\nthe\\ttheorem).\\tThe\\tonly\\tway\\tto\\tknow\\tfor\\tsure\\twhich\\tmodel\\tis\\tbest\\tis\\tto\\tevaluate\\tthem\\tall.\\tSince\\tthis\\tis\\tnot\\tpossible,\\tin\\tpractice\\tyou\\tmake\\nsome\\treasonable\\tassumptions\\tabout\\tthe\\tdata\\tand\\tyou\\tevaluate\\tonly\\ta\\tfew\\treasonable\\tmodels.\\tFor\\texample,\\tfor\\tsimple\\ttasks\\tyou\\tmay\\nevaluate\\tlinear\\tmodels\\twith\\tvarious\\tlevels\\tof\\tregularization,\\tand\\tfor\\ta\\tcomplex\\tproblem\\tyou\\tmay\\tevaluate\\tvarious\\tneural\\t\\nnetworks.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 52}), Document(page_content='Exercises\\nIn\\tthis\\tchapter\\twe\\thave\\tcovered\\tsome\\tof\\tthe\\tmost\\timportant\\tconcepts\\tin\\tMachine\\tLearning.\\tIn\\tthe\\tnext\\nchapters\\twe\\twill\\tdive\\tdeeper\\tand\\twrite\\tmore\\tcode,\\tbut\\tbefore\\twe\\tdo,\\tmake\\tsure\\tyou\\tknow\\thow\\tto\\tanswer\\nthe\\tfollowing\\tquestions:\\n1\\n.\\t\\nHow\\twould\\tyou\\tdefine\\tMachine\\tLearning?\\n2\\n.\\t\\nCan\\tyou\\tname\\tfour\\ttypes\\tof\\tproblems\\twhere\\tit\\tshines?\\n3\\n.\\t\\nWhat\\tis\\ta\\tlabeled\\ttraining\\tset?\\n4\\n.\\t\\nWhat\\tare\\tthe\\ttwo\\tmost\\tcommon\\tsupervised\\ttasks?\\n5\\n.\\t\\nCan\\tyou\\tname\\tfour\\tcommon\\tunsupervised\\ttasks?\\n6\\n.\\t\\nWhat\\ttype\\tof\\tMachine\\tLearning\\talgorithm\\twould\\tyou\\tuse\\tto\\tallow\\ta\\trobot\\tto\\twalk\\tin\\tvarious\\nunknown\\tterrains?\\n7\\n.\\t\\nWhat\\ttype\\tof\\talgorithm\\twould\\tyou\\tuse\\tto\\tsegment\\tyour\\tcustomers\\tinto\\tmultiple\\tgroups?\\n8\\n.\\t\\nWould\\tyou\\tframe\\tthe\\tproblem\\tof\\tspam\\tdetection\\tas\\ta\\tsupervised\\tlearning\\tproblem\\tor\\tan\\nunsupervised\\tlearning\\tproblem?\\n9\\n.\\t\\nWhat\\tis\\tan\\tonline\\tlearning\\tsystem?\\n10\\n.\\t\\nWhat\\tis\\tout-of-core\\tlearning?\\n11\\n.\\t\\nWhat\\ttype\\tof\\tlearning\\talgorithm\\trelies\\ton\\ta\\tsimilarity\\tmeasure\\tto\\tmake\\tpredictions?\\n12\\n.\\t\\nWhat\\tis\\tthe\\tdifference\\tbetween\\ta\\tmodel\\tparameter\\tand\\ta\\tlearning\\talgorithm’s\\thyperparameter?\\n13\\n.\\t\\nWhat\\tdo\\tmodel-based\\tlearning\\talgorithms\\tsearch\\tfor?\\tWhat\\tis\\tthe\\tmost\\tcommon\\tstrategy\\tthey\\tuse\\tto\\nsucceed?\\tHow\\tdo\\tthey\\tmake\\tpredictions?\\n14\\n.\\t\\nCan\\tyou\\tname\\tfour\\tof\\tthe\\tmain\\tchallenges\\tin\\tMachine\\tLearning?\\n15\\n.\\t\\nIf\\tyour\\tmodel\\tperforms\\tgreat\\ton\\tthe\\ttraining\\tdata\\tbut\\tgeneralizes\\tpoorly\\tto\\tnew\\tinstances,\\twhat\\tis\\nhappening?\\tCan\\tyou\\tname\\tthree\\tpossible\\tsolutions?\\n16\\n.\\t\\nWhat\\tis\\ta\\ttest\\tset\\tand\\twhy\\twould\\tyou\\twant\\tto\\tuse\\tit?\\n17\\n.\\t\\nWhat\\tis\\tthe\\tpurpose\\tof\\ta\\tvalidation\\tset?\\n18\\n.\\t\\nWhat\\tcan\\tgo\\twrong\\tif\\tyou\\ttune\\thyperparameters\\tusing\\tthe\\ttest\\tset?\\n19\\n.\\t\\nWhat\\tis\\tcross-validation\\tand\\twhy\\twould\\tyou\\tprefer\\tit\\tto\\ta\\tvalidation\\tset?', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 53}), Document(page_content='Solutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\nFun\\tfact:\\tthis\\todd-sounding\\tname\\tis\\ta\\tstatistics\\tterm\\tintroduced\\tby\\tFrancis\\tGalton\\twhile\\the\\twas\\tstudying\\tthe\\tfact\\tthat\\tthe\\tchildren\\tof\\ttall\\npeople\\ttend\\tto\\tbe\\tshorter\\tthan\\ttheir\\tparents.\\tSince\\tchildren\\twere\\tshorter,\\the\\tcalled\\tthis\\t\\nregression\\tto\\tthe\\tmean\\n.\\tThis\\tname\\twas\\tthen\\napplied\\tto\\tthe\\tmethods\\the\\tused\\tto\\tanalyze\\tcorrelations\\tbetween\\tvariables.\\nSome\\tneural\\tnetwork\\tarchitectures\\tcan\\tbe\\tunsupervised,\\tsuch\\tas\\tautoencoders\\tand\\trestricted\\tBoltzmann\\tmachines.\\tThey\\tcan\\talso\\tbe\\nsemisupervised,\\tsuch\\tas\\tin\\tdeep\\tbelief\\tnetworks\\tand\\tunsupervised\\tpretraining.\\nNotice\\thow\\tanimals\\tare\\trather\\twell\\tseparated\\tfrom\\tvehicles,\\thow\\thorses\\tare\\tclose\\tto\\tdeer\\tbut\\tfar\\tfrom\\tbirds,\\tand\\tso\\ton.\\tFigure\\treproduced\\nwith\\tpermission\\tfrom\\tSocher,\\tGanjoo,\\tManning,\\tand\\tNg\\t(2013),\\t“T-SNE\\tvisualization\\tof\\tthe\\tsemantic\\tword\\tspace.”\\nThat’s\\twhen\\tthe\\tsystem\\tworks\\tperfectly.\\tIn\\tpractice\\tit\\toften\\tcreates\\ta\\tfew\\tclusters\\tper\\tperson,\\tand\\tsometimes\\tmixes\\tup\\ttwo\\tpeople\\twho\\nlook\\talike,\\tso\\tyou\\tneed\\tto\\tprovide\\ta\\tfew\\tlabels\\tper\\tperson\\tand\\tmanually\\tclean\\tup\\tsome\\tclusters.\\nBy\\tconvention,\\tthe\\tGreek\\tletter\\tθ\\t(theta)\\tis\\tfrequently\\tused\\tto\\trepresent\\tmodel\\tparameters.\\nThe\\tcode\\tassumes\\tthat\\t\\nprepare_country_stats()\\n\\tis\\talready\\tdefined:\\tit\\tmerges\\tthe\\tGDP\\tand\\tlife\\tsatisfaction\\tdata\\tinto\\ta\\tsingle\\tPandas\\ndataframe.\\nIt’s\\tokay\\tif\\tyou\\tdon’t\\tunderstand\\tall\\tthe\\tcode\\tyet;\\twe\\twill\\tpresent\\tScikit-Learn\\tin\\tthe\\tfollowing\\tchapters.\\nFor\\texample,\\tknowing\\twhether\\tto\\twrite\\t“to,”\\t“two,”\\tor\\t“too”\\tdepending\\ton\\tthe\\tcontext.\\nFigure\\treproduced\\twith\\tpermission\\tfrom\\tBanko\\tand\\tBrill\\t(2001),\\t“Learning\\tCurves\\tfor\\tConfusion\\tSet\\tDisambiguation.”\\n“The\\tUnreasonable\\tEffectiveness\\tof\\tData,”\\tPeter\\tNorvig\\tet\\tal.\\t(2009).\\n“The\\tLack\\tof\\tA\\tPriori\\tDistinctions\\tBetween\\tLearning\\tAlgorithms,”\\tD.\\tWolperts\\t(1996).\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 54}), Document(page_content='Chapter\\t2.\\t\\nEnd-to-End\\tMachine\\tLearning\\nProject\\nIn\\t\\nthis\\tchapter,\\tyou\\twill\\tgo\\tthrough\\tan\\texample\\tproject\\tend\\tto\\tend,\\tpretending\\tto\\tbe\\ta\\trecently\\thired\\tdata\\nscientist\\tin\\ta\\treal\\testate\\tcompany.\\n1\\n\\tHere\\tare\\tthe\\tmain\\tsteps\\tyou\\twill\\tgo\\tthrough:\\n1\\n.\\t\\nLook\\tat\\tthe\\tbig\\tpicture.\\n2\\n.\\t\\nGet\\tthe\\tdata.\\n3\\n.\\t\\nDiscover\\tand\\tvisualize\\tthe\\tdata\\tto\\tgain\\tinsights.\\n4\\n.\\t\\nPrepare\\tthe\\tdata\\tfor\\tMachine\\tLearning\\talgorithms.\\n5\\n.\\t\\nSelect\\ta\\tmodel\\tand\\ttrain\\tit.\\n6\\n.\\t\\nFine-tune\\tyour\\tmodel.\\n7\\n.\\t\\nPresent\\tyour\\tsolution.\\n8\\n.\\t\\nLaunch,\\tmonitor,\\tand\\tmaintain\\tyour\\tsystem.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 55}), Document(page_content='Working\\twith\\tReal\\tData\\nWhen\\t\\nyou\\tare\\tlearning\\tabout\\tMachine\\tLearning\\tit\\tis\\tbest\\tto\\tactually\\texperiment\\twith\\treal-world\\tdata,\\tnot\\njust\\tartificial\\tdatasets.\\tFortunately,\\tthere\\tare\\tthousands\\tof\\topen\\tdatasets\\tto\\tchoose\\tfrom,\\tranging\\tacross\\tall\\nsorts\\tof\\tdomains.\\tHere\\tare\\ta\\tfew\\tplaces\\tyou\\tcan\\tlook\\tto\\tget\\tdata:\\nPopular\\topen\\tdata\\trepositories:\\nUC\\tIrvine\\tMachine\\tLearning\\tRepository\\nKaggle\\tdatasets\\nAmazon’s\\tAWS\\tdatasets\\nMeta\\tportals\\t(they\\tlist\\topen\\tdata\\trepositories):\\nhttp://dataportals.org/\\nhttp://opendatamonitor.eu/\\nhttp://quandl.com/\\nOther\\tpages\\tlisting\\tmany\\tpopular\\topen\\tdata\\trepositories:\\nWikipedia’s\\tlist\\tof\\tMachine\\tLearning\\tdatasets\\nQuora.com\\tquestion\\nDatasets\\tsubreddit\\nIn\\tthis\\tchapter\\twe\\tchose\\tthe\\tCalifornia\\tHousing\\tPrices\\tdataset\\tfrom\\tthe\\tStatLib\\trepository\\n2\\n\\t(see\\t\\nFigure\\t2-\\n1\\n).\\tThis\\tdataset\\twas\\tbased\\ton\\tdata\\tfrom\\tthe\\t1990\\tCalifornia\\tcensus.\\tIt\\tis\\tnot\\texactly\\trecent\\t(you\\tcould\\nstill\\tafford\\ta\\tnice\\thouse\\tin\\tthe\\tBay\\tArea\\tat\\tthe\\ttime),\\tbut\\tit\\thas\\tmany\\tqualities\\tfor\\tlearning,\\tso\\twe\\twill\\npretend\\tit\\tis\\trecent\\tdata.\\tWe\\talso\\tadded\\ta\\tcategorical\\tattribute\\tand\\tremoved\\ta\\tfew\\tfeatures\\tfor\\tteaching\\npurposes.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 56}), Document(page_content='Figure\\t2-1.\\t\\nCalifornia\\thousing\\tprices', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 57}), Document(page_content='Look\\tat\\tthe\\tBig\\tPicture\\nWelcome\\tto\\tMachine\\tLearning\\tHousing\\tCorporation!\\tThe\\tfirst\\ttask\\tyou\\tare\\tasked\\tto\\tperform\\tis\\tto\\tbuild\\ta\\nmodel\\tof\\thousing\\tprices\\tin\\tCalifornia\\tusing\\tthe\\tCalifornia\\tcensus\\tdata.\\tThis\\tdata\\thas\\tmetrics\\tsuch\\tas\\tthe\\npopulation,\\tmedian\\tincome,\\tmedian\\thousing\\tprice,\\tand\\tso\\ton\\tfor\\teach\\tblock\\tgroup\\tin\\tCalifornia.\\tBlock\\ngroups\\tare\\tthe\\tsmallest\\tgeographical\\tunit\\tfor\\twhich\\tthe\\tUS\\tCensus\\tBureau\\tpublishes\\tsample\\tdata\\t(a\\tblock\\ngroup\\ttypically\\thas\\ta\\tpopulation\\tof\\t600\\tto\\t3,000\\tpeople).\\tWe\\twill\\tjust\\tcall\\tthem\\t“districts”\\tfor\\tshort.\\nYour\\tmodel\\tshould\\tlearn\\tfrom\\tthis\\tdata\\tand\\tbe\\table\\tto\\tpredict\\tthe\\tmedian\\thousing\\tprice\\tin\\tany\\tdistrict,\\ngiven\\tall\\tthe\\tother\\tmetrics.\\nTIP\\nSince\\tyou\\tare\\ta\\twell-organized\\tdata\\tscientist,\\tthe\\tfirst\\tthing\\tyou\\tdo\\tis\\tto\\tpull\\tout\\tyour\\t\\nMachine\\tLearning\\tproject\\tchecklist.\\tYou\\tcan\\nstart\\twith\\tthe\\tone\\tin\\t\\nAppendix\\tB\\n;\\tit\\tshould\\twork\\treasonably\\twell\\tfor\\tmost\\tMachine\\tLearning\\tprojects\\tbut\\tmake\\tsure\\tto\\tadapt\\tit\\tto\\nyour\\tneeds.\\tIn\\tthis\\tchapter\\twe\\twill\\tgo\\tthrough\\tmany\\tchecklist\\titems,\\tbut\\twe\\twill\\talso\\tskip\\ta\\tfew,\\teither\\tbecause\\tthey\\tare\\tself-\\nexplanatory\\tor\\tbecause\\tthey\\twill\\tbe\\tdiscussed\\tin\\tlater\\tchapters.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 58}), Document(page_content='Frame\\tthe\\tProblem\\nThe\\t\\nfirst\\tquestion\\tto\\task\\tyour\\tboss\\tis\\twhat\\texactly\\tis\\tthe\\tbusiness\\tobjective;\\tbuilding\\ta\\tmodel\\tis\\tprobably\\nnot\\tthe\\tend\\tgoal.\\tHow\\tdoes\\tthe\\tcompany\\texpect\\tto\\tuse\\tand\\tbenefit\\tfrom\\tthis\\tmodel?\\tThis\\tis\\timportant\\nbecause\\tit\\twill\\tdetermine\\thow\\tyou\\tframe\\tthe\\tproblem,\\twhat\\talgorithms\\tyou\\twill\\tselect,\\twhat\\tperformance\\nmeasure\\tyou\\twill\\tuse\\tto\\tevaluate\\tyour\\tmodel,\\tand\\thow\\tmuch\\teffort\\tyou\\tshould\\tspend\\ttweaking\\tit.\\nYour\\tboss\\tanswers\\tthat\\tyour\\tmodel’s\\toutput\\t(a\\tprediction\\tof\\ta\\tdistrict’s\\tmedian\\thousing\\tprice)\\twill\\tbe\\tfed\\nto\\tanother\\tMachine\\tLearning\\tsystem\\t(see\\t\\nFigure\\t2-2\\n),\\talong\\twith\\tmany\\tother\\t\\nsignals\\n.\\n3\\n\\tThis\\tdownstream\\nsystem\\twill\\tdetermine\\twhether\\tit\\tis\\tworth\\tinvesting\\tin\\ta\\tgiven\\tarea\\tor\\tnot.\\tGetting\\tthis\\tright\\tis\\tcritical,\\tas\\nit\\tdirectly\\taffects\\trevenue.\\nFigure\\t2-2.\\t\\nA\\tMachine\\tLearning\\tpipeline\\tfor\\treal\\testate\\tinvestments\\nPIPELINES\\nA\\t\\nsequence\\tof\\tdata\\tprocessing\\t\\ncomponents\\n\\tis\\tcalled\\ta\\tdata\\t\\npipeline\\n.\\tPipelines\\tare\\tvery\\tcommon\\tin\\tMachine\\tLearning\\tsystems,\\tsince\\nthere\\tis\\ta\\tlot\\tof\\tdata\\tto\\tmanipulate\\tand\\tmany\\tdata\\ttransformations\\tto\\tapply.\\nComponents\\ttypically\\trun\\tasynchronously.\\tEach\\tcomponent\\tpulls\\tin\\ta\\tlarge\\tamount\\tof\\tdata,\\tprocesses\\tit,\\tand\\tspits\\tout\\tthe\\tresult\\tin\\tanother\\ndata\\tstore,\\tand\\tthen\\tsome\\ttime\\tlater\\tthe\\tnext\\tcomponent\\tin\\tthe\\tpipeline\\tpulls\\tthis\\tdata\\tand\\tspits\\tout\\tits\\town\\toutput,\\tand\\tso\\ton.\\tEach\\ncomponent\\tis\\tfairly\\tself-contained:\\tthe\\tinterface\\tbetween\\tcomponents\\tis\\tsimply\\tthe\\tdata\\tstore.\\tThis\\tmakes\\tthe\\tsystem\\tquite\\tsimple\\tto\\ngrasp\\t(with\\tthe\\thelp\\tof\\ta\\tdata\\tflow\\tgraph),\\tand\\tdifferent\\tteams\\tcan\\tfocus\\ton\\tdifferent\\tcomponents.\\tMoreover,\\tif\\ta\\tcomponent\\tbreaks\\ndown,\\tthe\\tdownstream\\tcomponents\\tcan\\toften\\tcontinue\\tto\\trun\\tnormally\\t(at\\tleast\\tfor\\ta\\twhile)\\tby\\tjust\\tusing\\tthe\\tlast\\toutput\\tfrom\\tthe\\tbroken\\ncomponent.\\tThis\\tmakes\\tthe\\tarchitecture\\tquite\\trobust.\\nOn\\tthe\\tother\\thand,\\ta\\tbroken\\tcomponent\\tcan\\tgo\\tunnoticed\\tfor\\tsome\\ttime\\tif\\tproper\\tmonitoring\\tis\\tnot\\timplemented.\\tThe\\tdata\\tgets\\tstale\\tand\\nthe\\toverall\\tsystem’s\\tperformance\\tdrops.\\nThe\\tnext\\tquestion\\tto\\task\\tis\\twhat\\tthe\\tcurrent\\tsolution\\tlooks\\tlike\\t(if\\tany).\\tIt\\twill\\toften\\tgive\\tyou\\ta\\treference\\nperformance,\\tas\\twell\\tas\\tinsights\\ton\\thow\\tto\\tsolve\\tthe\\tproblem.\\tYour\\tboss\\tanswers\\tthat\\tthe\\tdistrict\\thousing\\nprices\\tare\\tcurrently\\testimated\\tmanually\\tby\\texperts:\\ta\\tteam\\tgathers\\tup-to-date\\tinformation\\tabout\\ta\\tdistrict,\\nand\\twhen\\tthey\\tcannot\\tget\\tthe\\tmedian\\thousing\\tprice,\\tthey\\testimate\\tit\\tusing\\tcomplex\\trules.\\nThis\\tis\\tcostly\\tand\\ttime-consuming,\\tand\\ttheir\\testimates\\tare\\tnot\\tgreat;\\tin\\tcases\\twhere\\tthey\\tmanage\\tto\\tfind\\nout\\tthe\\tactual\\tmedian\\thousing\\tprice,\\tthey\\toften\\trealize\\tthat\\ttheir\\testimates\\twere\\toff\\tby\\tmore\\tthan\\t10%.\\nThis\\tis\\twhy\\tthe\\tcompany\\tthinks\\tthat\\tit\\twould\\tbe\\tuseful\\tto\\ttrain\\ta\\tmodel\\tto\\tpredict\\ta\\tdistrict’s\\tmedian\\nhousing\\tprice\\tgiven\\tother\\tdata\\tabout\\tthat\\tdistrict.\\tThe\\tcensus\\tdata\\tlooks\\tlike\\ta\\tgreat\\tdataset\\tto\\texploit\\tfor\\nthis\\tpurpose,\\tsince\\tit\\tincludes\\tthe\\tmedian\\thousing\\tprices\\tof\\tthousands\\tof\\tdistricts,\\tas\\twell\\tas\\tother\\tdata.\\nOkay,\\twith\\tall\\tthis\\tinformation\\tyou\\tare\\tnow\\tready\\tto\\tstart\\tdesigning\\tyour\\tsystem.\\tFirst,\\tyou\\tneed\\tto\\tframe', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 59}), Document(page_content='the\\tproblem:\\tis\\tit\\tsupervised,\\tunsupervised,\\tor\\tReinforcement\\tLearning?\\tIs\\tit\\ta\\tclassification\\ttask,\\ta\\nregression\\ttask,\\tor\\tsomething\\telse?\\tShould\\tyou\\tuse\\tbatch\\tlearning\\tor\\tonline\\tlearning\\ttechniques?\\tBefore\\nyou\\tread\\ton,\\tpause\\tand\\ttry\\tto\\tanswer\\tthese\\tquestions\\tfor\\tyourself.\\nHave\\tyou\\tfound\\tthe\\tanswers?\\tLet’s\\tsee:\\tit\\tis\\tclearly\\ta\\ttypical\\tsupervised\\tlearning\\ttask\\tsince\\tyou\\tare\\t\\ngiven\\nlabeled\\n\\ttraining\\texamples\\t(each\\tinstance\\tcomes\\twith\\tthe\\texpected\\toutput,\\ti.e.,\\tthe\\tdistrict’s\\tmedian\\nhousing\\tprice).\\tMoreover,\\tit\\tis\\talso\\ta\\ttypical\\tregression\\ttask,\\tsince\\tyou\\tare\\tasked\\tto\\tpredict\\ta\\tvalue.\\tMore\\nspecifically,\\tthis\\tis\\t\\na\\t\\nmultivariate\\tregression\\n\\tproblem\\tsince\\tthe\\tsystem\\twill\\tuse\\tmultiple\\tfeatures\\tto\\tmake\\na\\tprediction\\t(it\\twill\\tuse\\tthe\\tdistrict’s\\tpopulation,\\tthe\\tmedian\\tincome,\\tetc.).\\tIn\\tthe\\tfirst\\tchapter,\\tyou\\npredicted\\tlife\\tsatisfaction\\tbased\\ton\\tjust\\tone\\tfeature,\\tthe\\tGDP\\tper\\tcapita,\\tso\\tit\\twas\\t\\na\\t\\nunivariate\\tregression\\nproblem.\\tFinally,\\tthere\\tis\\tno\\tcontinuous\\tflow\\tof\\tdata\\tcoming\\tin\\tthe\\tsystem,\\tthere\\tis\\tno\\tparticular\\tneed\\tto\\nadjust\\tto\\tchanging\\tdata\\trapidly,\\tand\\tthe\\tdata\\tis\\tsmall\\tenough\\tto\\tfit\\tin\\tmemory,\\tso\\tplain\\tbatch\\tlearning\\nshould\\tdo\\tjust\\tfine.\\nTIP\\nIf\\tthe\\tdata\\twas\\thuge,\\tyou\\tcould\\teither\\tsplit\\tyour\\tbatch\\tlearning\\twork\\tacross\\tmultiple\\tservers\\t(using\\t\\nthe\\t\\nMapReduce\\n\\ttechnique,\\tas\\nwe\\twill\\tsee\\tlater),\\tor\\tyou\\tcould\\tuse\\tan\\tonline\\tlearning\\t\\ntechnique\\tinstead.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 60}), Document(page_content='Select\\ta\\tPerformance\\tMeasure\\nYour\\t\\nnext\\tstep\\tis\\tto\\tselect\\ta\\tperformance\\tmeasure.\\tA\\ttypical\\tperformance\\tmeasure\\tfor\\tregression\\nproblems\\tis\\tthe\\t\\nRoot\\tMean\\tSquare\\tError\\t(RMSE).\\tIt\\tgives\\tan\\tidea\\tof\\thow\\tmuch\\terror\\tthe\\tsystem\\ttypically\\nmakes\\tin\\tits\\tpredictions,\\twith\\ta\\thigher\\tweight\\tfor\\tlarge\\terrors.\\t\\nEquation\\t2-1\\n\\tshows\\tthe\\tmathematical\\nformula\\tto\\tcompute\\tthe\\tRMSE.\\nEquation\\t2-1.\\t\\nRoot\\tMean\\tSquare\\tError\\t(RMSE)\\nNOTATIONS\\nThis\\tequation\\tintroduces\\tseveral\\tvery\\tcommon\\t\\nMachine\\tLearning\\tnotations\\tthat\\twe\\twill\\tuse\\tthroughout\\tthis\\tbook:\\nm\\n\\tis\\tthe\\tnumber\\tof\\tinstances\\tin\\tthe\\tdataset\\tyou\\tare\\tmeasuring\\tthe\\tRMSE\\ton.\\nFor\\texample,\\tif\\tyou\\tare\\tevaluating\\tthe\\tRMSE\\ton\\ta\\tvalidation\\tset\\tof\\t2,000\\tdistricts,\\tthen\\t\\nm\\n\\t=\\t2,000.\\nx\\n(i)\\n\\tis\\ta\\tvector\\tof\\tall\\tthe\\tfeature\\tvalues\\t(excluding\\tthe\\tlabel)\\tof\\tthe\\t\\ni\\nth\\n\\tinstance\\tin\\tthe\\tdataset,\\tand\\t\\ny\\n(i)\\n\\tis\\tits\\tlabel\\t(the\\tdesired\\noutput\\tvalue\\tfor\\tthat\\tinstance).\\nFor\\texample,\\tif\\tthe\\tfirst\\tdistrict\\tin\\tthe\\tdataset\\tis\\tlocated\\tat\\tlongitude\\t–118.29°,\\tlatitude\\t33.91°,\\tand\\tit\\thas\\t1,416\\tinhabitants\\nwith\\ta\\tmedian\\tincome\\tof\\t$38,372,\\tand\\tthe\\tmedian\\thouse\\tvalue\\tis\\t$156,400\\t(ignoring\\tthe\\tother\\tfeatures\\tfor\\tnow),\\tthen:\\nand:\\nX\\n\\tis\\ta\\tmatrix\\tcontaining\\tall\\tthe\\tfeature\\tvalues\\t(excluding\\tlabels)\\tof\\tall\\tinstances\\tin\\tthe\\tdataset.\\tThere\\tis\\tone\\trow\\tper\\tinstance\\tand\\nthe\\t\\ni\\nth\\n\\trow\\tis\\tequal\\tto\\tthe\\ttranspose\\tof\\t\\nx\\n(i)\\n,\\tnoted\\t(\\nx\\n(i)\\n)\\nT\\n.\\n4\\nFor\\texample,\\tif\\tthe\\tfirst\\tdistrict\\tis\\tas\\tjust\\tdescribed,\\tthen\\tthe\\tmatrix\\t\\nX\\n\\tlooks\\tlike\\tthis:', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 61}), Document(page_content='h\\n\\tis\\tyour\\tsystem’s\\tprediction\\tfunction,\\talso\\tcalled\\t\\na\\t\\nhypothesis\\n.\\tWhen\\tyour\\tsystem\\tis\\tgiven\\tan\\t\\ninstance’s\\tfeature\\tvector\\t\\nx\\n(i)\\n,\\tit\\noutputs\\ta\\tpredicted\\tvalue\\t\\nŷ\\n(i)\\n\\t=\\t\\nh\\n(\\nx\\n(i)\\n)\\tfor\\tthat\\tinstance\\t(\\nŷ\\n\\tis\\tpronounced\\t“y-hat”).\\nFor\\texample,\\tif\\tyour\\tsystem\\tpredicts\\tthat\\tthe\\tmedian\\thousing\\tprice\\tin\\tthe\\tfirst\\tdistrict\\tis\\t$158,400,\\tthen\\t\\nŷ\\n(1)\\n\\t=\\t\\nh\\n(\\nx\\n(1)\\n)\\t=\\n158,400.\\tThe\\tprediction\\terror\\tfor\\tthis\\tdistrict\\tis\\t\\nŷ\\n(1)\\n\\t–\\t\\ny\\n(1)\\n\\t=\\t2,000.\\nRMSE(\\nX\\n,\\nh\\n)\\tis\\tthe\\t\\ncost\\tfunction\\tmeasured\\ton\\tthe\\tset\\tof\\texamples\\tusing\\tyour\\thypothesis\\t\\nh\\n.\\nWe\\tuse\\tlowercase\\titalic\\tfont\\tfor\\tscalar\\tvalues\\t(such\\tas\\t\\nm\\n\\tor\\t\\ny\\n(i)\\n)\\tand\\tfunction\\tnames\\t(such\\tas\\t\\nh\\n),\\tlowercase\\tbold\\tfont\\tfor\\tvectors\\t(such\\nas\\t\\nx\\n(i)\\n),\\tand\\tuppercase\\t\\nbold\\tfont\\tfor\\tmatrices\\t(such\\tas\\t\\nX\\n).\\nEven\\tthough\\tthe\\tRMSE\\tis\\tgenerally\\tthe\\tpreferred\\tperformance\\tmeasure\\tfor\\tregression\\ttasks,\\tin\\tsome\\ncontexts\\tyou\\tmay\\tprefer\\tto\\tuse\\tanother\\tfunction.\\tFor\\texample,\\tsuppose\\tthat\\tthere\\tare\\tmany\\toutlier\\tdistricts.\\nIn\\tthat\\tcase,\\tyou\\tmay\\tconsider\\tusing\\t\\nthe\\t\\nMean\\tAbsolute\\tError\\n\\t(also\\tcalled\\tthe\\tAverage\\tAbsolute\\nDeviation;\\tsee\\t\\nEquation\\t2-2\\n):\\nEquation\\t2-2.\\t\\nMean\\tAbsolute\\tError\\nBoth\\tthe\\tRMSE\\tand\\tthe\\tMAE\\tare\\tways\\tto\\tmeasure\\tthe\\tdistance\\tbetween\\ttwo\\tvectors:\\tthe\\tvector\\tof\\npredictions\\tand\\tthe\\tvector\\tof\\ttarget\\tvalues.\\tVarious\\tdistance\\tmeasures,\\tor\\t\\nnorms\\n,\\t\\nare\\tpossible:\\nComputing\\tthe\\troot\\tof\\ta\\tsum\\tof\\tsquares\\t(RMSE)\\tcorresponds\\tto\\tthe\\t\\nEuclidian\\tnorm\\n:\\tit\\t\\nis\\tthe\\tnotion\\nof\\tdistance\\tyou\\tare\\tfamiliar\\twith.\\tIt\\t\\nis\\talso\\tcalled\\tthe\\tℓ\\n2\\n\\t\\nnorm\\n,\\tnoted\\t\\t·\\t\\n2\\n\\t(or\\tjust\\t\\t·\\t).\\nComputing\\tthe\\tsum\\tof\\tabsolutes\\t(MAE)\\tcorresponds\\tto\\tthe\\tℓ\\n1\\n\\t\\nnorm\\n,\\t\\nnoted\\t\\t·\\t\\n1\\n.\\tIt\\tis\\tsometimes\\ncalled\\t\\nthe\\t\\nManhattan\\tnorm\\n\\tbecause\\tit\\tmeasures\\tthe\\tdistance\\tbetween\\ttwo\\tpoints\\tin\\ta\\tcity\\tif\\tyou\\tcan\\nonly\\ttravel\\talong\\torthogonal\\tcity\\tblocks.\\nMore\\tgenerally,\\t\\nthe\\tℓ\\nk\\n\\t\\nnorm\\n\\tof\\ta\\tvector\\t\\nv\\n\\tcontaining\\t\\nn\\n\\telements\\tis\\tdefined\\tas\\t\\n.\\tℓ\\n0\\n\\tjust\\t\\ngives\\tthe\\tnumber\\tof\\tnon-zero\\telements\\tin\\tthe\\tvector,\\nand\\tℓ\\n∞\\n\\t\\ngives\\tthe\\tmaximum\\tabsolute\\tvalue\\tin\\tthe\\tvector.\\nThe\\thigher\\tthe\\tnorm\\tindex,\\tthe\\tmore\\tit\\tfocuses\\ton\\tlarge\\tvalues\\tand\\tneglects\\tsmall\\tones.\\tThis\\tis\\twhy\\nthe\\tRMSE\\tis\\tmore\\tsensitive\\tto\\toutliers\\tthan\\tthe\\tMAE.\\tBut\\twhen\\toutliers\\tare\\texponentially\\trare\\t(like\\nin\\ta\\tbell-shaped\\tcurve),\\tthe\\tRMSE\\tperforms\\tvery\\twell\\tand\\tis\\t\\ngenerally\\tpreferred.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 62}), Document(page_content='', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 63}), Document(page_content='Check\\tthe\\tAssumptions\\nLastly,\\t\\nit\\tis\\tgood\\tpractice\\tto\\tlist\\tand\\tverify\\tthe\\tassumptions\\tthat\\twere\\tmade\\tso\\tfar\\t(by\\tyou\\tor\\tothers);\\tthis\\ncan\\tcatch\\tserious\\tissues\\tearly\\ton.\\tFor\\texample,\\tthe\\tdistrict\\tprices\\tthat\\tyour\\tsystem\\toutputs\\tare\\tgoing\\tto\\tbe\\nfed\\tinto\\ta\\tdownstream\\tMachine\\tLearning\\tsystem,\\tand\\twe\\tassume\\tthat\\tthese\\tprices\\tare\\tgoing\\tto\\tbe\\tused\\tas\\nsuch.\\tBut\\twhat\\tif\\tthe\\tdownstream\\tsystem\\tactually\\tconverts\\tthe\\tprices\\tinto\\tcategories\\t(e.g.,\\t“cheap,”\\n“medium,”\\tor\\t“expensive”)\\tand\\tthen\\tuses\\tthose\\tcategories\\tinstead\\tof\\tthe\\tprices\\tthemselves?\\tIn\\tthis\\tcase,\\ngetting\\tthe\\tprice\\tperfectly\\tright\\tis\\tnot\\timportant\\tat\\tall;\\tyour\\tsystem\\tjust\\tneeds\\tto\\tget\\tthe\\tcategory\\tright.\\tIf\\nthat’s\\tso,\\tthen\\tthe\\tproblem\\tshould\\thave\\tbeen\\tframed\\tas\\ta\\tclassification\\ttask,\\tnot\\ta\\tregression\\ttask.\\tYou\\ndon’t\\twant\\tto\\tfind\\tthis\\tout\\tafter\\tworking\\ton\\ta\\tregression\\tsystem\\tfor\\tmonths.\\nFortunately,\\tafter\\ttalking\\twith\\tthe\\tteam\\tin\\tcharge\\tof\\tthe\\tdownstream\\tsystem,\\tyou\\tare\\tconfident\\tthat\\tthey\\tdo\\nindeed\\tneed\\tthe\\tactual\\tprices,\\tnot\\tjust\\tcategories.\\tGreat!\\tYou’re\\tall\\tset,\\tthe\\tlights\\tare\\tgreen,\\tand\\tyou\\tcan\\nstart\\tcoding\\tnow!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 64}), Document(page_content='Get\\tthe\\tData\\nIt’s\\t\\ntime\\tto\\tget\\tyour\\thands\\tdirty.\\tDon’t\\thesitate\\tto\\tpick\\tup\\tyour\\tlaptop\\tand\\twalk\\tthrough\\tthe\\tfollowing\\tcode\\nexamples\\tin\\ta\\tJupyter\\tnotebook.\\tThe\\tfull\\tJupyter\\tnotebook\\tis\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 65}), Document(page_content='Create\\tthe\\tWorkspace\\nFirst\\tyou\\twill\\tneed\\tto\\thave\\tPython\\tinstalled.\\tIt\\tis\\tprobably\\talready\\tinstalled\\ton\\tyour\\tsystem.\\tIf\\tnot,\\tyou\\ncan\\tget\\tit\\tat\\t\\nhttps://www.python.org/\\n.\\n5\\nNext\\tyou\\tneed\\tto\\tcreate\\ta\\tworkspace\\tdirectory\\tfor\\tyour\\tMachine\\tLearning\\tcode\\tand\\tdatasets.\\tOpen\\ta\\nterminal\\tand\\ttype\\tthe\\tfollowing\\tcommands\\t(after\\tthe\\t\\n$\\n\\tprompts):\\n$\\texport\\tML_PATH=\"$HOME/ml\"\\t\\t\\t\\t\\t\\t#\\tYou\\tcan\\tchange\\tthe\\tpath\\tif\\tyou\\tprefer\\n$\\tmkdir\\t-p\\t$ML_PATH\\nYou\\twill\\tneed\\ta\\tnumber\\tof\\tPython\\tmodules:\\t\\nJupyter,\\tNumPy,\\tPandas,\\tMatplotlib,\\tand\\tScikit-Learn.\\tIf\\tyou\\nalready\\thave\\tJupyter\\trunning\\twith\\tall\\tthese\\tmodules\\tinstalled,\\tyou\\tcan\\tsafely\\tskip\\tto\\t\\n“Download\\tthe\\nData”\\n.\\tIf\\tyou\\tdon’t\\thave\\tthem\\tyet,\\tthere\\tare\\tmany\\tways\\tto\\tinstall\\tthem\\t(and\\ttheir\\tdependencies).\\tYou\\tcan\\nuse\\tyour\\tsystem’s\\tpackaging\\tsystem\\t(e.g.,\\tapt-get\\ton\\tUbuntu,\\tor\\tMacPorts\\tor\\tHomeBrew\\ton\\tmacOS),\\ninstall\\ta\\tScientific\\tPython\\tdistribution\\tsuch\\tas\\tAnaconda\\t\\nand\\tuse\\tits\\tpackaging\\tsystem,\\tor\\tjust\\tuse\\nPython’s\\town\\tpackaging\\tsystem,\\t\\npip,\\twhich\\tis\\tincluded\\tby\\tdefault\\twith\\tthe\\tPython\\tbinary\\tinstallers\\t(since\\nPython\\t2.7.9).\\n6\\n\\tYou\\tcan\\tcheck\\tto\\tsee\\tif\\tpip\\tis\\tinstalled\\tby\\ttyping\\tthe\\tfollowing\\tcommand:\\n$\\tpip3\\t--version\\npip\\t9.0.1\\tfrom\\t[...]/lib/python3.5/site-packages\\t(python\\t3.5)\\nYou\\tshould\\tmake\\tsure\\tyou\\thave\\ta\\trecent\\tversion\\tof\\tpip\\tinstalled,\\tat\\tthe\\tvery\\tleast\\t>1.4\\tto\\tsupport\\tbinary\\nmodule\\tinstallation\\t(a.k.a.\\twheels).\\tTo\\tupgrade\\tthe\\tpip\\tmodule,\\ttype:\\n7\\n$\\tpip3\\tinstall\\t--upgrade\\tpip\\nCollecting\\tpip\\n[...]\\nSuccessfully\\tinstalled\\tpip-9.0.1', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 66}), Document(page_content='CREATING\\tAN\\tISOLATED\\tENVIRONMENT\\nIf\\t\\nyou\\twould\\tlike\\tto\\twork\\tin\\tan\\tisolated\\tenvironment\\t(which\\tis\\tstrongly\\trecommended\\tso\\tyou\\tcan\\twork\\ton\\tdifferent\\tprojects\\twithout\\nhaving\\tconflicting\\tlibrary\\tversions),\\tinstall\\tvirtualenv\\tby\\trunning\\tthe\\tfollowing\\tpip\\tcommand:\\n$\\tpip3\\tinstall\\t--user\\t--upgrade\\tvirtualenv\\nCollecting\\tvirtualenv\\n[...]\\nSuccessfully\\tinstalled\\tvirtualenv\\nNow\\tyou\\tcan\\tcreate\\tan\\tisolated\\tPython\\tenvironment\\tby\\ttyping:\\n$\\tcd\\t$ML_PATH\\n$\\tvirtualenv\\tenv\\nUsing\\tbase\\tprefix\\t\\'[...]\\'\\nNew\\tpython\\texecutable\\tin\\t[...]/ml/env/bin/python3.5\\nAlso\\tcreating\\texecutable\\tin\\t[...]/ml/env/bin/python\\nInstalling\\tsetuptools,\\tpip,\\twheel...done.\\nNow\\tevery\\ttime\\tyou\\twant\\tto\\tactivate\\tthis\\tenvironment,\\tjust\\topen\\ta\\tterminal\\tand\\ttype:\\n$\\tcd\\t$ML_PATH\\n$\\tsource\\tenv/bin/activate\\nWhile\\tthe\\tenvironment\\tis\\tactive,\\tany\\tpackage\\tyou\\tinstall\\tusing\\tpip\\twill\\tbe\\tinstalled\\tin\\tthis\\tisolated\\tenvironment,\\tand\\tPython\\twill\\tonly\\thave\\naccess\\tto\\tthese\\tpackages\\t(if\\tyou\\talso\\twant\\taccess\\tto\\tthe\\tsystem’s\\tsite\\tpackages,\\tyou\\tshould\\tcreate\\tthe\\tenvironment\\tusing\\tvirtualenv’s\\t\\n--\\nsystem-site-packages\\n\\toption).\\tCheck\\tout\\tvirtualenv’s\\tdocumentation\\tfor\\tmore\\t\\ninformation.\\nNow\\tyou\\tcan\\tinstall\\tall\\tthe\\trequired\\tmodules\\tand\\ttheir\\tdependencies\\tusing\\tthis\\tsimple\\tpip\\tcommand:\\n$\\tpip3\\tinstall\\t--upgrade\\tjupyter\\tmatplotlib\\tnumpy\\tpandas\\tscipy\\tscikit-learn\\nCollecting\\tjupyter\\n\\t\\tDownloading\\tjupyter-1.0.0-py2.py3-none-any.whl\\nCollecting\\tmatplotlib\\n\\t\\t[...]\\nTo\\tcheck\\tyour\\tinstallation,\\ttry\\tto\\timport\\tevery\\tmodule\\tlike\\tthis:\\n$\\tpython3\\t-c\\t\"import\\tjupyter,\\tmatplotlib,\\tnumpy,\\tpandas,\\tscipy,\\tsklearn\"\\nThere\\tshould\\tbe\\tno\\toutput\\tand\\tno\\terror.\\tNow\\tyou\\tcan\\tfire\\tup\\t\\nJupyter\\tby\\ttyping:\\n$\\tjupyter\\tnotebook\\n[I\\t15:24\\tNotebookApp]\\tServing\\tnotebooks\\tfrom\\tlocal\\tdirectory:\\t[...]/ml\\n[I\\t15:24\\tNotebookApp]\\t0\\tactive\\tkernels\\n[I\\t15:24\\tNotebookApp]\\tThe\\tJupyter\\tNotebook\\tis\\trunning\\tat:\\thttp://localhost:8888/\\n[I\\t15:24\\tNotebookApp]\\tUse\\tControl-C\\tto\\tstop\\tthis\\tserver\\tand\\tshut\\tdown\\tall\\nkernels\\t(twice\\tto\\tskip\\tconfirmation).\\nA\\tJupyter\\tserver\\tis\\tnow\\trunning\\tin\\tyour\\tterminal,\\tlistening\\tto\\tport\\t8888.\\tYou\\tcan\\tvisit\\tthis\\tserver\\tby\\nopening\\tyour\\tweb\\tbrowser\\tto\\t\\nhttp://localhost:8888/\\n\\t(this\\tusually\\thappens\\tautomatically\\twhen\\tthe\\tserver\\nstarts).\\tYou\\tshould\\tsee\\tyour\\tempty\\tworkspace\\tdirectory\\t(containing\\tonly\\tthe\\t\\nenv\\n\\tdirectory\\tif\\tyou\\tfollowed\\nthe\\tpreceding\\tvirtualenv\\tinstructions).\\nNow\\tcreate\\ta\\tnew\\t\\nPython\\tnotebook\\tby\\tclicking\\ton\\tthe\\tNew\\tbutton\\tand\\tselecting\\tthe\\tappropriate\\tPython\\nversion\\n8\\n\\t(see\\t\\nFigure\\t2-3\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 67}), Document(page_content='This\\tdoes\\tthree\\tthings:\\tfirst,\\tit\\tcreates\\ta\\tnew\\tnotebook\\tfile\\tcalled\\t\\nUntitled.ipynb\\n\\tin\\tyour\\tworkspace;\\nsecond,\\tit\\tstarts\\ta\\tJupyter\\tPython\\tkernel\\tto\\trun\\tthis\\tnotebook;\\tand\\tthird,\\tit\\topens\\tthis\\tnotebook\\tin\\ta\\tnew\\ntab.\\tYou\\tshould\\tstart\\tby\\trenaming\\tthis\\tnotebook\\tto\\t“Housing”\\t(this\\twill\\tautomatically\\trename\\tthe\\tfile\\tto\\nHousing.ipynb\\n)\\tby\\tclicking\\tUntitled\\tand\\ttyping\\tthe\\tnew\\tname.\\nFigure\\t2-3.\\t\\nYour\\tworkspace\\tin\\tJupyter\\nA\\tnotebook\\tcontains\\ta\\tlist\\tof\\tcells.\\tEach\\tcell\\tcan\\tcontain\\texecutable\\tcode\\tor\\tformatted\\ttext.\\tRight\\tnow\\tthe\\nnotebook\\tcontains\\tonly\\tone\\tempty\\tcode\\tcell,\\tlabeled\\t“In\\t[1]:”.\\tTry\\ttyping\\t\\nprint(\"Hello\\tworld!\")\\n\\tin\\nthe\\tcell,\\tand\\tclick\\ton\\tthe\\tplay\\tbutton\\t(see\\t\\nFigure\\t2-4\\n)\\tor\\tpress\\tShift-Enter.\\tThis\\tsends\\tthe\\tcurrent\\tcell\\tto\\nthis\\tnotebook’s\\tPython\\tkernel,\\twhich\\truns\\tit\\tand\\treturns\\tthe\\toutput.\\tThe\\tresult\\tis\\tdisplayed\\tbelow\\tthe\\tcell,\\nand\\tsince\\twe\\treached\\tthe\\tend\\tof\\tthe\\tnotebook,\\ta\\tnew\\tcell\\tis\\tautomatically\\tcreated.\\tGo\\tthrough\\tthe\\tUser\\nInterface\\tTour\\tfrom\\tJupyter’s\\tHelp\\tmenu\\tto\\tlearn\\tthe\\tbasics.\\nFigure\\t2-4.\\t\\nHello\\tworld\\tPython\\tnotebook', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 68}), Document(page_content='Download\\tthe\\tData\\nIn\\t\\ntypical\\tenvironments\\tyour\\tdata\\twould\\tbe\\tavailable\\tin\\ta\\trelational\\tdatabase\\t(or\\tsome\\tother\\tcommon\\ndatastore)\\tand\\tspread\\tacross\\tmultiple\\ttables/documents/files.\\tTo\\taccess\\tit,\\tyou\\twould\\tfirst\\tneed\\tto\\tget\\nyour\\tcredentials\\tand\\taccess\\tauthorizations,\\n9\\n\\tand\\tfamiliarize\\tyourself\\twith\\tthe\\tdata\\tschema.\\tIn\\tthis\\tproject,\\nhowever,\\tthings\\tare\\tmuch\\tsimpler:\\tyou\\twill\\tjust\\tdownload\\ta\\tsingle\\tcompressed\\tfile,\\t\\nhousing.tgz\\n,\\twhich\\ncontains\\ta\\tcomma-separated\\tvalue\\t(CSV)\\tfile\\tcalled\\t\\nhousing.csv\\n\\twith\\tall\\tthe\\tdata.\\nYou\\tcould\\tuse\\tyour\\tweb\\tbrowser\\tto\\tdownload\\tit,\\tand\\trun\\t\\ntar\\txzf\\thousing.tgz\\n\\tto\\tdecompress\\tthe\\tfile\\nand\\textract\\tthe\\tCSV\\tfile,\\tbut\\tit\\tis\\tpreferable\\tto\\tcreate\\ta\\tsmall\\tfunction\\tto\\tdo\\tthat.\\tIt\\tis\\tuseful\\tin\\tparticular\\tif\\ndata\\tchanges\\tregularly,\\tas\\tit\\tallows\\tyou\\tto\\twrite\\ta\\tsmall\\tscript\\tthat\\tyou\\tcan\\trun\\twhenever\\tyou\\tneed\\tto\\nfetch\\tthe\\tlatest\\tdata\\t(or\\tyou\\tcan\\tset\\tup\\ta\\tscheduled\\tjob\\tto\\tdo\\tthat\\tautomatically\\tat\\tregular\\tintervals).\\nAutomating\\tthe\\tprocess\\tof\\tfetching\\tthe\\tdata\\tis\\talso\\tuseful\\tif\\tyou\\tneed\\tto\\tinstall\\tthe\\tdataset\\ton\\tmultiple\\nmachines.\\nHere\\tis\\tthe\\tfunction\\tto\\tfetch\\tthe\\tdata:\\n10\\nimport\\n\\t\\nos\\nimport\\n\\t\\ntarfile\\nfrom\\n\\t\\nsix.moves\\n\\t\\nimport\\n\\t\\nurllib\\nDOWNLOAD_ROOT\\n\\t\\n=\\n\\t\\n\"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\\nHOUSING_PATH\\n\\t\\n=\\n\\t\\nos\\n.\\npath\\n.\\njoin\\n(\\n\"datasets\"\\n,\\n\\t\\n\"housing\"\\n)\\nHOUSING_URL\\n\\t\\n=\\n\\t\\nDOWNLOAD_ROOT\\n\\t\\n+\\n\\t\\n\"datasets/housing/housing.tgz\"\\ndef\\n\\t\\nfetch_housing_data\\n(\\nhousing_url\\n=\\nHOUSING_URL\\n,\\n\\t\\nhousing_path\\n=\\nHOUSING_PATH\\n):\\n\\t\\t\\t\\t\\nif\\n\\t\\nnot\\n\\t\\nos\\n.\\npath\\n.\\nisdir\\n(\\nhousing_path\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nos\\n.\\nmakedirs\\n(\\nhousing_path\\n)\\n\\t\\t\\t\\t\\ntgz_path\\n\\t\\n=\\n\\t\\nos\\n.\\npath\\n.\\njoin\\n(\\nhousing_path\\n,\\n\\t\\n\"housing.tgz\"\\n)\\n\\t\\t\\t\\t\\nurllib\\n.\\nrequest\\n.\\nurlretrieve\\n(\\nhousing_url\\n,\\n\\t\\ntgz_path\\n)\\n\\t\\t\\t\\t\\nhousing_tgz\\n\\t\\n=\\n\\t\\ntarfile\\n.\\nopen\\n(\\ntgz_path\\n)\\n\\t\\t\\t\\t\\nhousing_tgz\\n.\\nextractall\\n(\\npath\\n=\\nhousing_path\\n)\\n\\t\\t\\t\\t\\nhousing_tgz\\n.\\nclose\\n()\\nNow\\twhen\\tyou\\tcall\\t\\nfetch_housing_data()\\n,\\tit\\tcreates\\ta\\t\\ndatasets/housing\\n\\tdirectory\\tin\\tyour\\tworkspace,\\ndownloads\\tthe\\t\\nhousing.tgz\\n\\tfile,\\tand\\textracts\\tthe\\t\\nhousing.csv\\n\\tfrom\\tit\\tin\\tthis\\tdirectory.\\nNow\\tlet’s\\tload\\tthe\\tdata\\tusing\\t\\nPandas.\\tOnce\\tagain\\tyou\\tshould\\twrite\\ta\\tsmall\\tfunction\\tto\\tload\\tthe\\tdata:\\nimport\\n\\t\\npandas\\n\\t\\nas\\n\\t\\npd\\ndef\\n\\t\\nload_housing_data\\n(\\nhousing_path\\n=\\nHOUSING_PATH\\n):\\n\\t\\t\\t\\t\\ncsv_path\\n\\t\\n=\\n\\t\\nos\\n.\\npath\\n.\\njoin\\n(\\nhousing_path\\n,\\n\\t\\n\"housing.csv\"\\n)\\n\\t\\t\\t\\t\\nreturn\\n\\t\\npd\\n.\\nread_csv\\n(\\ncsv_path\\n)\\nThis\\tfunction\\treturns\\ta\\tPandas\\tDataFrame\\tobject\\tcontaining\\tall\\tthe\\t\\ndata.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 69}), Document(page_content='Take\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\nLet’s\\t\\ntake\\ta\\tlook\\tat\\tthe\\ttop\\tfive\\trows\\tusing\\tthe\\tDataFrame’s\\t\\nhead()\\n\\tmethod\\t(see\\t\\nFigure\\t2-5\\n).\\nFigure\\t2-5.\\t\\nTop\\tfive\\trows\\tin\\tthe\\tdataset\\nEach\\trow\\trepresents\\tone\\tdistrict.\\tThere\\tare\\t10\\tattributes\\t(you\\tcan\\tsee\\tthe\\tfirst\\t6\\tin\\tthe\\tscreenshot):\\nlongitude\\n,\\t\\nlatitude\\n,\\t\\nhousing_median_age\\n,\\t\\ntotal_rooms\\n,\\t\\ntotal_bedrooms\\n,\\t\\npopulation\\n,\\nhouseholds\\n,\\t\\nmedian_income\\n,\\t\\nmedian_house_value\\n,\\tand\\t\\nocean_proximity\\n.\\nThe\\t\\ninfo()\\n\\t\\nmethod\\tis\\tuseful\\tto\\tget\\ta\\tquick\\tdescription\\tof\\tthe\\tdata,\\tin\\tparticular\\tthe\\ttotal\\tnumber\\tof\\trows,\\nand\\teach\\tattribute’s\\ttype\\tand\\tnumber\\tof\\tnon-null\\tvalues\\t(see\\t\\nFigure\\t2-6\\n).\\nFigure\\t2-6.\\t\\nHousing\\tinfo\\nThere\\tare\\t20,640\\tinstances\\tin\\tthe\\tdataset,\\twhich\\tmeans\\tthat\\tit\\tis\\tfairly\\tsmall\\tby\\tMachine\\tLearning\\nstandards,\\tbut\\tit’s\\tperfect\\tto\\tget\\tstarted.\\tNotice\\tthat\\tthe\\t\\ntotal_bedrooms\\n\\tattribute\\thas\\tonly\\t20,433\\tnon-\\nnull\\tvalues,\\tmeaning\\tthat\\t207\\tdistricts\\tare\\tmissing\\tthis\\tfeature.\\tWe\\twill\\tneed\\tto\\ttake\\tcare\\tof\\tthis\\tlater.\\nAll\\tattributes\\tare\\tnumerical,\\texcept\\tthe\\t\\nocean_proximity\\n\\tfield.\\tIts\\ttype\\tis\\t\\nobject\\n,\\tso\\tit\\tcould\\thold\\tany\\nkind\\tof\\tPython\\tobject,\\tbut\\tsince\\tyou\\tloaded\\tthis\\tdata\\tfrom\\ta\\tCSV\\tfile\\tyou\\tknow\\tthat\\tit\\tmust\\tbe\\ta\\ttext\\nattribute.\\tWhen\\tyou\\tlooked\\tat\\tthe\\ttop\\tfive\\trows,\\tyou\\tprobably\\tnoticed\\tthat\\tthe\\tvalues\\tin\\tthe\\nocean_proximity\\n\\tcolumn\\twere\\trepetitive,\\twhich\\tmeans\\tthat\\tit\\tis\\tprobably\\ta\\tcategorical\\tattribute.\\tYou\\ncan\\tfind\\tout\\twhat\\tcategories\\texist\\tand\\thow\\tmany\\tdistricts\\tbelong\\tto\\teach\\tcategory\\tby\\tusing\\tthe', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 70}), Document(page_content='value_counts()\\n\\t\\nmethod:\\n>>>\\t\\nhousing\\n[\\n\"ocean_proximity\"\\n]\\n.\\nvalue_counts\\n()\\n<1H\\tOCEAN\\t\\t\\t\\t\\t9136\\nINLAND\\t\\t\\t\\t\\t\\t\\t\\t6551\\nNEAR\\tOCEAN\\t\\t\\t\\t2658\\nNEAR\\tBAY\\t\\t\\t\\t\\t\\t2290\\nISLAND\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t5\\nName:\\tocean_proximity,\\tdtype:\\tint64\\nLet’s\\tlook\\tat\\tthe\\tother\\tfields.\\tThe\\t\\ndescribe()\\n\\t\\nmethod\\tshows\\ta\\tsummary\\tof\\tthe\\tnumerical\\tattributes\\n(\\nFigure\\t2-7\\n).\\nFigure\\t2-7.\\t\\nSummary\\tof\\teach\\tnumerical\\tattribute\\nThe\\t\\ncount\\n,\\t\\nmean\\n,\\t\\nmin\\n,\\tand\\t\\nmax\\n\\trows\\tare\\tself-explanatory.\\tNote\\tthat\\tthe\\tnull\\tvalues\\tare\\tignored\\t(so,\\tfor\\nexample,\\t\\ncount\\n\\tof\\t\\ntotal_bedrooms\\n\\tis\\t20,433,\\tnot\\t20,640).\\tThe\\t\\nstd\\n\\trow\\tshows\\tthe\\t\\nstandard\\tdeviation\\n,\\nwhich\\tmeasures\\thow\\tdispersed\\tthe\\tvalues\\tare.\\n11\\n\\tThe\\t25%,\\t50%,\\tand\\t75%\\trows\\tshow\\tthe\\tcorresponding\\npercentiles\\n:\\ta\\t\\npercentile\\tindicates\\tthe\\tvalue\\tbelow\\twhich\\ta\\tgiven\\tpercentage\\tof\\tobservations\\tin\\ta\\tgroup\\nof\\tobservations\\tfalls.\\tFor\\texample,\\t25%\\tof\\tthe\\tdistricts\\thave\\ta\\t\\nhousing_median_age\\n\\tlower\\tthan\\t18,\\nwhile\\t50%\\tare\\tlower\\tthan\\t29\\tand\\t75%\\tare\\tlower\\tthan\\t37.\\tThese\\tare\\toften\\tcalled\\tthe\\t25\\nth\\n\\tpercentile\\t(or\\n1\\nst\\n\\t\\nquartile\\n),\\tthe\\tmedian,\\tand\\tthe\\t75\\nth\\n\\tpercentile\\t(or\\t3\\nrd\\n\\tquartile).\\nAnother\\tquick\\tway\\tto\\tget\\ta\\tfeel\\tof\\tthe\\ttype\\tof\\tdata\\tyou\\tare\\tdealing\\twith\\tis\\tto\\tplot\\ta\\t\\nhistogram\\tfor\\teach\\nnumerical\\tattribute.\\tA\\thistogram\\tshows\\tthe\\tnumber\\tof\\tinstances\\t(on\\tthe\\tvertical\\taxis)\\tthat\\thave\\ta\\tgiven\\nvalue\\trange\\t(on\\tthe\\thorizontal\\taxis).\\tYou\\tcan\\teither\\tplot\\tthis\\tone\\tattribute\\tat\\ta\\ttime,\\tor\\tyou\\tcan\\tcall\\tthe\\nhist()\\n\\tmethod\\ton\\tthe\\twhole\\tdataset,\\tand\\tit\\twill\\tplot\\ta\\thistogram\\tfor\\teach\\tnumerical\\tattribute\\t(see\\nFigure\\t2-8\\n).\\tFor\\texample,\\tyou\\tcan\\tsee\\tthat\\tslightly\\tover\\t800\\tdistricts\\thave\\ta\\t\\nmedian_house_value\\n\\tequal\\nto\\tabout\\t$100,000.\\n%\\nmatplotlib\\n\\t\\ninline\\n\\t\\t\\t\\n#\\tonly\\tin\\ta\\tJupyter\\tnotebook\\nimport\\n\\t\\nmatplotlib.pyplot\\n\\t\\nas\\n\\t\\nplt\\nhousing\\n.\\nhist\\n(\\nbins\\n=\\n50\\n,\\n\\t\\nfigsize\\n=\\n(\\n20\\n,\\n15\\n))\\nplt\\n.\\nshow\\n()', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 71}), Document(page_content='NOTE\\nThe\\t\\nhist()\\n\\tmethod\\trelies\\ton\\t\\nMatplotlib,\\twhich\\tin\\tturn\\trelies\\ton\\ta\\tuser-specified\\tgraphical\\tbackend\\tto\\tdraw\\ton\\tyour\\tscreen.\\tSo\\nbefore\\tyou\\tcan\\tplot\\tanything,\\tyou\\tneed\\tto\\tspecify\\twhich\\tbackend\\tMatplotlib\\tshould\\tuse.\\tThe\\tsimplest\\toption\\tis\\tto\\tuse\\t\\nJupyter’s\\nmagic\\tcommand\\t\\n%matplotlib\\tinline\\n.\\tThis\\ttells\\tJupyter\\tto\\tset\\tup\\tMatplotlib\\tso\\tit\\tuses\\tJupyter’s\\town\\tbackend.\\tPlots\\tare\\tthen\\nrendered\\twithin\\tthe\\tnotebook\\titself.\\tNote\\tthat\\tcalling\\t\\nshow()\\n\\tis\\t\\noptional\\tin\\ta\\tJupyter\\tnotebook,\\tas\\tJupyter\\twill\\tautomatically\\ndisplay\\tplots\\twhen\\ta\\tcell\\tis\\texecuted.\\nFigure\\t2-8.\\t\\nA\\thistogram\\tfor\\teach\\tnumerical\\tattribute\\nNotice\\ta\\tfew\\tthings\\tin\\tthese\\thistograms:\\n1\\n.\\t\\nFirst,\\tthe\\tmedian\\tincome\\tattribute\\tdoes\\tnot\\tlook\\tlike\\tit\\tis\\texpressed\\tin\\tUS\\tdollars\\t(USD).\\tAfter\\nchecking\\twith\\tthe\\tteam\\tthat\\tcollected\\tthe\\tdata,\\tyou\\tare\\ttold\\tthat\\tthe\\tdata\\thas\\tbeen\\tscaled\\tand\\tcapped\\nat\\t15\\t(actually\\t15.0001)\\tfor\\thigher\\tmedian\\tincomes,\\tand\\tat\\t0.5\\t(actually\\t0.4999)\\tfor\\tlower\\tmedian\\nincomes.\\tWorking\\twith\\t\\npreprocessed\\tattributes\\tis\\tcommon\\tin\\tMachine\\tLearning,\\tand\\tit\\tis\\tnot\\nnecessarily\\ta\\tproblem,\\tbut\\tyou\\tshould\\ttry\\tto\\tunderstand\\thow\\tthe\\tdata\\twas\\tcomputed.\\n2\\n.\\t\\nThe\\thousing\\tmedian\\tage\\tand\\tthe\\tmedian\\thouse\\tvalue\\twere\\talso\\tcapped.\\tThe\\tlatter\\tmay\\tbe\\ta\\tserious\\nproblem\\tsince\\tit\\tis\\tyour\\t\\ntarget\\tattribute\\t(your\\tlabels).\\tYour\\tMachine\\tLearning\\talgorithms\\tmay\\tlearn\\nthat\\tprices\\tnever\\tgo\\tbeyond\\tthat\\tlimit.\\tYou\\tneed\\tto\\tcheck\\twith\\tyour\\tclient\\tteam\\t(the\\tteam\\tthat\\twill\\tuse\\nyour\\tsystem’s\\toutput)\\tto\\tsee\\tif\\tthis\\tis\\ta\\tproblem\\tor\\tnot.\\tIf\\tthey\\ttell\\tyou\\tthat\\tthey\\tneed\\tprecise\\npredictions\\teven\\tbeyond\\t$500,000,\\tthen\\tyou\\thave\\tmainly\\ttwo\\toptions:\\na\\n.\\t\\nCollect\\tproper\\tlabels\\tfor\\tthe\\tdistricts\\twhose\\tlabels\\twere\\tcapped.\\nb\\n.\\t\\nRemove\\tthose\\tdistricts\\tfrom\\tthe\\ttraining\\tset\\t(and\\talso\\tfrom\\tthe\\ttest\\tset,\\tsince\\tyour\\tsystem\\tshould\\nnot\\tbe\\tevaluated\\tpoorly\\tif\\tit\\tpredicts\\tvalues\\tbeyond\\t$500,000).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 72}), Document(page_content='3\\n.\\t\\nThese\\tattributes\\thave\\tvery\\tdifferent\\tscales.\\tWe\\twill\\tdiscuss\\tthis\\tlater\\tin\\tthis\\tchapter\\twhen\\twe\\nexplore\\tfeature\\tscaling.\\n4\\n.\\t\\nFinally,\\tmany\\thistograms\\tare\\t\\ntail\\theavy\\n:\\t\\nthey\\textend\\tmuch\\tfarther\\tto\\tthe\\tright\\tof\\tthe\\tmedian\\tthan\\tto\\nthe\\tleft.\\tThis\\tmay\\tmake\\tit\\ta\\tbit\\tharder\\tfor\\tsome\\tMachine\\tLearning\\talgorithms\\tto\\tdetect\\tpatterns.\\tWe\\nwill\\ttry\\ttransforming\\tthese\\tattributes\\tlater\\ton\\tto\\thave\\tmore\\tbell-shaped\\tdistributions.\\nHopefully\\tyou\\tnow\\thave\\ta\\tbetter\\tunderstanding\\tof\\tthe\\tkind\\tof\\tdata\\tyou\\t\\nare\\tdealing\\twith.\\nWARNING\\nWait!\\tBefore\\tyou\\tlook\\tat\\tthe\\tdata\\tany\\tfurther,\\tyou\\tneed\\tto\\tcreate\\ta\\ttest\\tset,\\tput\\tit\\taside,\\tand\\tnever\\tlook\\tat\\tit.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 73}), Document(page_content='Create\\ta\\tTest\\tSet\\nIt\\t\\nmay\\tsound\\tstrange\\tto\\tvoluntarily\\tset\\taside\\tpart\\tof\\tthe\\tdata\\tat\\tthis\\tstage.\\tAfter\\tall,\\tyou\\thave\\tonly\\ttaken\\ta\\nquick\\tglance\\tat\\tthe\\tdata,\\tand\\tsurely\\tyou\\tshould\\tlearn\\ta\\twhole\\tlot\\tmore\\tabout\\tit\\tbefore\\tyou\\tdecide\\twhat\\nalgorithms\\tto\\tuse,\\tright?\\tThis\\tis\\ttrue,\\tbut\\tyour\\tbrain\\tis\\tan\\tamazing\\tpattern\\tdetection\\tsystem,\\twhich\\tmeans\\nthat\\tit\\tis\\thighly\\tprone\\tto\\t\\noverfitting:\\tif\\tyou\\tlook\\tat\\tthe\\ttest\\tset,\\tyou\\tmay\\tstumble\\tupon\\tsome\\tseemingly\\ninteresting\\tpattern\\tin\\tthe\\ttest\\tdata\\tthat\\tleads\\tyou\\tto\\tselect\\ta\\tparticular\\tkind\\tof\\tMachine\\tLearning\\tmodel.\\nWhen\\tyou\\testimate\\tthe\\tgeneralization\\terror\\tusing\\tthe\\ttest\\tset,\\tyour\\testimate\\twill\\tbe\\ttoo\\toptimistic\\tand\\tyou\\nwill\\tlaunch\\ta\\tsystem\\tthat\\twill\\tnot\\tperform\\tas\\twell\\tas\\texpected.\\tThis\\tis\\t\\ncalled\\t\\ndata\\tsnooping\\n\\tbias.\\nCreating\\ta\\ttest\\tset\\tis\\ttheoretically\\tquite\\tsimple:\\tjust\\tpick\\tsome\\tinstances\\trandomly,\\ttypically\\t20%\\tof\\tthe\\ndataset,\\tand\\tset\\tthem\\taside:\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\ndef\\n\\t\\nsplit_train_test\\n(\\ndata\\n,\\n\\t\\ntest_ratio\\n):\\n\\t\\t\\t\\t\\nshuffled_indices\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\npermutation\\n(\\nlen\\n(\\ndata\\n))\\n\\t\\t\\t\\t\\ntest_set_size\\n\\t\\n=\\n\\t\\nint\\n(\\nlen\\n(\\ndata\\n)\\n\\t\\n*\\n\\t\\ntest_ratio\\n)\\n\\t\\t\\t\\t\\ntest_indices\\n\\t\\n=\\n\\t\\nshuffled_indices\\n[:\\ntest_set_size\\n]\\n\\t\\t\\t\\t\\ntrain_indices\\n\\t\\n=\\n\\t\\nshuffled_indices\\n[\\ntest_set_size\\n:]\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ndata\\n.\\niloc\\n[\\ntrain_indices\\n],\\n\\t\\ndata\\n.\\niloc\\n[\\ntest_indices\\n]\\nYou\\tcan\\tthen\\tuse\\tthis\\tfunction\\tlike\\tthis:\\n>>>\\t\\ntrain_set\\n,\\n\\t\\ntest_set\\n\\t\\n=\\n\\t\\nsplit_train_test\\n(\\nhousing\\n,\\n\\t\\n0.2\\n)\\n>>>\\t\\nprint\\n(\\nlen\\n(\\ntrain_set\\n),\\n\\t\\n\"train\\t+\"\\n,\\n\\t\\nlen\\n(\\ntest_set\\n),\\n\\t\\n\"test\"\\n)\\n16512\\ttrain\\t+\\t4128\\ttest\\nWell,\\tthis\\tworks,\\tbut\\tit\\tis\\tnot\\tperfect:\\tif\\tyou\\trun\\tthe\\tprogram\\tagain,\\tit\\twill\\tgenerate\\ta\\tdifferent\\ttest\\tset!\\nOver\\ttime,\\tyou\\t(or\\tyour\\tMachine\\tLearning\\talgorithms)\\twill\\tget\\tto\\tsee\\tthe\\twhole\\tdataset,\\twhich\\tis\\twhat\\nyou\\twant\\tto\\tavoid.\\nOne\\tsolution\\tis\\tto\\tsave\\tthe\\ttest\\tset\\ton\\tthe\\tfirst\\trun\\tand\\tthen\\tload\\tit\\tin\\tsubsequent\\truns.\\tAnother\\toption\\tis\\tto\\nset\\tthe\\trandom\\tnumber\\tgenerator’s\\tseed\\t(e.g.,\\t\\nnp.random.seed(42)\\n)\\n12\\n\\tbefore\\tcalling\\nnp.random.permutation()\\n,\\tso\\t\\nthat\\tit\\talways\\tgenerates\\tthe\\tsame\\tshuffled\\tindices.\\nBut\\tboth\\tthese\\tsolutions\\twill\\tbreak\\tnext\\ttime\\tyou\\tfetch\\tan\\tupdated\\tdataset.\\tA\\tcommon\\tsolution\\tis\\tto\\tuse\\neach\\tinstance’s\\tidentifier\\tto\\tdecide\\twhether\\tor\\tnot\\tit\\tshould\\tgo\\tin\\tthe\\ttest\\tset\\t(assuming\\tinstances\\thave\\ta\\nunique\\tand\\timmutable\\tidentifier).\\tFor\\texample,\\tyou\\tcould\\tcompute\\ta\\thash\\tof\\teach\\tinstance’s\\tidentifier,\\nkeep\\tonly\\tthe\\tlast\\tbyte\\tof\\tthe\\thash,\\tand\\tput\\tthe\\tinstance\\tin\\tthe\\ttest\\tset\\tif\\tthis\\tvalue\\tis\\tlower\\tor\\tequal\\tto\\t51\\n(~20%\\tof\\t256).\\tThis\\tensures\\tthat\\tthe\\ttest\\tset\\twill\\tremain\\tconsistent\\tacross\\tmultiple\\truns,\\teven\\tif\\tyou\\nrefresh\\tthe\\tdataset.\\tThe\\tnew\\ttest\\tset\\twill\\tcontain\\t20%\\tof\\tthe\\tnew\\tinstances,\\tbut\\tit\\twill\\tnot\\tcontain\\tany\\ninstance\\tthat\\twas\\tpreviously\\tin\\tthe\\ttraining\\tset.\\tHere\\tis\\ta\\tpossible\\timplementation:\\nimport\\n\\t\\nhashlib\\ndef\\n\\t\\ntest_set_check\\n(\\nidentifier\\n,\\n\\t\\ntest_ratio\\n,\\n\\t\\nhash\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nhash\\n(\\nnp\\n.\\nint64\\n(\\nidentifier\\n))\\n.\\ndigest\\n()[\\n-\\n1\\n]\\n\\t\\n<\\n\\t\\n256\\n\\t\\n*\\n\\t\\ntest_ratio\\ndef\\n\\t\\nsplit_train_test_by_id\\n(\\ndata\\n,\\n\\t\\ntest_ratio\\n,\\n\\t\\nid_column\\n,\\n\\t\\nhash\\n=\\nhashlib\\n.\\nmd5\\n):\\n\\t\\t\\t\\t\\nids\\n\\t\\n=\\n\\t\\ndata\\n[\\nid_column\\n]\\n\\t\\t\\t\\t\\nin_test_set\\n\\t\\n=\\n\\t\\nids\\n.\\napply\\n(\\nlambda\\n\\t\\nid_\\n:\\n\\t\\ntest_set_check\\n(\\nid_\\n,\\n\\t\\ntest_ratio\\n,\\n\\t\\nhash\\n))\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ndata\\n.\\nloc\\n[\\n~\\nin_test_set\\n],\\n\\t\\ndata\\n.\\nloc\\n[\\nin_test_set\\n]', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 74}), Document(page_content='Unfortunately,\\tthe\\thousing\\tdataset\\tdoes\\tnot\\thave\\tan\\tidentifier\\tcolumn.\\tThe\\tsimplest\\tsolution\\tis\\tto\\tuse\\tthe\\nrow\\tindex\\tas\\tthe\\tID:\\nhousing_with_id\\n\\t\\n=\\n\\t\\nhousing\\n.\\nreset_index\\n()\\n\\t\\t\\t\\n#\\tadds\\tan\\t`index`\\tcolumn\\ntrain_set\\n,\\n\\t\\ntest_set\\n\\t\\n=\\n\\t\\nsplit_train_test_by_id\\n(\\nhousing_with_id\\n,\\n\\t\\n0.2\\n,\\n\\t\\n\"index\"\\n)\\nIf\\tyou\\tuse\\tthe\\trow\\tindex\\tas\\ta\\tunique\\tidentifier,\\tyou\\tneed\\tto\\tmake\\tsure\\tthat\\tnew\\tdata\\tgets\\tappended\\tto\\tthe\\nend\\tof\\tthe\\tdataset,\\tand\\tno\\trow\\tever\\tgets\\tdeleted.\\tIf\\tthis\\tis\\tnot\\tpossible,\\tthen\\tyou\\tcan\\ttry\\tto\\tuse\\tthe\\tmost\\nstable\\tfeatures\\tto\\tbuild\\ta\\tunique\\tidentifier.\\tFor\\texample,\\ta\\tdistrict’s\\tlatitude\\tand\\tlongitude\\tare\\tguaranteed\\nto\\tbe\\tstable\\tfor\\ta\\tfew\\tmillion\\tyears,\\tso\\tyou\\tcould\\tcombine\\tthem\\tinto\\tan\\tID\\tlike\\tso:\\n13\\nhousing_with_id\\n[\\n\"id\"\\n]\\n\\t\\n=\\n\\t\\nhousing\\n[\\n\"longitude\"\\n]\\n\\t\\n*\\n\\t\\n1000\\n\\t\\n+\\n\\t\\nhousing\\n[\\n\"latitude\"\\n]\\ntrain_set\\n,\\n\\t\\ntest_set\\n\\t\\n=\\n\\t\\nsplit_train_test_by_id\\n(\\nhousing_with_id\\n,\\n\\t\\n0.2\\n,\\n\\t\\n\"id\"\\n)\\nScikit-Learn\\tprovides\\ta\\tfew\\tfunctions\\tto\\tsplit\\tdatasets\\tinto\\tmultiple\\tsubsets\\tin\\tvarious\\tways.\\tThe\\t\\nsimplest\\nfunction\\tis\\t\\ntrain_test_split\\n,\\twhich\\tdoes\\tpretty\\tmuch\\tthe\\tsame\\tthing\\tas\\tthe\\tfunction\\nsplit_train_test\\n\\tdefined\\tearlier,\\twith\\ta\\tcouple\\tof\\tadditional\\tfeatures.\\tFirst\\tthere\\tis\\ta\\t\\nrandom_state\\nparameter\\tthat\\tallows\\tyou\\tto\\tset\\tthe\\trandom\\tgenerator\\tseed\\tas\\texplained\\tpreviously,\\tand\\tsecond\\tyou\\tcan\\npass\\tit\\tmultiple\\tdatasets\\twith\\tan\\tidentical\\tnumber\\tof\\trows,\\tand\\tit\\twill\\tsplit\\tthem\\ton\\tthe\\tsame\\tindices\\t(this\\nis\\tvery\\tuseful,\\tfor\\texample,\\tif\\tyou\\thave\\ta\\tseparate\\tDataFrame\\tfor\\tlabels):\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\ntrain_test_split\\ntrain_set\\n,\\n\\t\\ntest_set\\n\\t\\n=\\n\\t\\ntrain_test_split\\n(\\nhousing\\n,\\n\\t\\ntest_size\\n=\\n0.2\\n,\\n\\t\\nrandom_state\\n=\\n42\\n)\\nSo\\tfar\\twe\\thave\\tconsidered\\tpurely\\trandom\\tsampling\\tmethods.\\tThis\\tis\\tgenerally\\tfine\\tif\\tyour\\tdataset\\tis\\tlarge\\nenough\\t(especially\\trelative\\tto\\tthe\\tnumber\\tof\\tattributes),\\tbut\\tif\\tit\\tis\\tnot,\\tyou\\trun\\tthe\\trisk\\tof\\tintroducing\\ta\\nsignificant\\t\\nsampling\\tbias.\\tWhen\\ta\\tsurvey\\tcompany\\tdecides\\tto\\tcall\\t1,000\\tpeople\\tto\\task\\tthem\\ta\\tfew\\nquestions,\\tthey\\tdon’t\\tjust\\tpick\\t1,000\\tpeople\\trandomly\\tin\\ta\\tphone\\tbooth.\\tThey\\ttry\\tto\\tensure\\tthat\\tthese\\t1,000\\npeople\\tare\\trepresentative\\tof\\tthe\\twhole\\tpopulation.\\tFor\\texample,\\tthe\\tUS\\tpopulation\\tis\\tcomposed\\tof\\t51.3%\\nfemale\\tand\\t48.7%\\tmale,\\tso\\ta\\twell-conducted\\tsurvey\\tin\\tthe\\tUS\\twould\\ttry\\tto\\tmaintain\\tthis\\tratio\\tin\\tthe\\nsample:\\t513\\tfemale\\tand\\t487\\tmale.\\tThis\\tis\\tcalled\\t\\nstratified\\tsampling\\n:\\t\\nthe\\tpopulation\\tis\\tdivided\\tinto\\nhomogeneous\\tsubgroups\\tcalled\\t\\nstrata\\n,\\tand\\tthe\\tright\\tnumber\\tof\\tinstances\\tis\\tsampled\\tfrom\\teach\\tstratum\\tto\\nguarantee\\tthat\\tthe\\ttest\\tset\\tis\\trepresentative\\tof\\tthe\\toverall\\tpopulation.\\tIf\\tthey\\tused\\tpurely\\trandom\\tsampling,\\nthere\\twould\\tbe\\tabout\\t12%\\tchance\\tof\\tsampling\\ta\\tskewed\\ttest\\tset\\twith\\teither\\tless\\tthan\\t49%\\tfemale\\tor\\tmore\\nthan\\t54%\\tfemale.\\tEither\\tway,\\tthe\\tsurvey\\tresults\\twould\\tbe\\tsignificantly\\tbiased.\\nSuppose\\tyou\\tchatted\\twith\\texperts\\twho\\ttold\\tyou\\tthat\\tthe\\tmedian\\tincome\\tis\\ta\\tvery\\timportant\\tattribute\\tto\\npredict\\tmedian\\thousing\\tprices.\\tYou\\tmay\\twant\\tto\\tensure\\tthat\\tthe\\ttest\\tset\\tis\\trepresentative\\tof\\tthe\\tvarious\\ncategories\\tof\\tincomes\\tin\\tthe\\twhole\\tdataset.\\tSince\\tthe\\tmedian\\tincome\\tis\\ta\\tcontinuous\\tnumerical\\tattribute,\\nyou\\tfirst\\tneed\\tto\\tcreate\\tan\\tincome\\tcategory\\tattribute.\\tLet’s\\tlook\\tat\\tthe\\tmedian\\tincome\\thistogram\\tmore\\nclosely\\t(see\\t\\nFigure\\t2-8\\n):\\tmost\\tmedian\\tincome\\tvalues\\tare\\tclustered\\taround\\t$20,000–$50,000,\\tbut\\tsome\\nmedian\\tincomes\\tgo\\tfar\\tbeyond\\t$60,000.\\tIt\\tis\\timportant\\tto\\thave\\ta\\tsufficient\\tnumber\\tof\\tinstances\\tin\\tyour\\ndataset\\tfor\\teach\\tstratum,\\tor\\telse\\tthe\\testimate\\tof\\tthe\\tstratum’s\\timportance\\tmay\\tbe\\tbiased.\\tThis\\tmeans\\tthat\\nyou\\tshould\\tnot\\thave\\ttoo\\tmany\\tstrata,\\tand\\teach\\tstratum\\tshould\\tbe\\tlarge\\tenough.\\tThe\\tfollowing\\tcode\\tcreates\\nan\\tincome\\tcategory\\tattribute\\tby\\tdividing\\tthe\\tmedian\\tincome\\tby\\t1.5\\t(to\\tlimit\\tthe\\tnumber\\tof\\tincome', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 75}), Document(page_content='categories),\\tand\\trounding\\tup\\tusing\\t\\nceil\\n\\t(to\\thave\\tdiscrete\\tcategories),\\tand\\tthen\\tmerging\\tall\\tthe\\tcategories\\ngreater\\tthan\\t5\\tinto\\tcategory\\t5:\\nhousing\\n[\\n\"income_cat\"\\n]\\n\\t\\n=\\n\\t\\nnp\\n.\\nceil\\n(\\nhousing\\n[\\n\"median_income\"\\n]\\n\\t\\n/\\n\\t\\n1.5\\n)\\nhousing\\n[\\n\"income_cat\"\\n]\\n.\\nwhere\\n(\\nhousing\\n[\\n\"income_cat\"\\n]\\n\\t\\n<\\n\\t\\n5\\n,\\n\\t\\n5.0\\n,\\n\\t\\ninplace\\n=\\nTrue\\n)\\nThese\\tincome\\tcategories\\tare\\trepresented\\ton\\t\\nFigure\\t2-9\\n):\\nFigure\\t2-9.\\t\\nHistogram\\tof\\tincome\\tcategories\\nNow\\tyou\\tare\\tready\\tto\\tdo\\t\\nstratified\\tsampling\\tbased\\ton\\tthe\\tincome\\tcategory.\\tFor\\tthis\\tyou\\tcan\\tuse\\tScikit-\\nLearn’s\\t\\nStratifiedShuffleSplit\\n\\tclass:\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\nStratifiedShuffleSplit\\nsplit\\n\\t\\n=\\n\\t\\nStratifiedShuffleSplit\\n(\\nn_splits\\n=\\n1\\n,\\n\\t\\ntest_size\\n=\\n0.2\\n,\\n\\t\\nrandom_state\\n=\\n42\\n)\\nfor\\n\\t\\ntrain_index\\n,\\n\\t\\ntest_index\\n\\t\\nin\\n\\t\\nsplit\\n.\\nsplit\\n(\\nhousing\\n,\\n\\t\\nhousing\\n[\\n\"income_cat\"\\n]):\\n\\t\\t\\t\\t\\nstrat_train_set\\n\\t\\n=\\n\\t\\nhousing\\n.\\nloc\\n[\\ntrain_index\\n]\\n\\t\\t\\t\\t\\nstrat_test_set\\n\\t\\n=\\n\\t\\nhousing\\n.\\nloc\\n[\\ntest_index\\n]\\nLet’s\\tsee\\tif\\tthis\\tworked\\tas\\texpected.\\tYou\\tcan\\tstart\\tby\\tlooking\\tat\\tthe\\tincome\\tcategory\\tproportions\\tin\\tthe\\nfull\\thousing\\tdataset:\\n>>>\\t\\nhousing\\n[\\n\"income_cat\"\\n]\\n.\\nvalue_counts\\n()\\n\\t\\n/\\n\\t\\nlen\\n(\\nhousing\\n)\\n3.0\\t\\t\\t\\t0.350581\\n2.0\\t\\t\\t\\t0.318847\\n4.0\\t\\t\\t\\t0.176308\\n5.0\\t\\t\\t\\t0.114438\\n1.0\\t\\t\\t\\t0.039826\\nName:\\tincome_cat,\\tdtype:\\tfloat64\\nWith\\tsimilar\\tcode\\tyou\\tcan\\tmeasure\\tthe\\tincome\\tcategory\\tproportions\\tin\\tthe\\ttest\\tset.\\t\\nFigure\\t2-10\\n\\tcompares\\nthe\\tincome\\tcategory\\tproportions\\tin\\tthe\\toverall\\tdataset,\\tin\\tthe\\ttest\\tset\\tgenerated\\twith\\tstratified\\tsampling,\\nand\\tin\\ta\\ttest\\tset\\tgenerated\\tusing\\tpurely\\trandom\\tsampling.\\tAs\\tyou\\tcan\\tsee,\\tthe\\ttest\\tset\\tgenerated\\tusing', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 76}), Document(page_content='stratified\\tsampling\\thas\\tincome\\tcategory\\tproportions\\talmost\\tidentical\\tto\\tthose\\tin\\tthe\\tfull\\tdataset,\\twhereas\\nthe\\ttest\\tset\\tgenerated\\tusing\\tpurely\\trandom\\tsampling\\tis\\tquite\\tskewed.\\nFigure\\t2-10.\\t\\nSampling\\tbias\\tcomparison\\tof\\tstratified\\tversus\\tpurely\\trandom\\tsampling\\nNow\\tyou\\tshould\\tremove\\tthe\\t\\nincome_cat\\n\\tattribute\\tso\\tthe\\tdata\\tis\\tback\\tto\\tits\\toriginal\\tstate:\\nfor\\n\\t\\nset_\\n\\t\\nin\\n\\t\\n(\\nstrat_train_set\\n,\\n\\t\\nstrat_test_set\\n):\\n\\t\\t\\t\\t\\nset_\\n.\\ndrop\\n(\\n\"income_cat\"\\n,\\n\\t\\naxis\\n=\\n1\\n,\\n\\t\\ninplace\\n=\\nTrue\\n)\\nWe\\tspent\\tquite\\ta\\tbit\\tof\\ttime\\ton\\ttest\\tset\\tgeneration\\tfor\\ta\\tgood\\treason:\\tthis\\tis\\tan\\toften\\tneglected\\tbut\\tcritical\\npart\\tof\\ta\\tMachine\\tLearning\\tproject.\\tMoreover,\\tmany\\tof\\tthese\\tideas\\twill\\tbe\\tuseful\\tlater\\twhen\\twe\\tdiscuss\\ncross-validation.\\tNow\\tit’s\\ttime\\tto\\tmove\\ton\\tto\\tthe\\tnext\\tstage:\\texploring\\tthe\\t\\ndata.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 77}), Document(page_content='Discover\\tand\\tVisualize\\tthe\\tData\\tto\\tGain\\tInsights\\nSo\\tfar\\tyou\\thave\\tonly\\ttaken\\ta\\tquick\\tglance\\tat\\tthe\\tdata\\tto\\tget\\ta\\tgeneral\\tunderstanding\\tof\\tthe\\tkind\\tof\\tdata\\tyou\\nare\\tmanipulating.\\tNow\\tthe\\tgoal\\tis\\tto\\tgo\\ta\\tlittle\\tbit\\tmore\\tin\\tdepth.\\nFirst,\\tmake\\tsure\\tyou\\thave\\tput\\tthe\\ttest\\tset\\taside\\tand\\tyou\\tare\\tonly\\texploring\\tthe\\ttraining\\tset.\\tAlso,\\tif\\tthe\\ntraining\\tset\\tis\\tvery\\tlarge,\\tyou\\tmay\\twant\\tto\\tsample\\tan\\texploration\\tset,\\tto\\tmake\\tmanipulations\\teasy\\tand\\tfast.\\nIn\\tour\\tcase,\\tthe\\tset\\tis\\tquite\\tsmall\\tso\\tyou\\tcan\\tjust\\twork\\tdirectly\\ton\\tthe\\tfull\\tset.\\tLet’s\\tcreate\\ta\\tcopy\\tso\\tyou\\ncan\\tplay\\twith\\tit\\twithout\\tharming\\tthe\\ttraining\\tset:\\nhousing\\n\\t\\n=\\n\\t\\nstrat_train_set\\n.\\ncopy\\n()', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 78}), Document(page_content='Visualizing\\tGeographical\\tData\\nSince\\t\\nthere\\tis\\tgeographical\\tinformation\\t(latitude\\tand\\tlongitude),\\tit\\tis\\ta\\tgood\\tidea\\tto\\tcreate\\ta\\tscatterplot\\tof\\nall\\tdistricts\\tto\\tvisualize\\tthe\\tdata\\t(\\nFigure\\t2-11\\n):\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n\\t\\nx\\n=\\n\"longitude\"\\n,\\n\\t\\ny\\n=\\n\"latitude\"\\n)\\nFigure\\t2-11.\\t\\nA\\tgeographical\\tscatterplot\\tof\\tthe\\tdata\\nThis\\tlooks\\tlike\\tCalifornia\\tall\\tright,\\tbut\\tother\\tthan\\tthat\\tit\\tis\\thard\\tto\\tsee\\tany\\tparticular\\tpattern.\\tSetting\\tthe\\nalpha\\n\\toption\\tto\\t\\n0.1\\n\\tmakes\\tit\\tmuch\\teasier\\tto\\tvisualize\\tthe\\tplaces\\twhere\\tthere\\tis\\ta\\thigh\\tdensity\\tof\\tdata\\npoints\\t(\\nFigure\\t2-12\\n):\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n\\t\\nx\\n=\\n\"longitude\"\\n,\\n\\t\\ny\\n=\\n\"latitude\"\\n,\\n\\t\\nalpha\\n=\\n0.1\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 79}), Document(page_content='Figure\\t2-12.\\t\\nA\\tbetter\\tvisualization\\thighlighting\\thigh-density\\tareas\\nNow\\tthat’s\\tmuch\\tbetter:\\tyou\\tcan\\tclearly\\tsee\\tthe\\thigh-density\\tareas,\\tnamely\\tthe\\tBay\\tArea\\tand\\taround\\tLos\\nAngeles\\tand\\tSan\\tDiego,\\tplus\\ta\\tlong\\tline\\tof\\tfairly\\thigh\\tdensity\\tin\\tthe\\tCentral\\tValley,\\tin\\tparticular\\taround\\nSacramento\\tand\\tFresno.\\nMore\\tgenerally,\\tour\\tbrains\\tare\\tvery\\tgood\\tat\\tspotting\\tpatterns\\ton\\tpictures,\\tbut\\tyou\\tmay\\tneed\\tto\\tplay\\taround\\nwith\\tvisualization\\tparameters\\tto\\tmake\\tthe\\tpatterns\\tstand\\tout.\\nNow\\tlet’s\\tlook\\tat\\tthe\\thousing\\tprices\\t(\\nFigure\\t2-13\\n).\\tThe\\tradius\\tof\\teach\\tcircle\\trepresents\\tthe\\tdistrict’s\\npopulation\\t(option\\t\\ns\\n),\\tand\\tthe\\tcolor\\trepresents\\tthe\\tprice\\t(option\\t\\nc\\n).\\tWe\\twill\\tuse\\ta\\tpredefined\\tcolor\\tmap\\n(option\\t\\ncmap\\n)\\tcalled\\t\\njet\\n,\\twhich\\tranges\\tfrom\\tblue\\t(low\\tvalues)\\tto\\tred\\t(high\\tprices):\\n14\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n\\t\\nx\\n=\\n\"longitude\"\\n,\\n\\t\\ny\\n=\\n\"latitude\"\\n,\\n\\t\\nalpha\\n=\\n0.4\\n,\\n\\t\\t\\t\\t\\ns\\n=\\nhousing\\n[\\n\"population\"\\n]\\n/\\n100\\n,\\n\\t\\nlabel\\n=\\n\"population\"\\n,\\n\\t\\nfigsize\\n=\\n(\\n10\\n,\\n7\\n),\\n\\t\\t\\t\\t\\nc\\n=\\n\"median_house_value\"\\n,\\n\\t\\ncmap\\n=\\nplt\\n.\\nget_cmap\\n(\\n\"jet\"\\n),\\n\\t\\ncolorbar\\n=\\nTrue\\n,\\n)\\nplt\\n.\\nlegend\\n()', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 80}), Document(page_content='Figure\\t2-13.\\t\\nCalifornia\\thousing\\tprices\\nThis\\timage\\ttells\\tyou\\tthat\\tthe\\thousing\\tprices\\tare\\tvery\\tmuch\\trelated\\tto\\tthe\\tlocation\\t(e.g.,\\tclose\\tto\\tthe\\tocean)\\nand\\tto\\tthe\\tpopulation\\tdensity,\\tas\\tyou\\tprobably\\tknew\\talready.\\tIt\\twill\\tprobably\\tbe\\tuseful\\tto\\tuse\\ta\\tclustering\\nalgorithm\\tto\\tdetect\\tthe\\tmain\\tclusters,\\tand\\tadd\\tnew\\tfeatures\\tthat\\tmeasure\\tthe\\tproximity\\tto\\tthe\\tcluster\\ncenters.\\tThe\\tocean\\tproximity\\tattribute\\tmay\\tbe\\tuseful\\tas\\twell,\\talthough\\tin\\tNorthern\\tCalifornia\\tthe\\thousing\\nprices\\tin\\tcoastal\\tdistricts\\tare\\tnot\\ttoo\\thigh,\\tso\\tit\\tis\\tnot\\ta\\t\\nsimple\\trule.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 81}), Document(page_content='Looking\\tfor\\tCorrelations\\nSince\\t\\nthe\\tdataset\\tis\\tnot\\ttoo\\tlarge,\\tyou\\tcan\\teasily\\tcompute\\tthe\\t\\nstandard\\tcorrelation\\tcoefficient\\n\\t(also\\ncalled\\t\\nPearson’s\\tr\\n)\\tbetween\\t\\nevery\\tpair\\tof\\tattributes\\tusing\\tthe\\t\\ncorr()\\n\\tmethod:\\ncorr_matrix\\n\\t\\n=\\n\\t\\nhousing\\n.\\ncorr\\n()\\nNow\\tlet’s\\tlook\\tat\\thow\\tmuch\\teach\\tattribute\\tcorrelates\\twith\\tthe\\tmedian\\thouse\\tvalue:\\n>>>\\t\\ncorr_matrix\\n[\\n\"median_house_value\"\\n]\\n.\\nsort_values\\n(\\nascending\\n=\\nFalse\\n)\\nmedian_house_value\\t\\t\\t\\t1.000000\\nmedian_income\\t\\t\\t\\t\\t\\t\\t\\t\\t0.687170\\ntotal_rooms\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.135231\\nhousing_median_age\\t\\t\\t\\t0.114220\\nhouseholds\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.064702\\ntotal_bedrooms\\t\\t\\t\\t\\t\\t\\t\\t0.047865\\npopulation\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.026699\\nlongitude\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.047279\\nlatitude\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.142826\\nName:\\tmedian_house_value,\\tdtype:\\tfloat64\\nThe\\tcorrelation\\tcoefficient\\tranges\\tfrom\\t–1\\tto\\t1.\\tWhen\\tit\\tis\\tclose\\tto\\t1,\\tit\\tmeans\\tthat\\tthere\\tis\\ta\\tstrong\\npositive\\tcorrelation;\\tfor\\texample,\\tthe\\tmedian\\thouse\\tvalue\\ttends\\tto\\tgo\\tup\\twhen\\tthe\\tmedian\\tincome\\tgoes\\tup.\\nWhen\\tthe\\tcoefficient\\tis\\tclose\\tto\\t–1,\\tit\\tmeans\\tthat\\tthere\\tis\\ta\\tstrong\\tnegative\\tcorrelation;\\tyou\\tcan\\tsee\\ta\\nsmall\\tnegative\\tcorrelation\\tbetween\\tthe\\tlatitude\\tand\\tthe\\tmedian\\thouse\\tvalue\\t(i.e.,\\tprices\\thave\\ta\\tslight\\ntendency\\tto\\tgo\\tdown\\twhen\\tyou\\tgo\\tnorth).\\tFinally,\\tcoefficients\\tclose\\tto\\tzero\\tmean\\tthat\\tthere\\tis\\tno\\tlinear\\ncorrelation.\\t\\nFigure\\t2-14\\n\\tshows\\tvarious\\tplots\\talong\\twith\\tthe\\tcorrelation\\tcoefficient\\tbetween\\ttheir\\nhorizontal\\tand\\tvertical\\taxes.\\nFigure\\t2-14.\\t\\nStandard\\tcorrelation\\tcoefficient\\tof\\tvarious\\tdatasets\\t(source:\\tWikipedia;\\tpublic\\tdomain\\timage)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 82}), Document(page_content='WARNING\\nThe\\tcorrelation\\tcoefficient\\tonly\\tmeasures\\tlinear\\tcorrelations\\t(“if\\t\\nx\\n\\tgoes\\tup,\\tthen\\t\\ny\\n\\tgenerally\\tgoes\\tup/down”).\\tIt\\tmay\\tcompletely\\nmiss\\tout\\ton\\tnonlinear\\trelationships\\t(e.g.,\\t“if\\t\\nx\\n\\tis\\tclose\\tto\\tzero\\tthen\\t\\ny\\n\\tgenerally\\tgoes\\tup”).\\tNote\\thow\\tall\\tthe\\tplots\\tof\\tthe\\tbottom\\trow\\nhave\\ta\\tcorrelation\\tcoefficient\\tequal\\tto\\tzero\\tdespite\\tthe\\tfact\\tthat\\ttheir\\taxes\\tare\\tclearly\\tnot\\tindependent:\\tthese\\tare\\texamples\\tof\\nnonlinear\\trelationships.\\tAlso,\\tthe\\tsecond\\trow\\tshows\\texamples\\twhere\\tthe\\tcorrelation\\tcoefficient\\tis\\tequal\\tto\\t1\\tor\\t–1;\\tnotice\\tthat\\tthis\\nhas\\tnothing\\tto\\tdo\\twith\\tthe\\tslope.\\tFor\\texample,\\tyour\\theight\\tin\\tinches\\thas\\ta\\tcorrelation\\tcoefficient\\tof\\t1\\twith\\tyour\\theight\\tin\\tfeet\\tor\\tin\\nnanometers.\\nAnother\\tway\\tto\\tcheck\\tfor\\tcorrelation\\tbetween\\tattributes\\tis\\tto\\tuse\\t\\nPandas’\\t\\nscatter_matrix\\n\\tfunction,\\nwhich\\tplots\\tevery\\tnumerical\\tattribute\\tagainst\\tevery\\tother\\tnumerical\\tattribute.\\tSince\\tthere\\tare\\tnow\\t11\\nnumerical\\tattributes,\\tyou\\twould\\tget\\t11\\n2\\n\\t=\\t121\\tplots,\\twhich\\twould\\tnot\\tfit\\ton\\ta\\tpage,\\tso\\tlet’s\\tjust\\tfocus\\ton\\ta\\nfew\\tpromising\\tattributes\\tthat\\tseem\\tmost\\tcorrelated\\twith\\tthe\\tmedian\\thousing\\tvalue\\t(\\nFigure\\t2-15\\n):\\nfrom\\n\\t\\npandas.tools.plotting\\n\\t\\nimport\\n\\t\\nscatter_matrix\\nattributes\\n\\t\\n=\\n\\t\\n[\\n\"median_house_value\"\\n,\\n\\t\\n\"median_income\"\\n,\\n\\t\\n\"total_rooms\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\"housing_median_age\"\\n]\\nscatter_matrix\\n(\\nhousing\\n[\\nattributes\\n],\\n\\t\\nfigsize\\n=\\n(\\n12\\n,\\n\\t\\n8\\n))\\nFigure\\t2-15.\\t\\nScatter\\tmatrix\\nThe\\tmain\\tdiagonal\\t(top\\tleft\\tto\\tbottom\\tright)\\twould\\tbe\\tfull\\tof\\tstraight\\tlines\\tif\\tPandas\\tplotted\\teach\\tvariable\\nagainst\\titself,\\twhich\\twould\\tnot\\tbe\\tvery\\tuseful.\\tSo\\tinstead\\tPandas\\tdisplays\\ta\\thistogram\\tof\\teach\\tattribute\\n(other\\toptions\\tare\\tavailable;\\tsee\\t\\nPandas’\\tdocumentation\\tfor\\tmore\\tdetails).\\nThe\\tmost\\tpromising\\tattribute\\tto\\tpredict\\tthe\\tmedian\\thouse\\tvalue\\tis\\tthe\\tmedian\\tincome,\\tso\\tlet’s\\tzoom\\tin\\ton\\ntheir\\tcorrelation\\tscatterplot\\t(\\nFigure\\t2-16\\n):\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n\\t\\nx\\n=\\n\"median_income\"\\n,\\n\\t\\ny\\n=\\n\"median_house_value\"\\n,', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 83}), Document(page_content='\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nalpha\\n=\\n0.1\\n)\\nThis\\tplot\\treveals\\ta\\tfew\\tthings.\\tFirst,\\tthe\\tcorrelation\\tis\\tindeed\\tvery\\tstrong;\\tyou\\tcan\\tclearly\\tsee\\tthe\\tupward\\ntrend\\tand\\tthe\\tpoints\\tare\\tnot\\ttoo\\tdispersed.\\tSecond,\\tthe\\tprice\\tcap\\tthat\\twe\\tnoticed\\tearlier\\tis\\tclearly\\tvisible\\nas\\ta\\thorizontal\\tline\\tat\\t$500,000.\\tBut\\tthis\\tplot\\treveals\\tother\\tless\\tobvious\\tstraight\\tlines:\\ta\\thorizontal\\tline\\naround\\t$450,000,\\tanother\\taround\\t$350,000,\\tperhaps\\tone\\taround\\t$280,000,\\tand\\ta\\tfew\\tmore\\tbelow\\tthat.\\nYou\\tmay\\twant\\tto\\ttry\\tremoving\\tthe\\tcorresponding\\tdistricts\\tto\\tprevent\\tyour\\talgorithms\\tfrom\\tlearning\\tto\\nreproduce\\tthese\\t\\ndata\\tquirks.\\nFigure\\t2-16.\\t\\nMedian\\tincome\\tversus\\tmedian\\thouse\\tvalue', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 84}), Document(page_content='Experimenting\\twith\\tAttribute\\tCombinations\\nHopefully\\t\\nthe\\tprevious\\tsections\\tgave\\tyou\\tan\\tidea\\tof\\ta\\tfew\\tways\\tyou\\tcan\\texplore\\tthe\\tdata\\tand\\tgain\\ninsights.\\tYou\\tidentified\\ta\\tfew\\tdata\\tquirks\\tthat\\tyou\\tmay\\twant\\tto\\tclean\\tup\\tbefore\\tfeeding\\tthe\\tdata\\tto\\ta\\nMachine\\tLearning\\talgorithm,\\tand\\tyou\\tfound\\tinteresting\\tcorrelations\\tbetween\\tattributes,\\tin\\tparticular\\twith\\nthe\\ttarget\\tattribute.\\tYou\\talso\\tnoticed\\tthat\\tsome\\tattributes\\thave\\ta\\ttail-heavy\\tdistribution,\\tso\\tyou\\tmay\\twant\\nto\\ttransform\\tthem\\t(e.g.,\\tby\\tcomputing\\ttheir\\tlogarithm).\\tOf\\tcourse,\\tyour\\tmileage\\twill\\tvary\\tconsiderably\\nwith\\teach\\tproject,\\tbut\\tthe\\tgeneral\\tideas\\tare\\tsimilar.\\nOne\\tlast\\tthing\\tyou\\tmay\\twant\\tto\\tdo\\tbefore\\tactually\\tpreparing\\tthe\\tdata\\tfor\\tMachine\\tLearning\\talgorithms\\tis\\nto\\ttry\\tout\\tvarious\\tattribute\\tcombinations.\\tFor\\texample,\\tthe\\ttotal\\tnumber\\tof\\trooms\\tin\\ta\\tdistrict\\tis\\tnot\\tvery\\nuseful\\tif\\tyou\\tdon’t\\tknow\\thow\\tmany\\thouseholds\\tthere\\tare.\\tWhat\\tyou\\treally\\twant\\tis\\tthe\\tnumber\\tof\\trooms\\nper\\thousehold.\\tSimilarly,\\tthe\\ttotal\\tnumber\\tof\\tbedrooms\\tby\\titself\\tis\\tnot\\tvery\\tuseful:\\tyou\\tprobably\\twant\\tto\\ncompare\\tit\\tto\\tthe\\tnumber\\tof\\trooms.\\tAnd\\tthe\\tpopulation\\tper\\thousehold\\talso\\tseems\\tlike\\tan\\tinteresting\\nattribute\\tcombination\\tto\\tlook\\tat.\\tLet’s\\tcreate\\tthese\\tnew\\tattributes:\\nhousing\\n[\\n\"rooms_per_household\"\\n]\\n\\t\\n=\\n\\t\\nhousing\\n[\\n\"total_rooms\"\\n]\\n/\\nhousing\\n[\\n\"households\"\\n]\\nhousing\\n[\\n\"bedrooms_per_room\"\\n]\\n\\t\\n=\\n\\t\\nhousing\\n[\\n\"total_bedrooms\"\\n]\\n/\\nhousing\\n[\\n\"total_rooms\"\\n]\\nhousing\\n[\\n\"population_per_household\"\\n]\\n=\\nhousing\\n[\\n\"population\"\\n]\\n/\\nhousing\\n[\\n\"households\"\\n]\\nAnd\\tnow\\tlet’s\\tlook\\tat\\tthe\\tcorrelation\\tmatrix\\tagain:\\n>>>\\t\\ncorr_matrix\\n\\t\\n=\\n\\t\\nhousing\\n.\\ncorr\\n()\\n>>>\\t\\ncorr_matrix\\n[\\n\"median_house_value\"\\n]\\n.\\nsort_values\\n(\\nascending\\n=\\nFalse\\n)\\nmedian_house_value\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1.000000\\nmedian_income\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.687160\\nrooms_per_household\\t\\t\\t\\t\\t\\t\\t\\t\\t0.146285\\ntotal_rooms\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.135097\\nhousing_median_age\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.114110\\nhouseholds\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.064506\\ntotal_bedrooms\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.047689\\npopulation_per_household\\t\\t\\t-0.021985\\npopulation\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.026920\\nlongitude\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.047432\\nlatitude\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.142724\\nbedrooms_per_room\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.259984\\nName:\\tmedian_house_value,\\tdtype:\\tfloat64\\nHey,\\tnot\\tbad!\\tThe\\tnew\\t\\nbedrooms_per_room\\n\\tattribute\\tis\\tmuch\\tmore\\tcorrelated\\twith\\tthe\\tmedian\\thouse\\nvalue\\tthan\\tthe\\ttotal\\tnumber\\tof\\trooms\\tor\\tbedrooms.\\tApparently\\thouses\\twith\\ta\\tlower\\tbedroom/room\\tratio\\ntend\\tto\\tbe\\tmore\\texpensive.\\tThe\\tnumber\\tof\\trooms\\tper\\thousehold\\tis\\talso\\tmore\\tinformative\\tthan\\tthe\\ttotal\\nnumber\\tof\\trooms\\tin\\ta\\tdistrict\\t—\\tobviously\\tthe\\tlarger\\tthe\\thouses,\\tthe\\tmore\\texpensive\\tthey\\tare.\\nThis\\tround\\tof\\texploration\\tdoes\\tnot\\thave\\tto\\tbe\\tabsolutely\\tthorough;\\tthe\\tpoint\\tis\\tto\\tstart\\toff\\ton\\tthe\\tright\\tfoot\\nand\\tquickly\\tgain\\tinsights\\tthat\\twill\\thelp\\tyou\\tget\\ta\\tfirst\\treasonably\\tgood\\tprototype.\\tBut\\tthis\\tis\\tan\\titerative\\nprocess:\\tonce\\tyou\\tget\\ta\\tprototype\\tup\\tand\\trunning,\\tyou\\tcan\\tanalyze\\tits\\toutput\\tto\\tgain\\tmore\\tinsights\\tand\\ncome\\tback\\tto\\tthis\\texploration\\t\\nstep.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 85}), Document(page_content='Prepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\tAlgorithms\\nIt’s\\t\\ntime\\tto\\tprepare\\tthe\\tdata\\tfor\\tyour\\tMachine\\tLearning\\talgorithms.\\tInstead\\tof\\tjust\\tdoing\\tthis\\tmanually,\\tyou\\nshould\\twrite\\tfunctions\\tto\\tdo\\tthat,\\tfor\\tseveral\\tgood\\treasons:\\nThis\\twill\\tallow\\tyou\\tto\\treproduce\\tthese\\ttransformations\\teasily\\ton\\tany\\tdataset\\t(e.g.,\\tthe\\tnext\\ttime\\tyou\\nget\\ta\\tfresh\\tdataset).\\nYou\\twill\\tgradually\\tbuild\\ta\\tlibrary\\tof\\ttransformation\\tfunctions\\tthat\\tyou\\tcan\\treuse\\tin\\tfuture\\tprojects.\\nYou\\tcan\\tuse\\tthese\\tfunctions\\tin\\tyour\\tlive\\tsystem\\tto\\ttransform\\tthe\\tnew\\tdata\\tbefore\\tfeeding\\tit\\tto\\tyour\\nalgorithms.\\nThis\\twill\\tmake\\tit\\tpossible\\tfor\\tyou\\tto\\teasily\\ttry\\tvarious\\ttransformations\\tand\\tsee\\twhich\\tcombination\\nof\\ttransformations\\tworks\\tbest.\\nBut\\tfirst\\tlet’s\\trevert\\tto\\ta\\tclean\\t\\ntraining\\tset\\t(by\\tcopying\\t\\nstrat_train_set\\n\\tonce\\tagain),\\tand\\tlet’s\\tseparate\\nthe\\tpredictors\\tand\\tthe\\tlabels\\tsince\\twe\\tdon’t\\tnecessarily\\twant\\tto\\tapply\\tthe\\tsame\\ttransformations\\tto\\tthe\\npredictors\\tand\\tthe\\ttarget\\tvalues\\t(note\\tthat\\t\\ndrop()\\n\\t\\ncreates\\ta\\tcopy\\tof\\tthe\\tdata\\tand\\tdoes\\tnot\\taffect\\nstrat_train_set\\n):\\nhousing\\n\\t\\n=\\n\\t\\nstrat_train_set\\n.\\ndrop\\n(\\n\"median_house_value\"\\n,\\n\\t\\naxis\\n=\\n1\\n)\\nhousing_labels\\n\\t\\n=\\n\\t\\nstrat_train_set\\n[\\n\"median_house_value\"\\n]\\n.\\ncopy\\n()', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 86}), Document(page_content='Data\\tCleaning\\nMost\\t\\nMachine\\tLearning\\talgorithms\\tcannot\\twork\\twith\\tmissing\\tfeatures,\\tso\\tlet’s\\tcreate\\ta\\tfew\\tfunctions\\tto\\ntake\\tcare\\tof\\tthem.\\tYou\\tnoticed\\tearlier\\tthat\\tthe\\t\\ntotal_bedrooms\\n\\tattribute\\thas\\tsome\\tmissing\\tvalues,\\tso\\nlet’s\\tfix\\tthis.\\tYou\\thave\\tthree\\toptions:\\nGet\\trid\\tof\\tthe\\tcorresponding\\tdistricts.\\nGet\\trid\\tof\\tthe\\twhole\\tattribute.\\nSet\\tthe\\tvalues\\tto\\tsome\\tvalue\\t(zero,\\tthe\\tmean,\\tthe\\tmedian,\\tetc.).\\nYou\\tcan\\taccomplish\\tthese\\teasily\\tusing\\t\\nDataFrame’s\\t\\ndropna()\\n,\\t\\ndrop()\\n,\\t\\nand\\t\\nfillna()\\n\\tmethods:\\nhousing\\n.\\ndropna\\n(\\nsubset\\n=\\n[\\n\"total_bedrooms\"\\n])\\n\\t\\t\\t\\t\\n#\\toption\\t1\\nhousing\\n.\\ndrop\\n(\\n\"total_bedrooms\"\\n,\\n\\t\\naxis\\n=\\n1\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\n#\\toption\\t2\\nmedian\\n\\t\\n=\\n\\t\\nhousing\\n[\\n\"total_bedrooms\"\\n]\\n.\\nmedian\\n()\\n\\t\\t\\n#\\toption\\t3\\nhousing\\n[\\n\"total_bedrooms\"\\n]\\n.\\nfillna\\n(\\nmedian\\n,\\n\\t\\ninplace\\n=\\nTrue\\n)\\nIf\\tyou\\tchoose\\toption\\t3,\\tyou\\tshould\\tcompute\\tthe\\tmedian\\tvalue\\ton\\tthe\\ttraining\\tset,\\tand\\tuse\\tit\\tto\\tfill\\tthe\\nmissing\\tvalues\\tin\\tthe\\ttraining\\tset,\\tbut\\talso\\tdon’t\\tforget\\tto\\tsave\\tthe\\tmedian\\tvalue\\tthat\\tyou\\thave\\tcomputed.\\nYou\\twill\\tneed\\tit\\tlater\\tto\\treplace\\tmissing\\tvalues\\tin\\tthe\\ttest\\tset\\twhen\\tyou\\twant\\tto\\tevaluate\\tyour\\tsystem,\\tand\\nalso\\tonce\\tthe\\tsystem\\tgoes\\tlive\\tto\\treplace\\tmissing\\tvalues\\tin\\tnew\\tdata.\\nScikit-Learn\\t\\nprovides\\ta\\thandy\\tclass\\tto\\ttake\\tcare\\tof\\tmissing\\tvalues:\\t\\nImputer\\n.\\tHere\\tis\\thow\\tto\\tuse\\tit.\\tFirst,\\nyou\\tneed\\tto\\tcreate\\tan\\t\\nImputer\\n\\tinstance,\\tspecifying\\tthat\\tyou\\twant\\tto\\treplace\\teach\\tattribute’s\\tmissing\\nvalues\\twith\\tthe\\tmedian\\tof\\tthat\\tattribute:\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nImputer\\nimputer\\n\\t\\n=\\n\\t\\nImputer\\n(\\nstrategy\\n=\\n\"median\"\\n)\\nSince\\tthe\\tmedian\\tcan\\tonly\\tbe\\tcomputed\\ton\\tnumerical\\tattributes,\\twe\\tneed\\tto\\tcreate\\ta\\tcopy\\tof\\tthe\\tdata\\nwithout\\tthe\\ttext\\tattribute\\t\\nocean_proximity\\n:\\nhousing_num\\n\\t\\n=\\n\\t\\nhousing\\n.\\ndrop\\n(\\n\"ocean_proximity\"\\n,\\n\\t\\naxis\\n=\\n1\\n)\\nNow\\tyou\\tcan\\tfit\\tthe\\t\\nimputer\\n\\tinstance\\tto\\tthe\\ttraining\\tdata\\tusing\\tthe\\t\\nfit()\\n\\t\\nmethod:\\nimputer\\n.\\nfit\\n(\\nhousing_num\\n)\\nThe\\t\\nimputer\\n\\thas\\tsimply\\tcomputed\\tthe\\tmedian\\tof\\teach\\tattribute\\tand\\tstored\\tthe\\tresult\\tin\\tits\\t\\nstatistics_\\ninstance\\tvariable.\\tOnly\\tthe\\t\\ntotal_bedrooms\\n\\tattribute\\thad\\tmissing\\tvalues,\\tbut\\twe\\tcannot\\tbe\\tsure\\tthat\\nthere\\twon’t\\tbe\\tany\\tmissing\\tvalues\\tin\\tnew\\tdata\\tafter\\tthe\\tsystem\\tgoes\\tlive,\\tso\\tit\\tis\\tsafer\\tto\\tapply\\tthe\\nimputer\\n\\tto\\tall\\tthe\\tnumerical\\tattributes:\\n>>>\\t\\nimputer\\n.\\nstatistics_\\narray([\\t-118.51\\t,\\t34.26\\t,\\t29.\\t,\\t2119.5\\t,\\t433.\\t,\\t1164.\\t,\\t408.\\t,\\t3.5409])\\n>>>\\t\\nhousing_num\\n.\\nmedian\\n()\\n.\\nvalues', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 87}), Document(page_content='array([\\t-118.51\\t,\\t34.26\\t,\\t29.\\t,\\t2119.5\\t,\\t433.\\t,\\t1164.\\t,\\t408.\\t,\\t3.5409])\\nNow\\tyou\\tcan\\tuse\\tthis\\t“trained”\\t\\nimputer\\n\\tto\\ttransform\\tthe\\ttraining\\tset\\tby\\treplacing\\tmissing\\tvalues\\tby\\tthe\\nlearned\\tmedians:\\nX\\n\\t\\n=\\n\\t\\nimputer\\n.\\ntransform\\n(\\nhousing_num\\n)\\nThe\\tresult\\tis\\ta\\tplain\\tNumpy\\tarray\\tcontaining\\tthe\\ttransformed\\tfeatures.\\tIf\\tyou\\twant\\tto\\tput\\tit\\tback\\tinto\\ta\\nPandas\\tDataFrame,\\tit’s\\tsimple:\\nhousing_tr\\n\\t\\n=\\n\\t\\npd\\n.\\nDataFrame\\n(\\nX\\n,\\n\\t\\ncolumns\\n=\\nhousing_num\\n.\\ncolumns\\n)\\nSCIKIT-LEARN\\tDESIGN\\nScikit-Learn’s\\t\\nAPI\\tis\\tremarkably\\twell\\tdesigned.\\tThe\\t\\nmain\\tdesign\\tprinciples\\n\\tare:\\n15\\nConsistency\\n.\\tAll\\tobjects\\tshare\\ta\\tconsistent\\tand\\tsimple\\tinterface:\\nEstimators\\n.\\tAny\\t\\nobject\\tthat\\tcan\\testimate\\tsome\\tparameters\\tbased\\ton\\ta\\tdataset\\tis\\tcalled\\tan\\t\\nestimator\\n\\t(e.g.,\\tan\\t\\nimputer\\n\\tis\\nan\\testimator).\\tThe\\testimation\\titself\\tis\\tperformed\\tby\\tthe\\t\\nfit()\\n\\tmethod,\\tand\\tit\\ttakes\\tonly\\ta\\tdataset\\tas\\ta\\tparameter\\t(or\\ttwo\\nfor\\tsupervised\\tlearning\\talgorithms;\\tthe\\tsecond\\tdataset\\tcontains\\tthe\\tlabels).\\tAny\\tother\\tparameter\\tneeded\\tto\\tguide\\tthe\\nestimation\\tprocess\\tis\\tconsidered\\ta\\thyperparameter\\t(such\\tas\\tan\\t\\nimputer\\n’s\\t\\nstrategy\\n),\\tand\\tit\\tmust\\tbe\\tset\\tas\\tan\\tinstance\\nvariable\\t(generally\\tvia\\ta\\tconstructor\\tparameter).\\nTransformers\\n.\\tSome\\t\\nestimators\\t(such\\tas\\tan\\t\\nimputer\\n)\\tcan\\talso\\ttransform\\ta\\tdataset;\\tthese\\tare\\tcalled\\t\\ntransformers\\n.\\tOnce\\nagain,\\tthe\\tAPI\\tis\\tquite\\tsimple:\\tthe\\ttransformation\\tis\\tperformed\\tby\\tthe\\t\\ntransform()\\n\\t\\nmethod\\twith\\tthe\\tdataset\\tto\\ttransform\\tas\\na\\tparameter.\\tIt\\treturns\\tthe\\ttransformed\\tdataset.\\tThis\\ttransformation\\tgenerally\\trelies\\ton\\tthe\\tlearned\\tparameters,\\tas\\tis\\tthe\\ncase\\tfor\\tan\\t\\nimputer\\n.\\tAll\\ttransformers\\talso\\thave\\ta\\tconvenience\\tmethod\\tcalled\\t\\nfit_transform()\\n\\t\\nthat\\tis\\tequivalent\\tto\\tcalling\\nfit()\\n\\tand\\tthen\\t\\ntransform()\\n\\t(but\\tsometimes\\t\\nfit_transform()\\n\\tis\\toptimized\\tand\\truns\\tmuch\\tfaster).\\nPredictors\\n.\\tFinally,\\t\\nsome\\testimators\\tare\\tcapable\\tof\\tmaking\\tpredictions\\tgiven\\ta\\tdataset;\\tthey\\tare\\tcalled\\t\\npredictors\\n.\\tFor\\nexample,\\tthe\\t\\nLinearRegression\\n\\tmodel\\t\\nin\\tthe\\tprevious\\tchapter\\twas\\ta\\tpredictor:\\tit\\tpredicted\\tlife\\tsatisfaction\\tgiven\\ta\\ncountry’s\\tGDP\\tper\\tcapita.\\tA\\tpredictor\\thas\\ta\\t\\npredict()\\n\\t\\nmethod\\tthat\\ttakes\\ta\\tdataset\\tof\\tnew\\tinstances\\tand\\treturns\\ta\\tdataset\\nof\\tcorresponding\\tpredictions.\\tIt\\talso\\thas\\ta\\t\\nscore()\\n\\t\\nmethod\\tthat\\tmeasures\\tthe\\tquality\\tof\\tthe\\tpredictions\\tgiven\\ta\\ttest\\tset\\t(and\\nthe\\tcorresponding\\tlabels\\tin\\tthe\\tcase\\tof\\tsupervised\\tlearning\\talgorithms).\\n16\\nInspection\\n.\\tAll\\tthe\\testimator’s\\thyperparameters\\tare\\taccessible\\tdirectly\\tvia\\tpublic\\tinstance\\tvariables\\t(e.g.,\\t\\nimputer.strategy\\n),\\nand\\tall\\tthe\\testimator’s\\tlearned\\tparameters\\tare\\talso\\taccessible\\tvia\\tpublic\\tinstance\\tvariables\\twith\\tan\\tunderscore\\tsuffix\\t(e.g.,\\nimputer.statistics_\\n).\\nNonproliferation\\tof\\tclasses\\n.\\tDatasets\\tare\\trepresented\\tas\\tNumPy\\tarrays\\tor\\tSciPy\\tsparse\\tmatrices,\\tinstead\\tof\\thomemade\\nclasses.\\tHyperparameters\\tare\\tjust\\tregular\\tPython\\tstrings\\tor\\tnumbers.\\nComposition\\n.\\tExisting\\tbuilding\\tblocks\\tare\\treused\\tas\\tmuch\\tas\\tpossible.\\tFor\\texample,\\tit\\tis\\teasy\\tto\\tcreate\\ta\\t\\nPipeline\\n\\testimator\\nfrom\\tan\\tarbitrary\\tsequence\\tof\\ttransformers\\tfollowed\\tby\\ta\\tfinal\\testimator,\\tas\\twe\\twill\\tsee.\\nSensible\\tdefaults\\n.\\tScikit-Learn\\tprovides\\treasonable\\tdefault\\tvalues\\tfor\\tmost\\tparameters,\\tmaking\\tit\\teasy\\tto\\tcreate\\ta\\tbaseline\\nworking\\tsystem\\tquickly.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 88}), Document(page_content='Handling\\tText\\tand\\tCategorical\\tAttributes\\nEarlier\\t\\nwe\\tleft\\tout\\tthe\\tcategorical\\tattribute\\t\\nocean_proximity\\n\\tbecause\\tit\\tis\\ta\\ttext\\tattribute\\tso\\twe\\tcannot\\ncompute\\tits\\tmedian.\\tMost\\tMachine\\tLearning\\talgorithms\\tprefer\\tto\\twork\\twith\\tnumbers\\tanyway,\\tso\\tlet’s\\nconvert\\tthese\\ttext\\tlabels\\tto\\tnumbers.\\nScikit-Learn\\tprovides\\ta\\ttransformer\\tfor\\tthis\\ttask\\t\\ncalled\\t\\nLabelEncoder\\n:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nLabelEncoder\\n>>>\\t\\nencoder\\n\\t\\n=\\n\\t\\nLabelEncoder\\n()\\n>>>\\t\\nhousing_cat\\n\\t\\n=\\n\\t\\nhousing\\n[\\n\"ocean_proximity\"\\n]\\n>>>\\t\\nhousing_cat_encoded\\n\\t\\n=\\n\\t\\nencoder\\n.\\nfit_transform\\n(\\nhousing_cat\\n)\\n>>>\\t\\nhousing_cat_encoded\\narray([0,\\t0,\\t4,\\t...,\\t1,\\t0,\\t3])\\nThis\\tis\\tbetter:\\tnow\\twe\\tcan\\tuse\\tthis\\tnumerical\\tdata\\tin\\tany\\tML\\talgorithm.\\tYou\\tcan\\tlook\\tat\\tthe\\tmapping\\tthat\\nthis\\tencoder\\thas\\tlearned\\tusing\\tthe\\t\\nclasses_\\n\\tattribute\\t(“<1H\\tOCEAN”\\tis\\tmapped\\tto\\t0,\\t“INLAND”\\tis\\nmapped\\tto\\t1,\\tetc.):\\n>>>\\t\\nprint\\n(\\nencoder\\n.\\nclasses_\\n)\\n[\\'<1H\\tOCEAN\\'\\t\\'INLAND\\'\\t\\'ISLAND\\'\\t\\'NEAR\\tBAY\\'\\t\\'NEAR\\tOCEAN\\']\\nOne\\tissue\\twith\\tthis\\trepresentation\\tis\\tthat\\tML\\talgorithms\\twill\\tassume\\tthat\\ttwo\\tnearby\\tvalues\\tare\\tmore\\nsimilar\\tthan\\ttwo\\tdistant\\tvalues.\\tObviously\\tthis\\tis\\tnot\\tthe\\tcase\\t(for\\texample,\\tcategories\\t0\\tand\\t4\\tare\\tmore\\nsimilar\\tthan\\tcategories\\t0\\tand\\t1).\\tTo\\tfix\\tthis\\tissue,\\ta\\tcommon\\tsolution\\tis\\tto\\tcreate\\tone\\tbinary\\tattribute\\tper\\ncategory:\\tone\\tattribute\\tequal\\tto\\t1\\twhen\\tthe\\tcategory\\tis\\t“<1H\\tOCEAN”\\t(and\\t0\\totherwise),\\tanother\\nattribute\\tequal\\tto\\t1\\twhen\\tthe\\tcategory\\tis\\t“INLAND”\\t(and\\t0\\totherwise),\\tand\\tso\\ton.\\tThis\\tis\\tcalled\\t\\none-hot\\nencoding\\n,\\t\\nbecause\\tonly\\tone\\tattribute\\twill\\tbe\\tequal\\tto\\t1\\t(hot),\\twhile\\tthe\\tothers\\twill\\tbe\\t0\\t(cold).\\nScikit-Learn\\tprovides\\ta\\t\\nOneHotEncoder\\n\\t\\nencoder\\tto\\tconvert\\tinteger\\tcategorical\\tvalues\\tinto\\tone-hot\\nvectors.\\tLet’s\\tencode\\tthe\\tcategories\\tas\\tone-hot\\tvectors.\\tNote\\tthat\\t\\nfit_transform()\\n\\texpects\\ta\\t2D\\tarray,\\nbut\\t\\nhousing_cat_encoded\\n\\tis\\ta\\t1D\\tarray,\\tso\\twe\\tneed\\tto\\treshape\\tit:\\n17\\n>>>\\t\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nOneHotEncoder\\n>>>\\t\\nencoder\\n\\t\\n=\\n\\t\\nOneHotEncoder\\n()\\n>>>\\t\\nhousing_cat_1hot\\n\\t\\n=\\n\\t\\nencoder\\n.\\nfit_transform\\n(\\nhousing_cat_encoded\\n.\\nreshape\\n(\\n-\\n1\\n,\\n1\\n))\\n>>>\\t\\nhousing_cat_1hot\\n<16512x5\\tsparse\\tmatrix\\tof\\ttype\\t\\'<class\\t\\'numpy.float64\\'>\\'\\n\\t with\\t16512\\tstored\\telements\\tin\\tCompressed\\tSparse\\tRow\\tformat>\\nNotice\\tthat\\tthe\\toutput\\tis\\ta\\tSciPy\\t\\nsparse\\tmatrix\\n,\\t\\ninstead\\tof\\ta\\t\\nNumPy\\tarray.\\tThis\\tis\\tvery\\tuseful\\twhen\\tyou\\nhave\\tcategorical\\tattributes\\twith\\tthousands\\tof\\tcategories.\\tAfter\\tone-hot\\tencoding\\twe\\tget\\ta\\tmatrix\\twith\\nthousands\\tof\\tcolumns,\\tand\\tthe\\tmatrix\\tis\\tfull\\tof\\tzeros\\texcept\\tfor\\tone\\t1\\tper\\trow.\\tUsing\\tup\\ttons\\tof\\tmemory\\nmostly\\tto\\tstore\\tzeros\\twould\\tbe\\tvery\\twasteful,\\tso\\tinstead\\ta\\tsparse\\tmatrix\\tonly\\tstores\\tthe\\tlocation\\tof\\tthe\\nnonzero\\telements.\\tYou\\tcan\\tuse\\tit\\tmostly\\tlike\\ta\\tnormal\\t2D\\tarray,\\n18\\n\\tbut\\tif\\tyou\\treally\\twant\\tto\\tconvert\\tit\\tto\\ta\\n(dense)\\tNumPy\\tarray,\\tjust\\tcall\\tthe\\t\\ntoarray()\\n\\t\\nmethod:\\n>>>\\t\\nhousing_cat_1hot\\n.\\ntoarray\\n()\\narray([[\\t1.,\\t\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t0.],\\n\\t\\t\\t\\t\\t\\t\\t[\\t1.,\\t\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t0.],\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t1.],', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 89}), Document(page_content='\\t\\t\\t\\t\\t\\t\\t...,\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.,\\t\\t1.,\\t\\t0.,\\t\\t0.,\\t\\t0.],\\n\\t\\t\\t\\t\\t\\t\\t[\\t1.,\\t\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t0.],\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t1.,\\t\\t0.]])\\nWe\\tcan\\tapply\\tboth\\ttransformations\\t(from\\ttext\\tcategories\\tto\\tinteger\\tcategories,\\tthen\\tfrom\\tinteger\\tcategories\\nto\\tone-hot\\tvectors)\\tin\\tone\\tshot\\tusing\\tthe\\t\\nLabelBinarizer\\n\\t\\nclass:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nLabelBinarizer\\n>>>\\t\\nencoder\\n\\t\\n=\\n\\t\\nLabelBinarizer\\n()\\n>>>\\t\\nhousing_cat_1hot\\n\\t\\n=\\n\\t\\nencoder\\n.\\nfit_transform\\n(\\nhousing_cat\\n)\\n>>>\\t\\nhousing_cat_1hot\\narray([[1,\\t0,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[1,\\t0,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t0,\\t1],\\n\\t\\t\\t\\t\\t\\t\\t...,\\n\\t\\t\\t\\t\\t\\t\\t[0,\\t1,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[1,\\t0,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t1,\\t0]])\\nNote\\tthat\\tthis\\treturns\\ta\\tdense\\tNumPy\\tarray\\tby\\tdefault.\\tYou\\tcan\\tget\\ta\\tsparse\\tmatrix\\tinstead\\tby\\tpassing\\nsparse_output=True\\n\\tto\\tthe\\t\\nLabelBinarizer\\n\\t\\nconstructor.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 90}), Document(page_content='Custom\\tTransformers\\nAlthough\\t\\nScikit-Learn\\tprovides\\tmany\\tuseful\\ttransformers,\\tyou\\twill\\tneed\\tto\\twrite\\tyour\\town\\tfor\\ttasks\\tsuch\\nas\\tcustom\\tcleanup\\toperations\\tor\\tcombining\\tspecific\\tattributes.\\tYou\\twill\\twant\\tyour\\ttransformer\\tto\\twork\\nseamlessly\\twith\\tScikit-Learn\\tfunctionalities\\t(such\\tas\\tpipelines),\\tand\\tsince\\tScikit-Learn\\trelies\\ton\\tduck\\ntyping\\t(not\\tinheritance),\\tall\\tyou\\tneed\\tis\\tto\\tcreate\\ta\\tclass\\tand\\timplement\\tthree\\tmethods:\\t\\nfit()\\n\\t(returning\\nself\\n),\\t\\ntransform()\\n,\\tand\\t\\nfit_transform()\\n.\\tYou\\tcan\\tget\\tthe\\tlast\\tone\\tfor\\tfree\\tby\\tsimply\\tadding\\nTransformerMixin\\n\\t\\nas\\ta\\tbase\\tclass.\\tAlso,\\tif\\tyou\\tadd\\t\\nBaseEstimator\\n\\tas\\ta\\t\\nbase\\tclass\\t(and\\tavoid\\t\\n*args\\nand\\t\\n**kargs\\n\\tin\\tyour\\tconstructor)\\tyou\\twill\\tget\\ttwo\\textra\\tmethods\\t(\\nget_params()\\n\\tand\\t\\nset_params()\\n)\\nthat\\twill\\tbe\\tuseful\\tfor\\tautomatic\\thyperparameter\\ttuning.\\tFor\\texample,\\there\\tis\\ta\\tsmall\\ttransformer\\tclass\\nthat\\tadds\\tthe\\tcombined\\tattributes\\twe\\tdiscussed\\tearlier:\\nfrom\\n\\t\\nsklearn.base\\n\\t\\nimport\\n\\t\\nBaseEstimator\\n,\\n\\t\\nTransformerMixin\\nrooms_ix\\n,\\n\\t\\nbedrooms_ix\\n,\\n\\t\\npopulation_ix\\n,\\n\\t\\nhousehold_ix\\n\\t\\n=\\n\\t\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n5\\n,\\n\\t\\n6\\nclass\\n\\t\\nCombinedAttributesAdder\\n(\\nBaseEstimator\\n,\\n\\t\\nTransformerMixin\\n):\\n\\t\\t\\t\\t\\ndef\\n\\t\\n__init__\\n(\\nself\\n,\\n\\t\\nadd_bedrooms_per_room\\n\\t\\n=\\n\\t\\nTrue\\n):\\n\\t\\n#\\tno\\t*args\\tor\\t**kargs\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nself\\n.\\nadd_bedrooms_per_room\\n\\t\\n=\\n\\t\\nadd_bedrooms_per_room\\n\\t\\t\\t\\t\\ndef\\n\\t\\nfit\\n(\\nself\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nself\\n\\t\\t\\n#\\tnothing\\telse\\tto\\tdo\\n\\t\\t\\t\\t\\ndef\\n\\t\\ntransform\\n(\\nself\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nrooms_per_household\\n\\t\\n=\\n\\t\\nX\\n[:,\\n\\t\\nrooms_ix\\n]\\n\\t\\n/\\n\\t\\nX\\n[:,\\n\\t\\nhousehold_ix\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npopulation_per_household\\n\\t\\n=\\n\\t\\nX\\n[:,\\n\\t\\npopulation_ix\\n]\\n\\t\\n/\\n\\t\\nX\\n[:,\\n\\t\\nhousehold_ix\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nself\\n.\\nadd_bedrooms_per_room\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbedrooms_per_room\\n\\t\\n=\\n\\t\\nX\\n[:,\\n\\t\\nbedrooms_ix\\n]\\n\\t\\n/\\n\\t\\nX\\n[:,\\n\\t\\nrooms_ix\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nnp\\n.\\nc_\\n[\\nX\\n,\\n\\t\\nrooms_per_household\\n,\\n\\t\\npopulation_per_household\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbedrooms_per_room\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nnp\\n.\\nc_\\n[\\nX\\n,\\n\\t\\nrooms_per_household\\n,\\n\\t\\npopulation_per_household\\n]\\nattr_adder\\n\\t\\n=\\n\\t\\nCombinedAttributesAdder\\n(\\nadd_bedrooms_per_room\\n=\\nFalse\\n)\\nhousing_extra_attribs\\n\\t\\n=\\n\\t\\nattr_adder\\n.\\ntransform\\n(\\nhousing\\n.\\nvalues\\n)\\nIn\\tthis\\texample\\tthe\\ttransformer\\thas\\tone\\thyperparameter,\\t\\nadd_bedrooms_per_room\\n,\\tset\\tto\\t\\nTrue\\n\\tby\\tdefault\\n(it\\tis\\toften\\thelpful\\tto\\tprovide\\tsensible\\tdefaults).\\tThis\\t\\nhyperparameter\\twill\\tallow\\tyou\\tto\\teasily\\tfind\\tout\\nwhether\\tadding\\tthis\\tattribute\\thelps\\tthe\\tMachine\\tLearning\\talgorithms\\tor\\tnot.\\tMore\\tgenerally,\\tyou\\tcan\\tadd\\ta\\nhyperparameter\\tto\\tgate\\tany\\tdata\\tpreparation\\tstep\\tthat\\tyou\\tare\\tnot\\t100%\\tsure\\tabout.\\tThe\\tmore\\tyou\\nautomate\\tthese\\tdata\\tpreparation\\tsteps,\\tthe\\tmore\\tcombinations\\tyou\\tcan\\tautomatically\\ttry\\tout,\\tmaking\\tit\\nmuch\\tmore\\tlikely\\tthat\\tyou\\twill\\tfind\\ta\\tgreat\\t\\ncombination\\t(and\\tsaving\\tyou\\ta\\tlot\\tof\\ttime).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 91}), Document(page_content='Feature\\tScaling\\nOne\\t\\nof\\tthe\\tmost\\timportant\\ttransformations\\tyou\\tneed\\tto\\tapply\\tto\\tyour\\tdata\\tis\\t\\nfeature\\tscaling\\n.\\tWith\\tfew\\nexceptions,\\tMachine\\tLearning\\talgorithms\\tdon’t\\tperform\\twell\\twhen\\tthe\\tinput\\tnumerical\\tattributes\\thave\\nvery\\tdifferent\\tscales.\\tThis\\tis\\tthe\\tcase\\tfor\\tthe\\thousing\\tdata:\\tthe\\ttotal\\tnumber\\tof\\trooms\\tranges\\tfrom\\tabout\\t6\\nto\\t39,320,\\twhile\\tthe\\tmedian\\tincomes\\tonly\\trange\\tfrom\\t0\\tto\\t15.\\tNote\\tthat\\tscaling\\tthe\\ttarget\\tvalues\\tis\\ngenerally\\tnot\\trequired.\\nThere\\tare\\ttwo\\tcommon\\tways\\tto\\tget\\tall\\tattributes\\tto\\thave\\tthe\\tsame\\tscale:\\t\\nmin-max\\tscaling\\n\\t\\nand\\nstandardization\\n.\\nMin-max\\tscaling\\t\\n(many\\tpeople\\tcall\\tthis\\t\\nnormalization\\n)\\tis\\tquite\\tsimple:\\tvalues\\tare\\tshifted\\tand\\trescaled\\nso\\tthat\\tthey\\tend\\tup\\tranging\\tfrom\\t0\\tto\\t1.\\tWe\\tdo\\tthis\\tby\\tsubtracting\\tthe\\tmin\\tvalue\\tand\\tdividing\\tby\\tthe\\tmax\\nminus\\tthe\\tmin.\\t\\nScikit-Learn\\tprovides\\ta\\ttransformer\\tcalled\\t\\nMinMaxScaler\\n\\tfor\\tthis.\\tIt\\thas\\ta\\nfeature_range\\n\\thyperparameter\\tthat\\tlets\\tyou\\tchange\\tthe\\trange\\tif\\tyou\\tdon’t\\twant\\t0–1\\tfor\\tsome\\treason.\\nStandardization\\tis\\tquite\\tdifferent:\\tfirst\\tit\\tsubtracts\\tthe\\tmean\\tvalue\\t(so\\tstandardized\\tvalues\\talways\\thave\\ta\\nzero\\tmean),\\tand\\tthen\\tit\\tdivides\\tby\\tthe\\tvariance\\tso\\tthat\\tthe\\tresulting\\tdistribution\\thas\\tunit\\tvariance.\\tUnlike\\nmin-max\\tscaling,\\tstandardization\\tdoes\\tnot\\tbound\\tvalues\\tto\\ta\\tspecific\\trange,\\twhich\\tmay\\tbe\\ta\\tproblem\\tfor\\nsome\\talgorithms\\t(e.g.,\\tneural\\tnetworks\\toften\\texpect\\tan\\tinput\\tvalue\\tranging\\tfrom\\t0\\tto\\t1).\\tHowever,\\nstandardization\\tis\\tmuch\\tless\\taffected\\tby\\toutliers.\\tFor\\texample,\\tsuppose\\ta\\tdistrict\\thad\\ta\\tmedian\\tincome\\nequal\\tto\\t100\\t(by\\tmistake).\\tMin-max\\tscaling\\twould\\tthen\\tcrush\\tall\\tthe\\tother\\tvalues\\tfrom\\t0–15\\tdown\\tto\\t0–\\n0.15,\\twhereas\\tstandardization\\twould\\tnot\\tbe\\tmuch\\taffected.\\tScikit-Learn\\tprovides\\ta\\ttransformer\\tcalled\\nStandardScaler\\n\\t\\nfor\\tstandardization.\\nWARNING\\nAs\\twith\\tall\\tthe\\ttransformations,\\tit\\tis\\timportant\\tto\\tfit\\tthe\\tscalers\\tto\\tthe\\ttraining\\tdata\\tonly,\\tnot\\tto\\tthe\\tfull\\tdataset\\t(including\\tthe\\ttest\\nset).\\tOnly\\tthen\\tcan\\tyou\\tuse\\tthem\\tto\\ttransform\\tthe\\ttraining\\tset\\tand\\tthe\\ttest\\tset\\t(and\\tnew\\tdata).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 92}), Document(page_content='Transformation\\tPipelines\\nAs\\t\\nyou\\tcan\\tsee,\\tthere\\tare\\tmany\\tdata\\ttransformation\\tsteps\\tthat\\tneed\\tto\\tbe\\texecuted\\tin\\tthe\\tright\\torder.\\nFortunately,\\tScikit-Learn\\tprovides\\tthe\\t\\nPipeline\\n\\tclass\\tto\\thelp\\twith\\tsuch\\tsequences\\tof\\ttransformations.\\nHere\\tis\\ta\\tsmall\\tpipeline\\tfor\\tthe\\t\\nnumerical\\tattributes:\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nPipeline\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nStandardScaler\\nnum_pipeline\\n\\t\\n=\\n\\t\\nPipeline\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'imputer\\'\\n,\\n\\t\\nImputer\\n(\\nstrategy\\n=\\n\"median\"\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'attribs_adder\\'\\n,\\n\\t\\nCombinedAttributesAdder\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'std_scaler\\'\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\n])\\nhousing_num_tr\\n\\t\\n=\\n\\t\\nnum_pipeline\\n.\\nfit_transform\\n(\\nhousing_num\\n)\\nThe\\t\\nPipeline\\n\\tconstructor\\ttakes\\ta\\tlist\\tof\\tname/estimator\\tpairs\\tdefining\\ta\\tsequence\\tof\\tsteps.\\tAll\\tbut\\tthe\\nlast\\testimator\\tmust\\tbe\\ttransformers\\t(i.e.,\\tthey\\tmust\\thave\\ta\\t\\nfit_transform()\\n\\tmethod).\\tThe\\tnames\\tcan\\tbe\\nanything\\tyou\\tlike\\t(as\\tlong\\tas\\tthey\\tdon’t\\tcontain\\tdouble\\tunderscores\\t“\\n__\\n”).\\nWhen\\tyou\\tcall\\tthe\\t\\npipeline’s\\t\\nfit()\\n\\tmethod,\\tit\\tcalls\\t\\nfit_transform()\\n\\tsequentially\\ton\\tall\\ttransformers,\\npassing\\tthe\\toutput\\tof\\teach\\tcall\\tas\\tthe\\tparameter\\tto\\tthe\\tnext\\tcall,\\tuntil\\tit\\treaches\\tthe\\tfinal\\testimator,\\tfor\\nwhich\\tit\\tjust\\tcalls\\tthe\\t\\nfit()\\n\\tmethod.\\nThe\\tpipeline\\texposes\\tthe\\tsame\\tmethods\\tas\\tthe\\tfinal\\testimator.\\tIn\\tthis\\texample,\\tthe\\tlast\\testimator\\tis\\t\\na\\nStandardScaler\\n,\\twhich\\tis\\ta\\ttransformer,\\tso\\tthe\\tpipeline\\thas\\t\\na\\t\\ntransform()\\n\\tmethod\\tthat\\tapplies\\tall\\tthe\\ntransforms\\tto\\tthe\\tdata\\tin\\tsequence\\t(it\\talso\\thas\\ta\\t\\nfit_transform\\n\\tmethod\\tthat\\twe\\tcould\\thave\\tused\\tinstead\\nof\\tcalling\\t\\nfit()\\n\\tand\\tthen\\t\\ntransform()\\n).\\nNow\\tit\\twould\\tbe\\tnice\\tif\\twe\\tcould\\tfeed\\ta\\tPandas\\tDataFrame\\tdirectly\\tinto\\tour\\tpipeline,\\tinstead\\tof\\thaving\\nto\\tfirst\\tmanually\\textract\\tthe\\tnumerical\\tcolumns\\tinto\\ta\\tNumPy\\tarray.\\tThere\\tis\\tnothing\\tin\\tScikit-Learn\\tto\\nhandle\\tPandas\\tDataFrames,\\n19\\n\\tbut\\twe\\tcan\\twrite\\ta\\tcustom\\ttransformer\\t\\nfor\\tthis\\ttask:\\nfrom\\n\\t\\nsklearn.base\\n\\t\\nimport\\n\\t\\nBaseEstimator\\n,\\n\\t\\nTransformerMixin\\nclass\\n\\t\\nDataFrameSelector\\n(\\nBaseEstimator\\n,\\n\\t\\nTransformerMixin\\n):\\n\\t\\t\\t\\t\\ndef\\n\\t\\n__init__\\n(\\nself\\n,\\n\\t\\nattribute_names\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nself\\n.\\nattribute_names\\n\\t\\n=\\n\\t\\nattribute_names\\n\\t\\t\\t\\t\\ndef\\n\\t\\nfit\\n(\\nself\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nself\\n\\t\\t\\t\\t\\ndef\\n\\t\\ntransform\\n(\\nself\\n,\\n\\t\\nX\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nX\\n[\\nself\\n.\\nattribute_names\\n]\\n.\\nvalues\\nOur\\t\\nDataFrameSelector\\n\\twill\\ttransform\\tthe\\tdata\\tby\\tselecting\\tthe\\tdesired\\tattributes,\\tdropping\\tthe\\trest,\\nand\\tconverting\\tthe\\tresulting\\tDataFrame\\tto\\ta\\tNumPy\\tarray.\\tWith\\tthis,\\tyou\\tcan\\teasily\\twrite\\ta\\tpipeline\\tthat\\nwill\\ttake\\ta\\tPandas\\tDataFrame\\tand\\thandle\\tonly\\tthe\\tnumerical\\tvalues:\\tthe\\tpipeline\\twould\\tjust\\tstart\\twith\\ta\\nDataFrameSelector\\n\\tto\\tpick\\tonly\\tthe\\tnumerical\\tattributes,\\tfollowed\\tby\\tthe\\tother\\tpreprocessing\\tsteps\\twe\\ndiscussed\\tearlier.\\tAnd\\tyou\\tcan\\tjust\\tas\\teasily\\twrite\\tanother\\tpipeline\\tfor\\tthe\\tcategorical\\tattributes\\tas\\twell\\nby\\tsimply\\tselecting\\tthe\\tcategorical\\tattributes\\tusing\\ta\\t\\nDataFrameSelector\\n\\tand\\tthen\\tapplying\\ta\\nLabelBinarizer\\n\\t\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 93}), Document(page_content='num_attribs\\n\\t\\n=\\n\\t\\nlist\\n(\\nhousing_num\\n)\\ncat_attribs\\n\\t\\n=\\n\\t\\n[\\n\"ocean_proximity\"\\n]\\nnum_pipeline\\n\\t\\n=\\n\\t\\nPipeline\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'selector\\'\\n,\\n\\t\\nDataFrameSelector\\n(\\nnum_attribs\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'imputer\\'\\n,\\n\\t\\nImputer\\n(\\nstrategy\\n=\\n\"median\"\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'attribs_adder\\'\\n,\\n\\t\\nCombinedAttributesAdder\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'std_scaler\\'\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\n])\\ncat_pipeline\\n\\t\\n=\\n\\t\\nPipeline\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'selector\\'\\n,\\n\\t\\nDataFrameSelector\\n(\\ncat_attribs\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'label_binarizer\\'\\n,\\n\\t\\nLabelBinarizer\\n()),\\n\\t\\t\\t\\t\\n])\\nBut\\thow\\tcan\\tyou\\tjoin\\tthese\\ttwo\\tpipelines\\tinto\\ta\\tsingle\\tpipeline?\\tThe\\tanswer\\tis\\tto\\tuse\\tScikit-Learn’s\\nFeatureUnion\\n\\t\\nclass.\\tYou\\tgive\\tit\\ta\\tlist\\tof\\ttransformers\\t(which\\tcan\\tbe\\tentire\\ttransformer\\tpipelines);\\twhen\\nits\\t\\ntransform()\\n\\tmethod\\tis\\tcalled,\\tit\\truns\\teach\\ttransformer’s\\t\\ntransform()\\n\\tmethod\\tin\\tparallel,\\twaits\\tfor\\ntheir\\toutput,\\tand\\tthen\\tconcatenates\\tthem\\tand\\treturns\\tthe\\tresult\\t(and\\tof\\tcourse\\tcalling\\tits\\t\\nfit()\\n\\tmethod\\ncalls\\teach\\ttransformer’s\\t\\nfit()\\n\\tmethod).\\tA\\tfull\\tpipeline\\thandling\\tboth\\tnumerical\\tand\\tcategorical\\nattributes\\tmay\\tlook\\tlike\\t\\nthis:\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nFeatureUnion\\nfull_pipeline\\n\\t\\n=\\n\\t\\nFeatureUnion\\n(\\ntransformer_list\\n=\\n[\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"num_pipeline\"\\n,\\n\\t\\nnum_pipeline\\n),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"cat_pipeline\"\\n,\\n\\t\\ncat_pipeline\\n),\\n\\t\\t\\t\\t\\n])\\nAnd\\tyou\\tcan\\trun\\tthe\\twhole\\tpipeline\\tsimply:\\n>>>\\t\\nhousing_prepared\\n\\t\\n=\\n\\t\\nfull_pipeline\\n.\\nfit_transform\\n(\\nhousing\\n)\\n>>>\\t\\nhousing_prepared\\narray([[-1.15604281,\\t\\t0.77194962,\\t\\t0.74333089,\\t...,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t],\\n\\t\\t\\t\\t\\t\\t\\t[-1.17602483,\\t\\t0.6596948\\t,\\t-1.1653172\\t,\\t...,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t],\\n\\t\\t\\t\\t\\t\\t\\t[...]\\n>>>\\t\\nhousing_prepared\\n.\\nshape\\n(16512,\\t16)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 94}), Document(page_content='Select\\tand\\tTrain\\ta\\tModel\\nAt\\tlast!\\t\\nYou\\tframed\\tthe\\tproblem,\\tyou\\tgot\\tthe\\tdata\\tand\\texplored\\tit,\\tyou\\tsampled\\ta\\ttraining\\tset\\tand\\ta\\ttest\\tset,\\nand\\tyou\\twrote\\ttransformation\\tpipelines\\tto\\tclean\\tup\\tand\\tprepare\\tyour\\tdata\\tfor\\tMachine\\tLearning\\nalgorithms\\tautomatically.\\tYou\\tare\\tnow\\tready\\tto\\tselect\\tand\\ttrain\\ta\\tMachine\\tLearning\\tmodel.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 95}), Document(page_content='Training\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\nThe\\t\\ngood\\tnews\\tis\\tthat\\tthanks\\tto\\tall\\tthese\\tprevious\\tsteps,\\tthings\\tare\\tnow\\tgoing\\tto\\tbe\\tmuch\\tsimpler\\tthan\\tyou\\nmight\\tthink.\\tLet’s\\tfirst\\ttrain\\ta\\t\\nLinear\\tRegression\\tmodel,\\tlike\\twe\\tdid\\tin\\tthe\\tprevious\\tchapter:\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLinearRegression\\nlin_reg\\n\\t\\n=\\n\\t\\nLinearRegression\\n()\\nlin_reg\\n.\\nfit\\n(\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n)\\nDone!\\tYou\\tnow\\thave\\ta\\tworking\\tLinear\\tRegression\\tmodel.\\tLet’s\\ttry\\tit\\tout\\ton\\ta\\tfew\\tinstances\\tfrom\\tthe\\ntraining\\tset:\\n>>>\\t\\nsome_data\\n\\t\\n=\\n\\t\\nhousing\\n.\\niloc\\n[:\\n5\\n]\\n>>>\\t\\nsome_labels\\n\\t\\n=\\n\\t\\nhousing_labels\\n.\\niloc\\n[:\\n5\\n]\\n>>>\\t\\nsome_data_prepared\\n\\t\\n=\\n\\t\\nfull_pipeline\\n.\\ntransform\\n(\\nsome_data\\n)\\n>>>\\t\\nprint\\n(\\n\"Predictions:\"\\n,\\n\\t\\nlin_reg\\n.\\npredict\\n(\\nsome_data_prepared\\n))\\nPredictions:\\t[\\t210644.6045\\t\\t317768.8069\\t\\t210956.4333\\t\\t59218.9888\\t\\t189747.5584]\\n>>>\\t\\nprint\\n(\\n\"Labels:\"\\n,\\n\\t\\nlist\\n(\\nsome_labels\\n))\\nLabels:\\t[286600.0,\\t340600.0,\\t196900.0,\\t46300.0,\\t254500.0]\\nIt\\tworks,\\talthough\\tthe\\tpredictions\\tare\\tnot\\texactly\\taccurate\\t(e.g.,\\tthe\\tfirst\\tprediction\\tis\\toff\\tby\\tclose\\tto\\n40%!).\\tLet’s\\tmeasure\\tthis\\tregression\\tmodel’s\\tRMSE\\ton\\tthe\\twhole\\ttraining\\tset\\tusing\\tScikit-Learn’s\\nmean_squared_error\\n\\t\\nfunction:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nmean_squared_error\\n>>>\\t\\nhousing_predictions\\n\\t\\n=\\n\\t\\nlin_reg\\n.\\npredict\\n(\\nhousing_prepared\\n)\\n>>>\\t\\nlin_mse\\n\\t\\n=\\n\\t\\nmean_squared_error\\n(\\nhousing_labels\\n,\\n\\t\\nhousing_predictions\\n)\\n>>>\\t\\nlin_rmse\\n\\t\\n=\\n\\t\\nnp\\n.\\nsqrt\\n(\\nlin_mse\\n)\\n>>>\\t\\nlin_rmse\\n68628.198198489219\\nOkay,\\tthis\\tis\\tbetter\\tthan\\tnothing\\tbut\\tclearly\\tnot\\ta\\tgreat\\tscore:\\tmost\\tdistricts’\\t\\nmedian_housing_values\\nrange\\tbetween\\t$120,000\\tand\\t$265,000,\\tso\\ta\\ttypical\\tprediction\\terror\\tof\\t$68,628\\tis\\tnot\\tvery\\tsatisfying.\\nThis\\tis\\tan\\texample\\tof\\ta\\tmodel\\t\\nunderfitting\\tthe\\ttraining\\tdata.\\tWhen\\tthis\\thappens\\tit\\tcan\\tmean\\tthat\\tthe\\nfeatures\\tdo\\tnot\\tprovide\\tenough\\tinformation\\tto\\tmake\\tgood\\tpredictions,\\tor\\tthat\\tthe\\tmodel\\tis\\tnot\\tpowerful\\nenough.\\tAs\\twe\\tsaw\\tin\\tthe\\tprevious\\tchapter,\\tthe\\tmain\\tways\\tto\\tfix\\tunderfitting\\tare\\tto\\tselect\\ta\\tmore\\npowerful\\tmodel,\\tto\\tfeed\\tthe\\ttraining\\talgorithm\\twith\\tbetter\\tfeatures,\\tor\\tto\\treduce\\tthe\\tconstraints\\ton\\tthe\\nmodel.\\tThis\\tmodel\\tis\\tnot\\tregularized,\\tso\\tthis\\trules\\tout\\tthe\\tlast\\toption.\\tYou\\tcould\\ttry\\tto\\tadd\\tmore\\tfeatures\\n(e.g.,\\tthe\\tlog\\tof\\tthe\\tpopulation),\\tbut\\tfirst\\tlet’s\\ttry\\ta\\tmore\\tcomplex\\tmodel\\tto\\tsee\\thow\\tit\\tdoes.\\nLet’s\\ttrain\\ta\\t\\nDecisionTreeRegressor\\n.\\tThis\\tis\\ta\\tpowerful\\tmodel,\\tcapable\\tof\\tfinding\\tcomplex\\tnonlinear\\nrelationships\\tin\\tthe\\t\\ndata\\t(Decision\\tTrees\\tare\\tpresented\\tin\\tmore\\tdetail\\tin\\t\\nChapter\\t6\\n).\\tThe\\tcode\\tshould\\tlook\\nfamiliar\\tby\\tnow:\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nDecisionTreeRegressor\\ntree_reg\\n\\t\\n=\\n\\t\\nDecisionTreeRegressor\\n()\\ntree_reg\\n.\\nfit\\n(\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n)\\nNow\\tthat\\tthe\\tmodel\\tis\\ttrained,\\tlet’s\\tevaluate\\tit\\ton\\tthe\\ttraining\\t\\nset:', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 96}), Document(page_content='>>>\\t\\nhousing_predictions\\n\\t\\n=\\n\\t\\ntree_reg\\n.\\npredict\\n(\\nhousing_prepared\\n)\\n>>>\\t\\ntree_mse\\n\\t\\n=\\n\\t\\nmean_squared_error\\n(\\nhousing_labels\\n,\\n\\t\\nhousing_predictions\\n)\\n>>>\\t\\ntree_rmse\\n\\t\\n=\\n\\t\\nnp\\n.\\nsqrt\\n(\\ntree_mse\\n)\\n>>>\\t\\ntree_rmse\\n0.0\\nWait,\\twhat!?\\tNo\\terror\\tat\\tall?\\tCould\\tthis\\tmodel\\treally\\tbe\\tabsolutely\\tperfect?\\tOf\\tcourse,\\tit\\tis\\tmuch\\tmore\\nlikely\\tthat\\tthe\\tmodel\\thas\\tbadly\\toverfit\\tthe\\tdata.\\tHow\\tcan\\tyou\\tbe\\tsure?\\tAs\\twe\\tsaw\\tearlier,\\tyou\\tdon’t\\twant\\nto\\ttouch\\tthe\\ttest\\tset\\tuntil\\tyou\\tare\\tready\\tto\\tlaunch\\ta\\tmodel\\tyou\\tare\\tconfident\\tabout,\\tso\\tyou\\tneed\\tto\\tuse\\tpart\\nof\\tthe\\ttraining\\tset\\tfor\\ttraining,\\tand\\tpart\\tfor\\t\\nmodel\\tvalidation.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 97}), Document(page_content='Better\\tEvaluation\\tUsing\\tCross-Validation\\nOne\\t\\nway\\tto\\tevaluate\\tthe\\tDecision\\tTree\\tmodel\\twould\\tbe\\tto\\tuse\\tthe\\t\\ntrain_test_split\\n\\tfunction\\tto\\tsplit\\nthe\\ttraining\\tset\\tinto\\ta\\tsmaller\\ttraining\\tset\\tand\\ta\\tvalidation\\tset,\\tthen\\ttrain\\tyour\\tmodels\\tagainst\\tthe\\tsmaller\\ntraining\\tset\\tand\\tevaluate\\tthem\\tagainst\\tthe\\tvalidation\\tset.\\tIt’s\\ta\\tbit\\tof\\twork,\\tbut\\tnothing\\ttoo\\tdifficult\\tand\\tit\\nwould\\twork\\tfairly\\twell.\\nA\\tgreat\\talternative\\tis\\tto\\tuse\\t\\nScikit-Learn’s\\t\\ncross-validation\\n\\tfeature.\\tThe\\tfollowing\\tcode\\tperforms\\t\\nK-fold\\ncross-validation\\n:\\tit\\trandomly\\tsplits\\tthe\\ttraining\\tset\\tinto\\t10\\tdistinct\\tsubsets\\tcalled\\t\\nfolds\\n,\\t\\nthen\\tit\\ttrains\\tand\\nevaluates\\tthe\\tDecision\\tTree\\tmodel\\t10\\ttimes,\\tpicking\\ta\\tdifferent\\tfold\\tfor\\tevaluation\\tevery\\ttime\\tand\\ntraining\\ton\\tthe\\tother\\t9\\tfolds.\\tThe\\tresult\\tis\\tan\\tarray\\tcontaining\\tthe\\t\\n10\\tevaluation\\tscores:\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\ncross_val_score\\nscores\\n\\t\\n=\\n\\t\\ncross_val_score\\n(\\ntree_reg\\n,\\n\\t\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscoring\\n=\\n\"neg_mean_squared_error\"\\n,\\n\\t\\ncv\\n=\\n10\\n)\\ntree_rmse_scores\\n\\t\\n=\\n\\t\\nnp\\n.\\nsqrt\\n(\\n-\\nscores\\n)\\nWARNING\\nScikit-Learn\\tcross-validation\\tfeatures\\texpect\\ta\\tutility\\tfunction\\t(greater\\tis\\tbetter)\\trather\\tthan\\ta\\tcost\\tfunction\\t(lower\\tis\\tbetter),\\tso\\nthe\\tscoring\\tfunction\\tis\\tactually\\tthe\\topposite\\tof\\tthe\\tMSE\\t(i.e.,\\ta\\tnegative\\tvalue),\\twhich\\tis\\twhy\\tthe\\tpreceding\\tcode\\tcomputes\\t\\n-\\nscores\\n\\tbefore\\tcalculating\\tthe\\tsquare\\troot.\\nLet’s\\tlook\\tat\\tthe\\tresults:\\n>>>\\t\\ndef\\n\\t\\ndisplay_scores\\n(\\nscores\\n):\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\n\"Scores:\"\\n,\\n\\t\\nscores\\n)\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\n\"Mean:\"\\n,\\n\\t\\nscores\\n.\\nmean\\n())\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\n\"Standard\\tdeviation:\"\\n,\\n\\t\\nscores\\n.\\nstd\\n())\\n...\\n>>>\\t\\ndisplay_scores\\n(\\ntree_rmse_scores\\n)\\nScores:\\t[\\t70232.0136482\\t\\t\\t66828.46839892\\t\\t72444.08721003\\t\\t70761.50186201\\n\\t\\t71125.52697653\\t\\t75581.29319857\\t\\t70169.59286164\\t\\t70055.37863456\\n\\t\\t75370.49116773\\t\\t71222.39081244]\\nMean:\\t71379.0744771\\nStandard\\tdeviation:\\t2458.31882043\\nNow\\tthe\\tDecision\\tTree\\tdoesn’t\\tlook\\tas\\tgood\\tas\\tit\\tdid\\tearlier.\\tIn\\tfact,\\tit\\tseems\\tto\\tperform\\tworse\\tthan\\tthe\\nLinear\\tRegression\\tmodel!\\tNotice\\tthat\\tcross-validation\\tallows\\tyou\\tto\\tget\\tnot\\tonly\\tan\\testimate\\tof\\tthe\\nperformance\\tof\\tyour\\tmodel,\\tbut\\talso\\ta\\tmeasure\\tof\\thow\\tprecise\\tthis\\testimate\\tis\\t(i.e.,\\tits\\tstandard\\ndeviation).\\tThe\\tDecision\\tTree\\thas\\ta\\tscore\\tof\\tapproximately\\t71,379,\\tgenerally\\t±2,458.\\tYou\\twould\\tnot\\nhave\\tthis\\tinformation\\tif\\tyou\\tjust\\tused\\tone\\tvalidation\\tset.\\tBut\\tcross-validation\\tcomes\\tat\\tthe\\tcost\\tof\\ttraining\\nthe\\tmodel\\tseveral\\ttimes,\\tso\\tit\\tis\\tnot\\talways\\tpossible.\\nLet’s\\tcompute\\tthe\\tsame\\tscores\\tfor\\tthe\\tLinear\\tRegression\\tmodel\\tjust\\tto\\tbe\\t\\nsure:\\n>>>\\t\\nlin_scores\\n\\t\\n=\\n\\t\\ncross_val_score\\n(\\nlin_reg\\n,\\n\\t\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n,\\n...\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscoring\\n=\\n\"neg_mean_squared_error\"\\n,\\n\\t\\ncv\\n=\\n10\\n)\\n...\\n>>>\\t\\nlin_rmse_scores\\n\\t\\n=\\n\\t\\nnp\\n.\\nsqrt\\n(\\n-\\nlin_scores\\n)\\n>>>\\t\\ndisplay_scores\\n(\\nlin_rmse_scores\\n)\\nScores:\\t[\\t66760.97371572\\t\\t66962.61914244\\t\\t70349.94853401\\t\\t74757.02629506\\n\\t\\t68031.13388938\\t\\t71193.84183426\\t\\t64968.13706527\\t\\t68261.95557897', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 98}), Document(page_content='\\t\\t71527.64217874\\t\\t67665.10082067]\\nMean:\\t69047.8379055\\nStandard\\tdeviation:\\t2735.51074287\\nThat’s\\tright:\\tthe\\tDecision\\tTree\\tmodel\\tis\\toverfitting\\tso\\tbadly\\tthat\\tit\\tperforms\\tworse\\tthan\\t\\nthe\\tLinear\\nRegression\\tmodel.\\nLet’s\\ttry\\tone\\tlast\\tmodel\\tnow:\\tthe\\t\\nRandomForestRegressor\\n.\\tAs\\twe\\twill\\tsee\\tin\\t\\nChapter\\t7\\n,\\t\\nRandom\\nForests\\twork\\tby\\ttraining\\tmany\\tDecision\\tTrees\\ton\\trandom\\tsubsets\\tof\\tthe\\tfeatures,\\tthen\\taveraging\\tout\\ttheir\\npredictions.\\tBuilding\\ta\\tmodel\\ton\\ttop\\tof\\tmany\\tother\\tmodels\\tis\\t\\ncalled\\t\\nEnsemble\\tLearning\\n,\\tand\\tit\\tis\\toften\\ta\\ngreat\\tway\\tto\\tpush\\tML\\talgorithms\\teven\\tfurther.\\tWe\\twill\\tskip\\tmost\\tof\\tthe\\tcode\\tsince\\tit\\tis\\tessentially\\tthe\\nsame\\tas\\tfor\\tthe\\tother\\tmodels:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nRandomForestRegressor\\n>>>\\t\\nforest_reg\\n\\t\\n=\\n\\t\\nRandomForestRegressor\\n()\\n>>>\\t\\nforest_reg\\n.\\nfit\\n(\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n)\\n>>>\\t\\n[\\n...\\n]\\n>>>\\t\\nforest_rmse\\n21941.911027380233\\n>>>\\t\\ndisplay_scores\\n(\\nforest_rmse_scores\\n)\\nScores:\\t[\\t51650.94405471\\t\\t48920.80645498\\t\\t52979.16096752\\t\\t54412.74042021\\n\\t\\t50861.29381163\\t\\t56488.55699727\\t\\t51866.90120786\\t\\t49752.24599537\\n\\t\\t55399.50713191\\t\\t53309.74548294]\\nMean:\\t52564.1902524\\nStandard\\tdeviation:\\t2301.87380392\\nWow,\\tthis\\tis\\tmuch\\tbetter:\\tRandom\\tForests\\tlook\\tvery\\tpromising.\\tHowever,\\tnote\\tthat\\tthe\\tscore\\ton\\tthe\\ntraining\\tset\\tis\\tstill\\tmuch\\tlower\\tthan\\ton\\tthe\\tvalidation\\tsets,\\tmeaning\\tthat\\tthe\\tmodel\\tis\\tstill\\toverfitting\\tthe\\ntraining\\tset.\\tPossible\\tsolutions\\tfor\\toverfitting\\tare\\tto\\tsimplify\\tthe\\tmodel,\\tconstrain\\tit\\t(i.e.,\\tregularize\\tit),\\tor\\nget\\ta\\tlot\\tmore\\ttraining\\tdata.\\tHowever,\\tbefore\\tyou\\tdive\\tmuch\\tdeeper\\tin\\tRandom\\tForests,\\tyou\\tshould\\ttry\\tout\\nmany\\tother\\tmodels\\tfrom\\tvarious\\tcategories\\tof\\tMachine\\tLearning\\talgorithms\\t(several\\tSupport\\tVector\\nMachines\\twith\\tdifferent\\tkernels,\\tpossibly\\ta\\tneural\\tnetwork,\\tetc.),\\twithout\\tspending\\ttoo\\tmuch\\ttime\\ntweaking\\tthe\\thyperparameters.\\tThe\\tgoal\\tis\\tto\\tshortlist\\ta\\tfew\\t(two\\tto\\tfive)\\tpromising\\tmodels.\\nTIP\\nYou\\tshould\\tsave\\tevery\\tmodel\\tyou\\texperiment\\twith,\\tso\\tyou\\tcan\\tcome\\tback\\teasily\\tto\\tany\\tmodel\\tyou\\twant.\\tMake\\tsure\\tyou\\tsave\\nboth\\tthe\\thyperparameters\\tand\\tthe\\ttrained\\tparameters,\\tas\\twell\\tas\\tthe\\tcross-validation\\tscores\\tand\\tperhaps\\tthe\\tactual\\tpredictions\\tas\\nwell.\\tThis\\twill\\tallow\\tyou\\tto\\teasily\\tcompare\\tscores\\tacross\\tmodel\\ttypes,\\tand\\tcompare\\tthe\\ttypes\\tof\\terrors\\tthey\\tmake.\\tYou\\tcan\\neasily\\tsave\\tScikit-Learn\\tmodels\\tby\\tusing\\t\\nPython’s\\t\\npickle\\n\\tmodule,\\tor\\t\\nusing\\t\\nsklearn.externals.joblib\\n,\\twhich\\tis\\tmore\\tefficient\\nat\\tserializing\\t\\nlarge\\tNumPy\\tarrays:\\nfrom\\n\\t\\nsklearn.externals\\n\\t\\nimport\\n\\t\\njoblib\\njoblib\\n.\\ndump\\n(\\nmy_model\\n,\\n\\t\\n\"my_model.pkl\"\\n)\\n#\\tand\\tlater...\\nmy_model_loaded\\n\\t\\n=\\n\\t\\njoblib\\n.\\nload\\n(\\n\"my_model.pkl\"\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 99}), Document(page_content='Fine-Tune\\tYour\\tModel\\nLet’s\\t\\nassume\\tthat\\tyou\\tnow\\thave\\ta\\tshortlist\\tof\\tpromising\\tmodels.\\tYou\\tnow\\tneed\\tto\\tfine-tune\\tthem.\\tLet’s\\nlook\\tat\\ta\\tfew\\tways\\tyou\\tcan\\tdo\\tthat.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 100}), Document(page_content=\"Grid\\tSearch\\nOne\\tway\\tto\\tdo\\tthat\\twould\\tbe\\tto\\tfiddle\\twith\\tthe\\t\\nhyperparameters\\tmanually,\\tuntil\\tyou\\tfind\\ta\\tgreat\\ncombination\\tof\\thyperparameter\\tvalues.\\tThis\\twould\\tbe\\tvery\\ttedious\\twork,\\tand\\tyou\\tmay\\tnot\\thave\\ttime\\tto\\nexplore\\tmany\\tcombinations.\\nInstead\\tyou\\tshould\\tget\\t\\nScikit-Learn’s\\t\\nGridSearchCV\\n\\tto\\tsearch\\tfor\\tyou.\\tAll\\tyou\\tneed\\tto\\tdo\\tis\\ttell\\tit\\twhich\\nhyperparameters\\tyou\\twant\\tit\\tto\\texperiment\\twith,\\tand\\twhat\\tvalues\\tto\\ttry\\tout,\\tand\\tit\\twill\\tevaluate\\tall\\tthe\\npossible\\tcombinations\\tof\\thyperparameter\\tvalues,\\tusing\\tcross-validation.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\nsearches\\tfor\\tthe\\tbest\\tcombination\\tof\\thyperparameter\\tvalues\\t\\nfor\\tthe\\t\\nRandomForestRegressor\\n:\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\nGridSearchCV\\nparam_grid\\n\\t\\n=\\n\\t\\n[\\n\\t\\t\\t\\t\\n{\\n'n_estimators'\\n:\\n\\t\\n[\\n3\\n,\\n\\t\\n10\\n,\\n\\t\\n30\\n],\\n\\t\\n'max_features'\\n:\\n\\t\\n[\\n2\\n,\\n\\t\\n4\\n,\\n\\t\\n6\\n,\\n\\t\\n8\\n]},\\n\\t\\t\\t\\t\\n{\\n'bootstrap'\\n:\\n\\t\\n[\\nFalse\\n],\\n\\t\\n'n_estimators'\\n:\\n\\t\\n[\\n3\\n,\\n\\t\\n10\\n],\\n\\t\\n'max_features'\\n:\\n\\t\\n[\\n2\\n,\\n\\t\\n3\\n,\\n\\t\\n4\\n]},\\n\\t\\t\\n]\\nforest_reg\\n\\t\\n=\\n\\t\\nRandomForestRegressor\\n()\\ngrid_search\\n\\t\\n=\\n\\t\\nGridSearchCV\\n(\\nforest_reg\\n,\\n\\t\\nparam_grid\\n,\\n\\t\\ncv\\n=\\n5\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscoring\\n=\\n'neg_mean_squared_error'\\n)\\ngrid_search\\n.\\nfit\\n(\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n)\\nTIP\\nWhen\\tyou\\thave\\tno\\tidea\\twhat\\tvalue\\ta\\thyperparameter\\tshould\\thave,\\ta\\tsimple\\tapproach\\tis\\tto\\ttry\\tout\\tconsecutive\\tpowers\\tof\\t10\\t(or\\ta\\nsmaller\\tnumber\\tif\\tyou\\twant\\ta\\tmore\\tfine-grained\\tsearch,\\tas\\tshown\\tin\\tthis\\texample\\twith\\tthe\\t\\nn_estimators\\n\\thyperparameter).\\nThis\\t\\nparam_grid\\n\\ttells\\tScikit-Learn\\tto\\tfirst\\tevaluate\\tall\\t3\\t×\\t4\\t=\\t12\\tcombinations\\tof\\t\\nn_estimators\\n\\tand\\nmax_features\\n\\thyperparameter\\tvalues\\tspecified\\tin\\tthe\\tfirst\\t\\ndict\\n\\t(don’t\\tworry\\tabout\\twhat\\tthese\\nhyperparameters\\tmean\\tfor\\tnow;\\tthey\\twill\\tbe\\texplained\\tin\\t\\nChapter\\t7\\n),\\tthen\\ttry\\tall\\t2\\t×\\t3\\t=\\t6\\tcombinations\\nof\\thyperparameter\\tvalues\\tin\\tthe\\tsecond\\t\\ndict\\n,\\tbut\\tthis\\ttime\\twith\\tthe\\t\\nbootstrap\\n\\t\\nhyperparameter\\tset\\tto\\nFalse\\n\\tinstead\\tof\\t\\nTrue\\n\\t(which\\tis\\tthe\\tdefault\\tvalue\\tfor\\tthis\\thyperparameter).\\nAll\\tin\\tall,\\tthe\\tgrid\\tsearch\\twill\\texplore\\t12\\t+\\t6\\t=\\t18\\tcombinations\\tof\\t\\nRandomForestRegressor\\nhyperparameter\\tvalues,\\tand\\tit\\twill\\ttrain\\teach\\tmodel\\tfive\\ttimes\\t(since\\twe\\tare\\tusing\\tfive-fold\\tcross\\nvalidation).\\tIn\\tother\\twords,\\tall\\tin\\tall,\\tthere\\twill\\tbe\\t18\\t×\\t5\\t=\\t90\\trounds\\tof\\ttraining!\\tIt\\tmay\\ttake\\tquite\\ta\\tlong\\ntime,\\tbut\\twhen\\tit\\tis\\tdone\\tyou\\tcan\\tget\\tthe\\tbest\\tcombination\\tof\\tparameters\\tlike\\tthis:\\n>>>\\t\\ngrid_search\\n.\\nbest_params_\\n{'max_features':\\t8,\\t'n_estimators':\\t30}\\nTIP\\nSince\\t8\\tand\\t30\\tare\\tthe\\tmaximum\\tvalues\\tthat\\twere\\tevaluated,\\tyou\\tshould\\tprobably\\ttry\\tsearching\\tagain\\twith\\thigher\\tvalues,\\tsince\\nthe\\tscore\\tmay\\tcontinue\\tto\\timprove.\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 101}), Document(page_content='You\\tcan\\talso\\tget\\tthe\\tbest\\testimator\\tdirectly:\\n>>>\\t\\ngrid_search\\n.\\nbest_estimator_\\nRandomForestRegressor(bootstrap=True,\\tcriterion=\\'mse\\',\\tmax_depth=None,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tmax_features=8,\\tmax_leaf_nodes=None,\\tmin_impurity_split=1e-07,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tmin_samples_leaf=1,\\tmin_samples_split=2,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tmin_weight_fraction_leaf=0.0,\\tn_estimators=30,\\tn_jobs=1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toob_score=False,\\trandom_state=42,\\tverbose=0,\\twarm_start=False)\\nNOTE\\nIf\\t\\nGridSearchCV\\n\\tis\\tinitialized\\twith\\t\\nrefit=True\\n\\t(which\\tis\\tthe\\tdefault),\\tthen\\tonce\\tit\\tfinds\\tthe\\tbest\\testimator\\tusing\\tcross-validation,\\tit\\nretrains\\tit\\ton\\tthe\\twhole\\ttraining\\tset.\\tThis\\tis\\tusually\\ta\\tgood\\tidea\\tsince\\tfeeding\\tit\\tmore\\tdata\\twill\\tlikely\\timprove\\tits\\tperformance.\\nAnd\\tof\\tcourse\\tthe\\tevaluation\\tscores\\tare\\talso\\tavailable:\\n>>>\\t\\ncvres\\n\\t\\n=\\n\\t\\ngrid_search\\n.\\ncv_results_\\n>>>\\t\\nfor\\n\\t\\nmean_score\\n,\\n\\t\\nparams\\n\\t\\nin\\n\\t\\nzip\\n(\\ncvres\\n[\\n\"mean_test_score\"\\n],\\n\\t\\ncvres\\n[\\n\"params\"\\n]):\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\nnp\\n.\\nsqrt\\n(\\n-\\nmean_score\\n),\\n\\t\\nparams\\n)\\n...\\n63825.0479302\\t{\\'max_features\\':\\t2,\\t\\'n_estimators\\':\\t3}\\n55643.8429091\\t{\\'max_features\\':\\t2,\\t\\'n_estimators\\':\\t10}\\n53380.6566859\\t{\\'max_features\\':\\t2,\\t\\'n_estimators\\':\\t30}\\n60959.1388585\\t{\\'max_features\\':\\t4,\\t\\'n_estimators\\':\\t3}\\n52740.5841667\\t{\\'max_features\\':\\t4,\\t\\'n_estimators\\':\\t10}\\n50374.1421461\\t{\\'max_features\\':\\t4,\\t\\'n_estimators\\':\\t30}\\n58661.2866462\\t{\\'max_features\\':\\t6,\\t\\'n_estimators\\':\\t3}\\n52009.9739798\\t{\\'max_features\\':\\t6,\\t\\'n_estimators\\':\\t10}\\n50154.1177737\\t{\\'max_features\\':\\t6,\\t\\'n_estimators\\':\\t30}\\n57865.3616801\\t{\\'max_features\\':\\t8,\\t\\'n_estimators\\':\\t3}\\n51730.0755087\\t{\\'max_features\\':\\t8,\\t\\'n_estimators\\':\\t10}\\n49694.8514333\\t{\\'max_features\\':\\t8,\\t\\'n_estimators\\':\\t30}\\n62874.4073931\\t{\\'max_features\\':\\t2,\\t\\'n_estimators\\':\\t3,\\t\\'bootstrap\\':\\tFalse}\\n54561.9398157\\t{\\'max_features\\':\\t2,\\t\\'n_estimators\\':\\t10,\\t\\'bootstrap\\':\\tFalse}\\n59416.6463145\\t{\\'max_features\\':\\t3,\\t\\'n_estimators\\':\\t3,\\t\\'bootstrap\\':\\tFalse}\\n52660.245911\\t{\\'max_features\\':\\t3,\\t\\'n_estimators\\':\\t10,\\t\\'bootstrap\\':\\tFalse}\\n57490.0168279\\t{\\'max_features\\':\\t4,\\t\\'n_estimators\\':\\t3,\\t\\'bootstrap\\':\\tFalse}\\n51093.9059428\\t{\\'max_features\\':\\t4,\\t\\'n_estimators\\':\\t10,\\t\\'bootstrap\\':\\tFalse}\\nIn\\tthis\\texample,\\twe\\tobtain\\tthe\\tbest\\tsolution\\tby\\tsetting\\tthe\\t\\nmax_features\\n\\thyperparameter\\tto\\t\\n8\\n,\\tand\\tthe\\nn_estimators\\n\\thyperparameter\\tto\\t\\n30\\n.\\tThe\\tRMSE\\tscore\\tfor\\tthis\\tcombination\\tis\\t49,694,\\twhich\\tis\\tslightly\\nbetter\\tthan\\tthe\\tscore\\tyou\\tgot\\tearlier\\tusing\\tthe\\tdefault\\thyperparameter\\tvalues\\t(which\\twas\\t52,564).\\nCongratulations,\\tyou\\thave\\tsuccessfully\\tfine-tuned\\tyour\\tbest\\tmodel!\\nTIP\\nDon’t\\tforget\\tthat\\tyou\\tcan\\ttreat\\tsome\\tof\\tthe\\tdata\\tpreparation\\tsteps\\tas\\thyperparameters.\\tFor\\texample,\\tthe\\tgrid\\tsearch\\twill\\nautomatically\\tfind\\tout\\twhether\\tor\\tnot\\tto\\tadd\\ta\\tfeature\\tyou\\twere\\tnot\\tsure\\tabout\\t(e.g.,\\tusing\\tthe\\t\\nadd_bedrooms_per_room\\nhyperparameter\\tof\\tyour\\t\\nCombinedAttributesAdder\\n\\ttransformer).\\tIt\\tmay\\tsimilarly\\tbe\\tused\\tto\\tautomatically\\tfind\\tthe\\tbest\\tway\\tto\\nhandle\\toutliers,\\tmissing\\tfeatures,\\tfeature\\tselection,\\t\\nand\\t\\nmore.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 102}), Document(page_content='Randomized\\tSearch\\nThe\\t\\ngrid\\tsearch\\tapproach\\t\\nis\\tfine\\twhen\\tyou\\tare\\texploring\\trelatively\\tfew\\tcombinations,\\tlike\\tin\\tthe\\tprevious\\nexample,\\tbut\\twhen\\tthe\\thyperparameter\\t\\nsearch\\tspace\\n\\t\\nis\\tlarge,\\tit\\tis\\toften\\tpreferable\\tto\\tuse\\nRandomizedSearchCV\\n\\tinstead.\\tThis\\tclass\\tcan\\tbe\\tused\\tin\\tmuch\\tthe\\tsame\\tway\\tas\\tthe\\t\\nGridSearchCV\\n\\tclass,\\nbut\\tinstead\\tof\\ttrying\\tout\\tall\\tpossible\\tcombinations,\\tit\\tevaluates\\ta\\tgiven\\tnumber\\tof\\trandom\\tcombinations\\nby\\tselecting\\ta\\trandom\\tvalue\\tfor\\teach\\thyperparameter\\tat\\tevery\\titeration.\\tThis\\tapproach\\thas\\ttwo\\tmain\\nbenefits:\\nIf\\tyou\\tlet\\tthe\\trandomized\\tsearch\\trun\\tfor,\\tsay,\\t1,000\\titerations,\\tthis\\tapproach\\twill\\texplore\\t1,000\\ndifferent\\tvalues\\tfor\\teach\\thyperparameter\\t(instead\\tof\\tjust\\ta\\tfew\\tvalues\\tper\\thyperparameter\\twith\\tthe\\ngrid\\tsearch\\tapproach).\\nYou\\thave\\tmore\\tcontrol\\tover\\tthe\\tcomputing\\tbudget\\tyou\\twant\\tto\\tallocate\\tto\\thyperparameter\\tsearch,\\nsimply\\tby\\tsetting\\tthe\\tnumber\\tof\\titerations.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 103}), Document(page_content='Ensemble\\tMethods\\nAnother\\t\\nway\\tto\\tfine-tune\\tyour\\tsystem\\tis\\tto\\ttry\\tto\\tcombine\\tthe\\tmodels\\tthat\\tperform\\tbest.\\tThe\\tgroup\\t(or\\n“ensemble”)\\twill\\toften\\tperform\\tbetter\\tthan\\tthe\\tbest\\tindividual\\tmodel\\t(just\\tlike\\tRandom\\tForests\\tperform\\nbetter\\tthan\\tthe\\tindividual\\tDecision\\tTrees\\tthey\\trely\\ton),\\tespecially\\tif\\tthe\\tindividual\\tmodels\\tmake\\tvery\\ndifferent\\ttypes\\tof\\terrors.\\tWe\\twill\\tcover\\tthis\\ttopic\\tin\\tmore\\tdetail\\tin\\t\\nChapter\\t7\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 104}), Document(page_content='Analyze\\tthe\\tBest\\tModels\\tand\\tTheir\\tErrors\\nYou\\t\\nwill\\toften\\tgain\\tgood\\tinsights\\ton\\tthe\\tproblem\\tby\\tinspecting\\tthe\\tbest\\tmodels.\\tFor\\texample,\\tthe\\nRandomForestRegressor\\n\\t\\ncan\\tindicate\\tthe\\trelative\\timportance\\tof\\teach\\tattribute\\tfor\\tmaking\\taccurate\\npredictions:\\n>>>\\t\\nfeature_importances\\n\\t\\n=\\n\\t\\ngrid_search\\n.\\nbest_estimator_\\n.\\nfeature_importances_\\n>>>\\t\\nfeature_importances\\narray([\\t\\t7.33442355e-02,\\t\\t\\t6.29090705e-02,\\t\\t\\t4.11437985e-02,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t1.46726854e-02,\\t\\t\\t1.41064835e-02,\\t\\t\\t1.48742809e-02,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t1.42575993e-02,\\t\\t\\t3.66158981e-01,\\t\\t\\t5.64191792e-02,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t1.08792957e-01,\\t\\t\\t5.33510773e-02,\\t\\t\\t1.03114883e-02,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t1.64780994e-01,\\t\\t\\t6.02803867e-05,\\t\\t\\t1.96041560e-03,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t2.85647464e-03])\\nLet’s\\tdisplay\\tthese\\timportance\\tscores\\tnext\\tto\\ttheir\\tcorresponding\\tattribute\\tnames:\\n>>>\\t\\nextra_attribs\\n\\t\\n=\\n\\t\\n[\\n\"rooms_per_hhold\"\\n,\\n\\t\\n\"pop_per_hhold\"\\n,\\n\\t\\n\"bedrooms_per_room\"\\n]\\n>>>\\t\\ncat_one_hot_attribs\\n\\t\\n=\\n\\t\\nlist\\n(\\nencoder\\n.\\nclasses_\\n)\\n>>>\\t\\nattributes\\n\\t\\n=\\n\\t\\nnum_attribs\\n\\t\\n+\\n\\t\\nextra_attribs\\n\\t\\n+\\n\\t\\ncat_one_hot_attribs\\n>>>\\t\\nsorted\\n(\\nzip\\n(\\nfeature_importances\\n,\\n\\t\\nattributes\\n),\\n\\t\\nreverse\\n=\\nTrue\\n)\\n[(0.36615898061813418,\\t\\'median_income\\'),\\n\\t(0.16478099356159051,\\t\\'INLAND\\'),\\n\\t(0.10879295677551573,\\t\\'pop_per_hhold\\'),\\n\\t(0.073344235516012421,\\t\\'longitude\\'),\\n\\t(0.062909070482620302,\\t\\'latitude\\'),\\n\\t(0.056419179181954007,\\t\\'rooms_per_hhold\\'),\\n\\t(0.053351077347675809,\\t\\'bedrooms_per_room\\'),\\n\\t(0.041143798478729635,\\t\\'housing_median_age\\'),\\n\\t(0.014874280890402767,\\t\\'population\\'),\\n\\t(0.014672685420543237,\\t\\'total_rooms\\'),\\n\\t(0.014257599323407807,\\t\\'households\\'),\\n\\t(0.014106483453584102,\\t\\'total_bedrooms\\'),\\n\\t(0.010311488326303787,\\t\\'<1H\\tOCEAN\\'),\\n\\t(0.0028564746373201579,\\t\\'NEAR\\tOCEAN\\'),\\n\\t(0.0019604155994780701,\\t\\'NEAR\\tBAY\\'),\\n\\t(6.0280386727365991e-05,\\t\\'ISLAND\\')]\\nWith\\tthis\\tinformation,\\tyou\\tmay\\twant\\tto\\ttry\\tdropping\\tsome\\tof\\tthe\\tless\\tuseful\\tfeatures\\t(e.g.,\\tapparently\\tonly\\none\\t\\nocean_proximity\\n\\tcategory\\tis\\treally\\tuseful,\\tso\\tyou\\tcould\\ttry\\tdropping\\tthe\\tothers).\\nYou\\tshould\\talso\\tlook\\tat\\tthe\\tspecific\\terrors\\tthat\\tyour\\tsystem\\tmakes,\\tthen\\ttry\\tto\\tunderstand\\twhy\\tit\\tmakes\\nthem\\tand\\twhat\\tcould\\tfix\\tthe\\tproblem\\t(adding\\textra\\tfeatures\\tor,\\ton\\tthe\\tcontrary,\\t\\ngetting\\trid\\tof\\tuninformative\\nones,\\tcleaning\\tup\\toutliers,\\tetc.).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 105}), Document(page_content='Evaluate\\tYour\\tSystem\\ton\\tthe\\tTest\\tSet\\nAfter\\t\\ntweaking\\tyour\\tmodels\\tfor\\ta\\twhile,\\tyou\\teventually\\thave\\ta\\tsystem\\tthat\\tperforms\\tsufficiently\\twell.\\nNow\\tis\\tthe\\ttime\\tto\\tevaluate\\tthe\\tfinal\\tmodel\\ton\\tthe\\ttest\\tset.\\tThere\\tis\\tnothing\\tspecial\\tabout\\tthis\\tprocess;\\tjust\\nget\\tthe\\tpredictors\\tand\\tthe\\tlabels\\tfrom\\tyour\\ttest\\tset,\\trun\\tyour\\t\\nfull_pipeline\\n\\tto\\ttransform\\tthe\\tdata\\t(call\\ntransform()\\n,\\t\\nnot\\n\\t\\nfit_transform()\\n!),\\tand\\tevaluate\\tthe\\tfinal\\tmodel\\ton\\tthe\\ttest\\tset:\\nfinal_model\\n\\t\\n=\\n\\t\\ngrid_search\\n.\\nbest_estimator_\\nX_test\\n\\t\\n=\\n\\t\\nstrat_test_set\\n.\\ndrop\\n(\\n\"median_house_value\"\\n,\\n\\t\\naxis\\n=\\n1\\n)\\ny_test\\n\\t\\n=\\n\\t\\nstrat_test_set\\n[\\n\"median_house_value\"\\n]\\n.\\ncopy\\n()\\nX_test_prepared\\n\\t\\n=\\n\\t\\nfull_pipeline\\n.\\ntransform\\n(\\nX_test\\n)\\nfinal_predictions\\n\\t\\n=\\n\\t\\nfinal_model\\n.\\npredict\\n(\\nX_test_prepared\\n)\\nfinal_mse\\n\\t\\n=\\n\\t\\nmean_squared_error\\n(\\ny_test\\n,\\n\\t\\nfinal_predictions\\n)\\nfinal_rmse\\n\\t\\n=\\n\\t\\nnp\\n.\\nsqrt\\n(\\nfinal_mse\\n)\\n\\t\\t\\t\\n#\\t=>\\tevaluates\\tto\\t47,766.0\\nThe\\t\\nperformance\\twill\\tusually\\tbe\\tslightly\\tworse\\tthan\\twhat\\tyou\\tmeasured\\tusing\\tcross-validation\\tif\\tyou\\tdid\\na\\tlot\\tof\\thyperparameter\\ttuning\\t(because\\tyour\\tsystem\\tends\\tup\\tfine-tuned\\tto\\tperform\\twell\\ton\\tthe\\tvalidation\\ndata,\\tand\\twill\\tlikely\\tnot\\tperform\\tas\\twell\\ton\\tunknown\\tdatasets).\\tIt\\tis\\tnot\\tthe\\tcase\\tin\\tthis\\texample,\\tbut\\twhen\\nthis\\thappens\\tyou\\tmust\\tresist\\tthe\\ttemptation\\tto\\ttweak\\tthe\\t\\nhyperparameters\\tto\\tmake\\tthe\\tnumbers\\tlook\\tgood\\non\\tthe\\ttest\\tset;\\tthe\\timprovements\\twould\\tbe\\tunlikely\\tto\\tgeneralize\\tto\\tnew\\tdata.\\nNow\\tcomes\\tthe\\tproject\\tprelaunch\\tphase:\\tyou\\tneed\\tto\\tpresent\\tyour\\tsolution\\t(highlighting\\twhat\\tyou\\thave\\nlearned,\\twhat\\tworked\\tand\\twhat\\tdid\\tnot,\\twhat\\tassumptions\\twere\\tmade,\\tand\\twhat\\tyour\\tsystem’s\\tlimitations\\nare),\\tdocument\\teverything,\\tand\\tcreate\\tnice\\tpresentations\\twith\\tclear\\tvisualizations\\tand\\teasy-to-remember\\nstatements\\t(e.g.,\\t“the\\tmedian\\tincome\\tis\\tthe\\tnumber\\tone\\tpredictor\\tof\\thousing\\tprices”).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 106}), Document(page_content='Launch,\\tMonitor,\\tand\\tMaintain\\tYour\\tSystem\\nPerfect,\\tyou\\tgot\\tapproval\\tto\\tlaunch!\\tYou\\tneed\\tto\\tget\\tyour\\tsolution\\tready\\tfor\\tproduction,\\tin\\tparticular\\tby\\nplugging\\tthe\\tproduction\\tinput\\tdata\\tsources\\tinto\\tyour\\tsystem\\tand\\twriting\\ttests.\\nYou\\talso\\tneed\\tto\\twrite\\tmonitoring\\tcode\\tto\\tcheck\\tyour\\tsystem’s\\tlive\\tperformance\\tat\\tregular\\tintervals\\tand\\ntrigger\\talerts\\twhen\\tit\\tdrops.\\tThis\\tis\\timportant\\tto\\tcatch\\tnot\\tonly\\tsudden\\tbreakage,\\tbut\\talso\\tperformance\\ndegradation.\\tThis\\tis\\tquite\\tcommon\\tbecause\\tmodels\\ttend\\tto\\t“rot”\\tas\\tdata\\tevolves\\tover\\ttime,\\tunless\\tthe\\nmodels\\tare\\tregularly\\ttrained\\ton\\tfresh\\tdata.\\nEvaluating\\tyour\\tsystem’s\\tperformance\\twill\\trequire\\tsampling\\tthe\\tsystem’s\\tpredictions\\tand\\tevaluating\\tthem.\\nThis\\twill\\tgenerally\\trequire\\ta\\thuman\\tanalysis.\\tThese\\tanalysts\\tmay\\tbe\\tfield\\texperts,\\tor\\tworkers\\ton\\ta\\ncrowdsourcing\\tplatform\\t(such\\tas\\tAmazon\\tMechanical\\tTurk\\tor\\tCrowdFlower).\\tEither\\tway,\\tyou\\tneed\\tto\\nplug\\tthe\\thuman\\tevaluation\\tpipeline\\tinto\\tyour\\tsystem.\\nYou\\tshould\\talso\\tmake\\tsure\\tyou\\tevaluate\\tthe\\tsystem’s\\tinput\\tdata\\tquality.\\tSometimes\\tperformance\\twill\\ndegrade\\tslightly\\tbecause\\tof\\ta\\tpoor\\tquality\\tsignal\\t(e.g.,\\ta\\tmalfunctioning\\tsensor\\tsending\\trandom\\tvalues,\\tor\\nanother\\tteam’s\\toutput\\tbecoming\\tstale),\\tbut\\tit\\tmay\\ttake\\ta\\twhile\\tbefore\\tyour\\tsystem’s\\tperformance\\tdegrades\\nenough\\tto\\ttrigger\\tan\\talert.\\tIf\\tyou\\tmonitor\\tyour\\tsystem’s\\tinputs,\\tyou\\tmay\\tcatch\\tthis\\tearlier.\\tMonitoring\\tthe\\ninputs\\tis\\tparticularly\\timportant\\tfor\\tonline\\tlearning\\tsystems.\\nFinally,\\tyou\\twill\\tgenerally\\twant\\tto\\ttrain\\tyour\\tmodels\\ton\\ta\\tregular\\tbasis\\tusing\\tfresh\\tdata.\\tYou\\tshould\\nautomate\\tthis\\tprocess\\tas\\tmuch\\tas\\tpossible.\\tIf\\tyou\\tdon’t,\\tyou\\tare\\tvery\\tlikely\\tto\\trefresh\\tyour\\tmodel\\tonly\\nevery\\tsix\\tmonths\\t(at\\tbest),\\tand\\tyour\\tsystem’s\\tperformance\\tmay\\tfluctuate\\tseverely\\tover\\ttime.\\tIf\\tyour\\nsystem\\tis\\tan\\tonline\\tlearning\\tsystem,\\tyou\\tshould\\tmake\\tsure\\tyou\\tsave\\tsnapshots\\tof\\tits\\tstate\\tat\\tregular\\nintervals\\tso\\tyou\\tcan\\teasily\\troll\\tback\\tto\\ta\\tpreviously\\tworking\\tstate.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 107}), Document(page_content='Try\\tIt\\tOut!\\nHopefully\\tthis\\tchapter\\tgave\\tyou\\ta\\tgood\\tidea\\tof\\twhat\\ta\\tMachine\\tLearning\\tproject\\tlooks\\tlike,\\tand\\tshowed\\nyou\\tsome\\tof\\tthe\\ttools\\tyou\\tcan\\tuse\\tto\\ttrain\\ta\\tgreat\\tsystem.\\tAs\\tyou\\tcan\\tsee,\\tmuch\\tof\\tthe\\twork\\tis\\tin\\tthe\\tdata\\npreparation\\tstep,\\tbuilding\\tmonitoring\\ttools,\\tsetting\\tup\\thuman\\tevaluation\\tpipelines,\\tand\\tautomating\\tregular\\nmodel\\ttraining.\\tThe\\tMachine\\tLearning\\talgorithms\\tare\\talso\\timportant,\\tof\\tcourse,\\tbut\\tit\\tis\\tprobably\\npreferable\\tto\\tbe\\tcomfortable\\twith\\tthe\\toverall\\tprocess\\tand\\tknow\\tthree\\tor\\tfour\\talgorithms\\twell\\trather\\tthan\\nto\\tspend\\tall\\tyour\\ttime\\texploring\\tadvanced\\talgorithms\\tand\\tnot\\tenough\\ttime\\ton\\tthe\\toverall\\tprocess.\\nSo,\\tif\\tyou\\thave\\tnot\\talready\\tdone\\tso,\\tnow\\tis\\ta\\tgood\\ttime\\tto\\tpick\\tup\\ta\\tlaptop,\\tselect\\ta\\tdataset\\tthat\\tyou\\tare\\ninterested\\tin,\\tand\\ttry\\tto\\tgo\\tthrough\\tthe\\twhole\\tprocess\\tfrom\\tA\\tto\\tZ.\\tA\\tgood\\tplace\\tto\\tstart\\tis\\ton\\ta\\ncompetition\\twebsite\\tsuch\\tas\\t\\nhttp://kaggle.com/\\n:\\tyou\\twill\\thave\\ta\\tdataset\\tto\\tplay\\twith,\\ta\\tclear\\tgoal,\\tand\\npeople\\tto\\tshare\\tthe\\texperience\\twith.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 108}), Document(page_content='Exercises\\nUsing\\tthis\\tchapter’s\\thousing\\tdataset:\\n1\\n.\\t\\nTry\\ta\\tSupport\\tVector\\tMachine\\tregressor\\t(\\nsklearn.svm.SVR\\n),\\t\\nwith\\tvarious\\thyperparameters\\tsuch\\tas\\nkernel=\"linear\"\\n\\t(with\\tvarious\\tvalues\\tfor\\tthe\\t\\nC\\n\\thyperparameter)\\tor\\t\\nkernel=\"rbf\"\\n\\t(with\\tvarious\\nvalues\\tfor\\tthe\\t\\nC\\n\\tand\\t\\ngamma\\n\\thyperparameters).\\tDon’t\\tworry\\tabout\\twhat\\tthese\\thyperparameters\\tmean\\nfor\\tnow.\\tHow\\tdoes\\tthe\\tbest\\t\\nSVR\\n\\tpredictor\\tperform?\\n2\\n.\\t\\nTry\\treplacing\\t\\nGridSearchCV\\n\\t\\nwith\\t\\nRandomizedSearchCV\\n.\\n3\\n.\\t\\nTry\\tadding\\ta\\ttransformer\\tin\\tthe\\tpreparation\\tpipeline\\tto\\tselect\\tonly\\tthe\\tmost\\timportant\\tattributes.\\n4\\n.\\t\\nTry\\tcreating\\ta\\tsingle\\tpipeline\\tthat\\tdoes\\tthe\\tfull\\tdata\\tpreparation\\tplus\\tthe\\tfinal\\tprediction.\\n5\\n.\\t\\nAutomatically\\texplore\\tsome\\tpreparation\\toptions\\tusing\\t\\nGridSearchCV\\n.\\nSolutions\\t\\nto\\tthese\\texercises\\tare\\tavailable\\tin\\tthe\\tonline\\tJupyter\\tnotebooks\\tat\\nhttps://github.com/ageron/handson-ml\\n.\\nThe\\texample\\tproject\\tis\\tcompletely\\tfictitious;\\tthe\\tgoal\\tis\\tjust\\tto\\tillustrate\\tthe\\tmain\\tsteps\\tof\\ta\\tMachine\\tLearning\\tproject,\\tnot\\tto\\tlearn\\tanything\\nabout\\tthe\\treal\\testate\\tbusiness.\\nThe\\toriginal\\tdataset\\tappeared\\tin\\tR.\\tKelley\\tPace\\tand\\tRonald\\tBarry,\\t“Sparse\\tSpatial\\tAutoregressions,”\\t\\nStatistics\\t&\\tProbability\\tLetters\\n\\t33,\\nno.\\t3\\t(1997):\\t291–297.\\nA\\tpiece\\tof\\tinformation\\tfed\\tto\\ta\\tMachine\\tLearning\\tsystem\\tis\\toften\\tcalled\\ta\\t\\nsignal\\n\\tin\\treference\\tto\\tShannon’s\\tinformation\\ttheory:\\tyou\\twant\\ta\\nhigh\\tsignal/noise\\tratio.\\nRecall\\tthat\\tthe\\ttranspose\\toperator\\tflips\\ta\\tcolumn\\tvector\\tinto\\ta\\trow\\tvector\\t(and\\tvice\\tversa).\\nThe\\tlatest\\tversion\\tof\\tPython\\t3\\tis\\trecommended.\\tPython\\t2.7+\\tshould\\twork\\tfine\\ttoo,\\tbut\\tit\\tis\\tdeprecated.\\tIf\\tyou\\tuse\\tPython\\t2,\\tyou\\tmust\\tadd\\nfrom\\t__future__\\timport\\tdivision,\\tprint_function,\\tunicode_literals\\n\\tat\\tthe\\tbeginning\\tof\\tyour\\tcode.\\nWe\\twill\\tshow\\tthe\\tinstallation\\tsteps\\tusing\\tpip\\tin\\ta\\tbash\\tshell\\ton\\ta\\tLinux\\tor\\tmacOS\\tsystem.\\tYou\\tmay\\tneed\\tto\\tadapt\\tthese\\tcommands\\tto\\tyour\\nown\\tsystem.\\tOn\\tWindows,\\twe\\trecommend\\tinstalling\\tAnaconda\\tinstead.\\nYou\\tmay\\tneed\\tto\\thave\\tadministrator\\trights\\tto\\trun\\tthis\\tcommand;\\tif\\tso,\\ttry\\tprefixing\\tit\\twith\\t\\nsudo\\n.\\nNote\\tthat\\tJupyter\\tcan\\thandle\\tmultiple\\tversions\\tof\\tPython,\\tand\\teven\\tmany\\tother\\tlanguages\\tsuch\\tas\\tR\\tor\\tOctave.\\nYou\\tmight\\talso\\tneed\\tto\\tcheck\\tlegal\\tconstraints,\\tsuch\\tas\\tprivate\\tfields\\tthat\\tshould\\tnever\\tbe\\tcopied\\tto\\tunsafe\\tdatastores.\\nIn\\ta\\treal\\tproject\\tyou\\twould\\tsave\\tthis\\tcode\\tin\\ta\\tPython\\tfile,\\tbut\\tfor\\tnow\\tyou\\tcan\\tjust\\twrite\\tit\\tin\\tyour\\tJupyter\\tnotebook.\\nThe\\tstandard\\tdeviation\\tis\\tgenerally\\tdenoted\\tσ\\t(the\\tGreek\\tletter\\tsigma),\\tand\\tit\\tis\\tthe\\tsquare\\troot\\tof\\tthe\\t\\nvariance\\n,\\twhich\\tis\\tthe\\taverage\\tof\\nthe\\tsquared\\tdeviation\\tfrom\\tthe\\tmean.\\tWhen\\ta\\tfeature\\thas\\ta\\tbell-shaped\\t\\nnormal\\tdistribution\\n\\t(also\\tcalled\\ta\\t\\nGaussian\\tdistribution\\n),\\twhich\\nis\\tvery\\tcommon,\\tthe\\t“68-95-99.7”\\trule\\tapplies:\\tabout\\t68%\\tof\\tthe\\tvalues\\tfall\\twithin\\t1σ\\tof\\tthe\\tmean,\\t95%\\twithin\\t2σ,\\tand\\t99.7%\\twithin\\t3σ.\\nYou\\twill\\toften\\tsee\\tpeople\\tset\\tthe\\trandom\\tseed\\tto\\t42.\\tThis\\tnumber\\thas\\tno\\tspecial\\tproperty,\\tother\\tthan\\tto\\tbe\\tThe\\tAnswer\\tto\\tthe\\tUltimate\\nQuestion\\tof\\tLife,\\tthe\\tUniverse,\\tand\\tEverything.\\nThe\\tlocation\\tinformation\\tis\\tactually\\tquite\\tcoarse,\\tand\\tas\\ta\\tresult\\tmany\\tdistricts\\twill\\thave\\tthe\\texact\\tsame\\tID,\\tso\\tthey\\twill\\tend\\tup\\tin\\tthe\\tsame\\nset\\t(test\\tor\\ttrain).\\tThis\\tintroduces\\tsome\\tunfortunate\\tsampling\\tbias.\\nIf\\tyou\\tare\\treading\\tthis\\tin\\tgrayscale,\\tgrab\\ta\\tred\\tpen\\tand\\tscribble\\tover\\tmost\\tof\\tthe\\tcoastline\\tfrom\\tthe\\tBay\\tArea\\tdown\\tto\\tSan\\tDiego\\t(as\\tyou\\nmight\\texpect).\\tYou\\tcan\\tadd\\ta\\tpatch\\tof\\tyellow\\taround\\tSacramento\\tas\\twell.\\nFor\\tmore\\tdetails\\ton\\tthe\\tdesign\\tprinciples,\\tsee\\t“API\\tdesign\\tfor\\tmachine\\tlearning\\tsoftware:\\texperiences\\tfrom\\tthe\\tscikit-learn\\tproject,”\\tL.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 109}), Document(page_content='Buitinck,\\tG.\\tLouppe,\\tM.\\tBlondel,\\tF.\\tPedregosa,\\tA.\\tMüller,\\tet\\tal.\\t(2013).\\nSome\\tpredictors\\talso\\tprovide\\tmethods\\tto\\tmeasure\\tthe\\tconfidence\\tof\\ttheir\\tpredictions.\\nNumPy’s\\t\\nreshape()\\n\\tfunction\\tallows\\tone\\tdimension\\tto\\tbe\\t–1,\\twhich\\tmeans\\t“unspecified”:\\tthe\\tvalue\\tis\\tinferred\\tfrom\\tthe\\tlength\\tof\\tthe\\tarray\\nand\\tthe\\tremaining\\tdimensions.\\nSee\\tSciPy’s\\tdocumentation\\tfor\\tmore\\tdetails.\\nBut\\tcheck\\tout\\tPull\\tRequest\\t#3886,\\twhich\\tmay\\tintroduce\\ta\\t\\nColumnTransformer\\n\\tclass\\tmaking\\tattribute-specific\\ttransformations\\teasy.\\tYou\\ncould\\talso\\trun\\t\\npip3\\tinstall\\tsklearn-pandas\\n\\tto\\tget\\ta\\t\\nDataFrameMapper\\n\\tclass\\twith\\ta\\tsimilar\\tobjective.\\n16\\n17\\n18\\n19', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 110}), Document(page_content='Chapter\\t3.\\t\\nClassification\\nIn\\t\\nChapter\\t1\\n\\twe\\tmentioned\\tthat\\tthe\\tmost\\tcommon\\tsupervised\\tlearning\\ttasks\\tare\\tregression\\t(predicting\\nvalues)\\tand\\tclassification\\t(predicting\\tclasses).\\tIn\\t\\nChapter\\t2\\n\\twe\\texplored\\ta\\tregression\\ttask,\\tpredicting\\nhousing\\tvalues,\\tusing\\tvarious\\talgorithms\\tsuch\\tas\\tLinear\\tRegression,\\tDecision\\tTrees,\\tand\\tRandom\\tForests\\n(which\\twill\\tbe\\texplained\\tin\\tfurther\\tdetail\\tin\\tlater\\tchapters).\\tNow\\twe\\twill\\tturn\\tour\\tattention\\tto\\nclassification\\tsystems.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 111}), Document(page_content='MNIST\\nIn\\tthis\\t\\nchapter,\\twe\\twill\\tbe\\tusing\\tthe\\tMNIST\\tdataset,\\twhich\\tis\\ta\\tset\\tof\\t70,000\\tsmall\\timages\\tof\\tdigits\\nhandwritten\\tby\\thigh\\tschool\\tstudents\\tand\\temployees\\tof\\tthe\\tUS\\tCensus\\tBureau.\\tEach\\timage\\tis\\tlabeled\\twith\\nthe\\tdigit\\tit\\trepresents.\\tThis\\tset\\thas\\tbeen\\tstudied\\tso\\tmuch\\tthat\\tit\\tis\\toften\\tcalled\\tthe\\t“Hello\\tWorld”\\tof\\nMachine\\tLearning:\\twhenever\\tpeople\\tcome\\tup\\twith\\ta\\tnew\\tclassification\\talgorithm,\\tthey\\tare\\tcurious\\tto\\tsee\\nhow\\tit\\twill\\tperform\\ton\\tMNIST.\\tWhenever\\tsomeone\\tlearns\\tMachine\\tLearning,\\tsooner\\tor\\tlater\\tthey\\ttackle\\nMNIST.\\nScikit-Learn\\tprovides\\tmany\\thelper\\tfunctions\\tto\\tdownload\\tpopular\\tdatasets.\\tMNIST\\tis\\tone\\tof\\tthem.\\tThe\\nfollowing\\tcode\\tfetches\\tthe\\tMNIST\\tdataset:\\n1\\n>>>\\t\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nfetch_mldata\\n>>>\\t\\nmnist\\n\\t\\n=\\n\\t\\nfetch_mldata\\n(\\n\\'MNIST\\toriginal\\'\\n)\\n>>>\\t\\nmnist\\n{\\'COL_NAMES\\':\\t[\\'label\\',\\t\\'data\\'],\\n\\t\\'DESCR\\':\\t\\'mldata.org\\tdataset:\\tmnist-original\\',\\n\\t\\'data\\':\\tarray([[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t\\t...,\\n\\t\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0]],\\tdtype=uint8),\\n\\t\\'target\\':\\tarray([\\t0.,\\t\\t0.,\\t\\t0.,\\t...,\\t\\t9.,\\t\\t9.,\\t\\t9.])}\\nDatasets\\tloaded\\tby\\tScikit-Learn\\tgenerally\\thave\\ta\\tsimilar\\tdictionary\\tstructure\\tincluding:\\nA\\t\\nDESCR\\n\\tkey\\tdescribing\\tthe\\tdataset\\nA\\t\\ndata\\n\\tkey\\tcontaining\\tan\\tarray\\twith\\tone\\trow\\tper\\tinstance\\tand\\tone\\tcolumn\\tper\\tfeature\\nA\\t\\ntarget\\n\\tkey\\tcontaining\\tan\\tarray\\twith\\tthe\\tlabels\\nLet’s\\tlook\\tat\\tthese\\tarrays:\\n>>>\\t\\nX\\n,\\n\\t\\ny\\n\\t\\n=\\n\\t\\nmnist\\n[\\n\"data\"\\n],\\n\\t\\nmnist\\n[\\n\"target\"\\n]\\n>>>\\t\\nX\\n.\\nshape\\n(70000,\\t784)\\n>>>\\t\\ny\\n.\\nshape\\n(70000,)\\nThere\\tare\\t70,000\\timages,\\tand\\teach\\timage\\thas\\t784\\tfeatures.\\tThis\\tis\\tbecause\\teach\\timage\\tis\\t28×28\\tpixels,\\nand\\teach\\tfeature\\tsimply\\trepresents\\tone\\tpixel’s\\tintensity,\\tfrom\\t0\\t(white)\\tto\\t255\\t(black).\\tLet’s\\ttake\\ta\\tpeek\\nat\\tone\\tdigit\\tfrom\\tthe\\tdataset.\\tAll\\tyou\\tneed\\tto\\tdo\\tis\\tgrab\\tan\\tinstance’s\\tfeature\\tvector,\\treshape\\tit\\tto\\ta\\t28×28\\narray,\\tand\\tdisplay\\tit\\tusing\\tMatplotlib’s\\t\\nimshow()\\n\\tfunction:\\n%\\nmatplotlib\\n\\t\\ninline\\nimport\\n\\t\\nmatplotlib\\nimport\\n\\t\\nmatplotlib.pyplot\\n\\t\\nas\\n\\t\\nplt\\nsome_digit\\n\\t\\n=\\n\\t\\nX\\n[\\n36000\\n]\\nsome_digit_image\\n\\t\\n=\\n\\t\\nsome_digit\\n.\\nreshape\\n(\\n28\\n,\\n\\t\\n28\\n)\\nplt\\n.\\nimshow\\n(\\nsome_digit_image\\n,\\n\\t\\ncmap\\n\\t\\n=\\n\\t\\nmatplotlib\\n.\\ncm\\n.\\nbinary\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ninterpolation\\n=\\n\"nearest\"\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 112}), Document(page_content='plt\\n.\\naxis\\n(\\n\"off\"\\n)\\nplt\\n.\\nshow\\n()\\nThis\\tlooks\\tlike\\ta\\t5,\\tand\\tindeed\\tthat’s\\twhat\\tthe\\tlabel\\ttells\\tus:\\n>>>\\t\\ny\\n[\\n36000\\n]\\n5.0\\nFigure\\t3-1\\n\\tshows\\ta\\tfew\\tmore\\timages\\tfrom\\tthe\\tMNIST\\tdataset\\tto\\tgive\\tyou\\ta\\tfeel\\tfor\\tthe\\tcomplexity\\tof\\tthe\\nclassification\\ttask.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 113}), Document(page_content='Figure\\t3-1.\\t\\nA\\tfew\\tdigits\\tfrom\\tthe\\tMNIST\\tdataset\\nBut\\twait!\\tYou\\tshould\\talways\\tcreate\\ta\\t\\ntest\\tset\\tand\\tset\\tit\\taside\\tbefore\\tinspecting\\tthe\\tdata\\tclosely.\\tThe\\nMNIST\\tdataset\\tis\\tactually\\talready\\tsplit\\tinto\\ta\\ttraining\\tset\\t(the\\tfirst\\t60,000\\timages)\\tand\\ta\\ttest\\tset\\t(the\\tlast\\n10,000\\timages):\\nX_train\\n,\\n\\t\\nX_test\\n,\\n\\t\\ny_train\\n,\\n\\t\\ny_test\\n\\t\\n=\\n\\t\\nX\\n[:\\n60000\\n],\\n\\t\\nX\\n[\\n60000\\n:],\\n\\t\\ny\\n[:\\n60000\\n],\\n\\t\\ny\\n[\\n60000\\n:]\\nLet’s\\talso\\tshuffle\\tthe\\t\\ntraining\\tset;\\tthis\\twill\\tguarantee\\tthat\\tall\\tcross-validation\\t\\nfolds\\twill\\tbe\\tsimilar\\t(you\\ndon’t\\twant\\tone\\tfold\\tto\\tbe\\tmissing\\tsome\\tdigits).\\tMoreover,\\tsome\\tlearning\\talgorithms\\tare\\tsensitive\\tto\\tthe\\norder\\tof\\tthe\\ttraining\\tinstances,\\tand\\tthey\\tperform\\tpoorly\\tif\\tthey\\tget\\tmany\\tsimilar\\tinstances\\tin\\ta\\trow.\\nShuffling\\tthe\\tdataset\\tensures\\tthat\\tthis\\twon’t\\t\\nhappen:\\n2\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nshuffle_index\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\npermutation\\n(\\n60000\\n)\\nX_train\\n,\\n\\t\\ny_train\\n\\t\\n=\\n\\t\\nX_train\\n[\\nshuffle_index\\n],\\n\\t\\ny_train\\n[\\nshuffle_index\\n]', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 114}), Document(page_content='Training\\ta\\tBinary\\tClassifier\\nLet’s\\t\\nsimplify\\tthe\\tproblem\\tfor\\tnow\\tand\\tonly\\ttry\\tto\\tidentify\\tone\\tdigit\\t—\\tfor\\texample,\\tthe\\tnumber\\t5.\\tThis\\n“5-detector”\\twill\\tbe\\tan\\texample\\tof\\ta\\t\\nbinary\\tclassifier\\n,\\tcapable\\tof\\tdistinguishing\\tbetween\\tjust\\ttwo\\nclasses,\\t5\\tand\\tnot-5.\\tLet’s\\tcreate\\tthe\\ttarget\\tvectors\\tfor\\tthis\\tclassification\\ttask:\\ny_train_5\\n\\t\\n=\\n\\t\\n(\\ny_train\\n\\t\\n==\\n\\t\\n5\\n)\\n\\t\\t\\n#\\tTrue\\tfor\\tall\\t5s,\\tFalse\\tfor\\tall\\tother\\tdigits.\\ny_test_5\\n\\t\\n=\\n\\t\\n(\\ny_test\\n\\t\\n==\\n\\t\\n5\\n)\\nOkay,\\tnow\\tlet’s\\tpick\\ta\\tclassifier\\tand\\ttrain\\tit.\\tA\\tgood\\tplace\\tto\\tstart\\tis\\twith\\t\\na\\t\\nStochastic\\tGradient\\tDescent\\n(SGD)\\tclassifier,\\tusing\\t\\nScikit-Learn’s\\t\\nSGDClassifier\\n\\tclass.\\tThis\\tclassifier\\thas\\tthe\\tadvantage\\tof\\tbeing\\ncapable\\tof\\thandling\\tvery\\tlarge\\tdatasets\\tefficiently.\\tThis\\tis\\tin\\tpart\\tbecause\\tSGD\\tdeals\\twith\\ttraining\\ninstances\\tindependently,\\tone\\tat\\ta\\ttime\\t(which\\talso\\tmakes\\tSGD\\twell\\tsuited\\tfor\\t\\nonline\\tlearning\\n),\\tas\\twe\\nwill\\tsee\\tlater.\\tLet’s\\tcreate\\tan\\t\\nSGDClassifier\\n\\t\\nand\\ttrain\\tit\\ton\\tthe\\twhole\\ttraining\\tset:\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nSGDClassifier\\nsgd_clf\\n\\t\\n=\\n\\t\\nSGDClassifier\\n(\\nrandom_state\\n=\\n42\\n)\\nsgd_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train_5\\n)\\nTIP\\nThe\\t\\nSGDClassifier\\n\\trelies\\ton\\trandomness\\tduring\\ttraining\\t(hence\\tthe\\tname\\t“stochastic”).\\tIf\\tyou\\twant\\treproducible\\tresults,\\tyou\\nshould\\tset\\tthe\\t\\nrandom_state\\n\\tparameter.\\nNow\\tyou\\tcan\\tuse\\tit\\tto\\tdetect\\timages\\tof\\tthe\\tnumber\\t5:\\n>>>\\t\\nsgd_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([\\tTrue],\\tdtype=bool)\\nThe\\tclassifier\\tguesses\\tthat\\tthis\\timage\\trepresents\\ta\\t5\\t(\\nTrue\\n).\\tLooks\\tlike\\tit\\tguessed\\tright\\tin\\tthis\\tparticular\\ncase!\\tNow,\\tlet’s\\tevaluate\\tthis\\tmodel’s\\tperformance.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 115}), Document(page_content='Performance\\tMeasures\\nEvaluating\\t\\na\\tclassifier\\tis\\toften\\tsignificantly\\ttrickier\\tthan\\tevaluating\\ta\\tregressor,\\tso\\twe\\twill\\tspend\\ta\\tlarge\\npart\\tof\\tthis\\tchapter\\ton\\tthis\\ttopic.\\tThere\\tare\\tmany\\tperformance\\tmeasures\\tavailable,\\tso\\tgrab\\tanother\\tcoffee\\nand\\tget\\tready\\tto\\tlearn\\tmany\\tnew\\tconcepts\\tand\\tacronyms!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 116}), Document(page_content='Measuring\\tAccuracy\\tUsing\\tCross-Validation\\nA\\t\\ngood\\tway\\tto\\tevaluate\\ta\\tmodel\\tis\\tto\\tuse\\tcross-validation,\\tjust\\tas\\tyou\\tdid\\tin\\t\\nChapter\\t2\\n.\\nIMPLEMENTING\\tCROSS-VALIDATION\\nOccasionally\\tyou\\twill\\tneed\\tmore\\tcontrol\\tover\\tthe\\tcross-validation\\tprocess\\tthan\\twhat\\tScikit-Learn\\tprovides\\toff-the-shelf.\\tIn\\tthese\\tcases,\\nyou\\tcan\\timplement\\tcross-validation\\tyourself;\\tit\\tis\\tactually\\tfairly\\tstraightforward.\\tThe\\tfollowing\\tcode\\tdoes\\troughly\\tthe\\tsame\\tthing\\tas\\nScikit-Learn’s\\t\\ncross_val_score()\\n\\tfunction,\\tand\\tprints\\tthe\\t\\nsame\\tresult:\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\nStratifiedKFold\\nfrom\\n\\t\\nsklearn.base\\n\\t\\nimport\\n\\t\\nclone\\nskfolds\\n\\t\\n=\\n\\t\\nStratifiedKFold\\n(\\nn_splits\\n=\\n3\\n,\\n\\t\\nrandom_state\\n=\\n42\\n)\\nfor\\n\\t\\ntrain_index\\n,\\n\\t\\ntest_index\\n\\t\\nin\\n\\t\\nskfolds\\n.\\nsplit\\n(\\nX_train\\n,\\n\\t\\ny_train_5\\n):\\n\\t\\t\\t\\t\\nclone_clf\\n\\t\\n=\\n\\t\\nclone\\n(\\nsgd_clf\\n)\\n\\t\\t\\t\\t\\nX_train_folds\\n\\t\\n=\\n\\t\\nX_train\\n[\\ntrain_index\\n]\\n\\t\\t\\t\\t\\ny_train_folds\\n\\t\\n=\\n\\t\\n(\\ny_train_5\\n[\\ntrain_index\\n])\\n\\t\\t\\t\\t\\nX_test_fold\\n\\t\\n=\\n\\t\\nX_train\\n[\\ntest_index\\n]\\n\\t\\t\\t\\t\\ny_test_fold\\n\\t\\n=\\n\\t\\n(\\ny_train_5\\n[\\ntest_index\\n])\\n\\t\\t\\t\\t\\nclone_clf\\n.\\nfit\\n(\\nX_train_folds\\n,\\n\\t\\ny_train_folds\\n)\\n\\t\\t\\t\\t\\ny_pred\\n\\t\\n=\\n\\t\\nclone_clf\\n.\\npredict\\n(\\nX_test_fold\\n)\\n\\t\\t\\t\\t\\nn_correct\\n\\t\\n=\\n\\t\\nsum\\n(\\ny_pred\\n\\t\\n==\\n\\t\\ny_test_fold\\n)\\n\\t\\t\\t\\t\\nprint\\n(\\nn_correct\\n\\t\\n/\\n\\t\\nlen\\n(\\ny_pred\\n))\\n\\t\\t\\n#\\tprints\\t0.9502,\\t0.96565\\tand\\t0.96495\\nThe\\t\\nStratifiedKFold\\n\\tclass\\tperforms\\t\\nstratified\\tsampling\\t(as\\texplained\\tin\\t\\nChapter\\t2\\n)\\tto\\tproduce\\tfolds\\tthat\\tcontain\\ta\\trepresentative\\tratio\\nof\\teach\\tclass.\\tAt\\teach\\titeration\\tthe\\tcode\\tcreates\\ta\\tclone\\tof\\tthe\\tclassifier,\\ttrains\\tthat\\tclone\\ton\\tthe\\ttraining\\tfolds,\\tand\\tmakes\\tpredictions\\ton\\nthe\\ttest\\tfold.\\tThen\\tit\\tcounts\\tthe\\tnumber\\tof\\tcorrect\\tpredictions\\tand\\toutputs\\tthe\\tratio\\tof\\tcorrect\\tpredictions.\\nLet’s\\tuse\\tthe\\t\\ncross_val_score()\\n\\tfunction\\tto\\tevaluate\\tyour\\t\\nSGDClassifier\\n\\tmodel\\tusing\\t\\nK-fold\\tcross-\\nvalidation,\\twith\\tthree\\tfolds.\\tRemember\\tthat\\tK-fold\\tcross-validation\\tmeans\\tsplitting\\tthe\\ttraining\\tset\\tinto\\nK-folds\\t(in\\tthis\\tcase,\\tthree),\\tthen\\tmaking\\tpredictions\\tand\\tevaluating\\tthem\\ton\\teach\\tfold\\tusing\\ta\\tmodel\\ntrained\\ton\\tthe\\tremaining\\tfolds\\t(see\\t\\nChapter\\t2\\n):\\n>>>\\t\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\ncross_val_score\\n>>>\\t\\ncross_val_score\\n(\\nsgd_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train_5\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\nscoring\\n=\\n\"accuracy\"\\n)\\narray([\\t0.9502\\t,\\t\\t0.96565,\\t\\t0.96495])\\nWow!\\tAbove\\t95%\\t\\naccuracy\\n\\t(ratio\\tof\\tcorrect\\tpredictions)\\ton\\tall\\tcross-validation\\tfolds?\\t\\nThis\\tlooks\\namazing,\\tdoesn’t\\tit?\\tWell,\\tbefore\\tyou\\tget\\ttoo\\texcited,\\tlet’s\\tlook\\tat\\ta\\tvery\\tdumb\\tclassifier\\tthat\\tjust\\nclassifies\\n\\tevery\\tsingle\\timage\\tin\\tthe\\t“not-5”\\tclass:\\nfrom\\n\\t\\nsklearn.base\\n\\t\\nimport\\n\\t\\nBaseEstimator\\nclass\\n\\t\\nNever5Classifier\\n(\\nBaseEstimator\\n):\\n\\t\\t\\t\\t\\ndef\\n\\t\\nfit\\n(\\nself\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npass\\n\\t\\t\\t\\t\\ndef\\n\\t\\npredict\\n(\\nself\\n,\\n\\t\\nX\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nnp\\n.\\nzeros\\n((\\nlen\\n(\\nX\\n),\\n\\t\\n1\\n),\\n\\t\\ndtype\\n=\\nbool\\n)\\nCan\\tyou\\tguess\\tthis\\tmodel’s\\taccuracy?\\tLet’s\\tfind\\tout:\\n>>>\\t\\nnever_5_clf\\n\\t\\n=\\n\\t\\nNever5Classifier\\n()\\n>>>\\t\\ncross_val_score\\n(\\nnever_5_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train_5\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\nscoring\\n=\\n\"accuracy\"\\n)\\narray([\\t0.909\\t\\t,\\t\\t0.90715,\\t\\t0.9128\\t])', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 117}), Document(page_content='That’s\\tright,\\tit\\thas\\tover\\t90%\\taccuracy!\\tThis\\tis\\tsimply\\tbecause\\tonly\\tabout\\t10%\\tof\\tthe\\timages\\tare\\t5s,\\tso\\tif\\nyou\\talways\\tguess\\tthat\\tan\\timage\\tis\\t\\nnot\\n\\ta\\t5,\\tyou\\twill\\tbe\\tright\\tabout\\t90%\\tof\\tthe\\ttime.\\tBeats\\tNostradamus.\\nThis\\tdemonstrates\\twhy\\taccuracy\\tis\\tgenerally\\tnot\\tthe\\tpreferred\\tperformance\\tmeasure\\tfor\\tclassifiers,\\nespecially\\twhen\\tyou\\tare\\t\\ndealing\\t\\nwith\\t\\nskewed\\tdatasets\\n\\t(i.e.,\\twhen\\tsome\\tclasses\\tare\\tmuch\\tmore\\tfrequent\\nthan\\tothers).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 118}), Document(page_content='Confusion\\tMatrix\\nA\\t\\nmuch\\tbetter\\tway\\tto\\tevaluate\\tthe\\tperformance\\tof\\ta\\tclassifier\\tis\\tto\\tlook\\tat\\tthe\\t\\nconfusion\\tmatrix\\n.\\tThe\\ngeneral\\tidea\\tis\\tto\\tcount\\tthe\\tnumber\\tof\\ttimes\\tinstances\\tof\\tclass\\tA\\tare\\tclassified\\tas\\tclass\\tB.\\tFor\\texample,\\tto\\nknow\\tthe\\tnumber\\tof\\ttimes\\tthe\\tclassifier\\tconfused\\timages\\tof\\t5s\\twith\\t3s,\\tyou\\twould\\tlook\\tin\\tthe\\t5\\nth\\n\\trow\\tand\\n3\\nrd\\n\\tcolumn\\tof\\tthe\\tconfusion\\tmatrix.\\nTo\\tcompute\\tthe\\tconfusion\\tmatrix,\\tyou\\tfirst\\tneed\\tto\\thave\\ta\\tset\\tof\\tpredictions,\\t\\nso\\tthey\\tcan\\tbe\\tcompared\\tto\\nthe\\tactual\\ttargets.\\tYou\\tcould\\tmake\\tpredictions\\ton\\tthe\\ttest\\tset,\\tbut\\tlet’s\\tkeep\\tit\\tuntouched\\tfor\\tnow\\n(remember\\tthat\\tyou\\twant\\tto\\tuse\\tthe\\ttest\\tset\\tonly\\tat\\tthe\\tvery\\tend\\tof\\tyour\\tproject,\\tonce\\tyou\\thave\\ta\\tclassifier\\nthat\\tyou\\tare\\tready\\tto\\tlaunch).\\tInstead,\\tyou\\tcan\\tuse\\tthe\\t\\ncross_val_predict()\\n\\t\\nfunction:\\nfrom\\tsklearn.model_selection\\timport\\tcross_val_predict\\ny_train_pred\\t=\\tcross_val_predict(sgd_clf,\\tX_train,\\ty_train_5,\\tcv=3)\\nJust\\tlike\\tthe\\t\\ncross_val_score()\\n\\t\\nfunction,\\t\\ncross_val_predict()\\n\\tperforms\\tK-fold\\tcross-validation,\\nbut\\tinstead\\tof\\treturning\\tthe\\tevaluation\\tscores,\\tit\\treturns\\tthe\\tpredictions\\tmade\\ton\\teach\\ttest\\tfold.\\tThis\\tmeans\\nthat\\tyou\\tget\\ta\\tclean\\tprediction\\tfor\\teach\\tinstance\\tin\\tthe\\ttraining\\tset\\t(“clean”\\tmeaning\\tthat\\tthe\\tprediction\\tis\\nmade\\tby\\ta\\tmodel\\tthat\\tnever\\tsaw\\tthe\\tdata\\tduring\\ttraining).\\nNow\\tyou\\tare\\tready\\tto\\tget\\tthe\\t\\nconfusion\\tmatrix\\tusing\\tthe\\t\\nconfusion_matrix()\\n\\tfunction.\\tJust\\tpass\\tit\\tthe\\ntarget\\tclasses\\t(\\ny_train_5\\n)\\tand\\tthe\\tpredicted\\tclasses\\t(\\ny_train_pred\\n):\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nconfusion_matrix\\n>>>\\t\\nconfusion_matrix\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred\\n)\\narray([[53272,\\t\\t1307],\\n\\t\\t\\t\\t\\t\\t\\t[\\t1077,\\t\\t4344]])\\nEach\\trow\\tin\\ta\\tconfusion\\tmatrix\\trepresents\\tan\\t\\nactual\\tclass\\n,\\twhile\\teach\\tcolumn\\trepresents\\ta\\t\\npredicted\\nclass\\n.\\t\\nThe\\tfirst\\trow\\tof\\tthis\\tmatrix\\tconsiders\\tnon-5\\timages\\t(the\\t\\nnegative\\tclass\\n):\\t53,272\\tof\\tthem\\twere\\ncorrectly\\tclassified\\tas\\tnon-5s\\t(they\\tare\\tcalled\\t\\ntrue\\tnegatives\\n),\\twhile\\tthe\\tremaining\\t1,307\\twere\\twrongly\\nclassified\\tas\\t5s\\t(\\nfalse\\tpositives\\n).\\tThe\\tsecond\\trow\\tconsiders\\tthe\\timages\\tof\\t5s\\t(the\\t\\npositive\\tclass\\n):\\t1,077\\nwere\\twrongly\\tclassified\\tas\\tnon-5s\\t(\\nfalse\\tnegatives\\n),\\twhile\\tthe\\tremaining\\t4,344\\twere\\tcorrectly\\tclassified\\nas\\t5s\\t(\\ntrue\\tpositives\\n).\\tA\\tperfect\\tclassifier\\twould\\thave\\tonly\\ttrue\\tpositives\\tand\\ttrue\\tnegatives,\\tso\\tits\\nconfusion\\tmatrix\\twould\\thave\\tnonzero\\tvalues\\tonly\\ton\\tits\\tmain\\tdiagonal\\t(top\\tleft\\tto\\tbottom\\tright):\\n>>>\\t\\nconfusion_matrix\\n(\\ny_train_5\\n,\\n\\t\\ny_train_perfect_predictions\\n)\\narray([[54579,\\t\\t\\t\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t\\t\\t0,\\t5421]])\\nThe\\tconfusion\\tmatrix\\tgives\\tyou\\ta\\tlot\\tof\\tinformation,\\tbut\\tsometimes\\tyou\\tmay\\tprefer\\ta\\tmore\\tconcise\\tmetric.\\nAn\\tinteresting\\tone\\tto\\tlook\\tat\\tis\\tthe\\taccuracy\\tof\\tthe\\tpositive\\tpredictions;\\tthis\\tis\\tcalled\\t\\nthe\\t\\nprecision\\n\\tof\\tthe\\nclassifier\\t(\\nEquation\\t3-1\\n).\\nEquation\\t3-1.\\t\\nPrecision', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 119}), Document(page_content='TP\\tis\\tthe\\tnumber\\tof\\ttrue\\tpositives,\\tand\\tFP\\tis\\tthe\\tnumber\\tof\\tfalse\\tpositives.\\nA\\ttrivial\\tway\\tto\\thave\\tperfect\\tprecision\\tis\\tto\\tmake\\tone\\tsingle\\tpositive\\tprediction\\tand\\tensure\\tit\\tis\\tcorrect\\n(precision\\t=\\t1/1\\t=\\t100%).\\tThis\\twould\\tnot\\tbe\\tvery\\tuseful\\tsince\\tthe\\tclassifier\\twould\\tignore\\tall\\tbut\\tone\\npositive\\tinstance.\\tSo\\tprecision\\tis\\ttypically\\tused\\talong\\twith\\tanother\\tmetric\\tnamed\\t\\nrecall\\n,\\t\\nalso\\tcalled\\nsensitivity\\n\\tor\\t\\ntrue\\tpositive\\trate\\n\\t(\\nTPR\\n):\\tthis\\tis\\tthe\\tratio\\tof\\tpositive\\tinstances\\tthat\\tare\\tcorrectly\\tdetected\\tby\\nthe\\tclassifier\\t(\\nEquation\\t3-2\\n).\\nEquation\\t3-2.\\t\\nRecall\\nFN\\tis\\tof\\tcourse\\tthe\\tnumber\\tof\\tfalse\\tnegatives.\\nIf\\tyou\\tare\\tconfused\\tabout\\tthe\\tconfusion\\tmatrix,\\t\\nFigure\\t3-2\\n\\tmay\\t\\nhelp.\\nFigure\\t3-2.\\t\\nAn\\tillustrated\\tconfusion\\tmatrix', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 120}), Document(page_content='Precision\\tand\\tRecall\\nScikit-Learn\\t\\nprovides\\tseveral\\tfunctions\\tto\\tcompute\\tclassifier\\tmetrics,\\tincluding\\t\\nprecision\\tand\\trecall:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nprecision_score\\n,\\n\\t\\nrecall_score\\n>>>\\t\\nprecision_score\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred\\n)\\n\\t\\n#\\t==\\t4344\\t/\\t(4344\\t+\\t1307)\\n0.76871350203503808\\n>>>\\t\\nrecall_score\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred\\n)\\n\\t\\n#\\t==\\t4344\\t/\\t(4344\\t+\\t1077)\\n0.80132816823464303\\nNow\\tyour\\t5-detector\\tdoes\\tnot\\tlook\\tas\\tshiny\\tas\\tit\\tdid\\twhen\\tyou\\tlooked\\tat\\tits\\taccuracy.\\tWhen\\tit\\tclaims\\tan\\nimage\\trepresents\\ta\\t5,\\tit\\tis\\tcorrect\\tonly\\t77%\\tof\\tthe\\ttime.\\tMoreover,\\tit\\tonly\\tdetects\\t80%\\tof\\tthe\\t5s.\\nIt\\tis\\toften\\tconvenient\\tto\\tcombine\\tprecision\\tand\\trecall\\tinto\\ta\\tsingle\\tmetric\\tcalled\\tthe\\t\\nF\\n1\\n\\tscore\\n,\\tin\\nparticular\\tif\\tyou\\tneed\\ta\\tsimple\\tway\\tto\\tcompare\\ttwo\\tclassifiers.\\tThe\\t\\nF\\n1\\n\\tscore\\tis\\t\\nthe\\t\\nharmonic\\tmean\\n\\tof\\nprecision\\tand\\trecall\\t(\\nEquation\\t3-3\\n).\\tWhereas\\tthe\\tregular\\tmean\\ttreats\\tall\\tvalues\\tequally,\\tthe\\tharmonic\\nmean\\tgives\\tmuch\\tmore\\tweight\\tto\\tlow\\tvalues.\\tAs\\ta\\tresult,\\tthe\\tclassifier\\twill\\tonly\\tget\\ta\\thigh\\tF\\n1\\n\\tscore\\tif\\nboth\\trecall\\tand\\tprecision\\tare\\thigh.\\nEquation\\t3-3.\\t\\nF\\n1\\n\\tscore\\nTo\\tcompute\\t\\nthe\\tF\\n1\\n\\tscore,\\tsimply\\tcall\\tthe\\t\\nf1_score()\\n\\tfunction:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nf1_score\\n>>>\\t\\nf1_score\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred\\n)\\n0.78468208092485547\\nThe\\tF\\n1\\n\\tscore\\tfavors\\tclassifiers\\tthat\\thave\\tsimilar\\tprecision\\tand\\trecall.\\tThis\\tis\\tnot\\talways\\twhat\\tyou\\twant:\\nin\\tsome\\tcontexts\\tyou\\tmostly\\tcare\\tabout\\tprecision,\\tand\\tin\\tother\\tcontexts\\tyou\\treally\\tcare\\tabout\\trecall.\\tFor\\nexample,\\tif\\tyou\\ttrained\\ta\\tclassifier\\tto\\tdetect\\tvideos\\tthat\\tare\\tsafe\\tfor\\tkids,\\tyou\\twould\\tprobably\\tprefer\\ta\\nclassifier\\tthat\\trejects\\tmany\\tgood\\tvideos\\t(low\\trecall)\\tbut\\tkeeps\\tonly\\tsafe\\tones\\t(high\\tprecision),\\trather\\tthan\\na\\tclassifier\\tthat\\thas\\ta\\tmuch\\thigher\\trecall\\tbut\\tlets\\ta\\tfew\\treally\\tbad\\tvideos\\tshow\\tup\\tin\\tyour\\tproduct\\t(in\\tsuch\\ncases,\\tyou\\tmay\\teven\\twant\\tto\\tadd\\ta\\thuman\\tpipeline\\tto\\tcheck\\tthe\\tclassifier’s\\tvideo\\tselection).\\tOn\\tthe\\tother\\nhand,\\tsuppose\\tyou\\ttrain\\ta\\tclassifier\\tto\\tdetect\\tshoplifters\\ton\\tsurveillance\\timages:\\tit\\tis\\tprobably\\tfine\\tif\\tyour\\nclassifier\\thas\\tonly\\t30%\\tprecision\\tas\\tlong\\tas\\tit\\thas\\t99%\\trecall\\t(sure,\\tthe\\tsecurity\\tguards\\twill\\tget\\ta\\tfew\\nfalse\\talerts,\\tbut\\talmost\\tall\\tshoplifters\\twill\\tget\\tcaught).\\nUnfortunately,\\tyou\\tcan’t\\thave\\tit\\tboth\\tways:\\tincreasing\\tprecision\\treduces\\trecall,\\tand\\tvice\\tversa.\\tThis\\t\\nis\\ncalled\\tthe\\t\\nprecision/recall\\ttradeoff\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 121}), Document(page_content='Precision/Recall\\tTradeoff\\nTo\\t\\nunderstand\\tthis\\ttradeoff,\\tlet’s\\tlook\\tat\\thow\\tthe\\n\\t\\nSGDClassifier\\n\\tmakes\\tits\\tclassification\\tdecisions.\\tFor\\neach\\tinstance,\\tit\\tcomputes\\ta\\tscore\\tbased\\ton\\ta\\t\\ndecision\\tfunction\\n,\\t\\nand\\tif\\tthat\\tscore\\tis\\tgreater\\tthan\\ta\\nthreshold,\\tit\\tassigns\\tthe\\tinstance\\tto\\tthe\\tpositive\\tclass,\\tor\\telse\\tit\\tassigns\\tit\\tto\\tthe\\tnegative\\tclass.\\t\\nFigure\\t3-3\\nshows\\ta\\tfew\\tdigits\\tpositioned\\tfrom\\tthe\\tlowest\\tscore\\ton\\tthe\\tleft\\tto\\tthe\\thighest\\tscore\\ton\\tthe\\tright.\\tSuppose\\nthe\\t\\ndecision\\tthreshold\\n\\t\\nis\\tpositioned\\tat\\tthe\\tcentral\\tarrow\\t(between\\tthe\\ttwo\\t5s):\\tyou\\twill\\tfind\\t4\\ttrue\\npositives\\t(actual\\t5s)\\ton\\tthe\\tright\\tof\\tthat\\tthreshold,\\tand\\tone\\tfalse\\tpositive\\t(actually\\ta\\t6).\\tTherefore,\\twith\\nthat\\tthreshold,\\tthe\\tprecision\\tis\\t80%\\t(4\\tout\\tof\\t5).\\tBut\\tout\\tof\\t6\\tactual\\t5s,\\tthe\\tclassifier\\tonly\\tdetects\\t4,\\tso\\tthe\\nrecall\\tis\\t67%\\t(4\\tout\\tof\\t6).\\tNow\\tif\\tyou\\traise\\tthe\\tthreshold\\t(move\\tit\\tto\\tthe\\tarrow\\ton\\tthe\\tright),\\tthe\\tfalse\\npositive\\t(the\\t6)\\tbecomes\\ta\\ttrue\\tnegative,\\tthereby\\tincreasing\\tprecision\\t(up\\tto\\t100%\\tin\\tthis\\tcase),\\tbut\\tone\\ntrue\\tpositive\\tbecomes\\ta\\tfalse\\tnegative,\\tdecreasing\\trecall\\tdown\\tto\\t50%.\\tConversely,\\tlowering\\tthe\\nthreshold\\tincreases\\trecall\\tand\\treduces\\tprecision.\\nFigure\\t3-3.\\t\\nDecision\\tthreshold\\tand\\tprecision/recall\\ttradeoff\\nScikit-Learn\\tdoes\\tnot\\tlet\\tyou\\tset\\tthe\\tthreshold\\tdirectly,\\tbut\\tit\\tdoes\\tgive\\tyou\\taccess\\tto\\tthe\\tdecision\\tscores\\nthat\\tit\\tuses\\tto\\tmake\\tpredictions.\\tInstead\\tof\\tcalling\\tthe\\tclassifier’s\\t\\npredict()\\n\\tmethod,\\tyou\\tcan\\tcall\\tits\\ndecision_function()\\n\\tmethod,\\twhich\\treturns\\ta\\tscore\\tfor\\teach\\tinstance,\\tand\\tthen\\tmake\\tpredictions\\tbased\\non\\tthose\\tscores\\tusing\\tany\\tthreshold\\tyou\\twant:\\n>>>\\t\\ny_scores\\n\\t\\n=\\n\\t\\nsgd_clf\\n.\\ndecision_function\\n([\\nsome_digit\\n])\\n>>>\\t\\ny_scores\\narray([\\t161855.74572176])\\n>>>\\t\\nthreshold\\n\\t\\n=\\n\\t\\n0\\n>>>\\t\\ny_some_digit_pred\\n\\t\\n=\\n\\t\\n(\\ny_scores\\n\\t\\n>\\n\\t\\nthreshold\\n)\\narray([\\tTrue],\\tdtype=bool)\\nThe\\t\\nSGDClassifier\\n\\tuses\\ta\\tthreshold\\tequal\\tto\\t0,\\tso\\tthe\\tprevious\\tcode\\treturns\\tthe\\tsame\\tresult\\tas\\tthe\\npredict()\\n\\tmethod\\t(i.e.,\\t\\nTrue\\n).\\tLet’s\\traise\\tthe\\tthreshold:\\n>>>\\t\\nthreshold\\n\\t\\n=\\n\\t\\n200000\\n>>>\\t\\ny_some_digit_pred\\n\\t\\n=\\n\\t\\n(\\ny_scores\\n\\t\\n>\\n\\t\\nthreshold\\n)\\n>>>\\t\\ny_some_digit_pred\\narray([False],\\tdtype=bool)\\nThis\\tconfirms\\tthat\\traising\\tthe\\tthreshold\\tdecreases\\trecall.\\tThe\\timage\\tactually\\trepresents\\ta\\t5,\\tand\\tthe\\nclassifier\\tdetects\\tit\\twhen\\tthe\\tthreshold\\tis\\t0,\\tbut\\tit\\tmisses\\tit\\twhen\\tthe\\tthreshold\\tis\\t\\nincreased\\tto\\t200,000.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 122}), Document(page_content='So\\thow\\tcan\\tyou\\tdecide\\twhich\\tthreshold\\tto\\tuse?\\tFor\\tthis\\tyou\\twill\\tfirst\\tneed\\tto\\tget\\tthe\\tscores\\tof\\tall\\ninstances\\tin\\tthe\\ttraining\\tset\\tusing\\tthe\\t\\ncross_val_predict()\\n\\tfunction\\tagain,\\tbut\\tthis\\ttime\\tspecifying\\tthat\\nyou\\twant\\tit\\tto\\treturn\\tdecision\\tscores\\t\\ninstead\\tof\\tpredictions:\\ny_scores\\n\\t\\n=\\n\\t\\ncross_val_predict\\n(\\nsgd_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train_5\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmethod\\n=\\n\"decision_function\"\\n)\\nNow\\twith\\tthese\\tscores\\tyou\\tcan\\tcompute\\tprecision\\tand\\trecall\\tfor\\tall\\tpossible\\tthresholds\\tusing\\tthe\\nprecision_recall_curve()\\n\\t\\nfunction:\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nprecision_recall_curve\\nprecisions\\n,\\n\\t\\nrecalls\\n,\\n\\t\\nthresholds\\n\\t\\n=\\n\\t\\nprecision_recall_curve\\n(\\ny_train_5\\n,\\n\\t\\ny_scores\\n)\\nFinally,\\tyou\\tcan\\tplot\\tprecision\\tand\\trecall\\tas\\tfunctions\\tof\\tthe\\tthreshold\\tvalue\\tusing\\tMatplotlib\\t(\\nFigure\\t3-\\n4\\n):\\ndef\\n\\t\\nplot_precision_recall_vs_threshold\\n(\\nprecisions\\n,\\n\\t\\nrecalls\\n,\\n\\t\\nthresholds\\n):\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n(\\nthresholds\\n,\\n\\t\\nprecisions\\n[:\\n-\\n1\\n],\\n\\t\\n\"b--\"\\n,\\n\\t\\nlabel\\n=\\n\"Precision\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n(\\nthresholds\\n,\\n\\t\\nrecalls\\n[:\\n-\\n1\\n],\\n\\t\\n\"g-\"\\n,\\n\\t\\nlabel\\n=\\n\"Recall\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nxlabel\\n(\\n\"Threshold\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nlegend\\n(\\nloc\\n=\\n\"upper\\tleft\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nylim\\n([\\n0\\n,\\n\\t\\n1\\n])\\nplot_precision_recall_vs_threshold\\n(\\nprecisions\\n,\\n\\t\\nrecalls\\n,\\n\\t\\nthresholds\\n)\\nplt\\n.\\nshow\\n()\\nFigure\\t3-4.\\t\\nPrecision\\tand\\trecall\\tversus\\tthe\\tdecision\\tthreshold\\nNOTE\\nYou\\tmay\\twonder\\twhy\\tthe\\tprecision\\tcurve\\tis\\tbumpier\\tthan\\tthe\\trecall\\tcurve\\tin\\t\\nFigure\\t3-4\\n.\\tThe\\treason\\tis\\tthat\\tprecision\\tmay\\nsometimes\\tgo\\tdown\\twhen\\tyou\\traise\\tthe\\tthreshold\\t(although\\tin\\tgeneral\\tit\\twill\\tgo\\tup).\\tTo\\tunderstand\\twhy,\\tlook\\tback\\tat\\t\\nFigure\\t3-3\\nand\\tnotice\\twhat\\thappens\\twhen\\tyou\\tstart\\tfrom\\tthe\\tcentral\\tthreshold\\tand\\tmove\\tit\\tjust\\tone\\tdigit\\tto\\tthe\\tright:\\tprecision\\tgoes\\tfrom\\t4/5\\n(80%)\\tdown\\tto\\t3/4\\t(75%).\\tOn\\tthe\\tother\\thand,\\trecall\\tcan\\tonly\\tgo\\tdown\\twhen\\tthe\\tthreshold\\tis\\tincreased,\\twhich\\texplains\\twhy\\tits\\ncurve\\tlooks\\tsmooth.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 123}), Document(page_content='Now\\tyou\\tcan\\tsimply\\tselect\\tthe\\tthreshold\\tvalue\\tthat\\tgives\\tyou\\tthe\\tbest\\tprecision/recall\\ttradeoff\\tfor\\tyour\\ntask.\\tAnother\\tway\\tto\\tselect\\ta\\tgood\\tprecision/recall\\ttradeoff\\tis\\tto\\tplot\\tprecision\\tdirectly\\tagainst\\trecall,\\tas\\nshown\\tin\\t\\nFigure\\t3-5\\n.\\nFigure\\t3-5.\\t\\nPrecision\\tversus\\trecall\\nYou\\tcan\\tsee\\tthat\\tprecision\\treally\\tstarts\\tto\\tfall\\tsharply\\taround\\t80%\\trecall.\\tYou\\twill\\tprobably\\twant\\tto\\nselect\\ta\\tprecision/recall\\ttradeoff\\tjust\\tbefore\\tthat\\tdrop\\t—\\tfor\\texample,\\tat\\taround\\t60%\\trecall.\\tBut\\tof\\ncourse\\tthe\\tchoice\\tdepends\\ton\\tyour\\tproject.\\nSo\\tlet’s\\tsuppose\\tyou\\tdecide\\tto\\taim\\tfor\\t90%\\tprecision.\\tYou\\tlook\\tup\\tthe\\tfirst\\tplot\\t(zooming\\tin\\ta\\tbit)\\tand\\nfind\\tthat\\tyou\\tneed\\tto\\tuse\\ta\\tthreshold\\tof\\tabout\\t70,000.\\tTo\\tmake\\tpredictions\\t(on\\tthe\\ttraining\\tset\\tfor\\tnow),\\ninstead\\tof\\tcalling\\tthe\\tclassifier’s\\t\\npredict()\\n\\tmethod,\\tyou\\tcan\\tjust\\trun\\tthis\\tcode:\\ny_train_pred_90\\n\\t\\n=\\n\\t\\n(\\ny_scores\\n\\t\\n>\\n\\t\\n70000\\n)\\nLet’s\\tcheck\\tthese\\tpredictions’\\tprecision\\tand\\trecall:\\n>>>\\t\\nprecision_score\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred_90\\n)\\n0.86592051164915484\\n>>>\\t\\nrecall_score\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred_90\\n)\\n0.69931746910164172\\nGreat,\\tyou\\thave\\ta\\t90%\\tprecision\\tclassifier\\t\\n(or\\tclose\\tenough)!\\tAs\\tyou\\tcan\\tsee,\\tit\\tis\\tfairly\\teasy\\tto\\tcreate\\ta\\nclassifier\\twith\\tvirtually\\tany\\tprecision\\tyou\\twant:\\tjust\\tset\\ta\\thigh\\tenough\\tthreshold,\\tand\\tyou’re\\tdone.\\tHmm,\\nnot\\tso\\tfast.\\tA\\thigh-precision\\tclassifier\\tis\\tnot\\tvery\\tuseful\\tif\\tits\\t\\nrecall\\tis\\ttoo\\tlow!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 124}), Document(page_content='TIP\\nIf\\tsomeone\\tsays\\t“let’s\\treach\\t99%\\tprecision,”\\tyou\\tshould\\task,\\t“at\\twhat\\trecall?”', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 125}), Document(page_content=\"The\\tROC\\tCurve\\nThe\\n\\t\\nreceiver\\toperating\\tcharacteristic\\n\\t(ROC)\\tcurve\\tis\\tanother\\tcommon\\ttool\\tused\\twith\\tbinary\\tclassifiers.\\nIt\\tis\\tvery\\tsimilar\\tto\\tthe\\tprecision/recall\\tcurve,\\tbut\\tinstead\\tof\\tplotting\\tprecision\\tversus\\trecall,\\tthe\\tROC\\ncurve\\tplots\\tthe\\t\\ntrue\\tpositive\\trate\\n\\t\\n(another\\tname\\tfor\\trecall)\\tagainst\\tthe\\t\\nfalse\\tpositive\\trate\\n.\\t\\nThe\\tFPR\\tis\\tthe\\nratio\\tof\\tnegative\\tinstances\\tthat\\tare\\tincorrectly\\tclassified\\tas\\tpositive.\\tIt\\tis\\tequal\\tto\\tone\\tminus\\tthe\\t\\ntrue\\nnegative\\trate\\n,\\t\\nwhich\\tis\\tthe\\tratio\\tof\\tnegative\\tinstances\\tthat\\tare\\tcorrectly\\tclassified\\tas\\tnegative.\\tThe\\tTNR\\nis\\talso\\tcalled\\t\\nspecificity\\n.\\t\\nHence\\tthe\\tROC\\tcurve\\tplots\\t\\nsensitivity\\n\\t(recall)\\tversus\\t\\n1\\t–\\t\\nspecificity\\n.\\nTo\\tplot\\tthe\\tROC\\tcurve,\\t\\nyou\\tfirst\\tneed\\tto\\tcompute\\tthe\\tTPR\\tand\\tFPR\\tfor\\tvarious\\tthreshold\\tvalues,\\tusing\\tthe\\nroc_curve()\\n\\tfunction:\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nroc_curve\\nfpr\\n,\\n\\t\\ntpr\\n,\\n\\t\\nthresholds\\n\\t\\n=\\n\\t\\nroc_curve\\n(\\ny_train_5\\n,\\n\\t\\ny_scores\\n)\\nThen\\tyou\\tcan\\tplot\\tthe\\tFPR\\tagainst\\tthe\\tTPR\\tusing\\t\\nMatplotlib.\\tThis\\tcode\\tproduces\\tthe\\tplot\\tin\\t\\nFigure\\t3-6\\n:\\ndef\\n\\t\\nplot_roc_curve\\n(\\nfpr\\n,\\n\\t\\ntpr\\n,\\n\\t\\nlabel\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n(\\nfpr\\n,\\n\\t\\ntpr\\n,\\n\\t\\nlinewidth\\n=\\n2\\n,\\n\\t\\nlabel\\n=\\nlabel\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n([\\n0\\n,\\n\\t\\n1\\n],\\n\\t\\n[\\n0\\n,\\n\\t\\n1\\n],\\n\\t\\n'k--'\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\naxis\\n([\\n0\\n,\\n\\t\\n1\\n,\\n\\t\\n0\\n,\\n\\t\\n1\\n])\\n\\t\\t\\t\\t\\nplt\\n.\\nxlabel\\n(\\n'False\\tPositive\\tRate'\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nylabel\\n(\\n'True\\tPositive\\tRate'\\n)\\nplot_roc_curve\\n(\\nfpr\\n,\\n\\t\\ntpr\\n)\\nplt\\n.\\nshow\\n()\\nFigure\\t3-6.\\t\\nROC\\tcurve\\nOnce\\tagain\\tthere\\tis\\ta\\ttradeoff:\\tthe\\thigher\\tthe\\trecall\\t(TPR),\\tthe\\tmore\\tfalse\\tpositives\\t(FPR)\\tthe\\tclassifier\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 126}), Document(page_content='produces.\\tThe\\tdotted\\tline\\trepresents\\tthe\\tROC\\tcurve\\tof\\ta\\tpurely\\trandom\\tclassifier;\\ta\\tgood\\tclassifier\\tstays\\nas\\tfar\\taway\\tfrom\\tthat\\tline\\tas\\tpossible\\t(toward\\tthe\\ttop-left\\tcorner).\\nOne\\tway\\tto\\tcompare\\tclassifiers\\tis\\tto\\tmeasure\\tthe\\t\\narea\\tunder\\tthe\\tcurve\\n\\t(AUC).\\t\\nA\\tperfect\\tclassifier\\twill\\nhave\\ta\\t\\nROC\\tAUC\\n\\tequal\\tto\\t1,\\twhereas\\ta\\tpurely\\trandom\\tclassifier\\twill\\thave\\ta\\tROC\\tAUC\\tequal\\tto\\t0.5.\\nScikit-Learn\\tprovides\\ta\\tfunction\\t\\nto\\tcompute\\tthe\\tROC\\tAUC:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nroc_auc_score\\n>>>\\t\\nroc_auc_score\\n(\\ny_train_5\\n,\\n\\t\\ny_scores\\n)\\n0.96244965559671547\\nTIP\\nSince\\tthe\\tROC\\tcurve\\tis\\tso\\tsimilar\\tto\\tthe\\t\\nprecision/recall\\t(or\\tPR)\\tcurve,\\tyou\\tmay\\twonder\\thow\\tto\\tdecide\\twhich\\tone\\tto\\tuse.\\tAs\\ta\\nrule\\tof\\tthumb,\\tyou\\tshould\\tprefer\\tthe\\tPR\\tcurve\\twhenever\\tthe\\tpositive\\tclass\\tis\\trare\\tor\\twhen\\tyou\\tcare\\tmore\\tabout\\tthe\\tfalse\\npositives\\tthan\\tthe\\tfalse\\tnegatives,\\tand\\tthe\\tROC\\tcurve\\totherwise.\\tFor\\texample,\\tlooking\\tat\\tthe\\tprevious\\tROC\\tcurve\\t(and\\tthe\\tROC\\nAUC\\tscore),\\tyou\\tmay\\tthink\\tthat\\tthe\\tclassifier\\tis\\treally\\tgood.\\tBut\\tthis\\tis\\tmostly\\tbecause\\tthere\\tare\\tfew\\tpositives\\t(5s)\\tcompared\\tto\\nthe\\tnegatives\\t(non-5s).\\tIn\\tcontrast,\\tthe\\tPR\\tcurve\\tmakes\\tit\\tclear\\tthat\\tthe\\tclassifier\\thas\\troom\\tfor\\timprovement\\t(the\\tcurve\\tcould\\tbe\\ncloser\\tto\\tthe\\ttop-right\\tcorner).\\nLet’s\\ttrain\\ta\\t\\nRandomForestClassifier\\n\\t\\nand\\tcompare\\tits\\tROC\\tcurve\\tand\\tROC\\tAUC\\tscore\\tto\\tthe\\nSGDClassifier\\n.\\tFirst,\\tyou\\tneed\\tto\\tget\\tscores\\tfor\\teach\\tinstance\\tin\\tthe\\ttraining\\tset.\\tBut\\tdue\\tto\\tthe\\tway\\tit\\nworks\\t(see\\t\\nChapter\\t7\\n),\\tthe\\t\\nRandomForestClassifier\\n\\tclass\\tdoes\\tnot\\thave\\ta\\t\\ndecision_function()\\nmethod.\\t\\nInstead\\tit\\thas\\ta\\t\\npredict_proba()\\n\\tmethod.\\tScikit-Learn\\tclassifiers\\tgenerally\\thave\\tone\\tor\\tthe\\nother.\\tThe\\t\\npredict_proba()\\n\\tmethod\\treturns\\tan\\tarray\\tcontaining\\ta\\trow\\tper\\tinstance\\tand\\ta\\tcolumn\\tper\\nclass,\\teach\\tcontaining\\tthe\\tprobability\\tthat\\tthe\\tgiven\\tinstance\\tbelongs\\tto\\tthe\\tgiven\\tclass\\t(e.g.,\\t70%\\tchance\\nthat\\tthe\\timage\\trepresents\\ta\\t5):\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nRandomForestClassifier\\nforest_clf\\n\\t\\n=\\n\\t\\nRandomForestClassifier\\n(\\nrandom_state\\n=\\n42\\n)\\ny_probas_forest\\n\\t\\n=\\n\\t\\ncross_val_predict\\n(\\nforest_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train_5\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmethod\\n=\\n\"predict_proba\"\\n)\\nBut\\tto\\tplot\\ta\\tROC\\tcurve,\\tyou\\tneed\\tscores,\\tnot\\tprobabilities.\\tA\\tsimple\\tsolution\\tis\\tto\\tuse\\tthe\\tpositive\\nclass’s\\t\\nprobability\\tas\\tthe\\tscore:\\ny_scores_forest\\n\\t\\n=\\n\\t\\ny_probas_forest\\n[:,\\n\\t\\n1\\n]\\n\\t\\t\\t\\n#\\tscore\\t=\\tproba\\tof\\tpositive\\tclass\\nfpr_forest\\n,\\n\\t\\ntpr_forest\\n,\\n\\t\\nthresholds_forest\\n\\t\\n=\\n\\t\\nroc_curve\\n(\\ny_train_5\\n,\\ny_scores_forest\\n)\\nNow\\tyou\\tare\\tready\\tto\\tplot\\tthe\\tROC\\tcurve.\\tIt\\tis\\tuseful\\tto\\tplot\\tthe\\tfirst\\tROC\\tcurve\\tas\\twell\\tto\\tsee\\thow\\tthey\\ncompare\\t(\\nFigure\\t3-7\\n):\\nplt\\n.\\nplot\\n(\\nfpr\\n,\\n\\t\\ntpr\\n,\\n\\t\\n\"b:\"\\n,\\n\\t\\nlabel\\n=\\n\"SGD\"\\n)\\nplot_roc_curve\\n(\\nfpr_forest\\n,\\n\\t\\ntpr_forest\\n,\\n\\t\\n\"Random\\tForest\"\\n)\\nplt\\n.\\nlegend\\n(\\nloc\\n=\\n\"lower\\tright\"\\n)\\nplt\\n.\\nshow\\n()', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 127}), Document(page_content='Figure\\t3-7.\\t\\nComparing\\tROC\\tcurves\\nAs\\t\\nyou\\tcan\\tsee\\tin\\t\\nFigure\\t3-7\\n,\\tthe\\t\\nRandomForestClassifier\\n’s\\tROC\\tcurve\\tlooks\\tmuch\\tbetter\\tthan\\tthe\\nSGDClassifier\\n’s:\\tit\\tcomes\\tmuch\\tcloser\\tto\\tthe\\ttop-left\\tcorner.\\tAs\\ta\\tresult,\\tits\\tROC\\tAUC\\tscore\\tis\\talso\\nsignificantly\\t\\nbetter:\\n>>>\\t\\nroc_auc_score\\n(\\ny_train_5\\n,\\n\\t\\ny_scores_forest\\n)\\n0.99312433660038291\\nTry\\tmeasuring\\tthe\\tprecision\\tand\\trecall\\tscores:\\tyou\\tshould\\tfind\\t98.5%\\tprecision\\tand\\t82.8%\\trecall.\\tNot\\ntoo\\tbad!\\nHopefully\\tyou\\tnow\\tknow\\thow\\tto\\ttrain\\tbinary\\tclassifiers,\\tchoose\\tthe\\tappropriate\\tmetric\\tfor\\tyour\\ttask,\\nevaluate\\tyour\\tclassifiers\\tusing\\tcross-validation,\\tselect\\tthe\\tprecision/recall\\ttradeoff\\tthat\\tfits\\tyour\\tneeds,\\nand\\tcompare\\tvarious\\tmodels\\tusing\\tROC\\tcurves\\tand\\tROC\\tAUC\\tscores.\\tNow\\tlet’s\\ttry\\tto\\tdetect\\tmore\\tthan\\njust\\tthe\\t5s.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 128}), Document(page_content='Multiclass\\tClassification\\nWhereas\\t\\nbinary\\tclassifiers\\tdistinguish\\tbetween\\ttwo\\tclasses,\\t\\nmulticlass\\tclassifiers\\n\\t(also\\tcalled\\nmultinomial\\tclassifiers\\n)\\tcan\\tdistinguish\\tbetween\\tmore\\tthan\\ttwo\\tclasses.\\nSome\\talgorithms\\t(such\\tas\\t\\nRandom\\tForest\\tclassifiers\\tor\\t\\nnaive\\tBayes\\tclassifiers)\\tare\\tcapable\\tof\\thandling\\nmultiple\\tclasses\\tdirectly.\\tOthers\\t(such\\tas\\tSupport\\tVector\\tMachine\\tclassifiers\\tor\\tLinear\\tclassifiers)\\tare\\nstrictly\\tbinary\\tclassifiers.\\tHowever,\\tthere\\tare\\tvarious\\tstrategies\\tthat\\tyou\\tcan\\tuse\\tto\\tperform\\tmulticlass\\nclassification\\tusing\\tmultiple\\tbinary\\tclassifiers.\\nFor\\texample,\\tone\\tway\\tto\\tcreate\\ta\\tsystem\\tthat\\tcan\\tclassify\\tthe\\tdigit\\timages\\tinto\\t10\\tclasses\\t(from\\t0\\tto\\t9)\\tis\\nto\\ttrain\\t10\\tbinary\\tclassifiers,\\tone\\tfor\\teach\\tdigit\\t(a\\t0-detector,\\ta\\t1-detector,\\ta\\t2-detector,\\tand\\tso\\ton).\\tThen\\nwhen\\tyou\\twant\\tto\\tclassify\\tan\\timage,\\tyou\\tget\\tthe\\tdecision\\tscore\\tfrom\\teach\\tclassifier\\tfor\\tthat\\timage\\tand\\tyou\\nselect\\tthe\\tclass\\twhose\\tclassifier\\toutputs\\tthe\\thighest\\tscore.\\tThis\\tis\\tcalled\\tthe\\t\\none-versus-all\\n\\t(OvA)\\nstrategy\\t\\n(also\\tcalled\\t\\none-versus-the-rest\\n).\\nAnother\\tstrategy\\tis\\tto\\ttrain\\ta\\tbinary\\tclassifier\\tfor\\tevery\\tpair\\tof\\tdigits:\\tone\\tto\\tdistinguish\\t0s\\tand\\t1s,\\nanother\\tto\\tdistinguish\\t0s\\tand\\t2s,\\tanother\\tfor\\t1s\\tand\\t2s,\\tand\\tso\\ton.\\tThis\\tis\\tcalled\\t\\nthe\\t\\none-versus-one\\n(OvO)\\tstrategy.\\tIf\\tthere\\tare\\t\\nN\\n\\tclasses,\\tyou\\tneed\\tto\\ttrain\\t\\nN\\n\\t×\\t(\\nN\\n\\t–\\t1)\\t/\\t2\\tclassifiers.\\tFor\\tthe\\tMNIST\\nproblem,\\tthis\\tmeans\\ttraining\\t45\\tbinary\\tclassifiers!\\tWhen\\tyou\\twant\\tto\\tclassify\\tan\\timage,\\tyou\\thave\\tto\\trun\\nthe\\timage\\tthrough\\tall\\t45\\tclassifiers\\tand\\tsee\\twhich\\tclass\\twins\\tthe\\tmost\\tduels.\\tThe\\tmain\\tadvantage\\tof\\tOvO\\nis\\tthat\\teach\\tclassifier\\tonly\\tneeds\\tto\\tbe\\ttrained\\ton\\tthe\\tpart\\tof\\tthe\\ttraining\\tset\\tfor\\tthe\\ttwo\\tclasses\\tthat\\tit\\tmust\\ndistinguish.\\nSome\\talgorithms\\t(such\\tas\\t\\nSupport\\tVector\\tMachine\\tclassifiers)\\tscale\\tpoorly\\twith\\tthe\\tsize\\tof\\tthe\\ttraining\\nset,\\tso\\tfor\\tthese\\talgorithms\\tOvO\\tis\\tpreferred\\tsince\\tit\\tis\\tfaster\\tto\\ttrain\\tmany\\tclassifiers\\ton\\tsmall\\ttraining\\nsets\\tthan\\ttraining\\tfew\\tclassifiers\\ton\\tlarge\\ttraining\\tsets.\\tFor\\tmost\\tbinary\\tclassification\\talgorithms,\\nhowever,\\tOvA\\tis\\tpreferred.\\nScikit-Learn\\tdetects\\twhen\\tyou\\ttry\\tto\\tuse\\ta\\tbinary\\tclassification\\talgorithm\\tfor\\ta\\tmulticlass\\tclassification\\ntask,\\tand\\tit\\tautomatically\\truns\\tOvA\\t(except\\tfor\\tSVM\\tclassifiers\\tfor\\twhich\\tit\\tuses\\tOvO).\\tLet’s\\ttry\\tthis\\twith\\nthe\\t\\nSGDClassifier\\n:\\n>>>\\t\\nsgd_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n\\t\\t\\n#\\ty_train,\\tnot\\ty_train_5\\n>>>\\t\\nsgd_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([\\t5.])\\nThat\\twas\\teasy!\\tThis\\tcode\\ttrains\\tthe\\t\\nSGDClassifier\\n\\t\\non\\tthe\\ttraining\\tset\\tusing\\tthe\\toriginal\\ttarget\\tclasses\\nfrom\\t0\\tto\\t9\\t(\\ny_train\\n),\\tinstead\\tof\\tthe\\t5-versus-all\\ttarget\\tclasses\\t(\\ny_train_5\\n).\\tThen\\tit\\tmakes\\ta\\tprediction\\n(a\\tcorrect\\tone\\tin\\tthis\\tcase).\\tUnder\\tthe\\thood,\\tScikit-Learn\\tactually\\ttrained\\t10\\tbinary\\tclassifiers,\\tgot\\ttheir\\ndecision\\tscores\\tfor\\tthe\\timage,\\tand\\tselected\\tthe\\tclass\\twith\\tthe\\thighest\\tscore.\\nTo\\tsee\\tthat\\tthis\\tis\\tindeed\\tthe\\tcase,\\tyou\\tcan\\tcall\\tthe\\t\\ndecision_function()\\n\\tmethod.\\tInstead\\tof\\treturning\\njust\\tone\\tscore\\tper\\tinstance,\\tit\\tnow\\treturns\\t10\\tscores,\\tone\\tper\\tclass:\\n>>>\\t\\nsome_digit_scores\\n\\t\\n=\\n\\t\\nsgd_clf\\n.\\ndecision_function\\n([\\nsome_digit\\n])\\n>>>\\t\\nsome_digit_scores\\narray([[-311402.62954431,\\t-363517.28355739,\\t-446449.5306454\\t,', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 129}), Document(page_content='\\t\\t\\t\\t\\t\\t\\t\\t-183226.61023518,\\t-414337.15339485,\\t\\t161855.74572176,\\n\\t\\t\\t\\t\\t\\t\\t\\t-452576.39616343,\\t-471957.14962573,\\t-518542.33997148,\\n\\t\\t\\t\\t\\t\\t\\t\\t-536774.63961222]])\\nThe\\thighest\\tscore\\tis\\tindeed\\tthe\\tone\\tcorresponding\\tto\\tclass\\t5:\\n>>>\\t\\nnp\\n.\\nargmax\\n(\\nsome_digit_scores\\n)\\n5\\n>>>\\t\\nsgd_clf\\n.\\nclasses_\\narray([\\t0.,\\t\\t1.,\\t\\t2.,\\t\\t3.,\\t\\t4.,\\t\\t5.,\\t\\t6.,\\t\\t7.,\\t\\t8.,\\t\\t9.])\\n>>>\\t\\nsgd_clf\\n.\\nclasses_\\n[\\n5\\n]\\n5.0\\nWARNING\\nWhen\\ta\\tclassifier\\tis\\ttrained,\\tit\\tstores\\tthe\\tlist\\tof\\ttarget\\tclasses\\tin\\tits\\t\\nclasses_\\n\\tattribute,\\tordered\\tby\\tvalue.\\tIn\\tthis\\tcase,\\tthe\\tindex\\tof\\neach\\tclass\\tin\\tthe\\t\\nclasses_\\n\\tarray\\tconveniently\\tmatches\\tthe\\tclass\\titself\\t(e.g.,\\tthe\\tclass\\tat\\tindex\\t5\\thappens\\tto\\tbe\\tclass\\t5),\\tbut\\tin\\ngeneral\\tyou\\twon’t\\tbe\\tso\\tlucky.\\nIf\\tyou\\twant\\tto\\tforce\\tScikitLearn\\tto\\t\\nuse\\tone-versus-one\\tor\\tone-versus-all,\\tyou\\tcan\\tuse\\tthe\\nOneVsOneClassifier\\n\\tor\\t\\nOneVsRestClassifier\\n\\tclasses.\\tSimply\\tcreate\\tan\\tinstance\\tand\\tpass\\ta\\tbinary\\nclassifier\\tto\\tits\\tconstructor.\\tFor\\texample,\\tthis\\tcode\\tcreates\\ta\\tmulticlass\\tclassifier\\tusing\\tthe\\tOvO\\tstrategy,\\nbased\\ton\\ta\\t\\nSGDClassifier\\n:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.multiclass\\n\\t\\nimport\\n\\t\\nOneVsOneClassifier\\n>>>\\t\\novo_clf\\n\\t\\n=\\n\\t\\nOneVsOneClassifier\\n(\\nSGDClassifier\\n(\\nrandom_state\\n=\\n42\\n))\\n>>>\\t\\novo_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n>>>\\t\\novo_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([\\t5.])\\n>>>\\t\\nlen\\n(\\novo_clf\\n.\\nestimators_\\n)\\n45\\nTraining\\ta\\t\\nRandomForestClassifier\\n\\tis\\t\\njust\\tas\\teasy:\\n>>>\\t\\nforest_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n>>>\\t\\nforest_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([\\t5.])\\nThis\\ttime\\tScikit-Learn\\tdid\\tnot\\thave\\tto\\trun\\tOvA\\tor\\tOvO\\tbecause\\tRandom\\tForest\\t\\nclassifiers\\n\\tcan\\tdirectly\\nclassify\\tinstances\\tinto\\tmultiple\\tclasses.\\tYou\\tcan\\tcall\\t\\npredict_proba()\\n\\tto\\tget\\tthe\\tlist\\tof\\tprobabilities\\tthat\\nthe\\tclassifier\\tassigned\\tto\\teach\\tinstance\\tfor\\teach\\tclass:\\n>>>\\t\\nforest_clf\\n.\\npredict_proba\\n([\\nsome_digit\\n])\\narray([[\\t0.1,\\t\\t0.\\t,\\t\\t0.\\t,\\t\\t0.1,\\t\\t0.\\t,\\t\\t0.8,\\t\\t0.\\t,\\t\\t0.\\t,\\t\\t0.\\t,\\t\\t0.\\t]])\\nYou\\tcan\\tsee\\tthat\\tthe\\tclassifier\\tis\\tfairly\\tconfident\\tabout\\tits\\tprediction:\\tthe\\t0.8\\tat\\tthe\\t5\\nth\\n\\tindex\\tin\\tthe\\tarray\\nmeans\\tthat\\tthe\\tmodel\\testimates\\tan\\t80%\\tprobability\\tthat\\tthe\\timage\\trepresents\\ta\\t5.\\tIt\\talso\\tthinks\\tthat\\tthe\\nimage\\tcould\\tinstead\\tbe\\ta\\t0\\tor\\ta\\t3\\t(10%\\tchance\\teach).\\nNow\\tof\\t\\ncourse\\tyou\\twant\\tto\\tevaluate\\tthese\\tclassifiers.\\tAs\\tusual,\\tyou\\twant\\tto\\tuse\\tcross-validation.\\tLet’s\\nevaluate\\tthe\\t\\nSGDClassifier\\n’s\\taccuracy\\tusing\\tthe\\t\\ncross_val_score()\\n\\tfunction:', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 130}), Document(page_content='>>>\\t\\ncross_val_score\\n(\\nsgd_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\nscoring\\n=\\n\"accuracy\"\\n)\\narray([\\t0.84063187,\\t\\t0.84899245,\\t\\t0.86652998])\\nIt\\tgets\\tover\\t84%\\ton\\tall\\ttest\\tfolds.\\tIf\\tyou\\tused\\ta\\trandom\\tclassifier,\\tyou\\twould\\tget\\t10%\\taccuracy,\\tso\\tthis\\tis\\nnot\\tsuch\\ta\\tbad\\tscore,\\tbut\\tyou\\tcan\\tstill\\tdo\\tmuch\\t\\nbetter.\\tFor\\texample,\\tsimply\\tscaling\\tthe\\tinputs\\t(as\\ndiscussed\\tin\\t\\nChapter\\t2\\n)\\tincreases\\t\\naccuracy\\tabove\\t90%:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nStandardScaler\\n>>>\\t\\nscaler\\n\\t\\n=\\n\\t\\nStandardScaler\\n()\\n>>>\\t\\nX_train_scaled\\n\\t\\n=\\n\\t\\nscaler\\n.\\nfit_transform\\n(\\nX_train\\n.\\nastype\\n(\\nnp\\n.\\nfloat64\\n))\\n>>>\\t\\ncross_val_score\\n(\\nsgd_clf\\n,\\n\\t\\nX_train_scaled\\n,\\n\\t\\ny_train\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\nscoring\\n=\\n\"accuracy\"\\n)\\narray([\\t0.91011798,\\t\\t0.90874544,\\t\\t0.906636\\t\\t])', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 131}), Document(page_content='Error\\tAnalysis\\nOf\\t\\ncourse,\\tif\\tthis\\twere\\ta\\treal\\tproject,\\tyou\\twould\\tfollow\\tthe\\tsteps\\tin\\tyour\\tMachine\\tLearning\\tproject\\nchecklist\\t(see\\t\\nAppendix\\tB\\n):\\texploring\\tdata\\tpreparation\\toptions,\\ttrying\\tout\\tmultiple\\tmodels,\\tshortlisting\\nthe\\tbest\\tones\\tand\\tfine-tuning\\ttheir\\thyperparameters\\tusing\\t\\nGridSearchCV\\n,\\t\\nand\\tautomating\\tas\\tmuch\\tas\\npossible,\\tas\\tyou\\tdid\\tin\\tthe\\tprevious\\tchapter.\\tHere,\\twe\\twill\\tassume\\tthat\\tyou\\thave\\tfound\\ta\\tpromising\\tmodel\\nand\\tyou\\twant\\tto\\tfind\\tways\\tto\\timprove\\tit.\\tOne\\tway\\tto\\tdo\\tthis\\tis\\tto\\tanalyze\\tthe\\ttypes\\tof\\terrors\\tit\\tmakes.\\nFirst,\\tyou\\tcan\\tlook\\tat\\tthe\\t\\nconfusion\\tmatrix.\\tYou\\tneed\\tto\\tmake\\tpredictions\\tusing\\tthe\\ncross_val_predict()\\n\\tfunction,\\t\\nthen\\tcall\\tthe\\t\\nconfusion_matrix()\\n\\tfunction,\\tjust\\tlike\\tyou\\tdid\\tearlier:\\n>>>\\t\\ny_train_pred\\n\\t\\n=\\n\\t\\ncross_val_predict\\n(\\nsgd_clf\\n,\\n\\t\\nX_train_scaled\\n,\\n\\t\\ny_train\\n,\\n\\t\\ncv\\n=\\n3\\n)\\n>>>\\t\\nconf_mx\\n\\t\\n=\\n\\t\\nconfusion_matrix\\n(\\ny_train\\n,\\n\\t\\ny_train_pred\\n)\\n>>>\\t\\nconf_mx\\narray([[5725,\\t\\t\\t\\t3,\\t\\t\\t24,\\t\\t\\t\\t9,\\t\\t\\t10,\\t\\t\\t49,\\t\\t\\t50,\\t\\t\\t10,\\t\\t\\t39,\\t\\t\\t\\t4],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t\\t2,\\t6493,\\t\\t\\t43,\\t\\t\\t25,\\t\\t\\t\\t7,\\t\\t\\t40,\\t\\t\\t\\t5,\\t\\t\\t10,\\t\\t109,\\t\\t\\t\\t8],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t51,\\t\\t\\t41,\\t5321,\\t\\t104,\\t\\t\\t89,\\t\\t\\t26,\\t\\t\\t87,\\t\\t\\t60,\\t\\t166,\\t\\t\\t13],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t47,\\t\\t\\t46,\\t\\t141,\\t5342,\\t\\t\\t\\t1,\\t\\t231,\\t\\t\\t40,\\t\\t\\t50,\\t\\t141,\\t\\t\\t92],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t19,\\t\\t\\t29,\\t\\t\\t41,\\t\\t\\t10,\\t5366,\\t\\t\\t\\t9,\\t\\t\\t56,\\t\\t\\t37,\\t\\t\\t86,\\t\\t189],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t73,\\t\\t\\t45,\\t\\t\\t36,\\t\\t193,\\t\\t\\t64,\\t4582,\\t\\t111,\\t\\t\\t30,\\t\\t193,\\t\\t\\t94],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t29,\\t\\t\\t34,\\t\\t\\t44,\\t\\t\\t\\t2,\\t\\t\\t42,\\t\\t\\t85,\\t5627,\\t\\t\\t10,\\t\\t\\t45,\\t\\t\\t\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t25,\\t\\t\\t24,\\t\\t\\t74,\\t\\t\\t32,\\t\\t\\t54,\\t\\t\\t12,\\t\\t\\t\\t6,\\t5787,\\t\\t\\t15,\\t\\t236],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t52,\\t\\t161,\\t\\t\\t73,\\t\\t156,\\t\\t\\t10,\\t\\t163,\\t\\t\\t61,\\t\\t\\t25,\\t5027,\\t\\t123],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t43,\\t\\t\\t35,\\t\\t\\t26,\\t\\t\\t92,\\t\\t178,\\t\\t\\t28,\\t\\t\\t\\t2,\\t\\t223,\\t\\t\\t82,\\t5240]])\\nThat’s\\ta\\tlot\\tof\\tnumbers.\\tIt’s\\toften\\tmore\\tconvenient\\tto\\tlook\\tat\\tan\\timage\\trepresentation\\tof\\tthe\\tconfusion\\nmatrix,\\tusing\\t\\nMatplotlib’s\\t\\nmatshow()\\n\\tfunction:\\nplt\\n.\\nmatshow\\n(\\nconf_mx\\n,\\n\\t\\ncmap\\n=\\nplt\\n.\\ncm\\n.\\ngray\\n)\\nplt\\n.\\nshow\\n()', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 132}), Document(page_content='This\\tconfusion\\tmatrix\\tlooks\\tfairly\\tgood,\\tsince\\tmost\\timages\\tare\\ton\\tthe\\tmain\\tdiagonal,\\twhich\\tmeans\\tthat\\nthey\\twere\\tclassified\\tcorrectly.\\tThe\\t5s\\tlook\\tslightly\\tdarker\\tthan\\tthe\\tother\\tdigits,\\twhich\\tcould\\tmean\\tthat\\nthere\\tare\\tfewer\\timages\\tof\\t5s\\tin\\tthe\\tdataset\\tor\\tthat\\tthe\\tclassifier\\tdoes\\tnot\\tperform\\tas\\twell\\ton\\t5s\\tas\\ton\\tother\\ndigits.\\tIn\\tfact,\\tyou\\tcan\\tverify\\tthat\\tboth\\tare\\tthe\\tcase.\\nLet’s\\tfocus\\tthe\\tplot\\ton\\tthe\\terrors.\\tFirst,\\tyou\\tneed\\tto\\tdivide\\teach\\tvalue\\tin\\tthe\\tconfusion\\tmatrix\\tby\\tthe\\nnumber\\tof\\timages\\tin\\tthe\\tcorresponding\\tclass,\\tso\\tyou\\tcan\\tcompare\\terror\\trates\\tinstead\\tof\\tabsolute\\tnumber\\nof\\terrors\\t(which\\twould\\tmake\\tabundant\\tclasses\\tlook\\tunfairly\\tbad):\\nrow_sums\\n\\t\\n=\\n\\t\\nconf_mx\\n.\\nsum\\n(\\naxis\\n=\\n1\\n,\\n\\t\\nkeepdims\\n=\\nTrue\\n)\\nnorm_conf_mx\\n\\t\\n=\\n\\t\\nconf_mx\\n\\t\\n/\\n\\t\\nrow_sums\\nNow\\tlet’s\\tfill\\tthe\\tdiagonal\\twith\\tzeros\\tto\\tkeep\\tonly\\tthe\\terrors,\\tand\\tlet’s\\tplot\\tthe\\tresult:\\nnp\\n.\\nfill_diagonal\\n(\\nnorm_conf_mx\\n,\\n\\t\\n0\\n)\\nplt\\n.\\nmatshow\\n(\\nnorm_conf_mx\\n,\\n\\t\\ncmap\\n=\\nplt\\n.\\ncm\\n.\\ngray\\n)\\nplt\\n.\\nshow\\n()', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 133}), Document(page_content='Now\\tyou\\tcan\\tclearly\\tsee\\tthe\\tkinds\\tof\\terrors\\tthe\\tclassifier\\tmakes.\\tRemember\\tthat\\trows\\trepresent\\tactual\\nclasses,\\twhile\\tcolumns\\trepresent\\tpredicted\\tclasses.\\tThe\\tcolumns\\tfor\\tclasses\\t8\\tand\\t9\\tare\\tquite\\tbright,\\nwhich\\ttells\\tyou\\tthat\\tmany\\timages\\tget\\tmisclassified\\tas\\t8s\\tor\\t9s.\\tSimilarly,\\tthe\\trows\\tfor\\tclasses\\t8\\tand\\t9\\tare\\nalso\\tquite\\tbright,\\ttelling\\tyou\\tthat\\t8s\\tand\\t9s\\tare\\toften\\tconfused\\twith\\tother\\tdigits.\\tConversely,\\tsome\\trows\\nare\\tpretty\\tdark,\\tsuch\\tas\\trow\\t1:\\tthis\\tmeans\\tthat\\tmost\\t1s\\tare\\tclassified\\tcorrectly\\t(a\\tfew\\tare\\tconfused\\twith\\n8s,\\tbut\\tthat’s\\tabout\\tit).\\tNotice\\tthat\\tthe\\terrors\\tare\\tnot\\tperfectly\\tsymmetrical;\\tfor\\texample,\\tthere\\tare\\tmore\\t5s\\nmisclassified\\tas\\t8s\\tthan\\tthe\\treverse.\\nAnalyzing\\tthe\\tconfusion\\tmatrix\\tcan\\toften\\tgive\\tyou\\tinsights\\ton\\tways\\tto\\timprove\\tyour\\tclassifier.\\tLooking\\tat\\nthis\\tplot,\\tit\\tseems\\tthat\\tyour\\tefforts\\tshould\\tbe\\tspent\\ton\\timproving\\tclassification\\tof\\t8s\\tand\\t9s,\\tas\\twell\\tas\\nfixing\\tthe\\tspecific\\t3/5\\tconfusion.\\tFor\\texample,\\tyou\\tcould\\ttry\\tto\\tgather\\tmore\\ttraining\\tdata\\tfor\\tthese\\tdigits.\\nOr\\tyou\\tcould\\tengineer\\tnew\\tfeatures\\tthat\\twould\\thelp\\tthe\\tclassifier\\t—\\tfor\\texample,\\twriting\\tan\\talgorithm\\tto\\ncount\\tthe\\tnumber\\tof\\tclosed\\tloops\\t(e.g.,\\t8\\thas\\ttwo,\\t6\\thas\\tone,\\t5\\thas\\tnone).\\tOr\\tyou\\tcould\\tpreprocess\\tthe\\nimages\\t(e.g.,\\tusing\\tScikit-Image,\\tPillow,\\tor\\tOpenCV)\\tto\\tmake\\tsome\\tpatterns\\tstand\\tout\\tmore,\\tsuch\\tas\\nclosed\\tloops.\\nAnalyzing\\tindividual\\terrors\\tcan\\talso\\tbe\\ta\\tgood\\tway\\tto\\tgain\\tinsights\\ton\\twhat\\tyour\\tclassifier\\tis\\tdoing\\tand\\nwhy\\tit\\tis\\tfailing,\\tbut\\tit\\tis\\tmore\\tdifficult\\tand\\ttime-consuming.\\tFor\\texample,\\tlet’s\\tplot\\texamples\\tof\\t3s\\tand\\t5s\\n(the\\t\\nplot_digits()\\n\\tfunction\\tjust\\tuses\\tMatplotlib’s\\t\\nimshow()\\n\\tfunction;\\tsee\\tthis\\tchapter’s\\tJupyter', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 134}), Document(page_content='notebook\\tfor\\tdetails):\\ncl_a\\n,\\n\\t\\ncl_b\\n\\t\\n=\\n\\t\\n3\\n,\\n\\t\\n5\\nX_aa\\n\\t\\n=\\n\\t\\nX_train\\n[(\\ny_train\\n\\t\\n==\\n\\t\\ncl_a\\n)\\n\\t\\n&\\n\\t\\n(\\ny_train_pred\\n\\t\\n==\\n\\t\\ncl_a\\n)]\\nX_ab\\n\\t\\n=\\n\\t\\nX_train\\n[(\\ny_train\\n\\t\\n==\\n\\t\\ncl_a\\n)\\n\\t\\n&\\n\\t\\n(\\ny_train_pred\\n\\t\\n==\\n\\t\\ncl_b\\n)]\\nX_ba\\n\\t\\n=\\n\\t\\nX_train\\n[(\\ny_train\\n\\t\\n==\\n\\t\\ncl_b\\n)\\n\\t\\n&\\n\\t\\n(\\ny_train_pred\\n\\t\\n==\\n\\t\\ncl_a\\n)]\\nX_bb\\n\\t\\n=\\n\\t\\nX_train\\n[(\\ny_train\\n\\t\\n==\\n\\t\\ncl_b\\n)\\n\\t\\n&\\n\\t\\n(\\ny_train_pred\\n\\t\\n==\\n\\t\\ncl_b\\n)]\\nplt\\n.\\nfigure\\n(\\nfigsize\\n=\\n(\\n8\\n,\\n8\\n))\\nplt\\n.\\nsubplot\\n(\\n221\\n);\\n\\t\\nplot_digits\\n(\\nX_aa\\n[:\\n25\\n],\\n\\t\\nimages_per_row\\n=\\n5\\n)\\nplt\\n.\\nsubplot\\n(\\n222\\n);\\n\\t\\nplot_digits\\n(\\nX_ab\\n[:\\n25\\n],\\n\\t\\nimages_per_row\\n=\\n5\\n)\\nplt\\n.\\nsubplot\\n(\\n223\\n);\\n\\t\\nplot_digits\\n(\\nX_ba\\n[:\\n25\\n],\\n\\t\\nimages_per_row\\n=\\n5\\n)\\nplt\\n.\\nsubplot\\n(\\n224\\n);\\n\\t\\nplot_digits\\n(\\nX_bb\\n[:\\n25\\n],\\n\\t\\nimages_per_row\\n=\\n5\\n)\\nplt\\n.\\nshow\\n()\\nThe\\ttwo\\t5×5\\tblocks\\ton\\tthe\\tleft\\tshow\\tdigits\\tclassified\\tas\\t3s,\\tand\\tthe\\ttwo\\t5×5\\tblocks\\ton\\tthe\\tright\\tshow\\nimages\\tclassified\\tas\\t5s.\\tSome\\tof\\tthe\\tdigits\\tthat\\tthe\\tclassifier\\tgets\\twrong\\t(i.e.,\\tin\\tthe\\tbottom-left\\tand\\ttop-\\nright\\tblocks)\\tare\\tso\\tbadly\\twritten\\tthat\\teven\\ta\\thuman\\twould\\thave\\ttrouble\\tclassifying\\tthem\\t(e.g.,\\tthe\\t5\\ton\\nthe\\t8\\nth\\n\\trow\\tand\\t1\\nst\\n\\tcolumn\\ttruly\\tlooks\\tlike\\ta\\t3).\\tHowever,\\tmost\\tmisclassified\\timages\\tseem\\tlike\\tobvious\\nerrors\\tto\\tus,\\tand\\tit’s\\thard\\tto\\tunderstand\\twhy\\tthe\\tclassifier\\tmade\\tthe\\tmistakes\\tit\\tdid.\\n3\\n\\tThe\\treason\\tis\\tthat\\twe\\nused\\ta\\tsimple\\t\\nSGDClassifier\\n,\\twhich\\tis\\ta\\tlinear\\tmodel.\\tAll\\tit\\tdoes\\tis\\tassign\\ta\\tweight\\tper\\tclass\\tto\\teach\\npixel,\\tand\\twhen\\tit\\tsees\\ta\\tnew\\timage\\tit\\tjust\\tsums\\tup\\tthe\\tweighted\\tpixel\\tintensities\\tto\\tget\\ta\\tscore\\tfor\\teach\\nclass.\\tSo\\tsince\\t3s\\tand\\t5s\\tdiffer\\tonly\\tby\\ta\\tfew\\tpixels,\\tthis\\tmodel\\twill\\teasily\\tconfuse\\tthem.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 135}), Document(page_content='The\\tmain\\tdifference\\tbetween\\t3s\\tand\\t5s\\tis\\tthe\\tposition\\tof\\tthe\\tsmall\\tline\\tthat\\tjoins\\tthe\\ttop\\tline\\tto\\tthe\\tbottom\\narc.\\tIf\\tyou\\tdraw\\ta\\t3\\twith\\tthe\\tjunction\\tslightly\\tshifted\\tto\\tthe\\tleft,\\tthe\\tclassifier\\tmight\\tclassify\\tit\\tas\\ta\\t5,\\tand\\nvice\\tversa.\\tIn\\tother\\twords,\\tthis\\tclassifier\\tis\\tquite\\tsensitive\\tto\\timage\\tshifting\\tand\\trotation.\\tSo\\tone\\tway\\tto\\nreduce\\tthe\\t3/5\\tconfusion\\twould\\tbe\\tto\\tpreprocess\\tthe\\timages\\tto\\tensure\\tthat\\tthey\\tare\\twell\\tcentered\\tand\\tnot\\ntoo\\trotated.\\tThis\\twill\\tprobably\\thelp\\treduce\\tother\\terrors\\tas\\t\\nwell.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 136}), Document(page_content='Multilabel\\tClassification\\nUntil\\t\\nnow\\teach\\tinstance\\thas\\talways\\tbeen\\tassigned\\tto\\tjust\\tone\\tclass.\\tIn\\tsome\\tcases\\tyou\\tmay\\twant\\tyour\\nclassifier\\tto\\toutput\\tmultiple\\tclasses\\tfor\\teach\\tinstance.\\tFor\\texample,\\tconsider\\ta\\tface-recognition\\nclassifier:\\twhat\\tshould\\tit\\tdo\\tif\\tit\\trecognizes\\tseveral\\tpeople\\ton\\tthe\\tsame\\tpicture?\\tOf\\tcourse\\tit\\tshould\\nattach\\tone\\tlabel\\tper\\tperson\\tit\\trecognizes.\\tSay\\tthe\\tclassifier\\thas\\tbeen\\ttrained\\tto\\trecognize\\tthree\\tfaces,\\nAlice,\\tBob,\\tand\\tCharlie;\\tthen\\twhen\\tit\\tis\\tshown\\ta\\tpicture\\tof\\tAlice\\tand\\tCharlie,\\tit\\tshould\\toutput\\t[1,\\t0,\\t1]\\n(meaning\\t“Alice\\tyes,\\tBob\\tno,\\tCharlie\\tyes”).\\tSuch\\ta\\tclassification\\tsystem\\tthat\\toutputs\\tmultiple\\tbinary\\nlabels\\tis\\tcalled\\ta\\t\\nmultilabel\\tclassification\\n\\tsystem.\\nWe\\twon’t\\tgo\\tinto\\tface\\trecognition\\tjust\\tyet,\\tbut\\tlet’s\\tlook\\tat\\ta\\tsimpler\\texample,\\tjust\\tfor\\tillustration\\npurposes:\\nfrom\\n\\t\\nsklearn.neighbors\\n\\t\\nimport\\n\\t\\nKNeighborsClassifier\\ny_train_large\\n\\t\\n=\\n\\t\\n(\\ny_train\\n\\t\\n>=\\n\\t\\n7\\n)\\ny_train_odd\\n\\t\\n=\\n\\t\\n(\\ny_train\\n\\t\\n%\\n\\t\\n2\\n\\t\\n==\\n\\t\\n1\\n)\\ny_multilabel\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\ny_train_large\\n,\\n\\t\\ny_train_odd\\n]\\nknn_clf\\n\\t\\n=\\n\\t\\nKNeighborsClassifier\\n()\\nknn_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_multilabel\\n)\\nThis\\tcode\\tcreates\\ta\\t\\ny_multilabel\\n\\tarray\\tcontaining\\ttwo\\ttarget\\tlabels\\tfor\\teach\\tdigit\\timage:\\tthe\\tfirst\\nindicates\\twhether\\tor\\tnot\\tthe\\tdigit\\tis\\tlarge\\t(7,\\t8,\\tor\\t9)\\tand\\tthe\\tsecond\\tindicates\\twhether\\tor\\tnot\\tit\\tis\\todd.\\nThe\\tnext\\tlines\\tcreate\\ta\\t\\nKNeighborsClassifier\\n\\t\\ninstance\\t(which\\tsupports\\tmultilabel\\tclassification,\\tbut\\nnot\\tall\\tclassifiers\\tdo)\\tand\\twe\\ttrain\\tit\\tusing\\tthe\\tmultiple\\ttargets\\tarray.\\tNow\\tyou\\tcan\\tmake\\ta\\tprediction,\\tand\\nnotice\\tthat\\tit\\toutputs\\ttwo\\tlabels:\\n>>>\\t\\nknn_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([[False,\\t\\tTrue]],\\tdtype=bool)\\nAnd\\tit\\tgets\\tit\\tright!\\tThe\\tdigit\\t5\\tis\\tindeed\\tnot\\tlarge\\t(\\nFalse\\n)\\tand\\todd\\t(\\nTrue\\n).\\nThere\\tare\\tmany\\tways\\tto\\tevaluate\\ta\\tmultilabel\\tclassifier,\\tand\\tselecting\\tthe\\tright\\tmetric\\treally\\tdepends\\ton\\nyour\\tproject.\\tFor\\texample,\\tone\\tapproach\\tis\\tto\\tmeasure\\tthe\\tF\\n1\\n\\tscore\\tfor\\teach\\tindividual\\tlabel\\t(or\\tany\\tother\\nbinary\\tclassifier\\tmetric\\tdiscussed\\tearlier),\\tthen\\tsimply\\tcompute\\tthe\\taverage\\tscore.\\tThis\\tcode\\tcomputes\\nthe\\taverage\\tF\\n1\\n\\tscore\\tacross\\t\\nall\\tlabels:\\n>>>\\t\\ny_train_knn_pred\\n\\t\\n=\\n\\t\\ncross_val_predict\\n(\\nknn_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train\\n,\\n\\t\\ncv\\n=\\n3\\n)\\n>>>\\t\\nf1_score\\n(\\ny_train\\n,\\n\\t\\ny_train_knn_pred\\n,\\n\\t\\naverage\\n=\\n\"macro\"\\n)\\n0.96845540180280221\\nThis\\tassumes\\tthat\\tall\\tlabels\\tare\\tequally\\timportant,\\twhich\\tmay\\tnot\\tbe\\tthe\\tcase.\\tIn\\tparticular,\\tif\\tyou\\thave\\nmany\\tmore\\tpictures\\tof\\tAlice\\tthan\\tof\\tBob\\tor\\tCharlie,\\tyou\\tmay\\twant\\tto\\tgive\\tmore\\tweight\\tto\\tthe\\tclassifier’s\\nscore\\ton\\tpictures\\tof\\tAlice.\\tOne\\tsimple\\toption\\tis\\tto\\tgive\\teach\\tlabel\\ta\\tweight\\tequal\\tto\\tits\\t\\nsupport\\n\\t(i.e.,\\tthe\\nnumber\\tof\\tinstances\\twith\\tthat\\ttarget\\tlabel).\\t\\nTo\\tdo\\tthis,\\tsimply\\tset\\t\\naverage=\"weighted\"\\n\\tin\\tthe\\tpreceding\\ncode.\\n4', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 137}), Document(page_content='Multioutput\\tClassification\\nThe\\t\\nlast\\ttype\\tof\\tclassification\\ttask\\twe\\tare\\tgoing\\tto\\tdiscuss\\there\\tis\\tcalled\\t\\nmultioutput-multiclass\\nclassification\\n\\t(or\\tsimply\\t\\nmultioutput\\tclassification\\n).\\tIt\\tis\\tsimply\\ta\\tgeneralization\\tof\\tmultilabel\\nclassification\\twhere\\teach\\tlabel\\tcan\\tbe\\tmulticlass\\t(i.e.,\\tit\\tcan\\thave\\tmore\\tthan\\ttwo\\tpossible\\tvalues).\\nTo\\tillustrate\\tthis,\\tlet’s\\tbuild\\ta\\tsystem\\tthat\\tremoves\\tnoise\\tfrom\\timages.\\tIt\\twill\\ttake\\tas\\tinput\\ta\\tnoisy\\tdigit\\nimage,\\tand\\tit\\twill\\t(hopefully)\\toutput\\ta\\tclean\\tdigit\\timage,\\trepresented\\tas\\tan\\tarray\\tof\\tpixel\\tintensities,\\tjust\\nlike\\tthe\\tMNIST\\timages.\\tNotice\\tthat\\tthe\\tclassifier’s\\toutput\\tis\\tmultilabel\\t(one\\tlabel\\tper\\tpixel)\\tand\\teach\\nlabel\\tcan\\thave\\tmultiple\\tvalues\\t(pixel\\tintensity\\tranges\\tfrom\\t0\\tto\\t255).\\tIt\\tis\\tthus\\tan\\texample\\tof\\ta\\tmultioutput\\nclassification\\tsystem.\\nNOTE\\nThe\\tline\\tbetween\\t\\nclassification\\tand\\tregression\\tis\\tsometimes\\tblurry,\\tsuch\\tas\\tin\\tthis\\texample.\\tArguably,\\tpredicting\\tpixel\\tintensity\\tis\\nmore\\takin\\tto\\tregression\\tthan\\tto\\tclassification.\\tMoreover,\\tmultioutput\\tsystems\\tare\\tnot\\tlimited\\tto\\tclassification\\ttasks;\\tyou\\tcould\\teven\\nhave\\ta\\tsystem\\tthat\\toutputs\\tmultiple\\tlabels\\tper\\tinstance,\\tincluding\\tboth\\tclass\\tlabels\\tand\\tvalue\\tlabels.\\nLet’s\\tstart\\tby\\tcreating\\tthe\\ttraining\\tand\\ttest\\tsets\\tby\\ttaking\\tthe\\tMNIST\\timages\\tand\\tadding\\tnoise\\tto\\ttheir\\tpixel\\nintensities\\tusing\\tNumPy’s\\t\\nrandint()\\n\\tfunction.\\tThe\\ttarget\\timages\\twill\\tbe\\tthe\\toriginal\\timages:\\nnoise\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandint\\n(\\n0\\n,\\n\\t\\n100\\n,\\n\\t\\n(\\nlen\\n(\\nX_train\\n),\\n\\t\\n784\\n))\\nX_train_mod\\n\\t\\n=\\n\\t\\nX_train\\n\\t\\n+\\n\\t\\nnoise\\nnoise\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandint\\n(\\n0\\n,\\n\\t\\n100\\n,\\n\\t\\n(\\nlen\\n(\\nX_test\\n),\\n\\t\\n784\\n))\\nX_test_mod\\n\\t\\n=\\n\\t\\nX_test\\n\\t\\n+\\n\\t\\nnoise\\ny_train_mod\\n\\t\\n=\\n\\t\\nX_train\\ny_test_mod\\n\\t\\n=\\n\\t\\nX_test\\nLet’s\\ttake\\ta\\tpeek\\tat\\tan\\timage\\tfrom\\tthe\\ttest\\tset\\t(yes,\\twe’re\\tsnooping\\ton\\tthe\\ttest\\tdata,\\tso\\tyou\\tshould\\tbe\\nfrowning\\tright\\tnow):\\nOn\\tthe\\tleft\\tis\\tthe\\tnoisy\\tinput\\timage,\\tand\\ton\\tthe\\tright\\tis\\tthe\\tclean\\ttarget\\timage.\\tNow\\tlet’s\\ttrain\\tthe\\tclassifier', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 138}), Document(page_content='and\\tmake\\tit\\tclean\\tthis\\timage:\\nknn_clf\\n.\\nfit\\n(\\nX_train_mod\\n,\\n\\t\\ny_train_mod\\n)\\nclean_digit\\n\\t\\n=\\n\\t\\nknn_clf\\n.\\npredict\\n([\\nX_test_mod\\n[\\nsome_index\\n]])\\nplot_digit\\n(\\nclean_digit\\n)\\nLooks\\tclose\\tenough\\tto\\tthe\\ttarget!\\tThis\\tconcludes\\tour\\ttour\\tof\\tclassification.\\tHopefully\\tyou\\tshould\\tnow\\nknow\\thow\\tto\\tselect\\tgood\\tmetrics\\tfor\\tclassification\\ttasks,\\tpick\\tthe\\tappropriate\\tprecision/recall\\ttradeoff,\\ncompare\\tclassifiers,\\tand\\tmore\\tgenerally\\tbuild\\tgood\\tclassification\\tsystems\\tfor\\ta\\tvariety\\tof\\t\\ntasks.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 139}), Document(page_content='Exercises\\n1\\n.\\t\\nTry\\tto\\tbuild\\ta\\tclassifier\\tfor\\tthe\\tMNIST\\tdataset\\tthat\\tachieves\\tover\\t97%\\taccuracy\\ton\\tthe\\ttest\\tset.\\tHint:\\nthe\\t\\nKNeighborsClassifier\\n\\t\\nworks\\tquite\\twell\\tfor\\tthis\\ttask;\\tyou\\tjust\\tneed\\tto\\tfind\\tgood\\nhyperparameter\\tvalues\\t(try\\ta\\tgrid\\tsearch\\ton\\tthe\\t\\nweights\\n\\tand\\t\\nn_neighbors\\n\\thyperparameters).\\n2\\n.\\t\\nWrite\\ta\\tfunction\\tthat\\tcan\\tshift\\tan\\tMNIST\\timage\\tin\\tany\\tdirection\\t(left,\\tright,\\tup,\\tor\\tdown)\\tby\\tone\\npixel.\\n5\\n\\tThen,\\tfor\\teach\\timage\\tin\\tthe\\ttraining\\tset,\\tcreate\\tfour\\tshifted\\tcopies\\t(one\\tper\\tdirection)\\tand\\tadd\\nthem\\tto\\tthe\\ttraining\\tset.\\tFinally,\\ttrain\\tyour\\tbest\\tmodel\\ton\\tthis\\texpanded\\ttraining\\tset\\tand\\tmeasure\\tits\\naccuracy\\ton\\tthe\\ttest\\tset.\\tYou\\tshould\\tobserve\\tthat\\tyour\\tmodel\\tperforms\\teven\\tbetter\\tnow!\\tThis\\ntechnique\\tof\\tartificially\\tgrowing\\tthe\\ttraining\\tset\\tis\\tcalled\\t\\ndata\\taugmentation\\n\\tor\\t\\ntraining\\tset\\nexpansion\\n.\\n3\\n.\\t\\nTackle\\tthe\\t\\nTitanic\\n\\tdataset.\\tA\\tgreat\\tplace\\tto\\tstart\\tis\\ton\\t\\nKaggle\\n.\\n4\\n.\\t\\nBuild\\ta\\tspam\\tclassifier\\t(a\\tmore\\tchallenging\\texercise):\\nDownload\\texamples\\tof\\tspam\\tand\\tham\\tfrom\\t\\nApache\\tSpamAssassin’s\\tpublic\\tdatasets\\n.\\nUnzip\\tthe\\tdatasets\\tand\\tfamiliarize\\tyourself\\twith\\tthe\\tdata\\tformat.\\nSplit\\tthe\\tdatasets\\tinto\\ta\\ttraining\\tset\\tand\\ta\\ttest\\tset.\\nWrite\\ta\\tdata\\tpreparation\\tpipeline\\tto\\tconvert\\teach\\temail\\tinto\\ta\\tfeature\\tvector.\\tYour\\tpreparation\\npipeline\\tshould\\ttransform\\tan\\temail\\tinto\\ta\\t(sparse)\\tvector\\tindicating\\tthe\\tpresence\\tor\\tabsence\\tof\\neach\\tpossible\\tword.\\tFor\\texample,\\tif\\tall\\temails\\tonly\\tever\\tcontain\\tfour\\twords,\\t“Hello,”\\t“how,”\\n“are,”\\t“you,”\\tthen\\tthe\\temail\\t“Hello\\tyou\\tHello\\tHello\\tyou”\\twould\\tbe\\tconverted\\tinto\\ta\\tvector\\t[1,\\n0,\\t0,\\t1]\\t(meaning\\t[“Hello”\\tis\\tpresent,\\t“how”\\tis\\tabsent,\\t“are”\\tis\\tabsent,\\t“you”\\tis\\tpresent]),\\tor\\n[3,\\t0,\\t0,\\t2]\\tif\\tyou\\tprefer\\tto\\tcount\\tthe\\tnumber\\tof\\toccurrences\\tof\\teach\\tword.\\nYou\\tmay\\twant\\tto\\tadd\\thyperparameters\\tto\\tyour\\tpreparation\\tpipeline\\tto\\tcontrol\\twhether\\tor\\tnot\\tto\\nstrip\\toff\\temail\\theaders,\\tconvert\\teach\\temail\\tto\\tlowercase,\\tremove\\tpunctuation,\\treplace\\tall\\tURLs\\nwith\\t“URL,”\\treplace\\tall\\tnumbers\\twith\\t“NUMBER,”\\tor\\teven\\t\\nperform\\t\\nstemming\\n\\t(i.e.,\\ttrim\\toff\\nword\\tendings;\\tthere\\tare\\tPython\\tlibraries\\tavailable\\tto\\tdo\\tthis).\\nThen\\ttry\\tout\\tseveral\\tclassifiers\\tand\\tsee\\tif\\tyou\\tcan\\tbuild\\ta\\tgreat\\tspam\\tclassifier,\\twith\\tboth\\thigh\\nrecall\\tand\\thigh\\tprecision.\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\tthe\\tonline\\tJupyter\\tnotebooks\\tat\\nhttps://github.com/ageron/handson-ml\\n.\\nBy\\tdefault\\tScikit-Learn\\tcaches\\tdownloaded\\tdatasets\\tin\\ta\\t\\ndirectory\\tcalled\\t\\n$HOME/scikit_learn_data\\n.\\nShuffling\\tmay\\tbe\\ta\\tbad\\tidea\\tin\\tsome\\tcontexts\\t—\\tfor\\texample,\\tif\\tyou\\tare\\tworking\\ton\\ttime\\tseries\\tdata\\t(such\\tas\\tstock\\tmarket\\tprices\\tor\\nweather\\tconditions).\\tWe\\twill\\texplore\\tthis\\tin\\tthe\\tnext\\tchapters.\\nBut\\tremember\\tthat\\tour\\tbrain\\tis\\ta\\tfantastic\\tpattern\\trecognition\\tsystem,\\tand\\tour\\tvisual\\tsystem\\tdoes\\ta\\tlot\\tof\\tcomplex\\tpreprocessing\\tbefore\\nany\\tinformation\\treaches\\tour\\tconsciousness,\\tso\\tthe\\tfact\\tthat\\tit\\tfeels\\tsimple\\tdoes\\tnot\\tmean\\tthat\\tit\\tis.\\n1\\n2\\n3\\n4', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 140}), Document(page_content='Scikit-Learn\\toffers\\ta\\tfew\\tother\\taveraging\\toptions\\tand\\tmultilabel\\tclassifier\\tmetrics;\\tsee\\tthe\\tdocumentation\\tfor\\tmore\\tdetails.\\nYou\\tcan\\tuse\\tthe\\t\\nshift()\\n\\tfunction\\tfrom\\tthe\\t\\nscipy.ndimage.interpolation\\n\\tmodule.\\tFor\\texample,\\t\\nshift(image,\\t[2,\\t1],\\tcval=0)\\n\\tshifts\\nthe\\timage\\t2\\tpixels\\tdown\\tand\\t1\\tpixel\\tto\\tthe\\tright.\\n4\\n5', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 141}), Document(page_content='Chapter\\t4.\\t\\nTraining\\tModels\\nSo\\t\\nfar\\twe\\thave\\ttreated\\tMachine\\tLearning\\tmodels\\tand\\ttheir\\ttraining\\talgorithms\\tmostly\\tlike\\tblack\\tboxes.\\tIf\\nyou\\twent\\tthrough\\tsome\\tof\\tthe\\texercises\\tin\\tthe\\tprevious\\tchapters,\\tyou\\tmay\\thave\\tbeen\\tsurprised\\tby\\thow\\nmuch\\tyou\\tcan\\tget\\tdone\\twithout\\tknowing\\tanything\\tabout\\twhat’s\\tunder\\tthe\\thood:\\tyou\\toptimized\\ta\\tregression\\nsystem,\\tyou\\timproved\\ta\\tdigit\\timage\\tclassifier,\\tand\\tyou\\teven\\tbuilt\\ta\\tspam\\tclassifier\\tfrom\\tscratch\\t—\\tall\\nthis\\twithout\\tknowing\\thow\\tthey\\tactually\\twork.\\tIndeed,\\tin\\tmany\\tsituations\\tyou\\tdon’t\\treally\\tneed\\tto\\tknow\\tthe\\nimplementation\\tdetails.\\nHowever,\\thaving\\ta\\tgood\\tunderstanding\\tof\\thow\\tthings\\twork\\tcan\\thelp\\tyou\\tquickly\\thome\\tin\\ton\\tthe\\nappropriate\\tmodel,\\tthe\\tright\\ttraining\\talgorithm\\tto\\tuse,\\tand\\ta\\tgood\\tset\\tof\\thyperparameters\\tfor\\tyour\\ttask.\\nUnderstanding\\twhat’s\\tunder\\tthe\\thood\\twill\\talso\\thelp\\tyou\\tdebug\\tissues\\tand\\tperform\\terror\\tanalysis\\tmore\\nefficiently.\\tLastly,\\tmost\\tof\\tthe\\ttopics\\tdiscussed\\tin\\tthis\\tchapter\\twill\\tbe\\tessential\\tin\\tunderstanding,\\tbuilding,\\nand\\ttraining\\tneural\\tnetworks\\t(discussed\\tin\\t\\nPart\\tII\\n\\tof\\tthis\\tbook).\\nIn\\tthis\\tchapter,\\twe\\twill\\tstart\\tby\\tlooking\\tat\\tthe\\t\\nLinear\\tRegression\\tmodel,\\tone\\tof\\tthe\\tsimplest\\tmodels\\tthere\\nis.\\tWe\\twill\\tdiscuss\\ttwo\\tvery\\tdifferent\\tways\\tto\\ttrain\\tit:\\nUsing\\ta\\tdirect\\t“closed-form”\\t\\nequation\\tthat\\tdirectly\\tcomputes\\tthe\\tmodel\\tparameters\\tthat\\tbest\\tfit\\tthe\\nmodel\\tto\\tthe\\ttraining\\tset\\t(i.e.,\\tthe\\tmodel\\tparameters\\tthat\\tminimize\\tthe\\tcost\\tfunction\\tover\\tthe\\ttraining\\nset).\\nUsing\\tan\\titerative\\toptimization\\tapproach,\\tcalled\\t\\nGradient\\tDescent\\t(GD),\\tthat\\tgradually\\ttweaks\\tthe\\nmodel\\tparameters\\tto\\tminimize\\tthe\\tcost\\tfunction\\t\\nover\\tthe\\ttraining\\tset,\\teventually\\tconverging\\tto\\tthe\\nsame\\tset\\tof\\tparameters\\tas\\tthe\\tfirst\\tmethod.\\tWe\\twill\\tlook\\tat\\ta\\tfew\\tvariants\\tof\\tGradient\\tDescent\\tthat\\nwe\\twill\\tuse\\tagain\\tand\\tagain\\twhen\\twe\\tstudy\\tneural\\tnetworks\\tin\\t\\nPart\\tII\\n:\\tBatch\\tGD,\\tMini-batch\\tGD,\\nand\\tStochastic\\tGD.\\nNext\\twe\\twill\\tlook\\tat\\t\\nPolynomial\\tRegression,\\ta\\tmore\\tcomplex\\tmodel\\tthat\\tcan\\tfit\\tnonlinear\\tdatasets.\\tSince\\nthis\\tmodel\\thas\\tmore\\tparameters\\tthan\\tLinear\\tRegression,\\tit\\tis\\tmore\\tprone\\tto\\toverfitting\\tthe\\ttraining\\tdata,\\nso\\twe\\twill\\tlook\\tat\\thow\\tto\\tdetect\\twhether\\tor\\tnot\\tthis\\tis\\tthe\\tcase,\\tusing\\tlearning\\tcurves,\\tand\\tthen\\twe\\twill\\nlook\\tat\\tseveral\\tregularization\\ttechniques\\tthat\\tcan\\treduce\\tthe\\trisk\\tof\\toverfitting\\tthe\\ttraining\\tset.\\nFinally,\\twe\\twill\\tlook\\tat\\ttwo\\tmore\\tmodels\\tthat\\tare\\tcommonly\\tused\\tfor\\tclassification\\ttasks:\\tLogistic\\nRegression\\tand\\tSoftmax\\tRegression.\\nWARNING\\nThere\\twill\\tbe\\tquite\\ta\\tfew\\tmath\\tequations\\tin\\tthis\\tchapter,\\tusing\\tbasic\\tnotions\\tof\\tlinear\\talgebra\\tand\\tcalculus.\\tTo\\tunderstand\\tthese\\nequations,\\tyou\\twill\\tneed\\tto\\tknow\\twhat\\tvectors\\tand\\tmatrices\\tare,\\thow\\tto\\ttranspose\\tthem,\\twhat\\tthe\\tdot\\tproduct\\tis,\\twhat\\tmatrix\\ninverse\\tis,\\tand\\twhat\\tpartial\\tderivatives\\tare.\\tIf\\tyou\\tare\\tunfamiliar\\twith\\tthese\\tconcepts,\\tplease\\tgo\\tthrough\\tthe\\tlinear\\talgebra\\tand\\ncalculus\\tintroductory\\ttutorials\\tavailable\\tas\\tJupyter\\tnotebooks\\tin\\tthe\\tonline\\tsupplemental\\tmaterial.\\tFor\\tthose\\twho\\tare\\ttruly\\tallergic\\nto\\tmathematics,\\tyou\\tshould\\tstill\\tgo\\tthrough\\tthis\\tchapter\\tand\\tsimply\\tskip\\tthe\\tequations;\\thopefully,\\tthe\\ttext\\twill\\tbe\\tsufficient\\tto\\thelp\\nyou\\tunderstand\\tmost\\tof\\tthe\\t\\nconcepts.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 142}), Document(page_content='Linear\\tRegression\\nIn\\t\\nChapter\\t1\\n,\\t\\nwe\\tlooked\\tat\\ta\\tsimple\\tregression\\tmodel\\tof\\tlife\\tsatisfaction:\\t\\nlife_satisfaction\\n\\t=\\t\\nθ\\n0\\n\\t+\\t\\nθ\\n1\\n\\t×\\nGDP_per_capita\\n.\\nThis\\tmodel\\tis\\tjust\\ta\\tlinear\\tfunction\\tof\\tthe\\tinput\\tfeature\\t\\nGDP_per_capita\\n.\\t\\nθ\\n0\\n\\tand\\t\\nθ\\n1\\n\\tare\\tthe\\tmodel’s\\nparameters.\\nMore\\tgenerally,\\ta\\tlinear\\tmodel\\tmakes\\ta\\tprediction\\tby\\tsimply\\tcomputing\\ta\\tweighted\\tsum\\tof\\tthe\\tinput\\nfeatures,\\tplus\\ta\\tconstant\\tcalled\\t\\nthe\\t\\nbias\\tterm\\n\\t(also\\tcalled\\tthe\\t\\nintercept\\tterm\\n),\\tas\\tshown\\tin\\t\\nEquation\\t4-1\\n.\\nEquation\\t4-1.\\t\\nLinear\\tRegression\\tmodel\\tprediction\\nŷ\\n\\tis\\tthe\\tpredicted\\tvalue.\\nn\\n\\tis\\tthe\\tnumber\\tof\\tfeatures.\\nx\\ni\\n\\tis\\tthe\\ti\\nth\\n\\tfeature\\tvalue.\\nθ\\nj\\n\\tis\\tthe\\tj\\nth\\n\\tmodel\\tparameter\\t(including\\tthe\\tbias\\tterm\\t\\nθ\\n0\\n\\tand\\tthe\\tfeature\\tweights\\t\\nθ\\n1\\n,\\t\\nθ\\n2\\n,\\t,\\t\\nθ\\nn\\n).\\nThis\\tcan\\tbe\\twritten\\tmuch\\tmore\\tconcisely\\tusing\\ta\\tvectorized\\tform,\\tas\\tshown\\tin\\t\\nEquation\\t4-2\\n.\\nEquation\\t4-2.\\t\\nLinear\\tRegression\\tmodel\\tprediction\\t(vectorized\\tform)\\nθ\\n\\tis\\tthe\\tmodel’s\\t\\nparameter\\tvector\\n,\\t\\ncontaining\\tthe\\tbias\\tterm\\t\\nθ\\n0\\n\\tand\\tthe\\tfeature\\tweights\\t\\nθ\\n1\\n\\tto\\t\\nθ\\nn\\n.\\nθ\\nT\\n\\tis\\tthe\\ttranspose\\tof\\t\\nθ\\n\\t(a\\trow\\tvector\\tinstead\\tof\\ta\\tcolumn\\tvector).\\nx\\n\\tis\\tthe\\tinstance’s\\t\\nfeature\\tvector\\n,\\t\\ncontaining\\t\\nx\\n0\\n\\tto\\t\\nx\\nn\\n,\\twith\\t\\nx\\n0\\n\\talways\\tequal\\tto\\t1.\\nθ\\nT\\n\\t·\\t\\nx\\n\\tis\\tthe\\tdot\\tproduct\\tof\\t\\nθ\\nT\\n\\tand\\t\\nx\\n.\\nh\\nθ\\n\\tis\\tthe\\t\\nhypothesis\\tfunction,\\tusing\\tthe\\tmodel\\tparameters\\t\\nθ\\n.\\nOkay,\\tthat’s\\tthe\\tLinear\\tRegression\\tmodel,\\tso\\tnow\\thow\\tdo\\twe\\ttrain\\tit?\\tWell,\\trecall\\tthat\\ttraining\\ta\\tmodel\\nmeans\\tsetting\\tits\\tparameters\\tso\\tthat\\tthe\\tmodel\\tbest\\tfits\\tthe\\ttraining\\tset.\\tFor\\tthis\\tpurpose,\\twe\\tfirst\\tneed\\ta\\nmeasure\\tof\\thow\\twell\\t(or\\tpoorly)\\tthe\\tmodel\\tfits\\tthe\\ttraining\\tdata.\\tIn\\t\\nChapter\\t2\\n\\twe\\tsaw\\tthat\\tthe\\tmost\\ncommon\\tperformance\\tmeasure\\tof\\ta\\tregression\\tmodel\\tis\\tthe\\t\\nRoot\\tMean\\tSquare\\tError\\t(RMSE)\\t(\\nEquation\\n2-1\\n).\\tTherefore,\\tto\\ttrain\\ta\\tLinear\\tRegression\\tmodel,\\tyou\\tneed\\tto\\tfind\\tthe\\tvalue\\tof\\t\\nθ\\n\\tthat\\tminimizes\\tthe', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 143}), Document(page_content='RMSE.\\tIn\\tpractice,\\tit\\tis\\tsimpler\\tto\\tminimize\\tthe\\t\\nMean\\tSquare\\tError\\t(MSE)\\tthan\\tthe\\tRMSE,\\tand\\tit\\tleads\\nto\\tthe\\tsame\\tresult\\t(because\\tthe\\tvalue\\tthat\\tminimizes\\ta\\tfunction\\talso\\tminimizes\\tits\\tsquare\\troot).\\n1\\nThe\\tMSE\\tof\\ta\\tLinear\\tRegression\\thypothesis\\t\\nh\\nθ\\n\\ton\\ta\\ttraining\\tset\\t\\nX\\n\\tis\\tcalculated\\tusing\\t\\nEquation\\t4-3\\n.\\nEquation\\t4-3.\\t\\nMSE\\tcost\\tfunction\\tfor\\ta\\tLinear\\tRegression\\tmodel\\nMost\\tof\\tthese\\tnotations\\twere\\tpresented\\tin\\t\\nChapter\\t2\\n\\t(see\\t\\n“Notations”\\n).\\tThe\\tonly\\tdifference\\tis\\tthat\\twe\\nwrite\\t\\nh\\nθ\\n\\tinstead\\tof\\tjust\\t\\nh\\n\\tin\\torder\\tto\\tmake\\tit\\tclear\\tthat\\tthe\\tmodel\\tis\\tparametrized\\tby\\tthe\\tvector\\t\\nθ\\n.\\tTo\\nsimplify\\tnotations,\\twe\\twill\\tjust\\twrite\\tMSE(\\nθ\\n)\\tinstead\\tof\\tMSE(\\nX\\n,\\t\\nh\\nθ\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 144}), Document(page_content='The\\tNormal\\tEquation\\nTo\\t\\nfind\\tthe\\tvalue\\tof\\t\\nθ\\n\\tthat\\tminimizes\\tthe\\t\\ncost\\tfunction,\\tthere\\tis\\ta\\t\\nclosed-form\\tsolution\\n\\t—\\tin\\tother\\twords,\\na\\tmathematical\\tequation\\tthat\\tgives\\tthe\\tresult\\tdirectly.\\tThis\\tis\\tcalled\\tthe\\t\\nNormal\\tEquation\\n\\t(\\nEquation\\t4-4\\n).\\n2\\nEquation\\t4-4.\\t\\nNormal\\tEquation\\n\\tis\\tthe\\tvalue\\tof\\t\\n\\tthat\\tminimizes\\tthe\\tcost\\tfunction.\\ny\\n\\tis\\tthe\\tvector\\tof\\ttarget\\tvalues\\tcontaining\\t\\ny\\n(1)\\n\\tto\\t\\ny\\n(\\nm\\n)\\n.\\nLet’s\\tgenerate\\tsome\\tlinear-looking\\tdata\\tto\\ttest\\tthis\\tequation\\ton\\t(\\nFigure\\t4-1\\n):\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nX\\n\\t\\n=\\n\\t\\n2\\n\\t\\n*\\n\\t\\nnp\\n.\\nrandom\\n.\\nrand\\n(\\n100\\n,\\n\\t\\n1\\n)\\ny\\n\\t\\n=\\n\\t\\n4\\n\\t\\n+\\n\\t\\n3\\n\\t\\n*\\n\\t\\nX\\n\\t\\n+\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n100\\n,\\n\\t\\n1\\n)\\nFigure\\t4-1.\\t\\nRandomly\\tgenerated\\tlinear\\tdataset\\nNow\\tlet’s\\tcompute\\t\\n\\tusing\\tthe\\tNormal\\tEquation.\\tWe\\twill\\tuse\\tthe\\t\\ninv()\\n\\tfunction\\tfrom\\tNumPy’s\\tLinear\\nAlgebra\\tmodule\\t(\\nnp.linalg\\n)\\tto\\tcompute\\tthe\\tinverse\\tof\\ta\\tmatrix,\\tand\\tthe\\t\\ndot()\\n\\tmethod\\tfor\\tmatrix\\nmultiplication:\\nX_b\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\nnp\\n.\\nones\\n((\\n100\\n,\\n\\t\\n1\\n)),\\n\\t\\nX\\n]\\n\\t\\t\\n#\\tadd\\tx0\\t=\\t1\\tto\\teach\\tinstance\\ntheta_best\\n\\t\\n=\\n\\t\\nnp\\n.\\nlinalg\\n.\\ninv\\n(\\nX_b\\n.\\nT\\n.\\ndot\\n(\\nX_b\\n))\\n.\\ndot\\n(\\nX_b\\n.\\nT\\n)\\n.\\ndot\\n(\\ny\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 145}), Document(page_content='The\\tactual\\tfunction\\tthat\\twe\\tused\\tto\\tgenerate\\tthe\\tdata\\tis\\t\\ny\\n\\t=\\t4\\t+\\t3\\nx\\n0\\n\\t+\\tGaussian\\tnoise.\\tLet’s\\tsee\\twhat\\tthe\\nequation\\tfound:\\n>>>\\t\\ntheta_best\\narray([[\\t4.21509616],\\n\\t\\t\\t\\t\\t\\t\\t[\\t2.77011339]])\\nWe\\twould\\thave\\thoped\\tfor\\t\\nθ\\n0\\n\\t=\\t4\\tand\\t\\nθ\\n1\\n\\t=\\t3\\tinstead\\tof\\t\\nθ\\n0\\n\\t=\\t4.215\\tand\\t\\nθ\\n1\\n\\t=\\t2.770.\\tClose\\tenough,\\tbut\\tthe\\nnoise\\tmade\\tit\\timpossible\\tto\\trecover\\tthe\\texact\\tparameters\\tof\\tthe\\toriginal\\tfunction.\\nNow\\tyou\\tcan\\tmake\\tpredictions\\tusing\\t\\n:\\n>>>\\t\\nX_new\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([[\\n0\\n],\\n\\t\\n[\\n2\\n]])\\n>>>\\t\\nX_new_b\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\nnp\\n.\\nones\\n((\\n2\\n,\\n\\t\\n1\\n)),\\n\\t\\nX_new\\n]\\n\\t\\n#\\tadd\\tx0\\t=\\t1\\tto\\teach\\tinstance\\n>>>\\t\\ny_predict\\n\\t\\n=\\n\\t\\nX_new_b\\n.\\ndot\\n(\\ntheta_best\\n)\\n>>>\\t\\ny_predict\\narray([[\\t4.21509616],\\n\\t\\t\\t\\t\\t\\t\\t[\\t9.75532293]])\\nLet’s\\tplot\\tthis\\tmodel’s\\tpredictions\\t(\\nFigure\\t4-2\\n):\\nplt\\n.\\nplot\\n(\\nX_new\\n,\\n\\t\\ny_predict\\n,\\n\\t\\n\"r-\"\\n)\\nplt\\n.\\nplot\\n(\\nX\\n,\\n\\t\\ny\\n,\\n\\t\\n\"b.\"\\n)\\nplt\\n.\\naxis\\n([\\n0\\n,\\n\\t\\n2\\n,\\n\\t\\n0\\n,\\n\\t\\n15\\n])\\nplt\\n.\\nshow\\n()\\nFigure\\t4-2.\\t\\nLinear\\tRegression\\tmodel\\tpredictions\\nThe\\tequivalent\\tcode\\t\\nusing\\tScikit-Learn\\tlooks\\tlike\\tthis:\\n3\\n>>>\\t\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLinearRegression\\n>>>\\t\\nlin_reg\\n\\t\\n=\\n\\t\\nLinearRegression\\n()\\n>>>\\t\\nlin_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\n>>>\\t\\nlin_reg\\n.\\nintercept_\\n,\\n\\t\\nlin_reg\\n.\\ncoef_\\n(array([\\t4.21509616]),\\tarray([[\\t2.77011339]]))\\n>>>\\t\\nlin_reg\\n.\\npredict\\n(\\nX_new\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 146}), Document(page_content='array([[\\t4.21509616],\\n\\t\\t\\t\\t\\t\\t\\t[\\t9.75532293]])', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 147}), Document(page_content='Computational\\tComplexity\\nThe\\t\\nNormal\\tEquation\\tcomputes\\tthe\\tinverse\\tof\\t\\nX\\nT\\n\\t·\\t\\nX\\n,\\twhich\\tis\\tan\\t\\nn\\n\\t×\\t\\nn\\n\\tmatrix\\t(where\\t\\nn\\n\\tis\\tthe\\tnumber\\tof\\nfeatures).\\tThe\\t\\ncomputational\\tcomplexity\\n\\tof\\tinverting\\tsuch\\ta\\tmatrix\\tis\\ttypically\\tabout\\t\\nO\\n(\\nn\\n2.4\\n)\\tto\\t\\nO\\n(\\nn\\n3\\n)\\n(depending\\ton\\tthe\\timplementation).\\tIn\\tother\\twords,\\tif\\tyou\\tdouble\\tthe\\tnumber\\tof\\tfeatures,\\tyou\\tmultiply\\tthe\\ncomputation\\ttime\\tby\\troughly\\t2\\n2.4\\n\\t=\\t5.3\\tto\\t2\\n3\\n\\t=\\t8.\\nWARNING\\nThe\\tNormal\\tEquation\\tgets\\tvery\\tslow\\twhen\\tthe\\tnumber\\tof\\tfeatures\\tgrows\\tlarge\\t(e.g.,\\t100,000).\\nOn\\tthe\\tpositive\\tside,\\tthis\\tequation\\tis\\tlinear\\twith\\tregards\\tto\\tthe\\tnumber\\tof\\tinstances\\tin\\tthe\\ttraining\\tset\\t(it\\tis\\nO\\n(\\nm\\n)),\\tso\\tit\\thandles\\tlarge\\ttraining\\tsets\\tefficiently,\\tprovided\\tthey\\tcan\\tfit\\tin\\tmemory.\\nAlso,\\tonce\\tyou\\thave\\ttrained\\tyour\\tLinear\\tRegression\\tmodel\\t(using\\tthe\\tNormal\\tEquation\\tor\\tany\\tother\\nalgorithm),\\tpredictions\\tare\\tvery\\tfast:\\tthe\\tcomputational\\tcomplexity\\tis\\tlinear\\twith\\tregards\\tto\\tboth\\tthe\\nnumber\\tof\\tinstances\\tyou\\twant\\tto\\tmake\\tpredictions\\ton\\tand\\tthe\\tnumber\\tof\\tfeatures.\\tIn\\tother\\twords,\\tmaking\\npredictions\\ton\\ttwice\\tas\\tmany\\tinstances\\t(or\\ttwice\\tas\\tmany\\tfeatures)\\twill\\tjust\\ttake\\troughly\\ttwice\\tas\\tmuch\\ntime.\\nNow\\twe\\twill\\tlook\\tat\\tvery\\tdifferent\\tways\\tto\\ttrain\\ta\\tLinear\\tRegression\\tmodel,\\tbetter\\tsuited\\tfor\\tcases\\nwhere\\tthere\\tare\\ta\\tlarge\\tnumber\\tof\\tfeatures,\\tor\\ttoo\\tmany\\ttraining\\tinstances\\tto\\tfit\\tin\\tmemory.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 148}), Document(page_content='Gradient\\tDescent\\nGradient\\tDescent\\n\\tis\\ta\\t\\nvery\\tgeneric\\toptimization\\talgorithm\\t\\ncapable\\tof\\tfinding\\toptimal\\tsolutions\\tto\\ta\\twide\\nrange\\tof\\tproblems.\\tThe\\tgeneral\\tidea\\tof\\tGradient\\tDescent\\tis\\tto\\ttweak\\tparameters\\titeratively\\tin\\torder\\tto\\nminimize\\ta\\t\\ncost\\tfunction.\\nSuppose\\tyou\\tare\\tlost\\tin\\tthe\\tmountains\\tin\\ta\\tdense\\tfog;\\tyou\\tcan\\tonly\\tfeel\\tthe\\tslope\\tof\\tthe\\tground\\tbelow\\tyour\\nfeet.\\tA\\tgood\\tstrategy\\tto\\tget\\tto\\tthe\\tbottom\\tof\\tthe\\tvalley\\tquickly\\tis\\tto\\tgo\\tdownhill\\tin\\tthe\\tdirection\\tof\\tthe\\nsteepest\\tslope.\\tThis\\tis\\texactly\\twhat\\tGradient\\tDescent\\tdoes:\\tit\\tmeasures\\tthe\\tlocal\\tgradient\\tof\\tthe\\terror\\nfunction\\twith\\tregards\\tto\\tthe\\t\\nparameter\\tvector\\t\\nθ\\n,\\tand\\tit\\tgoes\\tin\\tthe\\tdirection\\tof\\tdescending\\tgradient.\\tOnce\\nthe\\tgradient\\tis\\tzero,\\tyou\\thave\\treached\\ta\\tminimum!\\nConcretely,\\tyou\\tstart\\tby\\tfilling\\t\\nθ\\n\\twith\\trandom\\tvalues\\t(this\\tis\\tcalled\\t\\nrandom\\tinitialization\\n),\\t\\nand\\tthen\\tyou\\nimprove\\tit\\tgradually,\\ttaking\\tone\\tbaby\\tstep\\tat\\ta\\ttime,\\teach\\tstep\\tattempting\\tto\\tdecrease\\tthe\\tcost\\tfunction\\n(e.g.,\\tthe\\tMSE),\\tuntil\\tthe\\talgorithm\\t\\nconverges\\n\\tto\\ta\\tminimum\\t(see\\t\\nFigure\\t4-3\\n).\\nFigure\\t4-3.\\t\\nGradient\\tDescent\\nAn\\timportant\\tparameter\\tin\\tGradient\\tDescent\\tis\\tthe\\tsize\\tof\\tthe\\tsteps,\\tdetermined\\tby\\t\\nthe\\t\\nlearning\\trate\\nhyperparameter.\\tIf\\tthe\\tlearning\\trate\\tis\\ttoo\\tsmall,\\tthen\\tthe\\talgorithm\\twill\\thave\\tto\\tgo\\tthrough\\tmany\\titerations\\nto\\tconverge,\\twhich\\twill\\ttake\\ta\\tlong\\ttime\\t(see\\t\\nFigure\\t4-4\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 149}), Document(page_content='Figure\\t4-4.\\t\\nLearning\\trate\\ttoo\\tsmall\\nOn\\tthe\\tother\\thand,\\tif\\tthe\\tlearning\\trate\\tis\\ttoo\\thigh,\\tyou\\tmight\\tjump\\tacross\\tthe\\tvalley\\tand\\tend\\tup\\ton\\tthe\\tother\\nside,\\tpossibly\\teven\\thigher\\tup\\tthan\\tyou\\twere\\tbefore.\\tThis\\tmight\\tmake\\tthe\\talgorithm\\tdiverge,\\twith\\tlarger\\nand\\tlarger\\tvalues,\\tfailing\\tto\\tfind\\ta\\tgood\\tsolution\\t(see\\t\\nFigure\\t4-5\\n).\\nFigure\\t4-5.\\t\\nLearning\\trate\\ttoo\\tlarge\\nFinally,\\tnot\\tall\\tcost\\tfunctions\\tlook\\tlike\\tnice\\tregular\\tbowls.\\tThere\\tmay\\tbe\\tholes,\\tridges,\\tplateaus,\\tand\\tall\\nsorts\\tof\\tirregular\\tterrains,\\tmaking\\tconvergence\\tto\\tthe\\tminimum\\tvery\\tdifficult.\\t\\nFigure\\t4-6\\n\\tshows\\tthe\\ttwo\\nmain\\tchallenges\\twith\\tGradient\\tDescent:\\tif\\tthe\\trandom\\tinitialization\\tstarts\\tthe\\talgorithm\\ton\\tthe\\tleft,\\tthen\\tit\\nwill\\tconverge\\tto\\ta\\t\\nlocal\\tminimum\\n,\\twhich\\t\\nis\\tnot\\tas\\tgood\\tas\\tthe\\t\\nglobal\\tminimum\\n.\\tIf\\tit\\tstarts\\ton\\tthe\\tright,\\nthen\\tit\\twill\\ttake\\ta\\tvery\\tlong\\ttime\\tto\\tcross\\tthe\\tplateau,\\tand\\tif\\tyou\\tstop\\ttoo\\tearly\\tyou\\twill\\tnever\\treach\\tthe\\nglobal\\t\\nminimum.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 150}), Document(page_content='Figure\\t4-6.\\t\\nGradient\\tDescent\\tpitfalls\\nFortunately,\\tthe\\tMSE\\tcost\\tfunction\\tfor\\ta\\t\\nLinear\\tRegression\\tmodel\\thappens\\tto\\tbe\\ta\\t\\nconvex\\tfunction\\n,\\t\\nwhich\\nmeans\\tthat\\tif\\tyou\\tpick\\tany\\ttwo\\tpoints\\ton\\tthe\\tcurve,\\tthe\\tline\\tsegment\\tjoining\\tthem\\tnever\\tcrosses\\tthe\\tcurve.\\nThis\\timplies\\tthat\\tthere\\tare\\tno\\tlocal\\tminima,\\tjust\\tone\\tglobal\\tminimum.\\tIt\\tis\\talso\\ta\\tcontinuous\\tfunction\\twith\\na\\tslope\\tthat\\tnever\\tchanges\\tabruptly.\\n4\\n\\t\\nThese\\ttwo\\tfacts\\thave\\ta\\tgreat\\tconsequence:\\tGradient\\tDescent\\tis\\nguaranteed\\tto\\tapproach\\tarbitrarily\\tclose\\tthe\\tglobal\\tminimum\\t(if\\tyou\\twait\\tlong\\tenough\\tand\\tif\\tthe\\tlearning\\nrate\\tis\\tnot\\ttoo\\thigh).\\nIn\\tfact,\\tthe\\tcost\\tfunction\\thas\\tthe\\tshape\\tof\\ta\\tbowl,\\tbut\\tit\\tcan\\tbe\\tan\\telongated\\tbowl\\tif\\tthe\\tfeatures\\thave\\tvery\\ndifferent\\tscales.\\t\\nFigure\\t4-7\\n\\tshows\\tGradient\\tDescent\\ton\\ta\\ttraining\\tset\\twhere\\tfeatures\\t1\\tand\\t2\\thave\\tthe\\nsame\\tscale\\t(on\\tthe\\tleft),\\tand\\ton\\ta\\ttraining\\tset\\twhere\\tfeature\\t1\\thas\\tmuch\\tsmaller\\tvalues\\tthan\\tfeature\\t2\\t(on\\nthe\\tright).\\n5\\nFigure\\t4-7.\\t\\nGradient\\tDescent\\twith\\tand\\twithout\\tfeature\\tscaling\\nAs\\tyou\\tcan\\tsee,\\ton\\tthe\\tleft\\tthe\\tGradient\\tDescent\\talgorithm\\tgoes\\tstraight\\ttoward\\tthe\\tminimum,\\tthereby\\nreaching\\tit\\tquickly,\\twhereas\\ton\\tthe\\tright\\tit\\tfirst\\tgoes\\tin\\ta\\tdirection\\talmost\\torthogonal\\tto\\tthe\\tdirection\\tof\\nthe\\tglobal\\tminimum,\\tand\\tit\\tends\\twith\\ta\\tlong\\tmarch\\tdown\\tan\\talmost\\tflat\\tvalley.\\tIt\\twill\\teventually\\treach\\tthe\\nminimum,\\tbut\\tit\\twill\\ttake\\ta\\tlong\\ttime.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 151}), Document(page_content='WARNING\\nWhen\\tusing\\tGradient\\tDescent,\\tyou\\tshould\\tensure\\tthat\\tall\\tfeatures\\thave\\ta\\tsimilar\\tscale\\t(e.g.,\\tusing\\tScikit-Learn’s\\t\\nStandardScaler\\nclass),\\tor\\t\\nelse\\tit\\twill\\ttake\\tmuch\\tlonger\\tto\\tconverge.\\nThis\\tdiagram\\talso\\tillustrates\\tthe\\tfact\\tthat\\ttraining\\ta\\tmodel\\tmeans\\tsearching\\tfor\\ta\\tcombination\\tof\\tmodel\\nparameters\\t\\nthat\\tminimizes\\ta\\tcost\\tfunction\\t(over\\tthe\\ttraining\\tset).\\tIt\\tis\\ta\\tsearch\\tin\\tthe\\tmodel’s\\t\\nparameter\\nspace\\n:\\tthe\\tmore\\t\\nparameters\\ta\\tmodel\\thas,\\tthe\\tmore\\tdimensions\\tthis\\tspace\\thas,\\tand\\tthe\\tharder\\tthe\\tsearch\\tis:\\nsearching\\tfor\\ta\\tneedle\\tin\\ta\\t300-dimensional\\thaystack\\tis\\tmuch\\ttrickier\\tthan\\tin\\tthree\\tdimensions.\\nFortunately,\\tsince\\tthe\\tcost\\tfunction\\tis\\tconvex\\tin\\tthe\\tcase\\tof\\tLinear\\tRegression,\\tthe\\tneedle\\tis\\tsimply\\tat\\tthe\\nbottom\\tof\\tthe\\tbowl.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 152}), Document(page_content='Batch\\tGradient\\tDescent\\nTo\\t\\nimplement\\tGradient\\tDescent,\\tyou\\tneed\\tto\\tcompute\\tthe\\tgradient\\tof\\tthe\\tcost\\tfunction\\t\\nwith\\tregards\\tto\\neach\\tmodel\\tparameter\\t\\nθ\\nj\\n.\\tIn\\tother\\twords,\\tyou\\tneed\\tto\\tcalculate\\thow\\tmuch\\tthe\\tcost\\tfunction\\twill\\tchange\\tif\\nyou\\tchange\\t\\nθ\\nj\\n\\tjust\\ta\\tlittle\\tbit.\\tThis\\tis\\tcalled\\t\\na\\t\\npartial\\tderivative\\n.\\tIt\\tis\\tlike\\tasking\\t“what\\tis\\tthe\\tslope\\tof\\tthe\\nmountain\\tunder\\tmy\\tfeet\\tif\\tI\\tface\\teast?”\\tand\\tthen\\tasking\\tthe\\tsame\\tquestion\\tfacing\\tnorth\\t(and\\tso\\ton\\tfor\\tall\\nother\\tdimensions,\\tif\\tyou\\tcan\\timagine\\ta\\tuniverse\\twith\\tmore\\tthan\\tthree\\tdimensions).\\t\\nEquation\\t4-5\\n\\tcomputes\\nthe\\tpartial\\tderivative\\tof\\tthe\\tcost\\tfunction\\twith\\tregards\\tto\\tparameter\\t\\nθ\\nj\\n,\\tnoted\\t\\n.\\nEquation\\t4-5.\\t\\nPartial\\tderivatives\\tof\\tthe\\tcost\\tfunction\\nInstead\\tof\\tcomputing\\tthese\\tpartial\\tderivatives\\tindividually,\\tyou\\tcan\\tuse\\t\\nEquation\\t4-6\\n\\tto\\tcompute\\tthem\\tall\\nin\\tone\\tgo.\\tThe\\tgradient\\tvector,\\tnoted\\t\\nθ\\nMSE(\\nθ\\n),\\tcontains\\tall\\tthe\\tpartial\\tderivatives\\tof\\tthe\\tcost\\tfunction\\n(one\\tfor\\teach\\tmodel\\tparameter).\\nEquation\\t4-6.\\t\\nGradient\\tvector\\tof\\tthe\\tcost\\tfunction\\nWARNING\\nNotice\\tthat\\tthis\\tformula\\tinvolves\\tcalculations\\tover\\tthe\\tfull\\ttraining\\tset\\t\\nX\\n,\\tat\\teach\\tGradient\\tDescent\\tstep!\\tThis\\tis\\twhy\\tthe\\talgorithm\\nis\\tcalled\\t\\nBatch\\tGradient\\tDescent\\n:\\tit\\tuses\\tthe\\twhole\\tbatch\\tof\\ttraining\\tdata\\tat\\tevery\\tstep.\\tAs\\ta\\tresult\\tit\\tis\\tterribly\\tslow\\ton\\tvery\\nlarge\\ttraining\\tsets\\t(but\\twe\\twill\\tsee\\tmuch\\tfaster\\tGradient\\tDescent\\talgorithms\\tshortly).\\tHowever,\\tGradient\\tDescent\\tscales\\twell\\nwith\\tthe\\tnumber\\tof\\tfeatures;\\ttraining\\ta\\tLinear\\tRegression\\tmodel\\twhen\\tthere\\tare\\thundreds\\tof\\tthousands\\tof\\tfeatures\\tis\\tmuch\\tfaster\\nusing\\tGradient\\tDescent\\tthan\\tusing\\tthe\\tNormal\\tEquation.\\nOnce\\tyou\\thave\\tthe\\tgradient\\tvector,\\twhich\\tpoints\\tuphill,\\tjust\\tgo\\tin\\tthe\\topposite\\tdirection\\tto\\tgo\\tdownhill.\\nThis\\tmeans\\tsubtracting\\t\\nθ\\nMSE(\\nθ\\n)\\tfrom\\t\\nθ\\n.\\tThis\\tis\\twhere\\tthe\\t\\nlearning\\trate\\t\\nη\\n\\tcomes\\tinto\\tplay:\\n6\\n\\tmultiply\\tthe\\ngradient\\tvector\\tby\\t\\nη\\n\\tto\\tdetermine\\tthe\\tsize\\tof\\tthe\\tdownhill\\tstep\\t(\\nEquation\\t4-7\\n).\\nEquation\\t4-7.\\t\\nGradient\\tDescent\\tstep', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 153}), Document(page_content='Let’s\\tlook\\tat\\ta\\tquick\\timplementation\\tof\\tthis\\talgorithm:\\neta\\n\\t\\n=\\n\\t\\n0.1\\n\\t\\t\\n#\\tlearning\\trate\\nn_iterations\\n\\t\\n=\\n\\t\\n1000\\nm\\n\\t\\n=\\n\\t\\n100\\ntheta\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n2\\n,\\n1\\n)\\n\\t\\t\\n#\\trandom\\tinitialization\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\ngradients\\n\\t\\n=\\n\\t\\n2\\n/\\nm\\n\\t\\n*\\n\\t\\nX_b\\n.\\nT\\n.\\ndot\\n(\\nX_b\\n.\\ndot\\n(\\ntheta\\n)\\n\\t\\n-\\n\\t\\ny\\n)\\n\\t\\t\\t\\t\\ntheta\\n\\t\\n=\\n\\t\\ntheta\\n\\t\\n-\\n\\t\\neta\\n\\t\\n*\\n\\t\\ngradients\\nThat\\twasn’t\\ttoo\\thard!\\tLet’s\\t\\nlook\\tat\\tthe\\tresulting\\t\\ntheta\\n:\\n>>>\\t\\ntheta\\narray([[\\t4.21509616],\\n\\t\\t\\t\\t\\t\\t\\t[\\t2.77011339]])\\nHey,\\tthat’s\\texactly\\twhat\\tthe\\tNormal\\tEquation\\tfound!\\tGradient\\tDescent\\tworked\\tperfectly.\\tBut\\twhat\\tif\\tyou\\nhad\\tused\\ta\\tdifferent\\tlearning\\trate\\t\\neta\\n?\\t\\nFigure\\t4-8\\n\\tshows\\tthe\\tfirst\\t10\\tsteps\\tof\\tGradient\\tDescent\\tusing\\tthree\\ndifferent\\tlearning\\trates\\t(the\\tdashed\\tline\\trepresents\\tthe\\tstarting\\tpoint).\\nFigure\\t4-8.\\t\\nGradient\\tDescent\\twith\\tvarious\\tlearning\\trates\\nOn\\tthe\\tleft,\\tthe\\tlearning\\trate\\tis\\ttoo\\tlow:\\tthe\\talgorithm\\twill\\teventually\\treach\\tthe\\tsolution,\\tbut\\tit\\twill\\ttake\\ta\\nlong\\ttime.\\tIn\\tthe\\tmiddle,\\tthe\\tlearning\\trate\\tlooks\\tpretty\\tgood:\\tin\\tjust\\ta\\tfew\\titerations,\\tit\\thas\\talready\\nconverged\\tto\\tthe\\tsolution.\\tOn\\tthe\\tright,\\tthe\\tlearning\\trate\\tis\\ttoo\\thigh:\\tthe\\talgorithm\\tdiverges,\\tjumping\\tall\\nover\\tthe\\tplace\\tand\\tactually\\tgetting\\tfurther\\tand\\tfurther\\taway\\tfrom\\tthe\\tsolution\\tat\\tevery\\tstep.\\nTo\\tfind\\ta\\tgood\\tlearning\\trate,\\tyou\\tcan\\tuse\\tgrid\\tsearch\\t(see\\t\\nChapter\\t2\\n).\\tHowever,\\tyou\\tmay\\twant\\tto\\tlimit\\tthe\\nnumber\\tof\\titerations\\tso\\tthat\\tgrid\\tsearch\\tcan\\teliminate\\tmodels\\tthat\\ttake\\ttoo\\tlong\\tto\\tconverge.\\nYou\\tmay\\twonder\\thow\\tto\\tset\\tthe\\tnumber\\tof\\titerations.\\tIf\\tit\\tis\\ttoo\\tlow,\\tyou\\twill\\tstill\\tbe\\tfar\\taway\\tfrom\\tthe\\noptimal\\tsolution\\twhen\\tthe\\talgorithm\\tstops,\\tbut\\tif\\tit\\tis\\ttoo\\thigh,\\tyou\\twill\\twaste\\ttime\\twhile\\tthe\\tmodel\\nparameters\\t\\ndo\\tnot\\tchange\\tanymore.\\tA\\tsimple\\tsolution\\tis\\tto\\tset\\ta\\tvery\\tlarge\\tnumber\\tof\\titerations\\tbut\\tto\\ninterrupt\\tthe\\talgorithm\\twhen\\tthe\\tgradient\\tvector\\tbecomes\\ttiny\\t—\\tthat\\tis,\\twhen\\tits\\tnorm\\tbecomes\\tsmaller\\nthan\\ta\\ttiny\\tnumber\\t\\nϵ\\n\\t(called\\tthe\\t\\ntolerance\\n)\\t—\\tbecause\\tthis\\thappens\\twhen\\tGradient\\tDescent\\thas\\t(almost)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 154}), Document(page_content='reached\\tthe\\tminimum.\\nCONVERGENCE\\tRATE\\nWhen\\tthe\\t\\ncost\\tfunction\\tis\\tconvex\\tand\\tits\\tslope\\tdoes\\tnot\\tchange\\tabruptly\\t(as\\tis\\tthe\\tcase\\tfor\\tthe\\tMSE\\tcost\\tfunction),\\t\\nit\\tcan\\tbe\\tshown\\tthat\\nBatch\\tGradient\\tDescent\\twith\\ta\\tfixed\\tlearning\\trate\\thas\\ta\\t\\nconvergence\\trate\\n\\tof\\t\\n.\\tIn\\tother\\twords,\\tif\\tyou\\tdivide\\tthe\\ntolerance\\t\\nϵ\\n\\tby\\t10\\t(to\\thave\\ta\\tmore\\tprecise\\tsolution),\\tthen\\tthe\\talgorithm\\twill\\thave\\tto\\trun\\tabout\\t10\\ttimes\\tmore\\t\\niterations.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 155}), Document(page_content='Stochastic\\tGradient\\tDescent\\nThe\\t\\nmain\\tproblem\\twith\\tBatch\\tGradient\\tDescent\\tis\\tthe\\tfact\\tthat\\tit\\tuses\\tthe\\twhole\\ttraining\\tset\\tto\\tcompute\\nthe\\tgradients\\tat\\tevery\\tstep,\\twhich\\tmakes\\tit\\tvery\\tslow\\twhen\\tthe\\ttraining\\tset\\tis\\tlarge.\\tAt\\tthe\\topposite\\nextreme,\\t\\nStochastic\\tGradient\\tDescent\\n\\tjust\\tpicks\\ta\\trandom\\tinstance\\tin\\tthe\\ttraining\\tset\\tat\\tevery\\tstep\\tand\\ncomputes\\tthe\\tgradients\\tbased\\tonly\\ton\\tthat\\tsingle\\tinstance.\\tObviously\\tthis\\tmakes\\tthe\\talgorithm\\tmuch\\tfaster\\nsince\\tit\\thas\\tvery\\tlittle\\tdata\\tto\\tmanipulate\\tat\\tevery\\titeration.\\tIt\\talso\\tmakes\\tit\\tpossible\\tto\\ttrain\\ton\\thuge\\ntraining\\tsets,\\tsince\\tonly\\tone\\tinstance\\tneeds\\tto\\tbe\\tin\\tmemory\\tat\\teach\\titeration\\t(SGD\\tcan\\tbe\\timplemented\\tas\\nan\\tout-of-core\\talgorithm.\\n7\\n)\\nOn\\tthe\\tother\\thand,\\tdue\\tto\\tits\\tstochastic\\t(i.e.,\\trandom)\\tnature,\\tthis\\talgorithm\\tis\\tmuch\\tless\\tregular\\tthan\\tBatch\\nGradient\\tDescent:\\tinstead\\tof\\tgently\\tdecreasing\\tuntil\\tit\\treaches\\tthe\\tminimum,\\tthe\\tcost\\tfunction\\twill\\tbounce\\nup\\tand\\tdown,\\tdecreasing\\tonly\\ton\\taverage.\\tOver\\ttime\\tit\\twill\\tend\\tup\\tvery\\tclose\\tto\\tthe\\tminimum,\\tbut\\tonce\\tit\\ngets\\tthere\\tit\\twill\\tcontinue\\tto\\tbounce\\taround,\\tnever\\tsettling\\tdown\\t(see\\t\\nFigure\\t4-9\\n).\\tSo\\tonce\\tthe\\talgorithm\\nstops,\\tthe\\tfinal\\tparameter\\tvalues\\tare\\tgood,\\tbut\\tnot\\toptimal.\\nFigure\\t4-9.\\t\\nStochastic\\tGradient\\tDescent\\nWhen\\tthe\\tcost\\tfunction\\tis\\tvery\\tirregular\\t(as\\tin\\t\\nFigure\\t4-6\\n),\\tthis\\tcan\\tactually\\thelp\\tthe\\talgorithm\\tjump\\tout\\tof\\nlocal\\tminima,\\tso\\tStochastic\\tGradient\\tDescent\\thas\\ta\\tbetter\\tchance\\tof\\tfinding\\tthe\\tglobal\\tminimum\\tthan\\nBatch\\tGradient\\tDescent\\tdoes.\\nTherefore\\trandomness\\tis\\tgood\\tto\\tescape\\tfrom\\tlocal\\toptima,\\tbut\\tbad\\tbecause\\tit\\tmeans\\tthat\\tthe\\talgorithm\\ncan\\tnever\\tsettle\\tat\\tthe\\tminimum.\\tOne\\tsolution\\tto\\tthis\\tdilemma\\tis\\tto\\tgradually\\treduce\\tthe\\t\\nlearning\\trate.\\tThe\\nsteps\\tstart\\tout\\tlarge\\t(which\\thelps\\tmake\\tquick\\tprogress\\tand\\tescape\\tlocal\\tminima),\\tthen\\tget\\tsmaller\\tand\\nsmaller,\\tallowing\\tthe\\talgorithm\\tto\\tsettle\\tat\\tthe\\tglobal\\tminimum.\\tThis\\tprocess\\tis\\tcalled\\n\\t\\nsimulated', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 156}), Document(page_content='annealing\\n,\\tbecause\\tit\\tresembles\\tthe\\tprocess\\tof\\tannealing\\tin\\tmetallurgy\\twhere\\tmolten\\tmetal\\tis\\tslowly\\ncooled\\tdown.\\tThe\\tfunction\\tthat\\tdetermines\\tthe\\tlearning\\trate\\tat\\teach\\titeration\\tis\\tcalled\\tthe\\t\\nlearning\\nschedule\\n.\\t\\nIf\\tthe\\tlearning\\trate\\tis\\treduced\\ttoo\\tquickly,\\tyou\\tmay\\tget\\tstuck\\tin\\ta\\tlocal\\tminimum,\\tor\\teven\\tend\\tup\\nfrozen\\thalfway\\tto\\tthe\\tminimum.\\tIf\\tthe\\tlearning\\trate\\tis\\treduced\\ttoo\\tslowly,\\tyou\\tmay\\tjump\\taround\\tthe\\nminimum\\tfor\\ta\\tlong\\ttime\\tand\\tend\\tup\\twith\\ta\\tsuboptimal\\tsolution\\tif\\tyou\\thalt\\ttraining\\ttoo\\tearly.\\nThis\\tcode\\timplements\\tStochastic\\tGradient\\tDescent\\tusing\\ta\\t\\nsimple\\tlearning\\tschedule:\\nn_epochs\\n\\t\\n=\\n\\t\\n50\\nt0\\n,\\n\\t\\nt1\\n\\t\\n=\\n\\t\\n5\\n,\\n\\t\\n50\\n\\t\\t\\n#\\tlearning\\tschedule\\thyperparameters\\ndef\\n\\t\\nlearning_schedule\\n(\\nt\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nt0\\n\\t\\n/\\n\\t\\n(\\nt\\n\\t\\n+\\n\\t\\nt1\\n)\\ntheta\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n2\\n,\\n1\\n)\\n\\t\\t\\n#\\trandom\\tinitialization\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\nm\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nrandom_index\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandint\\n(\\nm\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nxi\\n\\t\\n=\\n\\t\\nX_b\\n[\\nrandom_index\\n:\\nrandom_index\\n+\\n1\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nyi\\n\\t\\n=\\n\\t\\ny\\n[\\nrandom_index\\n:\\nrandom_index\\n+\\n1\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ngradients\\n\\t\\n=\\n\\t\\n2\\n\\t\\n*\\n\\t\\nxi\\n.\\nT\\n.\\ndot\\n(\\nxi\\n.\\ndot\\n(\\ntheta\\n)\\n\\t\\n-\\n\\t\\nyi\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\neta\\n\\t\\n=\\n\\t\\nlearning_schedule\\n(\\nepoch\\n\\t\\n*\\n\\t\\nm\\n\\t\\n+\\n\\t\\ni\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntheta\\n\\t\\n=\\n\\t\\ntheta\\n\\t\\n-\\n\\t\\neta\\n\\t\\n*\\n\\t\\ngradients\\nBy\\tconvention\\twe\\titerate\\tby\\trounds\\tof\\t\\nm\\n\\titerations;\\teach\\tround\\tis\\tcalled\\tan\\t\\nepoch\\n.\\t\\nWhile\\tthe\\tBatch\\nGradient\\tDescent\\tcode\\titerated\\t1,000\\ttimes\\tthrough\\tthe\\twhole\\ttraining\\tset,\\tthis\\tcode\\tgoes\\tthrough\\tthe\\ntraining\\tset\\tonly\\t50\\ttimes\\tand\\treaches\\ta\\tfairly\\tgood\\tsolution:\\n>>>\\t\\ntheta\\narray([[\\t4.21076011],\\n\\t\\t\\t\\t\\t\\t[\\t2.74856079]])\\nFigure\\t4-10\\n\\tshows\\tthe\\tfirst\\t10\\tsteps\\tof\\ttraining\\t(notice\\thow\\tirregular\\tthe\\tsteps\\tare).\\nFigure\\t4-10.\\t\\nStochastic\\tGradient\\tDescent\\tfirst\\t10\\tsteps', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 157}), Document(page_content='Note\\tthat\\tsince\\tinstances\\tare\\tpicked\\trandomly,\\tsome\\tinstances\\tmay\\tbe\\tpicked\\tseveral\\ttimes\\tper\\tepoch\\nwhile\\tothers\\tmay\\tnot\\tbe\\tpicked\\tat\\tall.\\tIf\\tyou\\twant\\tto\\tbe\\tsure\\tthat\\tthe\\talgorithm\\tgoes\\tthrough\\tevery\\tinstance\\nat\\teach\\tepoch,\\tanother\\tapproach\\tis\\tto\\tshuffle\\tthe\\ttraining\\tset,\\tthen\\tgo\\tthrough\\tit\\tinstance\\tby\\tinstance,\\tthen\\nshuffle\\tit\\tagain,\\tand\\tso\\ton.\\tHowever,\\tthis\\tgenerally\\tconverges\\tmore\\tslowly.\\nTo\\tperform\\t\\nLinear\\tRegression\\tusing\\tSGD\\twith\\t\\nScikit-Learn,\\tyou\\tcan\\tuse\\tthe\\t\\nSGDRegressor\\n\\tclass,\\twhich\\ndefaults\\tto\\toptimizing\\tthe\\tsquared\\terror\\tcost\\tfunction.\\tThe\\tfollowing\\tcode\\truns\\t50\\tepochs,\\tstarting\\twith\\ta\\nlearning\\trate\\tof\\t0.1\\t(\\neta0=0.1\\n),\\tusing\\tthe\\tdefault\\tlearning\\tschedule\\t(different\\tfrom\\tthe\\tpreceding\\tone),\\nand\\tit\\tdoes\\tnot\\tuse\\tany\\t\\nregularization\\t(\\npenalty=None\\n;\\tmore\\tdetails\\ton\\tthis\\tshortly):\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nSGDRegressor\\nsgd_reg\\n\\t\\n=\\n\\t\\nSGDRegressor\\n(\\nn_iter\\n=\\n50\\n,\\n\\t\\npenalty\\n=\\nNone\\n,\\n\\t\\neta0\\n=\\n0.1\\n)\\nsgd_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n.\\nravel\\n())\\nOnce\\tagain,\\tyou\\tfind\\ta\\tsolution\\tvery\\tclose\\tto\\tthe\\tone\\treturned\\tby\\tthe\\t\\nNormal\\tEquation:\\n>>>\\t\\nsgd_reg\\n.\\nintercept_\\n,\\n\\t\\nsgd_reg\\n.\\ncoef_\\n(array([\\t4.16782089]),\\tarray([\\t2.72603052]))', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 158}), Document(page_content='Mini-batch\\tGradient\\tDescent\\nThe\\t\\nlast\\tGradient\\tDescent\\talgorithm\\twe\\twill\\tlook\\tat\\tis\\tcalled\\t\\nMini-batch\\tGradient\\tDescent\\n.\\tIt\\tis\\tquite\\nsimple\\tto\\tunderstand\\tonce\\tyou\\tknow\\tBatch\\tand\\tStochastic\\tGradient\\tDescent:\\tat\\teach\\tstep,\\tinstead\\tof\\ncomputing\\tthe\\tgradients\\tbased\\ton\\tthe\\tfull\\ttraining\\tset\\t(as\\tin\\tBatch\\tGD)\\tor\\tbased\\ton\\tjust\\tone\\tinstance\\t(as\\tin\\nStochastic\\tGD),\\tMini-batch\\tGD\\tcomputes\\tthe\\tgradients\\ton\\tsmall\\trandom\\tsets\\tof\\tinstances\\tcalled\\t\\nmini-\\nbatches\\n.\\tThe\\tmain\\tadvantage\\tof\\tMini-batch\\tGD\\tover\\tStochastic\\tGD\\tis\\tthat\\tyou\\tcan\\tget\\ta\\tperformance\\nboost\\tfrom\\thardware\\toptimization\\tof\\tmatrix\\toperations,\\tespecially\\twhen\\tusing\\tGPUs.\\nThe\\talgorithm’s\\tprogress\\tin\\tparameter\\tspace\\tis\\tless\\terratic\\tthan\\twith\\tSGD,\\tespecially\\twith\\tfairly\\tlarge\\nmini-batches.\\tAs\\ta\\tresult,\\tMini-batch\\tGD\\twill\\tend\\tup\\twalking\\taround\\ta\\tbit\\tcloser\\tto\\tthe\\tminimum\\tthan\\nSGD.\\tBut,\\ton\\tthe\\tother\\thand,\\tit\\tmay\\tbe\\tharder\\tfor\\tit\\tto\\tescape\\tfrom\\tlocal\\tminima\\t(in\\tthe\\tcase\\tof\\tproblems\\nthat\\tsuffer\\tfrom\\tlocal\\tminima,\\tunlike\\tLinear\\tRegression\\tas\\twe\\tsaw\\tearlier).\\t\\nFigure\\t4-11\\n\\tshows\\tthe\\tpaths\\ntaken\\tby\\tthe\\tthree\\tGradient\\tDescent\\talgorithms\\tin\\tparameter\\tspace\\tduring\\ttraining.\\tThey\\tall\\tend\\tup\\tnear\\nthe\\tminimum,\\tbut\\tBatch\\tGD’s\\tpath\\tactually\\tstops\\tat\\tthe\\tminimum,\\twhile\\tboth\\tStochastic\\tGD\\tand\\tMini-\\nbatch\\tGD\\tcontinue\\tto\\twalk\\taround.\\tHowever,\\tdon’t\\tforget\\tthat\\tBatch\\tGD\\ttakes\\ta\\tlot\\tof\\ttime\\tto\\ttake\\teach\\nstep,\\tand\\tStochastic\\tGD\\tand\\tMini-batch\\tGD\\twould\\talso\\treach\\tthe\\tminimum\\tif\\tyou\\tused\\ta\\tgood\\tlearning\\nschedule.\\nFigure\\t4-11.\\t\\nGradient\\tDescent\\tpaths\\tin\\tparameter\\tspace\\nLet’s\\tcompare\\tthe\\talgorithms\\twe’ve\\tdiscussed\\tso\\tfar\\tfor\\tLinear\\tRegression\\n8\\n\\t(recall\\tthat\\t\\nm\\n\\tis\\tthe\\tnumber\\nof\\ttraining\\tinstances\\tand\\t\\nn\\n\\tis\\tthe\\tnumber\\tof\\tfeatures);\\tsee\\t\\nTable\\t4-1\\n.\\nTable\\t4-1.\\t\\nComparison\\tof\\talgorithms\\tfor\\tLinear\\tRegression\\nAlgorithm\\nLarge\\t\\nm\\nOut-of-core\\tsupport\\nLarge\\t\\nn\\nHyperparams\\nScaling\\trequired\\nScikit-Learn\\nNormal\\tEquation\\nFast\\nNo\\nSlow\\n0\\nNo\\nLinearRegression\\nBatch\\tGD\\nSlow\\nNo\\nFast\\n2\\nYes\\nn/a', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 159}), Document(page_content='Stochastic\\tGD\\nFast\\nYes\\nFast\\n≥2\\nYes\\nSGDRegressor\\nMini-batch\\tGD\\nFast\\nYes\\nFast\\n≥2\\nYes\\nn/a\\nNOTE\\nThere\\tis\\talmost\\tno\\tdifference\\tafter\\ttraining:\\tall\\tthese\\talgorithms\\tend\\tup\\twith\\tvery\\tsimilar\\tmodels\\tand\\tmake\\tpredictions\\tin\\texactly\\nthe\\tsame\\t\\nway.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 160}), Document(page_content='Polynomial\\tRegression\\nWhat\\t\\nif\\tyour\\tdata\\tis\\tactually\\tmore\\tcomplex\\tthan\\ta\\tsimple\\tstraight\\tline?\\tSurprisingly,\\tyou\\tcan\\tactually\\tuse\\na\\tlinear\\tmodel\\tto\\tfit\\tnonlinear\\tdata.\\tA\\tsimple\\tway\\tto\\tdo\\tthis\\tis\\tto\\tadd\\tpowers\\tof\\teach\\tfeature\\tas\\tnew\\nfeatures,\\tthen\\ttrain\\ta\\tlinear\\tmodel\\ton\\tthis\\textended\\tset\\tof\\tfeatures.\\tThis\\ttechnique\\tis\\tcalled\\t\\nPolynomial\\nRegression\\n.\\nLet’s\\tlook\\tat\\tan\\texample.\\tFirst,\\tlet’s\\tgenerate\\tsome\\tnonlinear\\tdata,\\tbased\\ton\\ta\\tsimple\\t\\nquadratic\\tequation\\n9\\n(plus\\tsome\\tnoise;\\tsee\\t\\nFigure\\t4-12\\n):\\nm\\n\\t\\n=\\n\\t\\n100\\nX\\n\\t\\n=\\n\\t\\n6\\n\\t\\n*\\n\\t\\nnp\\n.\\nrandom\\n.\\nrand\\n(\\nm\\n,\\n\\t\\n1\\n)\\n\\t\\n-\\n\\t\\n3\\ny\\n\\t\\n=\\n\\t\\n0.5\\n\\t\\n*\\n\\t\\nX\\n**\\n2\\n\\t\\n+\\n\\t\\nX\\n\\t\\n+\\n\\t\\n2\\n\\t\\n+\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\nm\\n,\\n\\t\\n1\\n)\\nFigure\\t4-12.\\t\\nGenerated\\tnonlinear\\tand\\tnoisy\\tdataset\\nClearly,\\ta\\tstraight\\tline\\twill\\tnever\\tfit\\tthis\\tdata\\tproperly.\\tSo\\tlet’s\\tuse\\tScikit-Learn’s\\t\\nPolynomialFeatures\\nclass\\t\\nto\\ttransform\\tour\\ttraining\\tdata,\\tadding\\tthe\\tsquare\\t(2\\nnd\\n-degree\\tpolynomial)\\tof\\teach\\tfeature\\tin\\tthe\\ntraining\\tset\\tas\\tnew\\tfeatures\\t(in\\tthis\\tcase\\tthere\\tis\\tjust\\tone\\tfeature):\\n>>>\\t\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nPolynomialFeatures\\n>>>\\t\\npoly_features\\n\\t\\n=\\n\\t\\nPolynomialFeatures\\n(\\ndegree\\n=\\n2\\n,\\n\\t\\ninclude_bias\\n=\\nFalse\\n)\\n>>>\\t\\nX_poly\\n\\t\\n=\\n\\t\\npoly_features\\n.\\nfit_transform\\n(\\nX\\n)\\n>>>\\t\\nX\\n[\\n0\\n]\\narray([-0.75275929])\\n>>>\\t\\nX_poly\\n[\\n0\\n]\\narray([-0.75275929,\\t\\t0.56664654])\\nX_poly\\n\\tnow\\tcontains\\tthe\\toriginal\\tfeature\\tof\\t\\nX\\n\\tplus\\tthe\\tsquare\\tof\\tthis\\tfeature.\\tNow\\tyou\\tcan\\tfit\\ta\\nLinearRegression\\n\\t\\nmodel\\tto\\tthis\\textended\\ttraining\\tdata\\t(\\nFigure\\t4-13\\n):', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 161}), Document(page_content='>>>\\t\\nlin_reg\\n\\t\\n=\\n\\t\\nLinearRegression\\n()\\n>>>\\t\\nlin_reg\\n.\\nfit\\n(\\nX_poly\\n,\\n\\t\\ny\\n)\\n>>>\\t\\nlin_reg\\n.\\nintercept_\\n,\\n\\t\\nlin_reg\\n.\\ncoef_\\n(array([\\t1.78134581]),\\tarray([[\\t0.93366893,\\t\\t0.56456263]]))\\nFigure\\t4-13.\\t\\nPolynomial\\tRegression\\tmodel\\tpredictions\\nNot\\tbad:\\tthe\\tmodel\\testimates\\t\\n\\twhen\\tin\\tfact\\tthe\\toriginal\\tfunction\\twas\\t\\n.\\nNote\\tthat\\twhen\\tthere\\tare\\tmultiple\\tfeatures,\\tPolynomial\\tRegression\\tis\\tcapable\\tof\\tfinding\\trelationships\\nbetween\\tfeatures\\t(which\\tis\\tsomething\\ta\\tplain\\tLinear\\tRegression\\tmodel\\tcannot\\tdo).\\tThis\\tis\\tmade\\tpossible\\nby\\tthe\\tfact\\tthat\\t\\nPolynomialFeatures\\n\\talso\\tadds\\tall\\tcombinations\\tof\\tfeatures\\tup\\tto\\tthe\\tgiven\\tdegree.\\tFor\\nexample,\\tif\\tthere\\twere\\ttwo\\tfeatures\\t\\na\\n\\tand\\t\\nb\\n,\\t\\nPolynomialFeatures\\n\\t\\nwith\\t\\ndegree=3\\n\\twould\\tnot\\tonly\\tadd\\nthe\\tfeatures\\t\\na\\n2\\n,\\t\\na\\n3\\n,\\t\\nb\\n2\\n,\\tand\\t\\nb\\n3\\n,\\tbut\\talso\\tthe\\tcombinations\\t\\nab\\n,\\t\\na\\n2\\nb\\n,\\tand\\t\\nab\\n2\\n.\\nWARNING\\nPolynomialFeatures(degree=d)\\n\\ttransforms\\tan\\tarray\\tcontaining\\t\\nn\\n\\tfeatures\\tinto\\tan\\tarray\\tcontaining\\t\\n\\tfeatures,\\twhere\\t\\nn\\n!\\nis\\tthe\\t\\nfactorial\\n\\tof\\t\\nn\\n,\\tequal\\tto\\t1\\t×\\t2\\t×\\t3\\t×\\t\\t×\\t\\nn\\n.\\tBeware\\tof\\tthe\\tcombinatorial\\texplosion\\tof\\tthe\\tnumber\\tof\\t\\nfeatures!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 162}), Document(page_content='Learning\\tCurves\\nIf\\tyou\\t\\nperform\\thigh-degree\\t\\nPolynomial\\tRegression,\\tyou\\twill\\tlikely\\tfit\\tthe\\ttraining\\tdata\\tmuch\\tbetter\\tthan\\nwith\\tplain\\tLinear\\tRegression.\\tFor\\texample,\\t\\nFigure\\t4-14\\n\\tapplies\\ta\\t300-degree\\tpolynomial\\tmodel\\tto\\tthe\\npreceding\\ttraining\\tdata,\\tand\\tcompares\\tthe\\tresult\\twith\\ta\\tpure\\tlinear\\tmodel\\tand\\ta\\tquadratic\\tmodel\\t(2\\nnd\\n-\\ndegree\\tpolynomial).\\tNotice\\thow\\tthe\\t300-degree\\tpolynomial\\tmodel\\twiggles\\taround\\tto\\tget\\tas\\tclose\\tas\\npossible\\tto\\tthe\\ttraining\\tinstances.\\nFigure\\t4-14.\\t\\nHigh-degree\\tPolynomial\\tRegression\\nOf\\tcourse,\\tthis\\thigh-degree\\tPolynomial\\tRegression\\tmodel\\tis\\tseverely\\toverfitting\\tthe\\ttraining\\tdata,\\twhile\\nthe\\tlinear\\tmodel\\tis\\tunderfitting\\tit.\\tThe\\tmodel\\tthat\\twill\\tgeneralize\\tbest\\tin\\tthis\\tcase\\tis\\tthe\\tquadratic\\tmodel.\\nIt\\tmakes\\tsense\\tsince\\tthe\\tdata\\twas\\tgenerated\\tusing\\ta\\tquadratic\\tmodel,\\tbut\\tin\\tgeneral\\tyou\\twon’t\\tknow\\twhat\\nfunction\\tgenerated\\tthe\\tdata,\\tso\\thow\\tcan\\tyou\\tdecide\\thow\\tcomplex\\tyour\\tmodel\\tshould\\tbe?\\tHow\\tcan\\tyou\\ttell\\nthat\\tyour\\tmodel\\tis\\toverfitting\\tor\\tunderfitting\\tthe\\tdata?\\nIn\\t\\nChapter\\t2\\n\\tyou\\tused\\tcross-validation\\tto\\tget\\tan\\testimate\\tof\\ta\\tmodel’s\\tgeneralization\\tperformance.\\tIf\\ta\\nmodel\\tperforms\\twell\\ton\\tthe\\ttraining\\tdata\\tbut\\tgeneralizes\\tpoorly\\taccording\\tto\\tthe\\tcross-validation\\tmetrics,\\nthen\\tyour\\tmodel\\tis\\toverfitting.\\tIf\\tit\\tperforms\\tpoorly\\ton\\tboth,\\tthen\\tit\\tis\\tunderfitting.\\tThis\\tis\\tone\\tway\\tto\\ttell\\nwhen\\ta\\tmodel\\tis\\ttoo\\tsimple\\tor\\ttoo\\tcomplex.\\nAnother\\tway\\tis\\tto\\tlook\\tat\\tthe\\t\\nlearning\\tcurves\\n:\\tthese\\tare\\tplots\\tof\\tthe\\tmodel’s\\tperformance\\ton\\tthe\\ttraining\\nset\\tand\\tthe\\tvalidation\\tset\\tas\\ta\\tfunction\\tof\\tthe\\ttraining\\tset\\tsize.\\tTo\\tgenerate\\tthe\\tplots,\\tsimply\\ttrain\\tthe\\tmodel\\nseveral\\ttimes\\ton\\tdifferent\\tsized\\tsubsets\\tof\\tthe\\ttraining\\tset.\\tThe\\tfollowing\\tcode\\tdefines\\ta\\tfunction\\tthat\\tplots\\nthe\\tlearning\\tcurves\\tof\\ta\\tmodel\\tgiven\\tsome\\t\\ntraining\\tdata:\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nmean_squared_error\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\ntrain_test_split\\ndef\\n\\t\\nplot_learning_curves\\n(\\nmodel\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n):', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 163}), Document(page_content='\\t\\t\\t\\t\\nX_train\\n,\\n\\t\\nX_val\\n,\\n\\t\\ny_train\\n,\\n\\t\\ny_val\\n\\t\\n=\\n\\t\\ntrain_test_split\\n(\\nX\\n,\\n\\t\\ny\\n,\\n\\t\\ntest_size\\n=\\n0.2\\n)\\n\\t\\t\\t\\t\\ntrain_errors\\n,\\n\\t\\nval_errors\\n\\t\\n=\\n\\t\\n[],\\n\\t\\n[]\\n\\t\\t\\t\\t\\nfor\\n\\t\\nm\\n\\t\\nin\\n\\t\\nrange\\n(\\n1\\n,\\n\\t\\nlen\\n(\\nX_train\\n)):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nmodel\\n.\\nfit\\n(\\nX_train\\n[:\\nm\\n],\\n\\t\\ny_train\\n[:\\nm\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ny_train_predict\\n\\t\\n=\\n\\t\\nmodel\\n.\\npredict\\n(\\nX_train\\n[:\\nm\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ny_val_predict\\n\\t\\n=\\n\\t\\nmodel\\n.\\npredict\\n(\\nX_val\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntrain_errors\\n.\\nappend\\n(\\nmean_squared_error\\n(\\ny_train_predict\\n,\\n\\t\\ny_train\\n[:\\nm\\n]))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nval_errors\\n.\\nappend\\n(\\nmean_squared_error\\n(\\ny_val_predict\\n,\\n\\t\\ny_val\\n))\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n(\\nnp\\n.\\nsqrt\\n(\\ntrain_errors\\n),\\n\\t\\n\"r-+\"\\n,\\n\\t\\nlinewidth\\n=\\n2\\n,\\n\\t\\nlabel\\n=\\n\"train\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n(\\nnp\\n.\\nsqrt\\n(\\nval_errors\\n),\\n\\t\\n\"b-\"\\n,\\n\\t\\nlinewidth\\n=\\n3\\n,\\n\\t\\nlabel\\n=\\n\"val\"\\n)\\nLet’s\\tlook\\tat\\tthe\\tlearning\\tcurves\\tof\\tthe\\tplain\\tLinear\\tRegression\\tmodel\\t\\n(a\\tstraight\\tline;\\t\\nFigure\\t4-15\\n):\\nlin_reg\\n\\t\\n=\\n\\t\\nLinearRegression\\n()\\nplot_learning_curves\\n(\\nlin_reg\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n)\\nFigure\\t4-15.\\t\\nLearning\\tcurves\\nThis\\tdeserves\\ta\\tbit\\tof\\texplanation.\\tFirst,\\tlet’s\\tlook\\tat\\tthe\\tperformance\\ton\\tthe\\ttraining\\tdata:\\twhen\\tthere\\tare\\njust\\tone\\tor\\ttwo\\tinstances\\tin\\tthe\\ttraining\\tset,\\tthe\\tmodel\\tcan\\tfit\\tthem\\tperfectly,\\twhich\\tis\\twhy\\tthe\\tcurve\\tstarts\\nat\\tzero.\\tBut\\tas\\tnew\\tinstances\\tare\\tadded\\tto\\tthe\\ttraining\\tset,\\tit\\tbecomes\\timpossible\\tfor\\tthe\\tmodel\\tto\\tfit\\tthe\\ntraining\\tdata\\tperfectly,\\tboth\\tbecause\\tthe\\tdata\\tis\\tnoisy\\tand\\tbecause\\tit\\tis\\tnot\\tlinear\\tat\\tall.\\tSo\\tthe\\terror\\ton\\tthe\\ntraining\\tdata\\tgoes\\tup\\tuntil\\tit\\treaches\\ta\\tplateau,\\tat\\twhich\\tpoint\\tadding\\tnew\\tinstances\\tto\\tthe\\ttraining\\tset\\ndoesn’t\\tmake\\tthe\\taverage\\terror\\tmuch\\tbetter\\tor\\tworse.\\tNow\\tlet’s\\tlook\\tat\\tthe\\tperformance\\tof\\tthe\\tmodel\\ton\\nthe\\tvalidation\\tdata.\\tWhen\\tthe\\tmodel\\tis\\ttrained\\ton\\tvery\\tfew\\ttraining\\tinstances,\\tit\\tis\\tincapable\\tof\\ngeneralizing\\tproperly,\\twhich\\tis\\twhy\\tthe\\tvalidation\\terror\\tis\\tinitially\\tquite\\tbig.\\tThen\\tas\\tthe\\tmodel\\tis\\tshown\\nmore\\ttraining\\texamples,\\tit\\tlearns\\tand\\tthus\\tthe\\tvalidation\\terror\\tslowly\\tgoes\\tdown.\\tHowever,\\tonce\\tagain\\ta\\nstraight\\tline\\tcannot\\tdo\\ta\\tgood\\tjob\\tmodeling\\tthe\\tdata,\\tso\\tthe\\terror\\tends\\tup\\tat\\ta\\tplateau,\\tvery\\tclose\\tto\\tthe\\nother\\tcurve.\\nThese\\tlearning\\tcurves\\tare\\ttypical\\tof\\tan\\tunderfitting\\tmodel.\\tBoth\\tcurves\\thave\\treached\\ta\\tplateau;\\tthey\\tare\\nclose\\tand\\tfairly\\thigh.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 164}), Document(page_content='TIP\\nIf\\tyour\\tmodel\\tis\\tunderfitting\\tthe\\ttraining\\tdata,\\tadding\\tmore\\ttraining\\texamples\\twill\\tnot\\thelp.\\tYou\\tneed\\tto\\tuse\\ta\\tmore\\tcomplex\\nmodel\\tor\\tcome\\tup\\twith\\tbetter\\tfeatures.\\nNow\\tlet’s\\tlook\\tat\\tthe\\tlearning\\tcurves\\tof\\ta\\t10\\nth\\n-degree\\tpolynomial\\tmodel\\ton\\tthe\\tsame\\tdata\\t\\n(\\nFigure\\t4-16\\n):\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nPipeline\\npolynomial_regression\\n\\t\\n=\\n\\t\\nPipeline\\n((\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"poly_features\"\\n,\\n\\t\\nPolynomialFeatures\\n(\\ndegree\\n=\\n10\\n,\\n\\t\\ninclude_bias\\n=\\nFalse\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"lin_reg\"\\n,\\n\\t\\nLinearRegression\\n()),\\n\\t\\t\\t\\t\\n))\\nplot_learning_curves\\n(\\npolynomial_regression\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n)\\nThese\\tlearning\\tcurves\\tlook\\ta\\tbit\\tlike\\tthe\\tprevious\\tones,\\tbut\\tthere\\tare\\ttwo\\tvery\\timportant\\tdifferences:\\nThe\\terror\\ton\\tthe\\ttraining\\tdata\\tis\\tmuch\\tlower\\tthan\\twith\\tthe\\tLinear\\tRegression\\tmodel.\\nThere\\tis\\ta\\tgap\\tbetween\\tthe\\tcurves.\\tThis\\tmeans\\tthat\\tthe\\tmodel\\tperforms\\tsignificantly\\tbetter\\ton\\tthe\\ntraining\\tdata\\tthan\\ton\\tthe\\tvalidation\\tdata,\\twhich\\tis\\tthe\\thallmark\\tof\\tan\\toverfitting\\tmodel.\\tHowever,\\tif\\nyou\\tused\\ta\\tmuch\\tlarger\\ttraining\\tset,\\tthe\\ttwo\\tcurves\\twould\\tcontinue\\tto\\tget\\tcloser.\\nFigure\\t4-16.\\t\\nLearning\\tcurves\\tfor\\tthe\\tpolynomial\\tmodel\\nTIP\\nOne\\tway\\tto\\timprove\\tan\\toverfitting\\tmodel\\tis\\tto\\tfeed\\tit\\tmore\\ttraining\\tdata\\tuntil\\tthe\\tvalidation\\terror\\treaches\\tthe\\ttraining\\terror.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 165}), Document(page_content='THE\\tBIAS/VARIANCE\\tTRADEOFF\\nAn\\timportant\\t\\ntheoretical\\tresult\\tof\\tstatistics\\tand\\tMachine\\tLearning\\tis\\tthe\\tfact\\tthat\\ta\\tmodel’s\\tgeneralization\\terror\\tcan\\tbe\\texpressed\\tas\\tthe\\nsum\\tof\\tthree\\tvery\\tdifferent\\terrors:\\nBias\\nThis\\tpart\\tof\\tthe\\tgeneralization\\terror\\tis\\tdue\\tto\\twrong\\tassumptions,\\tsuch\\tas\\tassuming\\tthat\\tthe\\tdata\\tis\\tlinear\\twhen\\tit\\tis\\tactually\\nquadratic.\\tA\\thigh-bias\\tmodel\\tis\\tmost\\tlikely\\tto\\tunderfit\\tthe\\ttraining\\tdata.\\n10\\nVariance\\nThis\\tpart\\tis\\tdue\\tto\\tthe\\tmodel’s\\texcessive\\tsensitivity\\tto\\tsmall\\tvariations\\tin\\tthe\\ttraining\\tdata.\\tA\\tmodel\\twith\\tmany\\tdegrees\\tof\\tfreedom\\n(such\\tas\\ta\\thigh-degree\\tpolynomial\\tmodel)\\tis\\tlikely\\tto\\thave\\thigh\\tvariance,\\tand\\tthus\\tto\\toverfit\\tthe\\ttraining\\tdata.\\nIrreducible\\terror\\nThis\\t\\npart\\tis\\tdue\\tto\\tthe\\tnoisiness\\tof\\tthe\\tdata\\titself.\\tThe\\tonly\\tway\\tto\\treduce\\tthis\\tpart\\tof\\tthe\\terror\\tis\\tto\\tclean\\tup\\tthe\\tdata\\t(e.g.,\\tfix\\tthe\\ndata\\tsources,\\tsuch\\tas\\tbroken\\tsensors,\\tor\\tdetect\\tand\\tremove\\toutliers).\\nIncreasing\\ta\\tmodel’s\\tcomplexity\\twill\\ttypically\\tincrease\\tits\\tvariance\\tand\\treduce\\tits\\tbias.\\tConversely,\\treducing\\ta\\tmodel’s\\tcomplexity\\nincreases\\tits\\tbias\\tand\\treduces\\tits\\tvariance.\\t\\nThis\\tis\\twhy\\tit\\tis\\tcalled\\ta\\ttradeoff.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 166}), Document(page_content='Regularized\\tLinear\\tModels\\nAs\\twe\\t\\nsaw\\tin\\tChapters\\t\\n1\\n\\tand\\t\\n2\\n,\\ta\\tgood\\tway\\tto\\treduce\\toverfitting\\tis\\tto\\tregularize\\tthe\\tmodel\\t(i.e.,\\tto\\nconstrain\\tit):\\tthe\\tfewer\\tdegrees\\tof\\tfreedom\\tit\\thas,\\tthe\\tharder\\tit\\twill\\tbe\\tfor\\tit\\tto\\toverfit\\tthe\\tdata.\\tFor\\nexample,\\ta\\tsimple\\tway\\tto\\tregularize\\ta\\tpolynomial\\tmodel\\tis\\tto\\treduce\\tthe\\tnumber\\tof\\tpolynomial\\tdegrees.\\nFor\\ta\\tlinear\\tmodel,\\tregularization\\tis\\ttypically\\tachieved\\tby\\tconstraining\\tthe\\tweights\\tof\\tthe\\tmodel.\\tWe\\twill\\nnow\\tlook\\tat\\tRidge\\tRegression,\\tLasso\\tRegression,\\tand\\tElastic\\tNet,\\twhich\\timplement\\tthree\\tdifferent\\tways\\nto\\tconstrain\\tthe\\tweights.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 167}), Document(page_content='Ridge\\tRegression\\nRidge\\tRegression\\n\\t\\n(also\\tcalled\\t\\nTikhonov\\tregularization\\n)\\tis\\ta\\tregularized\\tversion\\tof\\tLinear\\tRegression:\\ta\\nregularization\\tterm\\n\\tequal\\tto\\t\\n\\tis\\tadded\\tto\\tthe\\tcost\\tfunction.\\t\\nThis\\tforces\\tthe\\tlearning\\talgorithm\\tto\\nnot\\tonly\\tfit\\tthe\\tdata\\tbut\\talso\\tkeep\\tthe\\tmodel\\tweights\\tas\\tsmall\\tas\\tpossible.\\tNote\\tthat\\tthe\\tregularization\\tterm\\nshould\\tonly\\tbe\\tadded\\tto\\tthe\\tcost\\tfunction\\tduring\\ttraining.\\tOnce\\tthe\\tmodel\\tis\\ttrained,\\tyou\\twant\\tto\\tevaluate\\nthe\\tmodel’s\\tperformance\\tusing\\tthe\\tunregularized\\tperformance\\tmeasure.\\nNOTE\\nIt\\tis\\tquite\\tcommon\\tfor\\tthe\\tcost\\tfunction\\tused\\tduring\\ttraining\\tto\\tbe\\tdifferent\\tfrom\\tthe\\tperformance\\tmeasure\\tused\\tfor\\ttesting.\\tApart\\nfrom\\tregularization,\\tanother\\treason\\twhy\\tthey\\tmight\\tbe\\tdifferent\\tis\\tthat\\ta\\tgood\\ttraining\\tcost\\tfunction\\tshould\\thave\\toptimization-\\nfriendly\\tderivatives,\\twhile\\tthe\\tperformance\\tmeasure\\tused\\tfor\\ttesting\\tshould\\tbe\\tas\\tclose\\tas\\tpossible\\tto\\tthe\\tfinal\\tobjective.\\tA\\tgood\\nexample\\tof\\tthis\\tis\\ta\\tclassifier\\ttrained\\tusing\\ta\\tcost\\tfunction\\tsuch\\tas\\tthe\\tlog\\tloss\\t(discussed\\tin\\ta\\tmoment)\\tbut\\tevaluated\\tusing\\nprecision/recall.\\nThe\\thyperparameter\\t\\nα\\n\\tcontrols\\thow\\tmuch\\tyou\\twant\\tto\\tregularize\\tthe\\tmodel.\\tIf\\t\\nα\\n\\t=\\t0\\tthen\\tRidge\\nRegression\\tis\\tjust\\tLinear\\tRegression.\\tIf\\t\\nα\\n\\tis\\tvery\\tlarge,\\tthen\\tall\\tweights\\tend\\tup\\tvery\\tclose\\tto\\tzero\\tand\\tthe\\nresult\\tis\\ta\\tflat\\tline\\tgoing\\tthrough\\tthe\\tdata’s\\tmean.\\t\\nEquation\\t4-8\\n\\tpresents\\tthe\\tRidge\\tRegression\\tcost\\nfunction.\\n11\\nEquation\\t4-8.\\t\\nRidge\\tRegression\\tcost\\tfunction\\nNote\\tthat\\tthe\\tbias\\tterm\\t\\nθ\\n0\\n\\tis\\tnot\\tregularized\\t(the\\tsum\\tstarts\\tat\\t\\ni\\n\\t=\\t1,\\tnot\\t0).\\tIf\\twe\\tdefine\\t\\nw\\n\\tas\\tthe\\tvector\\tof\\nfeature\\tweights\\t(\\nθ\\n1\\n\\tto\\t\\nθ\\nn\\n),\\tthen\\tthe\\tregularization\\tterm\\tis\\tsimply\\tequal\\tto\\t½(\\t\\nw\\n\\t\\n2\\n)\\n2\\n,\\twhere\\t\\t·\\t\\n2\\nrepresents\\tthe\\tℓ\\n2\\n\\t\\nnorm\\tof\\tthe\\tweight\\tvector.\\n12\\n\\tFor\\tGradient\\tDescent,\\tjust\\tadd\\t\\nα\\nw\\n\\tto\\tthe\\tMSE\\tgradient\\nvector\\t(\\nEquation\\t4-6\\n).\\nWARNING\\nIt\\tis\\timportant\\tto\\tscale\\tthe\\tdata\\t(e.g.,\\tusing\\ta\\t\\nStandardScaler\\n)\\t\\nbefore\\tperforming\\tRidge\\tRegression,\\tas\\tit\\tis\\tsensitive\\tto\\tthe\\tscale\\nof\\tthe\\tinput\\tfeatures.\\tThis\\tis\\ttrue\\tof\\tmost\\tregularized\\tmodels.\\nFigure\\t4-17\\n\\tshows\\tseveral\\tRidge\\tmodels\\ttrained\\ton\\tsome\\tlinear\\tdata\\tusing\\tdifferent\\t\\nα\\n\\tvalue.\\tOn\\tthe\\tleft,\\nplain\\tRidge\\tmodels\\tare\\tused,\\tleading\\tto\\tlinear\\tpredictions.\\tOn\\tthe\\tright,\\tthe\\tdata\\tis\\tfirst\\texpanded\\tusing\\nPolynomialFeatures(degree=10)\\n,\\tthen\\tit\\tis\\tscaled\\tusing\\ta\\t\\nStandardScaler\\n,\\tand\\tfinally\\tthe\\tRidge', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 168}), Document(page_content='models\\tare\\tapplied\\tto\\tthe\\tresulting\\tfeatures:\\tthis\\tis\\tPolynomial\\tRegression\\twith\\tRidge\\tregularization.\\nNote\\thow\\tincreasing\\t\\nα\\n\\tleads\\tto\\tflatter\\t(i.e.,\\tless\\textreme,\\tmore\\treasonable)\\tpredictions;\\tthis\\treduces\\tthe\\nmodel’s\\tvariance\\tbut\\tincreases\\tits\\tbias.\\nAs\\twith\\tLinear\\tRegression,\\twe\\tcan\\tperform\\tRidge\\tRegression\\teither\\tby\\tcomputing\\ta\\t\\nclosed-form\\nequation\\tor\\tby\\tperforming\\tGradient\\tDescent.\\tThe\\tpros\\tand\\tcons\\tare\\tthe\\tsame.\\t\\nEquation\\t4-9\\n\\tshows\\tthe\\nclosed-form\\tsolution\\t(where\\t\\nA\\n\\tis\\tthe\\t\\nn\\n\\t×\\t\\nn\\n\\t\\nidentity\\tmatrix\\n13\\n\\texcept\\twith\\ta\\t0\\tin\\tthe\\ttop-left\\tcell,\\ncorresponding\\tto\\tthe\\tbias\\tterm).\\nFigure\\t4-17.\\t\\nRidge\\tRegression\\nEquation\\t4-9.\\t\\nRidge\\tRegression\\tclosed-form\\tsolution\\nHere\\tis\\thow\\tto\\tperform\\tRidge\\tRegression\\twith\\t\\nScikit-Learn\\tusing\\ta\\tclosed-form\\tsolution\\t(a\\tvariant\\tof\\nEquation\\t4-9\\n\\tusing\\ta\\tmatrix\\tfactorization\\ttechnique\\tby\\tAndré-Louis\\tCholesky):\\n>>>\\t\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nRidge\\n>>>\\t\\nridge_reg\\n\\t\\n=\\n\\t\\nRidge\\n(\\nalpha\\n=\\n1\\n,\\n\\t\\nsolver\\n=\\n\"cholesky\"\\n)\\n>>>\\t\\nridge_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\n>>>\\t\\nridge_reg\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([[\\t1.55071465]])\\nAnd\\tusing\\t\\nStochastic\\tGradient\\tDescent:\\n14\\n>>>\\t\\nsgd_reg\\n\\t\\n=\\n\\t\\nSGDRegressor\\n(\\npenalty\\n=\\n\"l2\"\\n)\\n>>>\\t\\nsgd_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n.\\nravel\\n())\\n>>>\\t\\nsgd_reg\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([\\t1.13500145])\\nThe\\t\\npenalty\\n\\thyperparameter\\tsets\\tthe\\ttype\\tof\\tregularization\\tterm\\tto\\tuse.\\tSpecifying\\t\\n\"l2\"\\n\\tindicates\\tthat\\nyou\\twant\\tSGD\\tto\\tadd\\ta\\tregularization\\tterm\\tto\\tthe\\tcost\\tfunction\\t\\nequal\\tto\\thalf\\tthe\\tsquare\\tof\\tthe\\tℓ\\n2\\n\\tnorm\\tof\\nthe\\tweight\\tvector:\\tthis\\tis\\tsimply\\t\\nRidge\\tRegression.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 169}), Document(page_content='Lasso\\tRegression\\nLeast\\tAbsolute\\tShrinkage\\tand\\tSelection\\tOperator\\tRegression\\n\\t(simply\\tcalled\\t\\nLasso\\tRegression\\n)\\tis\\nanother\\tregularized\\tversion\\tof\\tLinear\\tRegression:\\tjust\\tlike\\tRidge\\tRegression,\\tit\\tadds\\ta\\tregularization\\tterm\\nto\\tthe\\t\\ncost\\tfunction,\\tbut\\tit\\tuses\\tthe\\tℓ\\n1\\n\\t\\nnorm\\tof\\tthe\\tweight\\tvector\\tinstead\\tof\\thalf\\tthe\\tsquare\\tof\\tthe\\tℓ\\n2\\n\\t\\nnorm\\n(see\\t\\nEquation\\t4-10\\n).\\nEquation\\t4-10.\\t\\nLasso\\tRegression\\tcost\\tfunction\\nFigure\\t4-18\\n\\tshows\\tthe\\tsame\\tthing\\tas\\t\\nFigure\\t4-17\\n\\tbut\\treplaces\\tRidge\\tmodels\\twith\\tLasso\\tmodels\\tand\\tuses\\nsmaller\\t\\nα\\n\\tvalues.\\nFigure\\t4-18.\\t\\nLasso\\tRegression\\nAn\\timportant\\tcharacteristic\\tof\\tLasso\\tRegression\\tis\\tthat\\tit\\ttends\\tto\\tcompletely\\teliminate\\tthe\\tweights\\tof\\tthe\\nleast\\timportant\\tfeatures\\t(i.e.,\\tset\\tthem\\tto\\tzero).\\tFor\\texample,\\tthe\\tdashed\\tline\\tin\\tthe\\tright\\tplot\\ton\\t\\nFigure\\t4-\\n18\\n\\t(with\\t\\nα\\n\\t=\\t10\\n-7\\n)\\tlooks\\tquadratic,\\talmost\\tlinear:\\tall\\tthe\\tweights\\tfor\\tthe\\thigh-degree\\tpolynomial\\tfeatures\\nare\\tequal\\tto\\tzero.\\tIn\\tother\\twords,\\tLasso\\tRegression\\tautomatically\\tperforms\\t\\nfeature\\tselection\\tand\\toutputs\\t\\na\\nsparse\\tmodel\\n\\t(i.e.,\\twith\\tfew\\tnonzero\\tfeature\\tweights).\\nYou\\tcan\\tget\\ta\\tsense\\tof\\twhy\\tthis\\tis\\tthe\\tcase\\tby\\tlooking\\tat\\t\\nFigure\\t4-19\\n:\\ton\\tthe\\ttop-left\\tplot,\\tthe\\tbackground\\ncontours\\t(ellipses)\\trepresent\\tan\\tunregularized\\tMSE\\tcost\\tfunction\\t(\\nα\\n\\t=\\t0),\\tand\\tthe\\twhite\\tcircles\\tshow\\tthe\\nBatch\\tGradient\\tDescent\\tpath\\twith\\tthat\\tcost\\tfunction.\\tThe\\tforeground\\tcontours\\t(diamonds)\\trepresent\\tthe\\tℓ\\n1\\npenalty,\\tand\\tthe\\ttriangles\\tshow\\tthe\\tBGD\\tpath\\tfor\\tthis\\tpenalty\\tonly\\t(\\nα\\n\\t→\\t∞).\\tNotice\\thow\\tthe\\tpath\\tfirst\\nreaches\\t\\nθ\\n1\\n\\t=\\t0,\\tthen\\trolls\\tdown\\ta\\tgutter\\tuntil\\tit\\treaches\\t\\nθ\\n2\\n\\t=\\t0.\\tOn\\tthe\\ttop-right\\tplot,\\tthe\\tcontours\\trepresent\\nthe\\tsame\\tcost\\tfunction\\tplus\\tan\\tℓ\\n1\\n\\tpenalty\\twith\\t\\nα\\n\\t=\\t0.5.\\tThe\\tglobal\\tminimum\\tis\\ton\\tthe\\t\\nθ\\n2\\n\\t=\\t0\\taxis.\\tBGD', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 170}), Document(page_content='first\\treaches\\t\\nθ\\n2\\n\\t=\\t0,\\tthen\\trolls\\tdown\\tthe\\tgutter\\tuntil\\tit\\treaches\\tthe\\tglobal\\tminimum.\\tThe\\ttwo\\tbottom\\tplots\\nshow\\tthe\\tsame\\tthing\\tbut\\tuses\\tan\\tℓ\\n2\\n\\tpenalty\\tinstead.\\tThe\\tregularized\\tminimum\\tis\\tcloser\\tto\\t\\nθ\\n\\t=\\t0\\tthan\\tthe\\nunregularized\\tminimum,\\tbut\\tthe\\tweights\\tdo\\tnot\\tget\\tfully\\teliminated.\\nFigure\\t4-19.\\t\\nLasso\\tversus\\tRidge\\tregularization\\nTIP\\nOn\\tthe\\tLasso\\tcost\\tfunction,\\tthe\\tBGD\\tpath\\ttends\\tto\\tbounce\\tacross\\tthe\\tgutter\\ttoward\\tthe\\tend.\\tThis\\tis\\tbecause\\tthe\\tslope\\tchanges\\nabruptly\\tat\\t\\nθ\\n2\\n\\t=\\t0.\\tYou\\tneed\\tto\\tgradually\\treduce\\tthe\\tlearning\\trate\\tin\\torder\\tto\\tactually\\tconverge\\tto\\tthe\\tglobal\\tminimum.\\nThe\\tLasso\\tcost\\tfunction\\tis\\tnot\\tdifferentiable\\tat\\t\\nθ\\ni\\n\\t=\\t0\\t(for\\t\\ni\\n\\t=\\t1,\\t2,\\t,\\t\\nn\\n),\\tbut\\tGradient\\tDescent\\tstill\\tworks\\nfine\\tif\\tyou\\tuse\\t\\na\\t\\nsubgradient\\tvector\\n\\t\\ng\\n15\\n\\tinstead\\twhen\\tany\\t\\nθ\\ni\\n\\t=\\t0.\\t\\nEquation\\t4-11\\n\\tshows\\ta\\tsubgradient\\nvector\\tequation\\tyou\\tcan\\tuse\\tfor\\tGradient\\tDescent\\twith\\tthe\\tLasso\\t\\ncost\\tfunction.\\nEquation\\t4-11.\\t\\nLasso\\tRegression\\tsubgradient\\tvector\\nHere\\tis\\ta\\tsmall\\tScikit-Learn\\texample\\tusing\\tthe\\t\\nLasso\\n\\tclass.\\tNote\\tthat\\tyou\\tcould\\tinstead\\tuse\\t\\nan\\nSGDRegressor(penalty=\"l1\")\\n.\\n>>>\\t\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLasso\\n>>>\\t\\nlasso_reg\\n\\t\\n=\\n\\t\\nLasso\\n(\\nalpha\\n=\\n0.1\\n)\\n>>>\\t\\nlasso_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\n>>>\\t\\nlasso_reg\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([\\t1.53788174])', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 171}), Document(page_content='', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 172}), Document(page_content='Elastic\\tNet\\nElastic\\tNet\\t\\nis\\ta\\tmiddle\\tground\\tbetween\\tRidge\\tRegression\\tand\\tLasso\\tRegression.\\tThe\\tregularization\\tterm\\nis\\ta\\tsimple\\tmix\\tof\\tboth\\tRidge\\tand\\tLasso’s\\tregularization\\tterms,\\tand\\tyou\\tcan\\tcontrol\\tthe\\tmix\\tratio\\t\\nr\\n.\\tWhen\\nr\\n\\t=\\t0,\\tElastic\\tNet\\tis\\tequivalent\\tto\\tRidge\\tRegression,\\tand\\twhen\\t\\nr\\n\\t=\\t1,\\tit\\tis\\tequivalent\\tto\\t\\nLasso\\tRegression\\n(see\\t\\nEquation\\t4-12\\n).\\nEquation\\t4-12.\\t\\nElastic\\tNet\\tcost\\tfunction\\nSo\\twhen\\tshould\\tyou\\tuse\\t\\nplain\\tLinear\\tRegression\\t(i.e.,\\twithout\\tany\\tregularization),\\tRidge,\\tLasso,\\tor\\nElastic\\tNet?\\tIt\\tis\\talmost\\talways\\tpreferable\\tto\\thave\\tat\\tleast\\ta\\tlittle\\tbit\\tof\\tregularization,\\tso\\tgenerally\\tyou\\nshould\\tavoid\\tplain\\tLinear\\tRegression.\\tRidge\\tis\\ta\\tgood\\tdefault,\\tbut\\tif\\tyou\\tsuspect\\tthat\\tonly\\ta\\tfew\\tfeatures\\nare\\tactually\\tuseful,\\tyou\\tshould\\tprefer\\tLasso\\tor\\tElastic\\tNet\\tsince\\tthey\\ttend\\tto\\treduce\\tthe\\tuseless\\tfeatures’\\nweights\\tdown\\tto\\tzero\\tas\\twe\\thave\\tdiscussed.\\tIn\\tgeneral,\\t\\nElastic\\tNet\\tis\\tpreferred\\tover\\tLasso\\tsince\\tLasso\\nmay\\tbehave\\terratically\\twhen\\tthe\\tnumber\\tof\\tfeatures\\tis\\tgreater\\tthan\\tthe\\tnumber\\tof\\ttraining\\tinstances\\tor\\nwhen\\tseveral\\tfeatures\\tare\\tstrongly\\tcorrelated.\\nHere\\tis\\ta\\tshort\\texample\\tusing\\tScikit-Learn’s\\t\\nElasticNet\\n\\t(\\nl1_ratio\\n\\tcorresponds\\tto\\tthe\\tmix\\tratio\\t\\nr\\n):\\n>>>\\t\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nElasticNet\\n>>>\\t\\nelastic_net\\n\\t\\n=\\n\\t\\nElasticNet\\n(\\nalpha\\n=\\n0.1\\n,\\n\\t\\nl1_ratio\\n=\\n0.5\\n)\\n>>>\\t\\nelastic_net\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\n>>>\\t\\nelastic_net\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([\\t1.54333232])', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 173}), Document(page_content='Early\\tStopping\\nA\\t\\nvery\\tdifferent\\tway\\tto\\tregularize\\titerative\\tlearning\\talgorithms\\tsuch\\tas\\tGradient\\tDescent\\tis\\tto\\tstop\\ntraining\\tas\\tsoon\\tas\\tthe\\tvalidation\\terror\\treaches\\ta\\tminimum.\\tThis\\tis\\tcalled\\t\\nearly\\tstopping\\n.\\t\\nFigure\\t4-20\\nshows\\ta\\tcomplex\\tmodel\\t(in\\tthis\\tcase\\ta\\thigh-degree\\tPolynomial\\tRegression\\tmodel)\\tbeing\\ttrained\\tusing\\nBatch\\tGradient\\tDescent.\\tAs\\tthe\\tepochs\\tgo\\tby,\\tthe\\talgorithm\\tlearns\\tand\\tits\\tprediction\\terror\\t(RMSE)\\ton\\tthe\\ntraining\\tset\\tnaturally\\tgoes\\tdown,\\tand\\tso\\tdoes\\tits\\tprediction\\terror\\ton\\tthe\\tvalidation\\tset.\\tHowever,\\tafter\\ta\\nwhile\\tthe\\tvalidation\\terror\\tstops\\tdecreasing\\tand\\tactually\\tstarts\\tto\\tgo\\tback\\tup.\\tThis\\tindicates\\tthat\\tthe\\tmodel\\nhas\\tstarted\\tto\\toverfit\\tthe\\ttraining\\tdata.\\tWith\\tearly\\tstopping\\tyou\\tjust\\tstop\\ttraining\\tas\\tsoon\\tas\\tthe\\tvalidation\\nerror\\treaches\\tthe\\tminimum.\\tIt\\tis\\tsuch\\ta\\tsimple\\tand\\tefficient\\tregularization\\ttechnique\\tthat\\tGeoffrey\\tHinton\\ncalled\\tit\\ta\\t“beautiful\\tfree\\tlunch.”\\nFigure\\t4-20.\\t\\nEarly\\tstopping\\tregularization\\nTIP\\nWith\\tStochastic\\tand\\tMini-batch\\tGradient\\tDescent,\\tthe\\tcurves\\tare\\tnot\\tso\\tsmooth,\\tand\\tit\\tmay\\tbe\\thard\\tto\\tknow\\twhether\\tyou\\thave\\nreached\\tthe\\tminimum\\tor\\tnot.\\tOne\\tsolution\\tis\\tto\\tstop\\tonly\\tafter\\tthe\\tvalidation\\terror\\thas\\tbeen\\tabove\\tthe\\tminimum\\tfor\\tsome\\ttime\\n(when\\tyou\\tare\\tconfident\\tthat\\tthe\\tmodel\\twill\\tnot\\tdo\\tany\\tbetter),\\tthen\\troll\\tback\\tthe\\t\\nmodel\\tparameters\\tto\\tthe\\tpoint\\twhere\\tthe\\nvalidation\\terror\\twas\\tat\\ta\\tminimum.\\nHere\\tis\\ta\\tbasic\\timplementation\\tof\\tearly\\t\\nstopping:\\nfrom\\n\\t\\nsklearn.base\\n\\t\\nimport\\n\\t\\nclone\\nsgd_reg\\n\\t\\n=\\n\\t\\nSGDRegressor\\n(\\nn_iter\\n=\\n1\\n,\\n\\t\\nwarm_start\\n=\\nTrue\\n,\\n\\t\\npenalty\\n=\\nNone\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlearning_rate\\n=\\n\"constant\"\\n,\\n\\t\\neta0\\n=\\n0.0005\\n)\\nminimum_val_error\\n\\t\\n=\\n\\t\\nfloat\\n(\\n\"inf\"\\n)\\nbest_epoch\\n\\t\\n=\\n\\t\\nNone', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 174}), Document(page_content='best_model\\n\\t\\n=\\n\\t\\nNone\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\n1000\\n):\\n\\t\\t\\t\\t\\nsgd_reg\\n.\\nfit\\n(\\nX_train_poly_scaled\\n,\\n\\t\\ny_train\\n)\\n\\t\\t\\n#\\tcontinues\\twhere\\tit\\tleft\\toff\\n\\t\\t\\t\\t\\ny_val_predict\\n\\t\\n=\\n\\t\\nsgd_reg\\n.\\npredict\\n(\\nX_val_poly_scaled\\n)\\n\\t\\t\\t\\t\\nval_error\\n\\t\\n=\\n\\t\\nmean_squared_error\\n(\\ny_val_predict\\n,\\n\\t\\ny_val\\n)\\n\\t\\t\\t\\t\\nif\\n\\t\\nval_error\\n\\t\\n<\\n\\t\\nminimum_val_error\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nminimum_val_error\\n\\t\\n=\\n\\t\\nval_error\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nbest_epoch\\n\\t\\n=\\n\\t\\nepoch\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nbest_model\\n\\t\\n=\\n\\t\\nclone\\n(\\nsgd_reg\\n)\\nNote\\tthat\\twith\\t\\nwarm_start=True\\n,\\twhen\\tthe\\t\\nfit()\\n\\tmethod\\tis\\tcalled,\\tit\\tjust\\tcontinues\\ttraining\\twhere\\tit\\tleft\\noff\\tinstead\\tof\\trestarting\\t\\nfrom\\tscratch.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 175}), Document(page_content='Logistic\\tRegression\\nAs\\t\\nwe\\tdiscussed\\tin\\t\\nChapter\\t1\\n,\\tsome\\tregression\\talgorithms\\tcan\\tbe\\tused\\tfor\\tclassification\\tas\\twell\\t(and\\nvice\\tversa).\\t\\nLogistic\\tRegression\\n\\t(also\\tcalled\\t\\nLogit\\tRegression\\n)\\tis\\tcommonly\\tused\\tto\\testimate\\tthe\\nprobability\\tthat\\tan\\tinstance\\tbelongs\\tto\\ta\\tparticular\\tclass\\t(e.g.,\\twhat\\tis\\tthe\\tprobability\\tthat\\tthis\\temail\\tis\\nspam?).\\tIf\\tthe\\testimated\\tprobability\\tis\\tgreater\\tthan\\t50%,\\tthen\\tthe\\tmodel\\tpredicts\\tthat\\tthe\\tinstance\\tbelongs\\nto\\tthat\\tclass\\t(called\\tthe\\tpositive\\tclass,\\tlabeled\\t“1”),\\tor\\telse\\tit\\tpredicts\\tthat\\tit\\tdoes\\tnot\\t(i.e.,\\tit\\tbelongs\\tto\\nthe\\tnegative\\tclass,\\tlabeled\\t“0”).\\tThis\\tmakes\\tit\\ta\\t\\nbinary\\tclassifier.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 176}), Document(page_content='Estimating\\tProbabilities\\nSo\\t\\nhow\\tdoes\\tit\\twork?\\tJust\\tlike\\ta\\tLinear\\tRegression\\tmodel,\\ta\\tLogistic\\tRegression\\tmodel\\tcomputes\\ta\\nweighted\\tsum\\tof\\tthe\\tinput\\tfeatures\\t(plus\\ta\\tbias\\tterm),\\tbut\\tinstead\\tof\\toutputting\\tthe\\tresult\\tdirectly\\tlike\\tthe\\nLinear\\tRegression\\tmodel\\tdoes,\\tit\\toutputs\\tthe\\t\\nlogistic\\n\\tof\\tthis\\tresult\\t(see\\t\\nEquation\\t4-13\\n).\\nEquation\\t4-13.\\t\\nLogistic\\tRegression\\tmodel\\testimated\\tprobability\\t(vectorized\\tform)\\nThe\\tlogistic\\t—\\talso\\tcalled\\tthe\\t\\nlogit\\n,\\tnoted\\t\\nσ\\n(·)\\t—\\tis\\ta\\t\\nsigmoid\\tfunction\\n\\t(i.e.,\\t\\nS\\n-shaped)\\tthat\\toutputs\\t\\na\\nnumber\\tbetween\\t0\\tand\\t1.\\tIt\\tis\\tdefined\\tas\\tshown\\tin\\t\\nEquation\\t4-14\\n\\tand\\t\\nFigure\\t4-21\\n.\\nEquation\\t4-14.\\t\\nLogistic\\tfunction\\nFigure\\t4-21.\\t\\nLogistic\\tfunction\\nOnce\\tthe\\tLogistic\\tRegression\\tmodel\\thas\\testimated\\tthe\\tprobability\\t\\n\\t=\\t\\nh\\nθ\\n(\\nx\\n)\\t\\nthat\\tan\\tinstance\\t\\nx\\n\\tbelongs\\tto\\nthe\\tpositive\\tclass,\\tit\\tcan\\tmake\\tits\\tprediction\\t\\nŷ\\n\\teasily\\t(see\\t\\nEquation\\t4-15\\n).\\nEquation\\t4-15.\\t\\nLogistic\\tRegression\\tmodel\\tprediction\\nNotice\\tthat\\t\\nσ\\n(\\nt\\n)\\t<\\t0.5\\twhen\\t\\nt\\n\\t<\\t0,\\tand\\t\\nσ\\n(\\nt\\n)\\t≥\\t0.5\\twhen\\t\\nt\\n\\t≥\\t0,\\tso\\ta\\tLogistic\\tRegression\\tmodel\\tpredicts\\t1\\tif', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 177}), Document(page_content='θ\\nT\\n\\t·\\t\\nx\\n\\tis\\tpositive,\\tand\\t0\\tif\\tit\\tis\\t\\nnegative.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 178}), Document(page_content='Training\\tand\\tCost\\tFunction\\nGood,\\t\\nnow\\tyou\\tknow\\thow\\ta\\tLogistic\\tRegression\\tmodel\\testimates\\tprobabilities\\tand\\tmakes\\tpredictions.\\nBut\\thow\\tis\\tit\\ttrained?\\tThe\\tobjective\\tof\\ttraining\\tis\\tto\\tset\\tthe\\t\\nparameter\\tvector\\t\\nθ\\n\\tso\\tthat\\tthe\\tmodel\\testimates\\nhigh\\tprobabilities\\tfor\\tpositive\\tinstances\\t(\\ny\\n\\t=\\t1)\\tand\\tlow\\tprobabilities\\tfor\\tnegative\\tinstances\\t(\\ny\\n\\t=\\t0).\\tThis\\nidea\\tis\\tcaptured\\tby\\tthe\\tcost\\tfunction\\tshown\\tin\\t\\nEquation\\t4-16\\n\\tfor\\ta\\tsingle\\ttraining\\tinstance\\t\\nx\\n.\\nEquation\\t4-16.\\t\\nCost\\tfunction\\tof\\ta\\tsingle\\ttraining\\tinstance\\nThis\\tcost\\tfunction\\tmakes\\tsense\\tbecause\\t\\n–\\tlog(\\nt\\n)\\tgrows\\tvery\\tlarge\\twhen\\t\\nt\\n\\tapproaches\\t0,\\tso\\tthe\\tcost\\twill\\nbe\\tlarge\\tif\\tthe\\tmodel\\testimates\\ta\\tprobability\\tclose\\tto\\t0\\tfor\\ta\\tpositive\\tinstance,\\tand\\tit\\twill\\talso\\tbe\\tvery\\nlarge\\tif\\tthe\\tmodel\\testimates\\ta\\tprobability\\tclose\\tto\\t1\\tfor\\ta\\tnegative\\tinstance.\\tOn\\tthe\\tother\\thand,\\t–\\tlog(\\nt\\n)\\tis\\nclose\\tto\\t0\\twhen\\t\\nt\\n\\tis\\tclose\\tto\\t1,\\tso\\tthe\\tcost\\twill\\tbe\\tclose\\tto\\t0\\tif\\tthe\\testimated\\tprobability\\tis\\tclose\\tto\\t0\\tfor\\ta\\nnegative\\tinstance\\tor\\tclose\\tto\\t1\\tfor\\ta\\tpositive\\tinstance,\\twhich\\tis\\tprecisely\\twhat\\twe\\twant.\\nThe\\tcost\\tfunction\\tover\\tthe\\twhole\\ttraining\\tset\\tis\\tsimply\\tthe\\taverage\\tcost\\tover\\tall\\ttraining\\tinstances.\\tIt\\tcan\\nbe\\twritten\\tin\\ta\\tsingle\\texpression\\t(as\\tyou\\tcan\\tverify\\teasily),\\tcalled\\t\\nthe\\t\\nlog\\tloss\\n,\\tshown\\tin\\t\\nEquation\\t4-17\\n.\\nEquation\\t4-17.\\t\\nLogistic\\tRegression\\tcost\\tfunction\\t(log\\tloss)\\nThe\\tbad\\tnews\\tis\\tthat\\tthere\\tis\\tno\\tknown\\t\\nclosed-form\\tequation\\tto\\tcompute\\tthe\\tvalue\\tof\\t\\nθ\\n\\tthat\\tminimizes\\tthis\\ncost\\tfunction\\t(there\\tis\\tno\\tequivalent\\tof\\tthe\\tNormal\\tEquation).\\tBut\\tthe\\tgood\\tnews\\tis\\tthat\\tthis\\tcost\\tfunction\\nis\\tconvex,\\tso\\tGradient\\tDescent\\t(or\\tany\\tother\\toptimization\\talgorithm)\\tis\\tguaranteed\\tto\\tfind\\tthe\\tglobal\\nminimum\\t(if\\tthe\\tlearning\\trate\\tis\\tnot\\ttoo\\tlarge\\tand\\tyou\\twait\\tlong\\tenough).\\tThe\\tpartial\\tderivatives\\tof\\tthe\\ncost\\tfunction\\twith\\tregards\\tto\\tthe\\tj\\nth\\n\\tmodel\\tparameter\\t\\nθ\\nj\\n\\tis\\tgiven\\tby\\t\\nEquation\\t4-18\\n.\\nEquation\\t4-18.\\t\\nLogistic\\tcost\\tfunction\\tpartial\\tderivatives\\nThis\\tequation\\tlooks\\tvery\\tmuch\\tlike\\t\\nEquation\\t4-5\\n:\\tfor\\teach\\tinstance\\tit\\tcomputes\\tthe\\tprediction\\terror\\tand\\nmultiplies\\tit\\tby\\tthe\\tj\\nth\\n\\tfeature\\tvalue,\\tand\\tthen\\tit\\tcomputes\\tthe\\taverage\\tover\\tall\\ttraining\\tinstances.\\tOnce\\tyou\\nhave\\tthe\\tgradient\\tvector\\tcontaining\\tall\\tthe\\tpartial\\tderivatives\\tyou\\tcan\\tuse\\tit\\tin\\tthe\\tBatch\\tGradient\\tDescent\\nalgorithm.\\tThat’s\\tit:\\tyou\\tnow\\tknow\\thow\\tto\\ttrain\\ta\\tLogistic\\tRegression\\tmodel.\\tFor\\t\\nStochastic\\tGD\\tyou', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 179}), Document(page_content='would\\tof\\tcourse\\tjust\\ttake\\tone\\tinstance\\tat\\ta\\ttime,\\tand\\tfor\\t\\nMini-batch\\tGD\\tyou\\twould\\tuse\\ta\\t\\nmini-batch\\tat\\ta\\ntime.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 180}), Document(page_content='Decision\\tBoundaries\\nLet’s\\t\\nuse\\tthe\\tiris\\tdataset\\tto\\tillustrate\\tLogistic\\tRegression.\\tThis\\tis\\ta\\tfamous\\tdataset\\tthat\\tcontains\\tthe\\tsepal\\nand\\tpetal\\tlength\\tand\\twidth\\tof\\t150\\tiris\\tflowers\\tof\\tthree\\tdifferent\\tspecies:\\tIris-Setosa,\\tIris-Versicolor,\\tand\\nIris-Virginica\\t(see\\t\\nFigure\\t4-22\\n).\\nFigure\\t4-22.\\t\\nFlowers\\tof\\tthree\\tiris\\tplant\\tspecies\\n16\\nLet’s\\ttry\\tto\\tbuild\\ta\\tclassifier\\tto\\tdetect\\tthe\\tIris-Virginica\\ttype\\tbased\\tonly\\ton\\tthe\\tpetal\\twidth\\tfeature.\\t\\nFirst\\nlet’s\\tload\\tthe\\tdata:\\n>>>\\t\\nfrom\\n\\t\\nsklearn\\n\\t\\nimport\\n\\t\\ndatasets\\n>>>\\t\\niris\\n\\t\\n=\\n\\t\\ndatasets\\n.\\nload_iris\\n()\\n>>>\\t\\nlist\\n(\\niris\\n.\\nkeys\\n())\\n[\\'data\\',\\t\\'target_names\\',\\t\\'feature_names\\',\\t\\'target\\',\\t\\'DESCR\\']\\n>>>\\t\\nX\\n\\t\\n=\\n\\t\\niris\\n[\\n\"data\"\\n][:,\\n\\t\\n3\\n:]\\n\\t\\t\\n#\\tpetal\\twidth\\n>>>\\t\\ny\\n\\t\\n=\\n\\t\\n(\\niris\\n[\\n\"target\"\\n]\\n\\t\\n==\\n\\t\\n2\\n)\\n.\\nastype\\n(\\nnp\\n.\\nint\\n)\\n\\t\\t\\n#\\t1\\tif\\tIris-Virginica,\\telse\\t0\\nNow\\tlet’s\\ttrain\\ta\\t\\nLogistic\\tRegression\\tmodel:\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLogisticRegression\\nlog_reg\\n\\t\\n=\\n\\t\\nLogisticRegression\\n()\\nlog_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nLet’s\\tlook\\tat\\tthe\\tmodel’s\\testimated\\tprobabilities\\tfor\\tflowers\\twith\\tpetal\\twidths\\tvarying\\tfrom\\t0\\tto\\t3\\tcm\\n(\\nFigure\\t4-23\\n):\\nX_new\\n\\t\\n=\\n\\t\\nnp\\n.\\nlinspace\\n(\\n0\\n,\\n\\t\\n3\\n,\\n\\t\\n1000\\n)\\n.\\nreshape\\n(\\n-\\n1\\n,\\n\\t\\n1\\n)\\ny_proba\\n\\t\\n=\\n\\t\\nlog_reg\\n.\\npredict_proba\\n(\\nX_new\\n)\\nplt\\n.\\nplot\\n(\\nX_new\\n,\\n\\t\\ny_proba\\n[:,\\n\\t\\n1\\n],\\n\\t\\n\"g-\"\\n,\\n\\t\\nlabel\\n=\\n\"Iris-Virginica\"\\n)\\nplt\\n.\\nplot\\n(\\nX_new\\n,\\n\\t\\ny_proba\\n[:,\\n\\t\\n0\\n],\\n\\t\\n\"b--\"\\n,\\n\\t\\nlabel\\n=\\n\"Not\\tIris-Virginica\"\\n)\\n#\\t+\\tmore\\tMatplotlib\\tcode\\tto\\tmake\\tthe\\timage\\tlook\\tpretty', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 181}), Document(page_content='Figure\\t4-23.\\t\\nEstimated\\tprobabilities\\tand\\tdecision\\tboundary\\nThe\\tpetal\\twidth\\tof\\tIris-Virginica\\tflowers\\t(represented\\tby\\ttriangles)\\tranges\\tfrom\\t1.4\\tcm\\tto\\t2.5\\tcm,\\twhile\\nthe\\tother\\tiris\\tflowers\\t(represented\\tby\\tsquares)\\tgenerally\\thave\\ta\\tsmaller\\tpetal\\twidth,\\tranging\\tfrom\\t0.1\\tcm\\nto\\t1.8\\tcm.\\tNotice\\tthat\\tthere\\tis\\ta\\tbit\\tof\\toverlap.\\tAbove\\tabout\\t2\\tcm\\tthe\\tclassifier\\tis\\thighly\\tconfident\\tthat\\tthe\\nflower\\tis\\tan\\tIris-Virginica\\t(it\\toutputs\\ta\\thigh\\tprobability\\tto\\tthat\\tclass),\\twhile\\tbelow\\t1\\tcm\\tit\\tis\\thighly\\nconfident\\tthat\\tit\\tis\\tnot\\tan\\tIris-Virginica\\t(high\\tprobability\\tfor\\tthe\\t“Not\\tIris-Virginica”\\tclass).\\tIn\\tbetween\\nthese\\textremes,\\tthe\\tclassifier\\tis\\tunsure.\\tHowever,\\tif\\tyou\\task\\tit\\tto\\tpredict\\tthe\\tclass\\t(using\\tthe\\t\\npredict()\\nmethod\\trather\\tthan\\tthe\\t\\npredict_proba()\\n\\tmethod),\\tit\\twill\\treturn\\twhichever\\tclass\\tis\\tthe\\tmost\\tlikely.\\nTherefore,\\tthere\\tis\\ta\\t\\ndecision\\tboundary\\n\\tat\\taround\\t1.6\\tcm\\twhere\\tboth\\tprobabilities\\tare\\tequal\\tto\\t50%:\\tif\\nthe\\tpetal\\twidth\\tis\\thigher\\tthan\\t1.6\\tcm,\\tthe\\tclassifier\\twill\\tpredict\\tthat\\tthe\\tflower\\tis\\tan\\tIris-Virginica,\\tor\\telse\\nit\\twill\\tpredict\\tthat\\tit\\tis\\tnot\\t(even\\tif\\tit\\tis\\tnot\\tvery\\tconfident):\\n>>>\\t\\nlog_reg\\n.\\npredict\\n([[\\n1.7\\n],\\n\\t\\n[\\n1.5\\n]])\\narray([1,\\t0])\\nFigure\\t4-24\\n\\tshows\\tthe\\tsame\\tdataset\\tbut\\tthis\\ttime\\tdisplaying\\ttwo\\tfeatures:\\tpetal\\twidth\\tand\\tlength.\\tOnce\\ntrained,\\tthe\\tLogistic\\tRegression\\tclassifier\\tcan\\testimate\\tthe\\tprobability\\tthat\\ta\\tnew\\tflower\\tis\\tan\\tIris-\\nVirginica\\tbased\\ton\\tthese\\ttwo\\tfeatures.\\tThe\\tdashed\\tline\\trepresents\\tthe\\tpoints\\twhere\\tthe\\tmodel\\testimates\\ta\\n50%\\tprobability:\\tthis\\tis\\tthe\\tmodel’s\\tdecision\\tboundary.\\tNote\\tthat\\tit\\tis\\ta\\tlinear\\tboundary.\\n17\\n\\tEach\\tparallel\\nline\\trepresents\\tthe\\tpoints\\twhere\\tthe\\tmodel\\toutputs\\ta\\tspecific\\tprobability,\\tfrom\\t15%\\t(bottom\\tleft)\\tto\\t90%\\n(top\\tright).\\tAll\\tthe\\tflowers\\tbeyond\\tthe\\ttop-right\\tline\\thave\\tan\\tover\\t90%\\tchance\\tof\\tbeing\\tIris-Virginica\\naccording\\tto\\tthe\\tmodel.\\nFigure\\t4-24.\\t\\nLinear\\tdecision\\tboundary', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 182}), Document(page_content='Just\\tlike\\tthe\\tother\\tlinear\\tmodels,\\tLogistic\\tRegression\\tmodels\\tcan\\tbe\\tregularized\\tusing\\t\\nℓ\\n1\\n\\tor\\tℓ\\n2\\n\\tpenalties.\\nScitkit-Learn\\tactually\\tadds\\tan\\tℓ\\n2\\n\\tpenalty\\tby\\tdefault.\\nNOTE\\nThe\\thyperparameter\\tcontrolling\\tthe\\tregularization\\tstrength\\tof\\ta\\tScikit-Learn\\t\\nLogisticRegression\\n\\tmodel\\tis\\tnot\\t\\nalpha\\n\\t(as\\tin\\tother\\nlinear\\tmodels),\\tbut\\tits\\tinverse:\\t\\nC\\n.\\tThe\\thigher\\tthe\\tvalue\\tof\\t\\nC\\n,\\tthe\\t\\nless\\n\\tthe\\tmodel\\tis\\t\\nregularized.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 183}), Document(page_content='Softmax\\tRegression\\nThe\\t\\nLogistic\\tRegression\\tmodel\\tcan\\tbe\\tgeneralized\\tto\\tsupport\\tmultiple\\tclasses\\tdirectly,\\twithout\\thaving\\tto\\ntrain\\tand\\tcombine\\tmultiple\\tbinary\\tclassifiers\\t(as\\tdiscussed\\tin\\t\\nChapter\\t3\\n).\\tThis\\tis\\tcalled\\t\\nSoftmax\\nRegression\\n,\\tor\\t\\nMultinomial\\tLogistic\\tRegression\\n.\\nThe\\tidea\\tis\\tquite\\tsimple:\\twhen\\tgiven\\tan\\tinstance\\t\\nx\\n,\\tthe\\tSoftmax\\tRegression\\tmodel\\tfirst\\tcomputes\\ta\\tscore\\ns\\nk\\n(\\nx\\n)\\tfor\\teach\\tclass\\t\\nk\\n,\\tthen\\testimates\\tthe\\tprobability\\tof\\teach\\tclass\\tby\\tapplying\\t\\nthe\\t\\nsoftmax\\tfunction\\n\\t(also\\ncalled\\tthe\\t\\nnormalized\\texponential\\n)\\tto\\tthe\\tscores.\\tThe\\tequation\\tto\\tcompute\\t\\ns\\nk\\n(\\nx\\n)\\tshould\\tlook\\tfamiliar,\\tas\\nit\\tis\\tjust\\tlike\\tthe\\tequation\\tfor\\tLinear\\tRegression\\tprediction\\t(see\\t\\nEquation\\t4-19\\n).\\nEquation\\t4-19.\\t\\nSoftmax\\tscore\\tfor\\tclass\\tk\\nNote\\tthat\\teach\\tclass\\thas\\tits\\town\\tdedicated\\t\\nparameter\\tvector\\t\\nθ\\n(k)\\n.\\tAll\\tthese\\tvectors\\tare\\ttypically\\tstored\\tas\\nrows\\t\\nin\\ta\\t\\nparameter\\tmatrix\\n\\t\\nΘ\\n.\\nOnce\\tyou\\thave\\tcomputed\\tthe\\tscore\\tof\\tevery\\tclass\\tfor\\tthe\\tinstance\\t\\nx\\n,\\tyou\\tcan\\testimate\\tthe\\tprobability\\t\\nk\\nthat\\tthe\\tinstance\\tbelongs\\tto\\tclass\\t\\nk\\n\\tby\\trunning\\tthe\\tscores\\tthrough\\tthe\\tsoftmax\\tfunction\\t(\\nEquation\\t4-20\\n):\\tit\\ncomputes\\tthe\\texponential\\tof\\tevery\\tscore,\\tthen\\tnormalizes\\tthem\\t(dividing\\tby\\tthe\\tsum\\tof\\tall\\tthe\\nexponentials).\\nEquation\\t4-20.\\t\\nSoftmax\\tfunction\\nK\\n\\tis\\tthe\\tnumber\\tof\\tclasses.\\ns\\n(\\nx\\n)\\tis\\ta\\tvector\\tcontaining\\tthe\\tscores\\tof\\teach\\tclass\\tfor\\tthe\\tinstance\\t\\nx\\n.\\nσ\\n(\\ns\\n(\\nx\\n))\\nk\\n\\tis\\tthe\\testimated\\tprobability\\tthat\\tthe\\tinstance\\t\\nx\\n\\tbelongs\\tto\\tclass\\t\\nk\\n\\tgiven\\tthe\\tscores\\tof\\teach\\nclass\\tfor\\tthat\\tinstance.\\nJust\\tlike\\tthe\\tLogistic\\tRegression\\tclassifier,\\tthe\\tSoftmax\\tRegression\\tclassifier\\tpredicts\\tthe\\tclass\\twith\\tthe\\nhighest\\testimated\\tprobability\\t(which\\tis\\tsimply\\tthe\\tclass\\twith\\tthe\\thighest\\tscore),\\tas\\tshown\\tin\\t\\nEquation\\t4-\\n21\\n.\\nEquation\\t4-21.\\t\\nSoftmax\\tRegression\\tclassifier\\tprediction\\n', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 184}), Document(page_content='The\\t\\nargmax\\n\\toperator\\treturns\\tthe\\tvalue\\tof\\ta\\tvariable\\tthat\\tmaximizes\\ta\\tfunction.\\tIn\\tthis\\tequation,\\tit\\nreturns\\tthe\\tvalue\\tof\\t\\nk\\n\\tthat\\tmaximizes\\tthe\\testimated\\tprobability\\t\\nσ\\n(\\ns\\n(\\nx\\n))\\nk\\n.\\nTIP\\nThe\\tSoftmax\\tRegression\\tclassifier\\tpredicts\\tonly\\tone\\tclass\\tat\\ta\\ttime\\t(i.e.,\\tit\\tis\\tmulticlass,\\tnot\\tmultioutput)\\tso\\tit\\tshould\\tbe\\tused\\tonly\\nwith\\tmutually\\texclusive\\tclasses\\tsuch\\tas\\tdifferent\\ttypes\\tof\\tplants.\\tYou\\tcannot\\tuse\\tit\\tto\\trecognize\\tmultiple\\tpeople\\tin\\tone\\tpicture.\\nNow\\tthat\\tyou\\tknow\\thow\\tthe\\tmodel\\testimates\\tprobabilities\\tand\\tmakes\\tpredictions,\\tlet’s\\ttake\\ta\\tlook\\tat\\ntraining.\\tThe\\tobjective\\tis\\tto\\thave\\ta\\tmodel\\tthat\\testimates\\ta\\thigh\\tprobability\\tfor\\tthe\\ttarget\\tclass\\t(and\\nconsequently\\ta\\tlow\\tprobability\\tfor\\tthe\\tother\\tclasses).\\tMinimizing\\tthe\\tcost\\tfunction\\tshown\\tin\\t\\nEquation\\t4-\\n22\\n,\\tcalled\\t\\nthe\\t\\ncross\\tentropy\\n,\\tshould\\tlead\\tto\\tthis\\tobjective\\tbecause\\tit\\tpenalizes\\tthe\\tmodel\\twhen\\tit\\nestimates\\ta\\tlow\\tprobability\\tfor\\ta\\ttarget\\tclass.\\tCross\\tentropy\\tis\\tfrequently\\tused\\tto\\tmeasure\\thow\\twell\\ta\\tset\\nof\\testimated\\tclass\\tprobabilities\\tmatch\\tthe\\ttarget\\tclasses\\t(we\\twill\\tuse\\tit\\tagain\\tseveral\\ttimes\\tin\\tthe\\nfollowing\\tchapters).\\nEquation\\t4-22.\\t\\nCross\\tentropy\\tcost\\tfunction\\n\\tis\\tequal\\tto\\t1\\tif\\tthe\\ttarget\\tclass\\tfor\\tthe\\ti\\nth\\n\\tinstance\\tis\\t\\nk\\n;\\totherwise,\\tit\\tis\\tequal\\tto\\t0.\\nNotice\\tthat\\twhen\\tthere\\tare\\tjust\\ttwo\\tclasses\\t(\\nK\\n\\t=\\t2),\\tthis\\tcost\\tfunction\\tis\\tequivalent\\tto\\tthe\\tLogistic\\nRegression’s\\tcost\\tfunction\\t(log\\tloss;\\tsee\\t\\nEquation\\t4-17\\n).\\nCROSS\\tENTROPY\\nCross\\tentropy\\toriginated\\tfrom\\tinformation\\ttheory.\\tSuppose\\tyou\\twant\\tto\\tefficiently\\ttransmit\\tinformation\\tabout\\tthe\\tweather\\tevery\\tday.\\tIf\\nthere\\tare\\teight\\toptions\\t(sunny,\\trainy,\\tetc.),\\tyou\\tcould\\tencode\\teach\\toption\\tusing\\t3\\tbits\\tsince\\t2\\n3\\n\\t=\\t8.\\tHowever,\\tif\\tyou\\tthink\\tit\\twill\\tbe\\tsunny\\nalmost\\tevery\\tday,\\tit\\twould\\tbe\\tmuch\\tmore\\tefficient\\tto\\tcode\\t“sunny”\\ton\\tjust\\tone\\tbit\\t(0)\\tand\\tthe\\tother\\tseven\\toptions\\ton\\t4\\tbits\\t(starting\\twith\\na\\t1).\\tCross\\tentropy\\tmeasures\\tthe\\taverage\\tnumber\\tof\\tbits\\tyou\\tactually\\tsend\\tper\\toption.\\tIf\\tyour\\tassumption\\tabout\\tthe\\tweather\\tis\\tperfect,\\ncross\\tentropy\\twill\\tjust\\tbe\\tequal\\tto\\tthe\\tentropy\\tof\\tthe\\tweather\\titself\\t(i.e.,\\tits\\tintrinsic\\tunpredictability).\\tBut\\tif\\tyour\\tassumptions\\tare\\twrong\\n(e.g.,\\tif\\tit\\trains\\toften),\\tcross\\tentropy\\twill\\tbe\\tgreater\\tby\\tan\\tamount\\t\\ncalled\\tthe\\t\\nKullback–Leibler\\tdivergence\\n.\\nThe\\tcross\\tentropy\\tbetween\\ttwo\\tprobability\\tdistributions\\t\\np\\n\\tand\\t\\nq\\n\\tis\\tdefined\\tas\\t\\n\\t(at\\tleast\\nwhen\\tthe\\tdistributions\\tare\\tdiscrete).\\nThe\\t\\ngradient\\tvector\\tof\\tthis\\tcost\\tfunction\\twith\\tregards\\tto\\t\\nθ\\n(k)\\n\\tis\\tgiven\\tby\\t\\nEquation\\t4-23\\n:\\nEquation\\t4-23.\\t\\nCross\\tentropy\\tgradient\\tvector\\tfor\\tclass\\tk', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 185}), Document(page_content='Now\\tyou\\tcan\\tcompute\\tthe\\tgradient\\tvector\\tfor\\tevery\\tclass,\\tthen\\tuse\\tGradient\\tDescent\\t(or\\tany\\tother\\noptimization\\talgorithm)\\tto\\tfind\\tthe\\tparameter\\tmatrix\\t\\nΘ\\n\\tthat\\tminimizes\\tthe\\tcost\\tfunction.\\nLet’s\\tuse\\tSoftmax\\tRegression\\tto\\tclassify\\tthe\\tiris\\tflowers\\tinto\\tall\\tthree\\tclasses.\\tScikit-Learn’s\\nLogisticRegression\\n\\t\\nuses\\t\\none-versus-all\\tby\\tdefault\\twhen\\tyou\\ttrain\\tit\\ton\\tmore\\tthan\\ttwo\\tclasses,\\tbut\\tyou\\ncan\\tset\\tthe\\t\\nmulti_class\\n\\thyperparameter\\tto\\t\\n\"multinomial\"\\n\\tto\\tswitch\\tit\\tto\\tSoftmax\\tRegression\\tinstead.\\nYou\\tmust\\talso\\tspecify\\ta\\tsolver\\tthat\\tsupports\\tSoftmax\\tRegression,\\tsuch\\tas\\tthe\\t\\n\"lbfgs\"\\n\\tsolver\\t(see\\tScikit-\\nLearn’s\\tdocumentation\\tfor\\tmore\\tdetails).\\tIt\\talso\\tapplies\\tℓ\\n2\\n\\t\\nregularization\\tby\\tdefault,\\twhich\\tyou\\tcan\\ncontrol\\tusing\\tthe\\thyperparameter\\t\\nC\\n.\\nX\\n\\t\\n=\\n\\t\\niris\\n[\\n\"data\"\\n][:,\\n\\t\\n(\\n2\\n,\\n\\t\\n3\\n)]\\n\\t\\t\\n#\\tpetal\\tlength,\\tpetal\\twidth\\ny\\n\\t\\n=\\n\\t\\niris\\n[\\n\"target\"\\n]\\nsoftmax_reg\\n\\t\\n=\\n\\t\\nLogisticRegression\\n(\\nmulti_class\\n=\\n\"multinomial\"\\n,\\nsolver\\n=\\n\"lbfgs\"\\n,\\n\\t\\nC\\n=\\n10\\n)\\nsoftmax_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nSo\\tthe\\tnext\\ttime\\tyou\\tfind\\tan\\tiris\\twith\\t5\\tcm\\tlong\\tand\\t2\\tcm\\twide\\tpetals,\\tyou\\tcan\\task\\tyour\\tmodel\\tto\\ttell\\tyou\\nwhat\\ttype\\tof\\tiris\\tit\\tis,\\tand\\tit\\twill\\tanswer\\tIris-Virginica\\t(class\\t2)\\twith\\t94.2%\\tprobability\\t(or\\tIris-\\nVersicolor\\twith\\t5.8%\\tprobability):\\n>>>\\t\\nsoftmax_reg\\n.\\npredict\\n([[\\n5\\n,\\n\\t\\n2\\n]])\\narray([2])\\n>>>\\t\\nsoftmax_reg\\n.\\npredict_proba\\n([[\\n5\\n,\\n\\t\\n2\\n]])\\narray([[\\t\\t6.33134078e-07,\\t\\t\\t5.75276067e-02,\\t\\t\\t9.42471760e-01]])\\nFigure\\t4-25\\n\\tshows\\tthe\\tresulting\\t\\ndecision\\tboundaries,\\trepresented\\tby\\tthe\\tbackground\\tcolors.\\tNotice\\tthat\\nthe\\tdecision\\tboundaries\\tbetween\\tany\\ttwo\\tclasses\\tare\\tlinear.\\tThe\\tfigure\\talso\\tshows\\tthe\\tprobabilities\\tfor\\nthe\\tIris-Versicolor\\tclass,\\trepresented\\tby\\tthe\\tcurved\\tlines\\t(e.g.,\\tthe\\tline\\tlabeled\\twith\\t0.450\\trepresents\\tthe\\n45%\\tprobability\\tboundary).\\tNotice\\tthat\\tthe\\tmodel\\tcan\\tpredict\\ta\\tclass\\tthat\\thas\\tan\\testimated\\tprobability\\nbelow\\t50%.\\tFor\\texample,\\tat\\tthe\\tpoint\\twhere\\tall\\tdecision\\tboundaries\\tmeet,\\tall\\tclasses\\thave\\tan\\tequal\\nestimated\\tprobability\\t\\nof\\t33%.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 186}), Document(page_content='Figure\\t4-25.\\t\\nSoftmax\\tRegression\\tdecision\\tboundaries', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 187}), Document(page_content='Exercises\\n1\\n.\\t\\nWhat\\tLinear\\tRegression\\ttraining\\talgorithm\\tcan\\tyou\\tuse\\tif\\tyou\\thave\\ta\\ttraining\\tset\\twith\\tmillions\\tof\\nfeatures?\\n2\\n.\\t\\nSuppose\\tthe\\tfeatures\\tin\\tyour\\ttraining\\tset\\thave\\tvery\\tdifferent\\tscales.\\tWhat\\talgorithms\\tmight\\tsuffer\\nfrom\\tthis,\\tand\\thow?\\tWhat\\tcan\\tyou\\tdo\\tabout\\tit?\\n3\\n.\\t\\nCan\\tGradient\\tDescent\\tget\\tstuck\\tin\\ta\\tlocal\\tminimum\\twhen\\ttraining\\ta\\tLogistic\\tRegression\\tmodel?\\n4\\n.\\t\\nDo\\tall\\tGradient\\tDescent\\talgorithms\\tlead\\tto\\tthe\\tsame\\tmodel\\tprovided\\tyou\\tlet\\tthem\\trun\\tlong\\tenough?\\n5\\n.\\t\\nSuppose\\tyou\\tuse\\tBatch\\tGradient\\tDescent\\tand\\tyou\\tplot\\tthe\\tvalidation\\terror\\tat\\tevery\\tepoch.\\tIf\\tyou\\nnotice\\tthat\\tthe\\tvalidation\\terror\\tconsistently\\tgoes\\tup,\\twhat\\tis\\tlikely\\tgoing\\ton?\\tHow\\tcan\\tyou\\tfix\\tthis?\\n6\\n.\\t\\nIs\\tit\\ta\\tgood\\tidea\\tto\\tstop\\tMini-batch\\tGradient\\tDescent\\timmediately\\twhen\\tthe\\tvalidation\\terror\\tgoes\\nup?\\n7\\n.\\t\\nWhich\\tGradient\\tDescent\\talgorithm\\t(among\\tthose\\twe\\tdiscussed)\\twill\\treach\\tthe\\tvicinity\\tof\\tthe\\toptimal\\nsolution\\tthe\\tfastest?\\tWhich\\twill\\tactually\\tconverge?\\tHow\\tcan\\tyou\\tmake\\tthe\\tothers\\tconverge\\tas\\twell?\\n8\\n.\\t\\nSuppose\\tyou\\tare\\tusing\\tPolynomial\\tRegression.\\tYou\\tplot\\tthe\\tlearning\\tcurves\\tand\\tyou\\tnotice\\tthat\\tthere\\nis\\ta\\tlarge\\tgap\\tbetween\\tthe\\ttraining\\terror\\tand\\tthe\\tvalidation\\terror.\\tWhat\\tis\\thappening?\\tWhat\\tare\\tthree\\nways\\tto\\tsolve\\tthis?\\n9\\n.\\t\\nSuppose\\tyou\\tare\\tusing\\tRidge\\tRegression\\tand\\tyou\\tnotice\\tthat\\tthe\\ttraining\\terror\\tand\\tthe\\tvalidation\\nerror\\tare\\talmost\\tequal\\tand\\tfairly\\thigh.\\tWould\\tyou\\tsay\\tthat\\tthe\\tmodel\\tsuffers\\tfrom\\thigh\\tbias\\tor\\thigh\\nvariance?\\tShould\\tyou\\tincrease\\tthe\\tregularization\\thyperparameter\\t\\nα\\n\\tor\\treduce\\tit?\\n10\\n.\\t\\nWhy\\twould\\tyou\\twant\\tto\\tuse:\\nRidge\\tRegression\\tinstead\\tof\\tplain\\tLinear\\tRegression\\t(i.e.,\\twithout\\tany\\tregularization)?\\nLasso\\tinstead\\tof\\tRidge\\tRegression?\\nElastic\\tNet\\tinstead\\tof\\tLasso?\\n11\\n.\\t\\nSuppose\\tyou\\twant\\tto\\tclassify\\tpictures\\tas\\toutdoor/indoor\\tand\\tdaytime/nighttime.\\tShould\\tyou\\nimplement\\ttwo\\tLogistic\\tRegression\\tclassifiers\\tor\\tone\\tSoftmax\\tRegression\\tclassifier?\\n12\\n.\\t\\nImplement\\tBatch\\tGradient\\tDescent\\twith\\tearly\\tstopping\\tfor\\tSoftmax\\tRegression\\t\\n(without\\tusing\\tScikit-\\nLearn).\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\nIt\\tis\\toften\\tthe\\tcase\\tthat\\ta\\tlearning\\talgorithm\\twill\\ttry\\tto\\toptimize\\ta\\tdifferent\\tfunction\\tthan\\tthe\\tperformance\\tmeasure\\tused\\tto\\tevaluate\\tthe\\nfinal\\tmodel.\\tThis\\tis\\tgenerally\\tbecause\\tthat\\tfunction\\tis\\teasier\\tto\\tcompute,\\tbecause\\tit\\thas\\tuseful\\tdifferentiation\\tproperties\\tthat\\tthe\\nperformance\\tmeasure\\tlacks,\\tor\\tbecause\\twe\\twant\\tto\\tconstrain\\tthe\\tmodel\\tduring\\ttraining,\\tas\\twe\\twill\\tsee\\twhen\\twe\\tdiscuss\\tregularization.\\n1', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 188}), Document(page_content='The\\tdemonstration\\tthat\\tthis\\treturns\\tthe\\tvalue\\tof\\t\\nθ\\n\\tthat\\tminimizes\\tthe\\tcost\\tfunction\\tis\\toutside\\tthe\\tscope\\tof\\tthis\\tbook.\\nNote\\tthat\\tScikit-Learn\\tseparates\\tthe\\tbias\\tterm\\t(\\nintercept_\\n)\\tfrom\\tthe\\tfeature\\tweights\\t(\\ncoef_\\n).\\nTechnically\\tspeaking,\\tits\\tderivative\\tis\\t\\nLipschitz\\tcontinuous\\n.\\nSince\\tfeature\\t1\\tis\\tsmaller,\\tit\\ttakes\\ta\\tlarger\\tchange\\tin\\t\\nθ\\n1\\n\\tto\\taffect\\tthe\\tcost\\tfunction,\\twhich\\tis\\twhy\\tthe\\tbowl\\tis\\telongated\\talong\\tthe\\t\\nθ\\n1\\n\\taxis.\\nEta\\t(\\nη\\n)\\tis\\tthe\\t7\\nletter\\tof\\tthe\\tGreek\\talphabet.\\nOut-of-core\\talgorithms\\tare\\tdiscussed\\tin\\t\\nChapter\\t1\\n.\\nWhile\\tthe\\tNormal\\tEquation\\tcan\\tonly\\tperform\\tLinear\\tRegression,\\tthe\\tGradient\\tDescent\\talgorithms\\tcan\\tbe\\tused\\tto\\ttrain\\tmany\\tother\\tmodels,\\nas\\twe\\twill\\tsee.\\nA\\tquadratic\\tequation\\tis\\tof\\tthe\\tform\\t\\ny\\n\\t=\\t\\nax\\n+\\t\\nbx\\n\\t+\\t\\nc\\n.\\nThis\\tnotion\\tof\\tbias\\tis\\tnot\\tto\\tbe\\tconfused\\twith\\tthe\\tbias\\tterm\\tof\\tlinear\\tmodels.\\nIt\\tis\\tcommon\\tto\\tuse\\tthe\\tnotation\\t\\nJ\\n(\\nθ\\n)\\tfor\\tcost\\tfunctions\\tthat\\tdon’t\\thave\\ta\\tshort\\tname;\\twe\\twill\\toften\\tuse\\tthis\\tnotation\\tthroughout\\tthe\\trest\\tof\\nthis\\tbook.\\tThe\\tcontext\\twill\\tmake\\tit\\tclear\\twhich\\tcost\\tfunction\\tis\\tbeing\\tdiscussed.\\nNorms\\tare\\tdiscussed\\tin\\t\\nChapter\\t2\\n.\\nA\\tsquare\\tmatrix\\tfull\\tof\\t0s\\texcept\\tfor\\t1s\\ton\\tthe\\tmain\\tdiagonal\\t(top-left\\tto\\tbottom-right).\\nAlternatively\\tyou\\tcan\\tuse\\tthe\\t\\nRidge\\n\\tclass\\twith\\tthe\\t\\n\"sag\"\\n\\tsolver.\\tStochastic\\tAverage\\tGD\\t\\nis\\ta\\tvariant\\tof\\tSGD.\\tFor\\tmore\\tdetails,\\tsee\\tthe\\npresentation\\t\\n“Minimizing\\tFinite\\tSums\\twith\\tthe\\tStochastic\\tAverage\\tGradient\\tAlgorithm”\\n\\tby\\tMark\\tSchmidt\\tet\\tal.\\tfrom\\tthe\\tUniversity\\tof\\nBritish\\tColumbia.\\nYou\\tcan\\tthink\\tof\\ta\\tsubgradient\\tvector\\tat\\ta\\tnondifferentiable\\tpoint\\tas\\tan\\tintermediate\\tvector\\tbetween\\tthe\\tgradient\\tvectors\\taround\\tthat\\tpoint.\\nPhotos\\treproduced\\tfrom\\tthe\\tcorresponding\\tWikipedia\\tpages.\\tIris-Virginica\\tphoto\\tby\\tFrank\\tMayfield\\t(\\nCreative\\tCommons\\tBY-SA\\t2.0\\n),\\tIris-\\nVersicolor\\tphoto\\tby\\tD.\\tGordon\\tE.\\tRobertson\\t(\\nCreative\\tCommons\\tBY-SA\\t3.0\\n),\\tand\\tIris-Setosa\\tphoto\\tis\\tpublic\\tdomain.\\nIt\\tis\\tthe\\tthe\\tset\\tof\\tpoints\\t\\nx\\n\\tsuch\\tthat\\t\\nθ\\n0\\n\\t+\\t\\nθ\\n1\\nx\\n1\\n\\t+\\t\\nθ\\n2\\nx\\n2\\n\\t=\\t0,\\twhich\\tdefines\\ta\\tstraight\\tline.\\n2\\n3\\n4\\n5\\n6\\nth\\n7\\n8\\n9\\n2\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 189}), Document(page_content='Chapter\\t5.\\t\\nSupport\\tVector\\tMachines\\nA\\t\\nSupport\\tVector\\tMachine\\n\\t(SVM)\\tis\\t\\na\\tvery\\tpowerful\\tand\\tversatile\\tMachine\\tLearning\\tmodel,\\tcapable\\tof\\nperforming\\tlinear\\tor\\tnonlinear\\tclassification,\\tregression,\\tand\\teven\\toutlier\\tdetection.\\tIt\\tis\\tone\\tof\\tthe\\tmost\\npopular\\tmodels\\tin\\tMachine\\tLearning,\\tand\\tanyone\\tinterested\\tin\\tMachine\\tLearning\\tshould\\thave\\tit\\tin\\ttheir\\ntoolbox.\\tSVMs\\tare\\tparticularly\\twell\\tsuited\\tfor\\tclassification\\tof\\tcomplex\\tbut\\tsmall-\\tor\\tmedium-sized\\ndatasets.\\nThis\\tchapter\\twill\\texplain\\tthe\\tcore\\tconcepts\\tof\\tSVMs,\\thow\\tto\\tuse\\tthem,\\tand\\thow\\tthey\\twork.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 190}), Document(page_content='Linear\\tSVM\\tClassification\\nThe\\t\\nfundamental\\tidea\\tbehind\\tSVMs\\tis\\tbest\\texplained\\twith\\tsome\\tpictures.\\t\\nFigure\\t5-1\\n\\tshows\\tpart\\tof\\tthe\\niris\\tdataset\\tthat\\twas\\tintroduced\\tat\\tthe\\tend\\tof\\t\\nChapter\\t4\\n.\\tThe\\ttwo\\tclasses\\tcan\\tclearly\\tbe\\tseparated\\teasily\\nwith\\ta\\tstraight\\tline\\t(they\\tare\\t\\nlinearly\\tseparable\\n).\\tThe\\tleft\\tplot\\tshows\\tthe\\tdecision\\tboundaries\\tof\\tthree\\npossible\\tlinear\\tclassifiers.\\tThe\\tmodel\\twhose\\tdecision\\tboundary\\tis\\trepresented\\tby\\tthe\\tdashed\\tline\\tis\\tso\\nbad\\tthat\\tit\\tdoes\\tnot\\teven\\tseparate\\tthe\\tclasses\\tproperly.\\tThe\\tother\\ttwo\\tmodels\\twork\\tperfectly\\ton\\tthis\\ntraining\\tset,\\tbut\\ttheir\\tdecision\\tboundaries\\tcome\\tso\\tclose\\tto\\tthe\\tinstances\\tthat\\tthese\\tmodels\\twill\\tprobably\\nnot\\tperform\\tas\\twell\\ton\\tnew\\tinstances.\\tIn\\tcontrast,\\tthe\\tsolid\\tline\\tin\\tthe\\tplot\\ton\\tthe\\tright\\trepresents\\tthe\\ndecision\\tboundary\\tof\\tan\\tSVM\\tclassifier;\\tthis\\tline\\tnot\\tonly\\tseparates\\tthe\\ttwo\\tclasses\\tbut\\talso\\tstays\\tas\\tfar\\naway\\tfrom\\tthe\\tclosest\\ttraining\\tinstances\\tas\\tpossible.\\tYou\\tcan\\tthink\\tof\\tan\\tSVM\\tclassifier\\tas\\tfitting\\tthe\\nwidest\\tpossible\\tstreet\\t(represented\\tby\\tthe\\tparallel\\tdashed\\tlines)\\tbetween\\tthe\\tclasses.\\tThis\\tis\\t\\ncalled\\t\\nlarge\\nmargin\\tclassification\\n.\\nFigure\\t5-1.\\t\\nLarge\\tmargin\\tclassification\\nNotice\\tthat\\tadding\\tmore\\ttraining\\tinstances\\t“off\\tthe\\tstreet”\\twill\\tnot\\taffect\\tthe\\tdecision\\tboundary\\tat\\tall:\\tit\\tis\\nfully\\tdetermined\\t(or\\t“supported”)\\tby\\tthe\\tinstances\\tlocated\\ton\\tthe\\tedge\\tof\\tthe\\tstreet.\\tThese\\tinstances\\tare\\ncalled\\t\\nthe\\t\\nsupport\\tvectors\\n\\t(they\\tare\\tcircled\\tin\\t\\nFigure\\t5-1\\n).\\nWARNING\\nSVMs\\tare\\t\\nsensitive\\tto\\tthe\\tfeature\\tscales,\\tas\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t5-2\\n:\\ton\\tthe\\tleft\\tplot,\\tthe\\tvertical\\tscale\\tis\\tmuch\\tlarger\\tthan\\tthe\\nhorizontal\\tscale,\\tso\\tthe\\twidest\\tpossible\\tstreet\\tis\\tclose\\tto\\thorizontal.\\tAfter\\tfeature\\tscaling\\t(e.g.,\\tusing\\tScikit-Learn’s\\nStandardScaler\\n),\\t\\nthe\\tdecision\\tboundary\\tlooks\\tmuch\\tbetter\\t(on\\tthe\\tright\\tplot).\\nFigure\\t5-2.\\t\\nSensitivity\\tto\\tfeature\\tscales', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 191}), Document(page_content='Soft\\tMargin\\tClassification\\nIf\\twe\\t\\nstrictly\\timpose\\tthat\\tall\\tinstances\\tbe\\toff\\tthe\\tstreet\\tand\\ton\\tthe\\tright\\tside,\\tthis\\tis\\tcalled\\t\\nhard\\tmargin\\nclassification\\n.\\t\\nThere\\tare\\ttwo\\tmain\\tissues\\twith\\thard\\tmargin\\tclassification.\\tFirst,\\tit\\tonly\\tworks\\tif\\tthe\\tdata\\nis\\tlinearly\\tseparable,\\tand\\tsecond\\tit\\tis\\tquite\\tsensitive\\tto\\toutliers.\\t\\nFigure\\t5-3\\n\\tshows\\tthe\\tiris\\tdataset\\twith\\njust\\tone\\tadditional\\toutlier:\\ton\\tthe\\tleft,\\tit\\tis\\timpossible\\tto\\tfind\\ta\\thard\\tmargin,\\tand\\ton\\tthe\\tright\\tthe\\tdecision\\nboundary\\tends\\tup\\tvery\\tdifferent\\tfrom\\tthe\\tone\\twe\\tsaw\\tin\\t\\nFigure\\t5-1\\n\\twithout\\tthe\\toutlier,\\tand\\tit\\twill\\nprobably\\tnot\\tgeneralize\\tas\\twell.\\nFigure\\t5-3.\\t\\nHard\\tmargin\\tsensitivity\\tto\\toutliers\\nTo\\tavoid\\tthese\\tissues\\t\\nit\\tis\\tpreferable\\tto\\tuse\\ta\\tmore\\tflexible\\tmodel.\\tThe\\tobjective\\tis\\tto\\tfind\\ta\\tgood\\nbalance\\tbetween\\tkeeping\\tthe\\tstreet\\tas\\tlarge\\tas\\tpossible\\tand\\tlimiting\\tthe\\t\\nmargin\\tviolations\\n\\t\\n(i.e.,\\tinstances\\nthat\\tend\\tup\\tin\\tthe\\tmiddle\\tof\\tthe\\tstreet\\tor\\teven\\ton\\tthe\\twrong\\tside).\\tThis\\tis\\tcalled\\t\\nsoft\\tmargin\\nclassification\\n.\\nIn\\tScikit-Learn’s\\tSVM\\tclasses,\\tyou\\tcan\\tcontrol\\tthis\\tbalance\\tusing\\tthe\\t\\nC\\n\\thyperparameter:\\ta\\tsmaller\\t\\nC\\n\\tvalue\\nleads\\tto\\ta\\twider\\tstreet\\tbut\\tmore\\tmargin\\tviolations.\\t\\nFigure\\t5-4\\n\\tshows\\tthe\\tdecision\\tboundaries\\tand\\tmargins\\nof\\ttwo\\tsoft\\tmargin\\tSVM\\tclassifiers\\ton\\ta\\tnonlinearly\\tseparable\\tdataset.\\tOn\\tthe\\tleft,\\tusing\\ta\\thigh\\t\\nC\\n\\tvalue\\nthe\\tclassifier\\tmakes\\tfewer\\tmargin\\tviolations\\tbut\\tends\\tup\\twith\\ta\\tsmaller\\tmargin.\\tOn\\tthe\\tright,\\tusing\\ta\\tlow\\nC\\n\\tvalue\\tthe\\tmargin\\tis\\tmuch\\tlarger,\\tbut\\tmany\\tinstances\\tend\\tup\\ton\\tthe\\tstreet.\\tHowever,\\tit\\tseems\\tlikely\\tthat\\nthe\\tsecond\\tclassifier\\twill\\tgeneralize\\tbetter:\\tin\\tfact\\teven\\ton\\tthis\\ttraining\\tset\\tit\\tmakes\\tfewer\\tprediction\\nerrors,\\tsince\\tmost\\tof\\tthe\\tmargin\\tviolations\\tare\\tactually\\ton\\tthe\\tcorrect\\tside\\tof\\tthe\\tdecision\\tboundary.\\nFigure\\t5-4.\\t\\nFewer\\tmargin\\tviolations\\tversus\\tlarge\\tmargin\\nTIP\\nIf\\tyour\\tSVM\\tmodel\\tis\\t\\noverfitting,\\tyou\\tcan\\ttry\\tregularizing\\tit\\tby\\treducing\\t\\nC\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 192}), Document(page_content='The\\tfollowing\\tScikit-Learn\\tcode\\tloads\\tthe\\tiris\\tdataset,\\tscales\\tthe\\tfeatures,\\tand\\tthen\\ttrains\\ta\\tlinear\\tSVM\\nmodel\\t\\n(using\\tthe\\t\\nLinearSVC\\n\\tclass\\twith\\t\\nC\\n\\t=\\t0.1\\tand\\tthe\\t\\nhinge\\tloss\\n\\tfunction,\\tdescribed\\tshortly)\\tto\\tdetect\\nIris-Virginica\\tflowers.\\tThe\\tresulting\\tmodel\\tis\\trepresented\\ton\\t\\nthe\\tright\\tof\\t\\nFigure\\t5-4\\n.\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nfrom\\n\\t\\nsklearn\\n\\t\\nimport\\n\\t\\ndatasets\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nPipeline\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nStandardScaler\\nfrom\\n\\t\\nsklearn.svm\\n\\t\\nimport\\n\\t\\nLinearSVC\\niris\\n\\t\\n=\\n\\t\\ndatasets\\n.\\nload_iris\\n()\\nX\\n\\t\\n=\\n\\t\\niris\\n[\\n\"data\"\\n][:,\\n\\t\\n(\\n2\\n,\\n\\t\\n3\\n)]\\n\\t\\t\\n#\\tpetal\\tlength,\\tpetal\\twidth\\ny\\n\\t\\n=\\n\\t\\n(\\niris\\n[\\n\"target\"\\n]\\n\\t\\n==\\n\\t\\n2\\n)\\n.\\nastype\\n(\\nnp\\n.\\nfloat64\\n)\\n\\t\\t\\n#\\tIris-Virginica\\nsvm_clf\\n\\t\\n=\\n\\t\\nPipeline\\n((\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"scaler\"\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"linear_svc\"\\n,\\n\\t\\nLinearSVC\\n(\\nC\\n=\\n1\\n,\\n\\t\\nloss\\n=\\n\"hinge\"\\n)),\\n\\t\\t\\t\\t\\n))\\nsvm_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nThen,\\tas\\tusual,\\tyou\\tcan\\tuse\\tthe\\tmodel\\tto\\tmake\\tpredictions:\\n>>>\\t\\nsvm_clf\\n.\\npredict\\n([[\\n5.5\\n,\\n\\t\\n1.7\\n]])\\narray([\\t1.])\\nNOTE\\nUnlike\\tLogistic\\tRegression\\tclassifiers,\\tSVM\\tclassifiers\\tdo\\tnot\\toutput\\tprobabilities\\tfor\\teach\\tclass.\\nAlternatively,\\t\\nyou\\tcould\\tuse\\tthe\\t\\nSVC\\n\\tclass,\\tusing\\t\\nSVC(kernel=\"linear\",\\tC=1)\\n,\\tbut\\tit\\tis\\tmuch\\tslower,\\nespecially\\twith\\tlarge\\ttraining\\tsets,\\tso\\tit\\tis\\tnot\\trecommended.\\tAnother\\toption\\tis\\tto\\tuse\\tthe\\t\\nSGDClassifier\\nclass,\\twith\\t\\nSGDClassifier(loss=\"hinge\",\\talpha=1/(m*C))\\n.\\tThis\\tapplies\\tregular\\t\\nStochastic\\nGradient\\tDescent\\t(see\\t\\nChapter\\t4\\n)\\tto\\ttrain\\ta\\tlinear\\tSVM\\tclassifier.\\tIt\\tdoes\\tnot\\tconverge\\tas\\tfast\\tas\\tthe\\nLinearSVC\\n\\tclass,\\tbut\\tit\\tcan\\tbe\\tuseful\\tto\\thandle\\thuge\\tdatasets\\tthat\\tdo\\tnot\\tfit\\tin\\tmemory\\t(out-of-core\\ntraining),\\tor\\tto\\thandle\\tonline\\tclassification\\ttasks.\\nTIP\\nThe\\t\\nLinearSVC\\n\\tclass\\tregularizes\\tthe\\tbias\\tterm,\\tso\\tyou\\tshould\\tcenter\\tthe\\ttraining\\tset\\tfirst\\tby\\tsubtracting\\tits\\tmean.\\tThis\\tis\\nautomatic\\tif\\tyou\\tscale\\tthe\\tdata\\tusing\\tthe\\t\\nStandardScaler\\n.\\tMoreover,\\tmake\\tsure\\tyou\\tset\\tthe\\t\\nloss\\n\\thyperparameter\\tto\\t\\n\"hinge\"\\n,\\tas\\nit\\tis\\tnot\\tthe\\tdefault\\tvalue.\\tFinally,\\tfor\\tbetter\\tperformance\\tyou\\tshould\\tset\\tthe\\t\\ndual\\n\\thyperparameter\\tto\\t\\nFalse\\n,\\tunless\\tthere\\tare\\tmore\\nfeatures\\tthan\\ttraining\\tinstances\\t(we\\twill\\tdiscuss\\tduality\\tlater\\tin\\tthe\\t\\nchapter).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 193}), Document(page_content='Nonlinear\\tSVM\\tClassification\\nAlthough\\t\\nlinear\\tSVM\\tclassifiers\\tare\\tefficient\\tand\\twork\\tsurprisingly\\twell\\tin\\tmany\\tcases,\\tmany\\tdatasets\\nare\\tnot\\teven\\tclose\\tto\\tbeing\\tlinearly\\tseparable.\\tOne\\tapproach\\tto\\thandling\\tnonlinear\\tdatasets\\tis\\tto\\tadd\\tmore\\nfeatures,\\tsuch\\tas\\t\\npolynomial\\tfeatures\\t(as\\tyou\\tdid\\tin\\t\\nChapter\\t4\\n);\\tin\\tsome\\tcases\\tthis\\tcan\\tresult\\tin\\ta\\tlinearly\\nseparable\\tdataset.\\tConsider\\tthe\\tleft\\tplot\\tin\\t\\nFigure\\t5-5\\n:\\tit\\trepresents\\ta\\tsimple\\tdataset\\twith\\tjust\\tone\\tfeature\\nx\\n1\\n.\\tThis\\tdataset\\tis\\tnot\\tlinearly\\tseparable,\\tas\\tyou\\tcan\\tsee.\\tBut\\tif\\tyou\\tadd\\ta\\tsecond\\tfeature\\t\\nx\\n2\\n\\t=\\t(\\nx\\n1\\n)\\n2\\n,\\tthe\\nresulting\\t2D\\tdataset\\tis\\tperfectly\\tlinearly\\tseparable.\\nFigure\\t5-5.\\t\\nAdding\\tfeatures\\tto\\tmake\\ta\\tdataset\\tlinearly\\tseparable\\nTo\\timplement\\tthis\\tidea\\tusing\\t\\nScikit-Learn,\\tyou\\tcan\\tcreate\\ta\\t\\nPipeline\\n\\tcontaining\\ta\\t\\nPolynomialFeatures\\ntransformer\\t(discussed\\tin\\t\\n“Polynomial\\tRegression”\\n),\\tfollowed\\tby\\ta\\t\\nStandardScaler\\n\\tand\\ta\\t\\nLinearSVC\\n.\\nLet’s\\ttest\\tthis\\ton\\tthe\\tmoons\\tdataset\\t(see\\t\\nFigure\\t5-6\\n):\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nmake_moons\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nPipeline\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nPolynomialFeatures\\npolynomial_svm_clf\\n\\t\\n=\\n\\t\\nPipeline\\n((\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"poly_features\"\\n,\\n\\t\\nPolynomialFeatures\\n(\\ndegree\\n=\\n3\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"scaler\"\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"svm_clf\"\\n,\\n\\t\\nLinearSVC\\n(\\nC\\n=\\n10\\n,\\n\\t\\nloss\\n=\\n\"hinge\"\\n))\\n\\t\\t\\t\\t\\n))\\npolynomial_svm_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 194}), Document(page_content='Figure\\t5-6.\\t\\nLinear\\tSVM\\tclassifier\\tusing\\tpolynomial\\tfeatures', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 195}), Document(page_content='Polynomial\\tKernel\\nAdding\\t\\npolynomial\\tfeatures\\tis\\tsimple\\tto\\timplement\\tand\\tcan\\twork\\tgreat\\twith\\tall\\tsorts\\tof\\tMachine\\tLearning\\nalgorithms\\t(not\\tjust\\tSVMs),\\tbut\\tat\\ta\\tlow\\tpolynomial\\tdegree\\tit\\tcannot\\tdeal\\twith\\tvery\\tcomplex\\tdatasets,\\nand\\twith\\ta\\thigh\\tpolynomial\\tdegree\\tit\\tcreates\\ta\\thuge\\tnumber\\tof\\tfeatures,\\tmaking\\tthe\\tmodel\\ttoo\\tslow.\\nFortunately,\\twhen\\tusing\\tSVMs\\tyou\\tcan\\tapply\\tan\\talmost\\tmiraculous\\tmathematical\\ttechnique\\tcalled\\tthe\\nkernel\\ttrick\\n\\t(it\\tis\\texplained\\tin\\ta\\tmoment).\\tIt\\tmakes\\tit\\tpossible\\tto\\tget\\tthe\\tsame\\tresult\\tas\\tif\\tyou\\tadded\\tmany\\npolynomial\\tfeatures,\\teven\\twith\\tvery\\thigh-degree\\tpolynomials,\\twithout\\tactually\\thaving\\tto\\tadd\\tthem.\\tSo\\nthere\\tis\\tno\\tcombinatorial\\texplosion\\tof\\tthe\\tnumber\\tof\\tfeatures\\tsince\\tyou\\tdon’t\\tactually\\tadd\\tany\\tfeatures.\\nThis\\ttrick\\tis\\timplemented\\tby\\tthe\\t\\nSVC\\n\\tclass.\\t\\nLet’s\\ttest\\tit\\ton\\tthe\\tmoons\\tdataset:\\nfrom\\n\\t\\nsklearn.svm\\n\\t\\nimport\\n\\t\\nSVC\\npoly_kernel_svm_clf\\n\\t\\n=\\n\\t\\nPipeline\\n((\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"scaler\"\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"svm_clf\"\\n,\\n\\t\\nSVC\\n(\\nkernel\\n=\\n\"poly\"\\n,\\n\\t\\ndegree\\n=\\n3\\n,\\n\\t\\ncoef0\\n=\\n1\\n,\\n\\t\\nC\\n=\\n5\\n))\\n\\t\\t\\t\\t\\n))\\npoly_kernel_svm_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nThis\\tcode\\t\\ntrains\\tan\\tSVM\\tclassifier\\tusing\\ta\\t3\\nrd\\n-degree\\tpolynomial\\tkernel.\\tIt\\tis\\trepresented\\ton\\tthe\\tleft\\tof\\nFigure\\t5-7\\n.\\tOn\\tthe\\tright\\tis\\tanother\\tSVM\\tclassifier\\tusing\\ta\\t10\\nth\\n-degree\\tpolynomial\\tkernel.\\tObviously,\\tif\\nyour\\tmodel\\tis\\toverfitting,\\tyou\\tmight\\twant\\tto\\treduce\\tthe\\tpolynomial\\tdegree.\\tConversely,\\tif\\tit\\tis\\nunderfitting,\\tyou\\tcan\\ttry\\tincreasing\\tit.\\tThe\\thyperparameter\\t\\ncoef0\\n\\tcontrols\\thow\\tmuch\\tthe\\tmodel\\tis\\ninfluenced\\tby\\thigh-degree\\tpolynomials\\tversus\\tlow-degree\\tpolynomials.\\nFigure\\t5-7.\\t\\nSVM\\tclassifiers\\twith\\ta\\tpolynomial\\tkernel\\nTIP\\nA\\tcommon\\tapproach\\tto\\tfind\\tthe\\tright\\t\\nhyperparameter\\tvalues\\tis\\tto\\tuse\\tgrid\\t\\nsearch\\t(see\\t\\nChapter\\t2\\n).\\tIt\\tis\\toften\\tfaster\\tto\\tfirst\\tdo\\ta\\nvery\\tcoarse\\tgrid\\tsearch,\\tthen\\ta\\tfiner\\tgrid\\tsearch\\taround\\tthe\\tbest\\tvalues\\tfound.\\tHaving\\ta\\tgood\\tsense\\tof\\twhat\\teach\\nhyperparameter\\tactually\\tdoes\\tcan\\talso\\thelp\\tyou\\tsearch\\tin\\tthe\\tright\\tpart\\tof\\tthe\\t\\nhyperparameter\\tspace.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 196}), Document(page_content='Adding\\tSimilarity\\tFeatures\\nAnother\\t\\ntechnique\\tto\\ttackle\\tnonlinear\\tproblems\\tis\\tto\\tadd\\tfeatures\\tcomputed\\tusing\\ta\\t\\nsimilarity\\tfunction\\nthat\\tmeasures\\thow\\tmuch\\teach\\tinstance\\tresembles\\ta\\tparticular\\t\\nlandmark\\n.\\t\\nFor\\texample,\\tlet’s\\ttake\\tthe\\tone-\\ndimensional\\tdataset\\tdiscussed\\tearlier\\tand\\tadd\\ttwo\\tlandmarks\\tto\\tit\\tat\\t\\nx\\n1\\n\\t=\\t–2\\tand\\t\\nx\\n1\\n\\t=\\t1\\t(see\\tthe\\tleft\\tplot\\nin\\t\\nFigure\\t5-8\\n).\\tNext,\\tlet’s\\tdefine\\tthe\\tsimilarity\\tfunction\\tto\\tbe\\tthe\\t\\nGaussian\\t\\nRadial\\tBasis\\tFunction\\n\\t(\\nRBF\\n)\\nwith\\t\\nγ\\n\\t=\\t0.3\\t(see\\t\\nEquation\\t5-1\\n).\\nEquation\\t5-1.\\t\\nGaussian\\tRBF\\nIt\\tis\\ta\\tbell-shaped\\tfunction\\tvarying\\tfrom\\t0\\t(very\\tfar\\taway\\tfrom\\tthe\\tlandmark)\\tto\\t1\\t(at\\tthe\\tlandmark).\\tNow\\nwe\\tare\\tready\\tto\\tcompute\\tthe\\tnew\\tfeatures.\\tFor\\texample,\\tlet’s\\tlook\\tat\\tthe\\tinstance\\t\\nx\\n1\\n\\t=\\t–1:\\tit\\tis\\tlocated\\tat\\ta\\ndistance\\tof\\t1\\tfrom\\tthe\\tfirst\\tlandmark,\\tand\\t2\\tfrom\\tthe\\tsecond\\tlandmark.\\tTherefore\\tits\\tnew\\tfeatures\\tare\\t\\nx\\n2\\n\\t=\\nexp\\t(–0.3\\t×\\t1\\n2\\n)\\t≈\\t0.74\\tand\\t\\nx\\n3\\n\\t=\\texp\\t(–0.3\\t×\\t2\\n2\\n)\\t≈\\t0.30.\\tThe\\tplot\\ton\\tthe\\tright\\tof\\t\\nFigure\\t5-8\\n\\tshows\\tthe\\ntransformed\\tdataset\\t(dropping\\tthe\\toriginal\\tfeatures).\\tAs\\tyou\\tcan\\tsee,\\tit\\tis\\tnow\\tlinearly\\t\\nseparable\\n.\\nFigure\\t5-8.\\t\\nSimilarity\\tfeatures\\tusing\\tthe\\tGaussian\\tRBF\\nYou\\tmay\\twonder\\thow\\tto\\tselect\\tthe\\tlandmarks.\\tThe\\tsimplest\\tapproach\\tis\\tto\\tcreate\\ta\\tlandmark\\tat\\tthe\\nlocation\\tof\\teach\\tand\\tevery\\tinstance\\tin\\tthe\\tdataset.\\tThis\\tcreates\\tmany\\tdimensions\\tand\\tthus\\tincreases\\tthe\\nchances\\tthat\\tthe\\ttransformed\\ttraining\\tset\\twill\\tbe\\tlinearly\\tseparable.\\tThe\\tdownside\\tis\\tthat\\ta\\ttraining\\tset\\nwith\\t\\nm\\n\\tinstances\\tand\\t\\nn\\n\\tfeatures\\tgets\\ttransformed\\tinto\\ta\\ttraining\\tset\\twith\\t\\nm\\n\\tinstances\\tand\\t\\nm\\n\\tfeatures\\n(assuming\\tyou\\tdrop\\tthe\\toriginal\\tfeatures).\\tIf\\tyour\\ttraining\\tset\\tis\\tvery\\tlarge,\\tyou\\tend\\tup\\twith\\tan\\tequally\\nlarge\\tnumber\\tof\\t\\nfeatures.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 197}), Document(page_content='Gaussian\\tRBF\\tKernel\\nJust\\t\\nlike\\tthe\\tpolynomial\\tfeatures\\tmethod,\\tthe\\tsimilarity\\tfeatures\\tmethod\\tcan\\tbe\\tuseful\\twith\\tany\\tMachine\\nLearning\\talgorithm,\\tbut\\tit\\tmay\\tbe\\tcomputationally\\texpensive\\tto\\tcompute\\tall\\tthe\\tadditional\\tfeatures,\\nespecially\\ton\\tlarge\\ttraining\\tsets.\\tHowever,\\tonce\\tagain\\tthe\\t\\nkernel\\ttrick\\tdoes\\tits\\tSVM\\tmagic:\\tit\\tmakes\\tit\\npossible\\tto\\tobtain\\ta\\tsimilar\\tresult\\tas\\tif\\tyou\\thad\\tadded\\tmany\\tsimilarity\\tfeatures,\\twithout\\tactually\\thaving\\tto\\nadd\\tthem.\\tLet’s\\ttry\\tthe\\tGaussian\\tRBF\\tkernel\\tusing\\t\\nthe\\t\\nSVC\\n\\tclass:\\nrbf_kernel_svm_clf\\n\\t\\n=\\n\\t\\nPipeline\\n((\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"scaler\"\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"svm_clf\"\\n,\\n\\t\\nSVC\\n(\\nkernel\\n=\\n\"rbf\"\\n,\\n\\t\\ngamma\\n=\\n5\\n,\\n\\t\\nC\\n=\\n0.001\\n))\\n\\t\\t\\t\\t\\n))\\nrbf_kernel_svm_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nThis\\tmodel\\tis\\trepresented\\ton\\tthe\\tbottom\\tleft\\tof\\t\\nFigure\\t5-9\\n.\\tThe\\tother\\tplots\\tshow\\tmodels\\ttrained\\twith\\ndifferent\\tvalues\\tof\\thyperparameters\\t\\ngamma\\n\\t(\\nγ\\n)\\tand\\t\\nC\\n.\\tIncreasing\\t\\ngamma\\n\\tmakes\\tthe\\tbell-shape\\tcurve\\nnarrower\\t(see\\tthe\\tleft\\tplot\\tof\\t\\nFigure\\t5-8\\n),\\tand\\tas\\ta\\tresult\\teach\\tinstance’s\\trange\\tof\\tinfluence\\tis\\tsmaller:\\tthe\\ndecision\\tboundary\\tends\\tup\\tbeing\\tmore\\tirregular,\\twiggling\\taround\\tindividual\\tinstances.\\tConversely,\\ta\\nsmall\\t\\ngamma\\n\\t\\nvalue\\tmakes\\tthe\\tbell-shaped\\tcurve\\twider,\\tso\\tinstances\\thave\\ta\\tlarger\\trange\\tof\\tinfluence,\\tand\\nthe\\tdecision\\tboundary\\tends\\tup\\tsmoother.\\tSo\\t\\nγ\\n\\tacts\\tlike\\ta\\tregularization\\thyperparameter:\\tif\\tyour\\tmodel\\tis\\noverfitting,\\tyou\\tshould\\treduce\\tit,\\tand\\tif\\tit\\tis\\t\\nunderfitting,\\tyou\\tshould\\tincrease\\tit\\t(similar\\tto\\tthe\\t\\nC\\nhyperparameter).\\nFigure\\t5-9.\\t\\nSVM\\tclassifiers\\tusing\\tan\\tRBF\\tkernel\\nOther\\tkernels\\texist\\tbut\\tare\\tused\\tmuch\\tmore\\trarely.\\tFor\\texample,\\tsome\\tkernels\\tare\\tspecialized\\tfor\\tspecific\\ndata\\tstructures.\\t\\nString\\tkernels\\n\\t\\nare\\tsometimes\\tused\\twhen\\tclassifying\\ttext\\tdocuments\\tor\\tDNA\\tsequences\\n(e.g.,\\tusing\\tthe\\t\\nstring\\tsubsequence\\tkernel\\n\\tor\\tkernels\\tbased\\ton\\t\\nthe\\t\\nLevenshtein\\tdistance\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 198}), Document(page_content='TIP\\nWith\\tso\\tmany\\tkernels\\tto\\tchoose\\tfrom,\\thow\\tcan\\tyou\\tdecide\\twhich\\tone\\tto\\tuse?\\tAs\\ta\\trule\\tof\\tthumb,\\tyou\\tshould\\talways\\ttry\\tthe\\tlinear\\nkernel\\tfirst\\t(remember\\tthat\\t\\nLinearSVC\\n\\t\\nis\\tmuch\\tfaster\\tthan\\t\\nSVC(kernel=\"linear\")\\n),\\tespecially\\tif\\tthe\\ttraining\\tset\\tis\\tvery\\tlarge\\tor\\nif\\tit\\thas\\tplenty\\tof\\tfeatures.\\tIf\\tthe\\ttraining\\tset\\tis\\tnot\\ttoo\\tlarge,\\tyou\\tshould\\ttry\\tthe\\tGaussian\\tRBF\\tkernel\\tas\\twell;\\tit\\tworks\\twell\\tin\\nmost\\tcases.\\tThen\\tif\\tyou\\thave\\tspare\\ttime\\tand\\tcomputing\\tpower,\\tyou\\tcan\\talso\\texperiment\\twith\\ta\\tfew\\tother\\tkernels\\tusing\\tcross-\\nvalidation\\tand\\tgrid\\tsearch,\\tespecially\\tif\\tthere\\tare\\tkernels\\tspecialized\\tfor\\tyour\\ttraining\\tset’s\\tdata\\t\\nstructure.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 199}), Document(page_content='Computational\\tComplexity\\nThe\\n\\t\\nLinearSVC\\n\\tclass\\tis\\tbased\\ton\\tthe\\t\\nliblinear\\n\\tlibrary,\\t\\nwhich\\timplements\\tan\\t\\noptimized\\talgorithm\\n\\tfor\\nlinear\\tSVMs.\\n1\\n\\tIt\\tdoes\\tnot\\tsupport\\tthe\\tkernel\\ttrick,\\tbut\\tit\\tscales\\talmost\\tlinearly\\twith\\tthe\\tnumber\\tof\\ttraining\\ninstances\\tand\\tthe\\tnumber\\tof\\tfeatures:\\tits\\ttraining\\ttime\\tcomplexity\\tis\\troughly\\t\\nO\\n(\\nm\\n\\t×\\t\\nn\\n).\\nThe\\talgorithm\\ttakes\\tlonger\\tif\\tyou\\trequire\\ta\\tvery\\thigh\\tprecision.\\tThis\\tis\\tcontrolled\\tby\\tthe\\t\\ntolerance\\nhyperparameter\\t\\nϵ\\n\\t(called\\t\\ntol\\n\\tin\\tScikit-Learn).\\tIn\\tmost\\tclassification\\ttasks,\\tthe\\tdefault\\ttolerance\\tis\\tfine.\\nThe\\t\\nSVC\\n\\tclass\\tis\\tbased\\ton\\t\\nthe\\t\\nlibsvm\\n\\tlibrary,\\twhich\\timplements\\t\\nan\\talgorithm\\n\\tthat\\tsupports\\tthe\\tkernel\\ntrick.\\n2\\n\\tThe\\ttraining\\ttime\\tcomplexity\\tis\\tusually\\tbetween\\t\\nO\\n(\\nm\\n2\\n\\t×\\t\\nn\\n)\\tand\\t\\nO\\n(\\nm\\n3\\n\\t×\\t\\nn\\n).\\tUnfortunately,\\tthis\\nmeans\\tthat\\tit\\tgets\\tdreadfully\\tslow\\twhen\\tthe\\tnumber\\tof\\ttraining\\tinstances\\tgets\\tlarge\\t(e.g.,\\thundreds\\tof\\nthousands\\tof\\tinstances).\\tThis\\talgorithm\\tis\\tperfect\\tfor\\tcomplex\\tbut\\tsmall\\tor\\tmedium\\ttraining\\tsets.\\nHowever,\\tit\\tscales\\twell\\twith\\tthe\\tnumber\\tof\\tfeatures,\\tespecially\\twith\\t\\nsparse\\tfeatures\\n\\t(i.e.,\\twhen\\teach\\ninstance\\thas\\tfew\\tnonzero\\tfeatures).\\tIn\\tthis\\tcase,\\tthe\\talgorithm\\tscales\\troughly\\twith\\tthe\\taverage\\tnumber\\tof\\nnonzero\\tfeatures\\tper\\tinstance.\\t\\nTable\\t5-1\\n\\tcompares\\t\\nScikit-Learn’s\\tSVM\\tclassification\\t\\nclasses.\\nTable\\t5-1.\\t\\nComparison\\tof\\tScikit-Learn\\tclasses\\tfor\\tSVM\\t\\nclassification\\nClass\\nTime\\tcomplexity\\nOut-of-core\\tsupport\\nScaling\\trequired\\nKernel\\ttrick\\nLinearSVC\\nO(\\nm\\n\\t×\\t\\nn\\n)\\nNo\\nYes\\nNo\\nSGDClassifier\\nO(\\nm\\n\\t×\\t\\nn\\n)\\nYes\\nYes\\nNo\\nSVC\\nO(\\nm\\n²\\t×\\t\\nn\\n)\\tto\\tO(\\nm\\n³\\t×\\t\\nn\\n)\\nNo\\nYes\\nYes', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 200}), Document(page_content='SVM\\tRegression\\nAs\\t\\nwe\\tmentioned\\tearlier,\\tthe\\tSVM\\talgorithm\\tis\\tquite\\tversatile:\\tnot\\tonly\\tdoes\\tit\\tsupport\\tlinear\\tand\\nnonlinear\\tclassification,\\tbut\\tit\\talso\\tsupports\\tlinear\\tand\\tnonlinear\\tregression.\\tThe\\ttrick\\tis\\tto\\treverse\\tthe\\nobjective:\\tinstead\\tof\\ttrying\\tto\\tfit\\tthe\\tlargest\\tpossible\\tstreet\\tbetween\\ttwo\\tclasses\\twhile\\tlimiting\\tmargin\\nviolations,\\tSVM\\tRegression\\ttries\\tto\\tfit\\tas\\tmany\\tinstances\\tas\\tpossible\\t\\non\\n\\tthe\\tstreet\\twhile\\tlimiting\\tmargin\\nviolations\\t(i.e.,\\tinstances\\t\\noff\\n\\tthe\\tstreet).\\tThe\\twidth\\tof\\tthe\\tstreet\\tis\\tcontrolled\\tby\\ta\\thyperparameter\\t\\nϵ\\n.\\nFigure\\t5-10\\n\\tshows\\ttwo\\tlinear\\tSVM\\tRegression\\tmodels\\ttrained\\ton\\tsome\\trandom\\tlinear\\tdata,\\tone\\twith\\ta\\nlarge\\tmargin\\t(\\nϵ\\n\\t=\\t1.5)\\tand\\tthe\\tother\\twith\\ta\\tsmall\\tmargin\\t(\\nϵ\\n\\t=\\t0.5).\\nFigure\\t5-10.\\t\\nSVM\\tRegression\\nAdding\\tmore\\ttraining\\tinstances\\twithin\\tthe\\tmargin\\tdoes\\tnot\\taffect\\tthe\\tmodel’s\\tpredictions;\\tthus,\\tthe\\tmodel\\nis\\tsaid\\tto\\t\\nbe\\t\\nϵ-insensitive\\n.\\nYou\\tcan\\tuse\\tScikit-Learn’s\\t\\nLinearSVR\\n\\tclass\\t\\nto\\tperform\\tlinear\\tSVM\\tRegression.\\tThe\\tfollowing\\tcode\\nproduces\\tthe\\tmodel\\trepresented\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t5-10\\n\\t(the\\ttraining\\tdata\\tshould\\tbe\\tscaled\\tand\\tcentered\\nfirst):\\nfrom\\n\\t\\nsklearn.svm\\n\\t\\nimport\\n\\t\\nLinearSVR\\nsvm_reg\\n\\t\\n=\\n\\t\\nLinearSVR\\n(\\nepsilon\\n=\\n1.5\\n)\\nsvm_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nTo\\ttackle\\tnonlinear\\tregression\\ttasks,\\tyou\\tcan\\tuse\\ta\\tkernelized\\tSVM\\tmodel.\\tFor\\texample,\\t\\nFigure\\t5-11\\nshows\\tSVM\\tRegression\\ton\\ta\\trandom\\tquadratic\\ttraining\\tset,\\tusing\\ta\\t2\\nnd\\n-degree\\tpolynomial\\tkernel.\\tThere\\nis\\tlittle\\tregularization\\ton\\tthe\\tleft\\tplot\\t(i.e.,\\ta\\tlarge\\t\\nC\\n\\tvalue),\\tand\\tmuch\\tmore\\tregularization\\ton\\tthe\\tright\\tplot\\n(i.e.,\\ta\\tsmall\\t\\nC\\n\\tvalue).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 201}), Document(page_content='Figure\\t5-11.\\t\\nSVM\\tregression\\tusing\\ta\\t2\\nnd\\n-degree\\tpolynomial\\tkernel\\nThe\\tfollowing\\tcode\\tproduces\\tthe\\tmodel\\trepresented\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t5-11\\n\\tusing\\tScikit-Learn’s\\t\\nSVR\\nclass\\t(which\\tsupports\\tthe\\tkernel\\ttrick).\\tThe\\t\\nSVR\\n\\tclass\\tis\\tthe\\tregression\\tequivalent\\tof\\tthe\\t\\nSVC\\n\\tclass,\\tand\\nthe\\t\\nLinearSVR\\n\\tclass\\tis\\tthe\\tregression\\tequivalent\\tof\\tthe\\t\\nLinearSVC\\n\\tclass.\\tThe\\t\\nLinearSVR\\n\\tclass\\tscales\\nlinearly\\twith\\tthe\\tsize\\tof\\tthe\\ttraining\\tset\\t(just\\tlike\\tthe\\t\\nLinearSVC\\n\\tclass),\\twhile\\tthe\\t\\nSVR\\n\\tclass\\tgets\\tmuch\\ttoo\\nslow\\twhen\\tthe\\ttraining\\tset\\tgrows\\t\\nlarge\\t\\n(just\\tlike\\tthe\\t\\nSVC\\n\\tclass).\\nfrom\\n\\t\\nsklearn.svm\\n\\t\\nimport\\n\\t\\nSVR\\nsvm_poly_reg\\n\\t\\n=\\n\\t\\nSVR\\n(\\nkernel\\n=\\n\"poly\"\\n,\\n\\t\\ndegree\\n=\\n2\\n,\\n\\t\\nC\\n=\\n100\\n,\\n\\t\\nepsilon\\n=\\n0.1\\n)\\nsvm_poly_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nNOTE\\nSVMs\\tcan\\talso\\tbe\\tused\\tfor\\toutlier\\tdetection;\\tsee\\tScikit-Learn’s\\tdocumentation\\tfor\\tmore\\tdetails.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 202}), Document(page_content='Under\\tthe\\tHood\\nThis\\t\\nsection\\texplains\\thow\\tSVMs\\tmake\\tpredictions\\tand\\thow\\ttheir\\ttraining\\talgorithms\\twork,\\tstarting\\twith\\nlinear\\tSVM\\tclassifiers.\\tYou\\tcan\\tsafely\\tskip\\tit\\tand\\tgo\\tstraight\\tto\\tthe\\texercises\\tat\\tthe\\tend\\tof\\tthis\\tchapter\\tif\\nyou\\tare\\tjust\\tgetting\\tstarted\\twith\\tMachine\\tLearning,\\tand\\tcome\\tback\\tlater\\twhen\\tyou\\twant\\tto\\tget\\ta\\tdeeper\\nunderstanding\\tof\\tSVMs.\\nFirst,\\ta\\tword\\tabout\\tnotations:\\tin\\t\\nChapter\\t4\\n\\twe\\tused\\tthe\\tconvention\\tof\\tputting\\tall\\tthe\\t\\nmodel\\tparameters\\tin\\none\\tvector\\t\\nθ\\n,\\tincluding\\tthe\\tbias\\tterm\\t\\nθ\\n0\\n\\tand\\tthe\\tinput\\tfeature\\tweights\\t\\nθ\\n1\\n\\tto\\t\\nθ\\nn\\n,\\tand\\tadding\\ta\\tbias\\tinput\\t\\nx\\n0\\n\\t=\\n1\\tto\\tall\\tinstances.\\tIn\\tthis\\tchapter,\\twe\\twill\\tuse\\ta\\tdifferent\\tconvention,\\twhich\\tis\\tmore\\tconvenient\\t(and\\tmore\\ncommon)\\twhen\\tyou\\tare\\tdealing\\twith\\tSVMs:\\tthe\\tbias\\tterm\\twill\\tbe\\tcalled\\t\\nb\\n\\tand\\tthe\\tfeature\\tweights\\tvector\\nwill\\tbe\\tcalled\\t\\nw\\n.\\tNo\\tbias\\tfeature\\twill\\tbe\\tadded\\tto\\tthe\\tinput\\t\\nfeature\\tvectors.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 203}), Document(page_content='Decision\\tFunction\\tand\\tPredictions\\nThe\\t\\nlinear\\tSVM\\tclassifier\\tmodel\\tpredicts\\tthe\\tclass\\tof\\ta\\tnew\\tinstance\\t\\nx\\n\\tby\\tsimply\\tcomputing\\tthe\\tdecision\\nfunction\\t\\nw\\nT\\n\\t·\\t\\nx\\n\\t+\\t\\nb\\n\\t=\\t\\nw\\n1\\n\\t\\nx\\n1\\n\\t+\\t\\t+\\t\\nw\\nn\\n\\t\\nx\\nn\\n\\t+\\t\\nb\\n:\\tif\\tthe\\tresult\\tis\\tpositive,\\tthe\\tpredicted\\tclass\\t\\nŷ\\n\\tis\\tthe\\tpositive\\nclass\\t(1),\\tor\\telse\\tit\\tis\\tthe\\tnegative\\tclass\\t(0);\\tsee\\t\\nEquation\\t5-2\\n.\\nEquation\\t5-2.\\t\\nLinear\\tSVM\\tclassifier\\tprediction\\nFigure\\t5-12\\n\\tshows\\tthe\\tdecision\\tfunction\\tthat\\tcorresponds\\tto\\tthe\\tmodel\\ton\\tthe\\tright\\tof\\t\\nFigure\\t5-4\\n:\\tit\\tis\\ta\\ntwo-dimensional\\tplane\\tsince\\tthis\\tdataset\\thas\\ttwo\\tfeatures\\t(petal\\twidth\\tand\\tpetal\\tlength).\\tThe\\tdecision\\nboundary\\tis\\tthe\\tset\\tof\\tpoints\\twhere\\tthe\\tdecision\\tfunction\\tis\\tequal\\tto\\t0:\\tit\\tis\\tthe\\tintersection\\tof\\ttwo\\tplanes,\\nwhich\\tis\\ta\\tstraight\\tline\\t(represented\\tby\\tthe\\tthick\\tsolid\\tline).\\n3\\nFigure\\t5-12.\\t\\nDecision\\tfunction\\tfor\\tthe\\tiris\\tdataset\\nThe\\tdashed\\tlines\\trepresent\\tthe\\tpoints\\twhere\\tthe\\tdecision\\tfunction\\tis\\tequal\\tto\\t1\\tor\\t–1:\\tthey\\tare\\tparallel\\tand\\nat\\tequal\\tdistance\\tto\\tthe\\tdecision\\tboundary,\\tforming\\ta\\tmargin\\taround\\tit.\\tTraining\\ta\\tlinear\\tSVM\\tclassifier\\nmeans\\tfinding\\tthe\\tvalue\\tof\\t\\nw\\n\\tand\\t\\nb\\n\\tthat\\tmake\\tthis\\tmargin\\tas\\twide\\tas\\tpossible\\twhile\\tavoiding\\tmargin\\nviolations\\t(hard\\tmargin)\\tor\\t\\nlimiting\\tthem\\t(soft\\tmargin).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 204}), Document(page_content='Training\\tObjective\\nConsider\\t\\nthe\\tslope\\tof\\tthe\\tdecision\\tfunction:\\tit\\tis\\tequal\\tto\\tthe\\tnorm\\tof\\tthe\\tweight\\tvector,\\t\\t\\nw\\n\\t.\\tIf\\twe\\ndivide\\tthis\\tslope\\tby\\t2,\\tthe\\tpoints\\twhere\\tthe\\tdecision\\tfunction\\tis\\tequal\\tto\\t±1\\tare\\tgoing\\tto\\tbe\\ttwice\\tas\\tfar\\naway\\tfrom\\tthe\\tdecision\\tboundary.\\tIn\\tother\\twords,\\tdividing\\tthe\\tslope\\tby\\t2\\twill\\tmultiply\\tthe\\tmargin\\tby\\t2.\\nPerhaps\\tthis\\tis\\teasier\\tto\\tvisualize\\tin\\t2D\\tin\\t\\nFigure\\t5-13\\n.\\tThe\\tsmaller\\tthe\\tweight\\tvector\\t\\nw\\n,\\tthe\\tlarger\\tthe\\nmargin.\\nFigure\\t5-13.\\t\\nA\\tsmaller\\tweight\\tvector\\tresults\\tin\\ta\\tlarger\\tmargin\\nSo\\twe\\twant\\tto\\tminimize\\t\\t\\nw\\n\\t\\tto\\tget\\ta\\tlarge\\tmargin.\\tHowever,\\tif\\twe\\talso\\twant\\tto\\tavoid\\tany\\tmargin\\nviolation\\t(hard\\tmargin),\\tthen\\twe\\tneed\\tthe\\tdecision\\tfunction\\tto\\tbe\\tgreater\\tthan\\t1\\tfor\\tall\\tpositive\\ttraining\\ninstances,\\tand\\tlower\\tthan\\t–1\\tfor\\tnegative\\ttraining\\tinstances.\\tIf\\twe\\tdefine\\t\\nt\\n(i)\\n\\t=\\t\\n–1\\tfor\\tnegative\\tinstances\\t(if\\ny\\n(i)\\n\\t=\\t0)\\tand\\t\\nt\\n(i)\\n\\t=\\t1\\tfor\\tpositive\\tinstances\\t(if\\t\\ny\\n(i)\\n\\t=\\t1),\\tthen\\twe\\tcan\\texpress\\tthis\\tconstraint\\tas\\t\\nt\\n(i)\\n(\\nw\\nT\\n\\t·\\t\\nx\\n(i)\\n\\t+\\nb\\n)\\t≥\\t1\\tfor\\tall\\tinstances.\\nWe\\tcan\\ttherefore\\texpress\\tthe\\thard\\tmargin\\tlinear\\tSVM\\tclassifier\\tobjective\\tas\\tthe\\t\\nconstrained\\noptimization\\n\\t\\nproblem\\tin\\t\\nEquation\\t5-3\\n.\\nEquation\\t5-3.\\t\\nHard\\tmargin\\tlinear\\tSVM\\tclassifier\\tobjective\\nNOTE\\nWe\\tare\\tminimizing\\t\\nw\\nT\\n\\t·\\t\\nw\\n,\\twhich\\tis\\tequal\\tto\\t\\n\\t\\nw\\n\\t\\n2\\n,\\trather\\tthan\\tminimizing\\t\\t\\nw\\n\\t.\\tThis\\tis\\tbecause\\tit\\twill\\tgive\\tthe\\tsame\\nresult\\t(since\\tthe\\tvalues\\tof\\t\\nw\\n\\tand\\t\\nb\\n\\tthat\\tminimize\\ta\\tvalue\\talso\\tminimize\\thalf\\tof\\tits\\tsquare),\\tbut\\t\\n\\t\\nw\\n\\t\\n2\\n\\thas\\ta\\tnice\\tand\\tsimple\\nderivative\\t(it\\tis\\tjust\\t\\nw\\n)\\twhile\\t\\t\\nw\\n\\t\\tis\\tnot\\tdifferentiable\\tat\\t\\nw\\n\\t=\\t\\n0\\n.\\tOptimization\\talgorithms\\twork\\tmuch\\tbetter\\ton\\tdifferentiable\\nfunctions.\\nTo\\tget\\tthe\\tsoft\\tmargin\\tobjective,\\twe\\tneed\\tto\\tintroduce\\t\\na\\t\\nslack\\tvariable\\n\\t\\nζ\\n(i)\\n\\t≥\\t0\\tfor\\teach\\tinstance:\\n4\\n\\t\\nζ\\n(i)\\nmeasures\\thow\\tmuch\\tthe\\ti\\nth\\n\\tinstance\\tis\\tallowed\\tto\\tviolate\\tthe\\tmargin.\\tWe\\tnow\\thave\\ttwo\\tconflicting\\nobjectives:\\tmaking\\tthe\\tslack\\tvariables\\tas\\tsmall\\tas\\tpossible\\tto\\treduce\\tthe\\tmargin\\tviolations,\\tand\\tmaking\\t\\nw\\nT\\n\\t·\\t\\nw\\n\\tas\\tsmall\\tas\\tpossible\\tto\\tincrease\\tthe\\tmargin.\\tThis\\tis\\twhere\\tthe\\t\\nC\\n\\thyperparameter\\tcomes\\tin:\\tit', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 205}), Document(page_content='allows\\tus\\tto\\tdefine\\tthe\\ttradeoff\\tbetween\\tthese\\ttwo\\tobjectives.\\tThis\\tgives\\tus\\tthe\\tconstrained\\t\\noptimization\\nproblem\\tin\\t\\nEquation\\t5-4\\n.\\nEquation\\t5-4.\\t\\nSoft\\tmargin\\tlinear\\tSVM\\tclassifier\\tobjective\\n', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 206}), Document(page_content='Quadratic\\tProgramming\\nThe\\t\\nhard\\tmargin\\tand\\tsoft\\tmargin\\tproblems\\tare\\tboth\\tconvex\\tquadratic\\toptimization\\tproblems\\twith\\tlinear\\nconstraints.\\tSuch\\tproblems\\tare\\tknown\\tas\\t\\nQuadratic\\tProgramming\\n\\t(QP)\\tproblems.\\tMany\\toff-the-shelf\\nsolvers\\tare\\tavailable\\tto\\tsolve\\tQP\\tproblems\\tusing\\ta\\tvariety\\tof\\ttechniques\\tthat\\tare\\toutside\\tthe\\tscope\\tof\\tthis\\nbook.\\n5\\n\\tThe\\tgeneral\\tproblem\\tformulation\\tis\\tgiven\\tby\\t\\nEquation\\t5-5\\n.\\nEquation\\t5-5.\\t\\nQuadratic\\tProgramming\\tproblem\\nNote\\tthat\\tthe\\texpression\\t\\nA\\n\\t·\\t\\np\\n\\t≤\\t\\nb\\n\\tactually\\tdefines\\t\\nn\\nc\\n\\tconstraints:\\t\\np\\nT\\n\\t·\\t\\na\\n(i)\\n\\t≤\\t\\nb\\n(i)\\n\\tfor\\t\\ni\\n\\t=\\t1,\\t2,\\t,\\t\\nn\\nc\\n,\\twhere\\na\\n(i)\\n\\tis\\tthe\\tvector\\tcontaining\\tthe\\telements\\tof\\tthe\\ti\\nth\\n\\trow\\tof\\t\\nA\\n\\tand\\t\\nb\\n(i)\\n\\tis\\tthe\\ti\\nth\\n\\telement\\tof\\t\\nb\\n.\\nYou\\tcan\\teasily\\tverify\\tthat\\tif\\tyou\\tset\\tthe\\tQP\\t\\nparameters\\tin\\tthe\\tfollowing\\tway,\\tyou\\tget\\tthe\\thard\\tmargin\\nlinear\\tSVM\\tclassifier\\tobjective:\\nn\\np\\n\\t=\\t\\nn\\n\\t+\\t1,\\twhere\\t\\nn\\n\\tis\\tthe\\tnumber\\tof\\tfeatures\\t(the\\t+1\\tis\\tfor\\tthe\\tbias\\tterm).\\nn\\nc\\n\\t=\\t\\nm\\n,\\twhere\\t\\nm\\n\\tis\\tthe\\tnumber\\tof\\ttraining\\tinstances.\\nH\\n\\tis\\tthe\\t\\nn\\np\\n\\t×\\t\\nn\\np\\n\\t\\nidentity\\tmatrix,\\texcept\\twith\\ta\\tzero\\tin\\tthe\\ttop-left\\tcell\\t(to\\tignore\\tthe\\tbias\\tterm).\\nf\\n\\t=\\t\\n0\\n,\\tan\\t\\nn\\np\\n-dimensional\\tvector\\tfull\\tof\\t0s.\\nb\\n\\t=\\t\\n1\\n,\\tan\\t\\nn\\nc\\n-dimensional\\tvector\\tfull\\tof\\t1s.\\na\\n(i)\\n\\t=\\t–\\nt\\n(i)\\n\\t\\n\\t\\n(i)\\n,\\twhere\\t\\n\\t\\n(i)\\n\\tis\\tequal\\tto\\t\\nx\\n(i)\\n\\twith\\tan\\textra\\tbias\\tfeature\\t\\n\\t\\n0\\n\\t=\\t1.\\nSo\\tone\\tway\\tto\\ttrain\\ta\\thard\\tmargin\\tlinear\\tSVM\\tclassifier\\tis\\tjust\\tto\\tuse\\tan\\toff-the-shelf\\tQP\\tsolver\\tby\\npassing\\tit\\tthe\\tpreceding\\tparameters.\\tThe\\tresulting\\tvector\\t\\np\\n\\twill\\tcontain\\tthe\\tbias\\tterm\\t\\nb\\n\\t=\\t\\np\\n0\\n\\tand\\tthe\\nfeature\\tweights\\t\\nw\\ni\\n\\t=\\t\\np\\ni\\n\\tfor\\t\\ni\\n\\t=\\t1,\\t2,\\t,\\t\\nm\\n.\\tSimilarly,\\tyou\\tcan\\tuse\\ta\\tQP\\tsolver\\tto\\tsolve\\tthe\\tsoft\\tmargin\\nproblem\\t(see\\tthe\\texercises\\tat\\tthe\\tend\\tof\\tthe\\tchapter).\\nHowever,\\tto\\tuse\\tthe\\tkernel\\ttrick\\twe\\tare\\tgoing\\tto\\tlook\\tat\\ta\\tdifferent\\tconstrained\\toptimization\\t\\nproblem.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 207}), Document(page_content='The\\tDual\\tProblem\\nGiven\\t\\na\\tconstrained\\toptimization\\tproblem,\\tknown\\tas\\t\\nthe\\t\\nprimal\\tproblem\\n,\\tit\\tis\\tpossible\\tto\\texpress\\ta\\ndifferent\\tbut\\tclosely\\trelated\\tproblem,\\tcalled\\t\\nits\\t\\ndual\\tproblem\\n.\\tThe\\tsolution\\tto\\tthe\\tdual\\tproblem\\ttypically\\ngives\\ta\\tlower\\tbound\\tto\\tthe\\tsolution\\tof\\tthe\\tprimal\\tproblem,\\tbut\\tunder\\tsome\\tconditions\\tit\\tcan\\teven\\thave\\tthe\\nsame\\tsolutions\\tas\\tthe\\tprimal\\tproblem.\\tLuckily,\\tthe\\tSVM\\tproblem\\thappens\\tto\\tmeet\\tthese\\tconditions,\\n6\\n\\tso\\nyou\\tcan\\tchoose\\tto\\tsolve\\tthe\\tprimal\\tproblem\\tor\\tthe\\tdual\\tproblem;\\tboth\\twill\\thave\\tthe\\tsame\\tsolution.\\nEquation\\t5-6\\n\\tshows\\tthe\\tdual\\tform\\tof\\tthe\\tlinear\\tSVM\\tobjective\\t(if\\tyou\\tare\\tinterested\\tin\\tknowing\\thow\\tto\\nderive\\tthe\\tdual\\tproblem\\tfrom\\tthe\\tprimal\\tproblem,\\tsee\\t\\nAppendix\\tC\\n).\\nEquation\\t5-6.\\t\\nDual\\tform\\tof\\tthe\\tlinear\\tSVM\\tobjective\\nOnce\\tyou\\tfind\\tthe\\tvector\\t\\n\\tthat\\tminimizes\\tthis\\tequation\\t(using\\ta\\tQP\\tsolver),\\tyou\\tcan\\tcompute\\t\\n\\tand\\t\\nthat\\tminimize\\tthe\\tprimal\\tproblem\\tby\\tusing\\t\\nEquation\\t5-7\\n.\\nEquation\\t5-7.\\t\\nFrom\\tthe\\tdual\\tsolution\\tto\\tthe\\tprimal\\tsolution\\nThe\\tdual\\tproblem\\tis\\tfaster\\tto\\tsolve\\tthan\\tthe\\tprimal\\twhen\\tthe\\tnumber\\tof\\ttraining\\tinstances\\tis\\tsmaller\\tthan\\nthe\\tnumber\\tof\\tfeatures.\\tMore\\timportantly,\\tit\\tmakes\\tthe\\tkernel\\ttrick\\tpossible,\\twhile\\tthe\\tprimal\\tdoes\\tnot.\\tSo\\nwhat\\tis\\tthis\\t\\nkernel\\ttrick\\tanyway?', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 208}), Document(page_content='Kernelized\\tSVM\\nSuppose\\t\\nyou\\twant\\tto\\tapply\\ta\\t2\\nnd\\n-degree\\tpolynomial\\ttransformation\\tto\\ta\\ttwo-dimensional\\ttraining\\tset\\n(such\\tas\\tthe\\tmoons\\ttraining\\tset),\\tthen\\ttrain\\ta\\tlinear\\tSVM\\tclassifier\\ton\\tthe\\ttransformed\\ttraining\\tset.\\nEquation\\t5-8\\n\\tshows\\tthe\\t2\\nnd\\n-degree\\tpolynomial\\tmapping\\tfunction\\t\\nϕ\\n\\tthat\\tyou\\twant\\tto\\tapply.\\nEquation\\t5-8.\\t\\nSecond-degree\\tpolynomial\\tmapping\\nNotice\\tthat\\tthe\\ttransformed\\tvector\\tis\\tthree-dimensional\\tinstead\\tof\\ttwo-dimensional.\\tNow\\tlet’s\\tlook\\tat\\nwhat\\thappens\\tto\\ta\\tcouple\\tof\\ttwo-dimensional\\tvectors,\\t\\na\\n\\tand\\t\\nb\\n,\\tif\\twe\\tapply\\tthis\\t2\\nnd\\n-degree\\tpolynomial\\nmapping\\tand\\tthen\\tcompute\\tthe\\tdot\\tproduct\\tof\\tthe\\ttransformed\\tvectors\\t(See\\t\\nEquation\\t5-9\\n).\\nEquation\\t5-9.\\t\\nKernel\\ttrick\\tfor\\ta\\t2\\nnd\\n-degree\\tpolynomial\\tmapping\\nHow\\tabout\\tthat?\\tThe\\tdot\\tproduct\\tof\\tthe\\ttransformed\\tvectors\\tis\\tequal\\tto\\tthe\\tsquare\\tof\\tthe\\tdot\\tproduct\\tof\\tthe\\noriginal\\tvectors:\\t\\nϕ\\n(\\na\\n)\\nT\\n\\t·\\t\\nϕ\\n(\\nb\\n)\\t=\\t(\\na\\nT\\n\\t·\\t\\nb\\n)\\n2\\n.\\nNow\\there\\tis\\tthe\\tkey\\tinsight:\\tif\\tyou\\tapply\\tthe\\ttransformation\\t\\nϕ\\n\\tto\\tall\\ttraining\\tinstances,\\tthen\\tthe\\tdual\\nproblem\\t(see\\t\\nEquation\\t5-6\\n)\\twill\\tcontain\\tthe\\tdot\\tproduct\\t\\nϕ\\n(\\nx\\n(i)\\n)\\nT\\n\\t·\\t\\nϕ\\n(\\nx\\n(j)\\n).\\tBut\\tif\\t\\nϕ\\n\\tis\\tthe\\t2\\nnd\\n-degree\\npolynomial\\ttransformation\\tdefined\\tin\\t\\nEquation\\t5-8\\n,\\tthen\\tyou\\tcan\\treplace\\tthis\\tdot\\tproduct\\tof\\ttransformed\\nvectors\\tsimply\\tby\\t\\n.\\tSo\\tyou\\tdon’t\\tactually\\tneed\\tto\\ttransform\\tthe\\ttraining\\tinstances\\tat\\tall:\\tjust\\nreplace\\tthe\\tdot\\tproduct\\tby\\tits\\tsquare\\tin\\t\\nEquation\\t5-6\\n.\\tThe\\tresult\\twill\\tbe\\tstrictly\\tthe\\tsame\\tas\\tif\\tyou\\twent\\nthrough\\tthe\\ttrouble\\tof\\tactually\\ttransforming\\tthe\\ttraining\\tset\\tthen\\tfitting\\ta\\tlinear\\tSVM\\talgorithm,\\tbut\\tthis\\ntrick\\tmakes\\tthe\\twhole\\tprocess\\tmuch\\tmore\\tcomputationally\\tefficient.\\tThis\\tis\\tthe\\tessence\\tof\\tthe\\tkernel\\ntrick.\\nThe\\tfunction\\t\\nK\\n(\\na\\n,\\t\\nb\\n)\\t=\\t(\\na\\nT\\n\\t·\\t\\nb\\n)\\n2\\n\\tis\\tcalled\\ta\\t2\\nnd\\n-degree\\t\\npolynomial\\tkernel\\n.\\t\\nIn\\tMachine\\tLearning,\\ta\\t\\nkernel\\nis\\ta\\tfunction\\tcapable\\tof\\tcomputing\\tthe\\tdot\\tproduct\\t\\nϕ\\n(\\na\\n)\\nT\\n\\t·\\t\\nϕ\\n(\\nb\\n)\\tbased\\tonly\\ton\\tthe\\toriginal\\tvectors\\t\\na\\n\\tand\\t\\nb\\n,', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 209}), Document(page_content='without\\thaving\\tto\\tcompute\\t(or\\teven\\tto\\tknow\\tabout)\\tthe\\ttransformation\\t\\nϕ\\n.\\t\\nEquation\\t5-10\\n\\tlists\\tsome\\tof\\tthe\\nmost\\tcommonly\\tused\\tkernels.\\nEquation\\t5-10.\\t\\nCommon\\tkernels\\nMERCER’S\\tTHEOREM\\nAccording\\t\\nto\\t\\nMercer’s\\ttheorem\\n,\\tif\\ta\\tfunction\\t\\nK\\n(\\na\\n,\\t\\nb\\n)\\trespects\\ta\\tfew\\tmathematical\\tconditions\\tcalled\\t\\nMercer’s\\tconditions\\n\\t(\\nK\\n\\tmust\\tbe\\ncontinuous,\\tsymmetric\\tin\\tits\\targuments\\tso\\t\\nK\\n(\\na\\n,\\t\\nb\\n)\\t=\\t\\nK\\n(\\nb\\n,\\t\\na\\n),\\tetc.),\\tthen\\tthere\\texists\\ta\\tfunction\\t\\nϕ\\n\\tthat\\tmaps\\t\\na\\n\\tand\\t\\nb\\n\\tinto\\tanother\\tspace\\n(possibly\\twith\\tmuch\\thigher\\tdimensions)\\tsuch\\tthat\\t\\nK\\n(\\na\\n,\\t\\nb\\n)\\t=\\t\\nϕ\\n(\\na\\n)\\nT\\n\\t·\\t\\nϕ\\n(\\nb\\n).\\tSo\\tyou\\tcan\\tuse\\t\\nK\\n\\tas\\ta\\tkernel\\tsince\\tyou\\tknow\\t\\nϕ\\n\\texists,\\teven\\tif\\nyou\\tdon’t\\tknow\\twhat\\t\\nϕ\\n\\tis.\\tIn\\tthe\\tcase\\tof\\tthe\\t\\nGaussian\\tRBF\\tkernel,\\tit\\tcan\\tbe\\tshown\\tthat\\t\\nϕ\\n\\tactually\\tmaps\\teach\\ttraining\\tinstance\\tto\\tan\\ninfinite-dimensional\\tspace,\\tso\\tit’s\\ta\\tgood\\tthing\\tyou\\tdon’t\\tneed\\tto\\tactually\\tperform\\tthe\\tmapping!\\nNote\\tthat\\tsome\\tfrequently\\tused\\tkernels\\t(such\\tas\\tthe\\tSigmoid\\tkernel)\\tdon’t\\trespect\\tall\\tof\\tMercer’s\\tconditions,\\tyet\\tthey\\tgenerally\\twork\\nwell\\tin\\tpractice.\\nThere\\tis\\tstill\\tone\\tloose\\tend\\twe\\tmust\\ttie.\\t\\nEquation\\t5-7\\n\\tshows\\thow\\tto\\tgo\\tfrom\\tthe\\tdual\\tsolution\\tto\\tthe\\nprimal\\tsolution\\tin\\tthe\\tcase\\tof\\ta\\tlinear\\tSVM\\tclassifier,\\tbut\\tif\\tyou\\tapply\\tthe\\tkernel\\ttrick\\tyou\\tend\\tup\\twith\\nequations\\tthat\\tinclude\\t\\nϕ\\n(\\nx\\n(i)\\n).\\tIn\\tfact,\\t\\n\\tmust\\thave\\tthe\\tsame\\tnumber\\tof\\tdimensions\\tas\\t\\nϕ\\n(\\nx\\n(i)\\n),\\twhich\\tmay\\nbe\\thuge\\tor\\teven\\tinfinite,\\tso\\tyou\\tcan’t\\tcompute\\tit.\\tBut\\thow\\tcan\\tyou\\tmake\\tpredictions\\twithout\\tknowing\\t\\n?\\nWell,\\tthe\\tgood\\tnews\\tis\\tthat\\tyou\\tcan\\tplug\\tin\\tthe\\tformula\\tfor\\t\\n\\tfrom\\t\\nEquation\\t5-7\\n\\tinto\\tthe\\tdecision\\tfunction\\nfor\\ta\\tnew\\tinstance\\t\\nx\\n(n)\\n,\\tand\\tyou\\tget\\tan\\tequation\\twith\\tonly\\tdot\\tproducts\\tbetween\\tinput\\tvectors.\\tThis\\tmakes\\nit\\tpossible\\tto\\tuse\\tthe\\tkernel\\ttrick,\\tonce\\tagain\\t(\\nEquation\\t5-11\\n).\\nEquation\\t5-11.\\t\\nMaking\\tpredictions\\twith\\ta\\tkernelized\\tSVM\\nNote\\tthat\\tsince\\t\\nα\\n(i)\\n\\t≠\\t0\\tonly\\tfor\\tsupport\\tvectors,\\tmaking\\tpredictions\\tinvolves\\tcomputing\\tthe\\tdot\\tproduct\\tof\\nthe\\tnew\\tinput\\tvector\\t\\nx\\n(n)\\n\\twith\\tonly\\tthe\\tsupport\\tvectors,\\tnot\\tall\\tthe\\ttraining\\tinstances.\\tOf\\tcourse,\\tyou\\talso\\nneed\\tto\\tcompute\\tthe\\tbias\\tterm\\t\\n,\\tusing\\tthe\\tsame\\ttrick\\t(\\nEquation\\t5-12\\n).\\nEquation\\t5-12.\\t\\nComputing\\tthe\\tbias\\tterm\\tusing\\tthe\\tkernel\\ttrick', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 210}), Document(page_content='If\\tyou\\tare\\tstarting\\tto\\tget\\ta\\theadache,\\tit’s\\tperfectly\\tnormal:\\tit’s\\tan\\tunfortunate\\tside\\teffects\\tof\\tthe\\t\\nkernel\\ntrick.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 211}), Document(page_content='Online\\tSVMs\\nBefore\\t\\nconcluding\\tthis\\tchapter,\\tlet’s\\ttake\\ta\\tquick\\tlook\\tat\\tonline\\tSVM\\tclassifiers\\t(recall\\tthat\\tonline\\nlearning\\tmeans\\tlearning\\tincrementally,\\ttypically\\tas\\tnew\\tinstances\\tarrive).\\nFor\\tlinear\\tSVM\\tclassifiers,\\tone\\tmethod\\tis\\tto\\tuse\\t\\nGradient\\tDescent\\t(e.g.,\\tusing\\t\\nSGDClassifier\\n)\\tto\\nminimize\\tthe\\tcost\\tfunction\\tin\\t\\nEquation\\t5-13\\n,\\twhich\\tis\\tderived\\tfrom\\tthe\\tprimal\\tproblem.\\tUnfortunately\\tit\\nconverges\\tmuch\\tmore\\tslowly\\tthan\\tthe\\tmethods\\tbased\\ton\\tQP.\\nEquation\\t5-13.\\t\\nLinear\\tSVM\\tclassifier\\tcost\\tfunction\\nThe\\tfirst\\tsum\\tin\\tthe\\tcost\\tfunction\\twill\\tpush\\tthe\\tmodel\\tto\\thave\\ta\\tsmall\\tweight\\tvector\\t\\nw\\n,\\tleading\\tto\\ta\\tlarger\\nmargin.\\tThe\\tsecond\\tsum\\tcomputes\\tthe\\ttotal\\tof\\tall\\tmargin\\tviolations.\\tAn\\tinstance’s\\tmargin\\tviolation\\tis\\nequal\\tto\\t0\\tif\\tit\\tis\\tlocated\\toff\\tthe\\tstreet\\tand\\ton\\tthe\\tcorrect\\tside,\\tor\\telse\\tit\\tis\\tproportional\\tto\\tthe\\tdistance\\tto\\nthe\\tcorrect\\tside\\tof\\tthe\\tstreet.\\tMinimizing\\tthis\\tterm\\tensures\\tthat\\tthe\\tmodel\\tmakes\\tthe\\tmargin\\tviolations\\tas\\nsmall\\tand\\tas\\tfew\\tas\\tpossible\\nHINGE\\tLOSS\\nThe\\tfunction\\t\\nmax\\n(0,\\t1\\t–\\t\\nt\\n)\\tis\\tcalled\\t\\nthe\\t\\nhinge\\tloss\\n\\tfunction\\t(represented\\tbelow).\\tIt\\tis\\tequal\\tto\\t0\\twhen\\t\\nt\\n\\t≥\\t1.\\tIts\\tderivative\\t(slope)\\tis\\tequal\\nto\\t–1\\tif\\t\\nt\\n\\t<\\t1\\tand\\t0\\tif\\t\\nt\\n\\t>\\t1.\\tIt\\tis\\tnot\\tdifferentiable\\tat\\t\\nt\\n\\t=\\t1,\\tbut\\tjust\\tlike\\tfor\\tLasso\\tRegression\\t(see\\t\\n“Lasso\\tRegression”\\n)\\tyou\\tcan\\tstill\\tuse\\nGradient\\tDescent\\tusing\\t\\nany\\t\\nsubderivative\\n\\tat\\t\\nt\\n\\t=\\t1\\t(i.e.,\\tany\\tvalue\\tbetween\\t–1\\tand\\t0).\\nIt\\tis\\talso\\tpossible\\tto\\timplement\\tonline\\tkernelized\\tSVMs\\t—\\tfor\\texample,\\tusing\\t\\n“Incremental\\tand\\nDecremental\\tSVM\\tLearning”\\n7\\n\\tor\\t\\n“Fast\\tKernel\\tClassifiers\\twith\\tOnline\\tand\\tActive\\tLearning.”\\n8\\n\\tHowever,\\nthese\\tare\\timplemented\\tin\\tMatlab\\tand\\tC++.\\tFor\\tlarge-scale\\tnonlinear\\tproblems,\\tyou\\tmay\\twant\\tto\\tconsider\\nusing\\tneural\\tnetworks\\tinstead\\t\\n(see\\t\\nPart\\tII\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 212}), Document(page_content='Exercises\\n1\\n.\\t\\nWhat\\tis\\tthe\\tfundamental\\tidea\\tbehind\\tSupport\\tVector\\tMachines?\\n2\\n.\\t\\nWhat\\tis\\ta\\tsupport\\tvector?\\n3\\n.\\t\\nWhy\\tis\\tit\\timportant\\tto\\tscale\\tthe\\tinputs\\twhen\\tusing\\tSVMs?\\n4\\n.\\t\\nCan\\tan\\tSVM\\tclassifier\\toutput\\ta\\tconfidence\\tscore\\twhen\\tit\\tclassifies\\tan\\tinstance?\\tWhat\\tabout\\ta\\nprobability?\\n5\\n.\\t\\nShould\\tyou\\tuse\\tthe\\tprimal\\tor\\tthe\\tdual\\tform\\tof\\tthe\\tSVM\\tproblem\\tto\\ttrain\\ta\\tmodel\\ton\\ta\\ttraining\\tset\\nwith\\tmillions\\tof\\tinstances\\tand\\thundreds\\tof\\tfeatures?\\n6\\n.\\t\\nSay\\tyou\\ttrained\\tan\\tSVM\\tclassifier\\twith\\tan\\tRBF\\tkernel.\\tIt\\tseems\\tto\\tunderfit\\tthe\\ttraining\\tset:\\tshould\\nyou\\tincrease\\tor\\tdecrease\\t\\nγ\\n\\t(\\ngamma\\n)?\\tWhat\\tabout\\t\\nC\\n?\\n7\\n.\\t\\nHow\\tshould\\tyou\\tset\\tthe\\tQP\\tparameters\\t(\\nH\\n,\\t\\nf\\n,\\t\\nA\\n,\\tand\\t\\nb\\n)\\tto\\tsolve\\tthe\\tsoft\\tmargin\\tlinear\\tSVM\\nclassifier\\tproblem\\tusing\\tan\\toff-the-shelf\\tQP\\tsolver?\\n8\\n.\\t\\nTrain\\ta\\t\\nLinearSVC\\n\\ton\\ta\\tlinearly\\tseparable\\tdataset.\\tThen\\ttrain\\tan\\t\\nSVC\\n\\tand\\ta\\t\\nSGDClassifier\\n\\ton\\tthe\\nsame\\tdataset.\\t\\nSee\\tif\\tyou\\tcan\\tget\\tthem\\tto\\tproduce\\troughly\\tthe\\tsame\\tmodel.\\n9\\n.\\t\\nTrain\\tan\\tSVM\\tclassifier\\ton\\tthe\\tMNIST\\tdataset.\\tSince\\tSVM\\tclassifiers\\tare\\tbinary\\tclassifiers,\\tyou\\nwill\\tneed\\tto\\tuse\\t\\none-versus-all\\tto\\tclassify\\tall\\t10\\tdigits.\\tYou\\tmay\\twant\\tto\\ttune\\tthe\\thyperparameters\\nusing\\tsmall\\tvalidation\\tsets\\tto\\tspeed\\tup\\tthe\\tprocess.\\tWhat\\taccuracy\\tcan\\tyou\\treach?\\n10\\n.\\t\\nTrain\\tan\\tSVM\\tregressor\\ton\\tthe\\tCalifornia\\thousing\\t\\ndataset.\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\n“A\\tDual\\tCoordinate\\tDescent\\tMethod\\tfor\\tLarge-scale\\tLinear\\tSVM,”\\tLin\\tet\\tal.\\t(2008).\\n“Sequential\\tMinimal\\tOptimization\\t(SMO),”\\tJ.\\tPlatt\\t(1998).\\nMore\\tgenerally,\\twhen\\tthere\\tare\\t\\nn\\n\\tfeatures,\\tthe\\tdecision\\tfunction\\tis\\tan\\t\\nn\\n-dimensional\\t\\nhyperplane\\n,\\t\\nand\\tthe\\tdecision\\tboundary\\tis\\tan\\t(\\nn\\n\\t–\\t1)-\\ndimensional\\thyperplane.\\nZeta\\t(\\nζ\\n)\\tis\\tthe\\t8\\nletter\\tof\\tthe\\tGreek\\talphabet.\\nTo\\tlearn\\tmore\\tabout\\tQuadratic\\tProgramming,\\tyou\\tcan\\tstart\\tby\\treading\\tStephen\\tBoyd\\tand\\tLieven\\tVandenberghe,\\t\\nConvex\\tOptimization\\n(Cambridge,\\tUK:\\tCambridge\\tUniversity\\tPress,\\t2004)\\tor\\twatch\\tRichard\\tBrown’s\\t\\nseries\\tof\\tvideo\\tlectures\\n.\\nThe\\tobjective\\tfunction\\tis\\tconvex,\\tand\\tthe\\tinequality\\tconstraints\\tare\\tcontinuously\\tdifferentiable\\tand\\tconvex\\tfunctions.\\n“Incremental\\tand\\tDecremental\\tSupport\\tVector\\tMachine\\tLearning,”\\tG.\\tCauwenberghs,\\tT.\\tPoggio\\t(2001).\\n“Fast\\tKernel\\tClassifiers\\twith\\tOnline\\tand\\tActive\\tLearning,“\\tA.\\tBordes,\\tS.\\tErtekin,\\tJ.\\tWeston,\\tL.\\tBottou\\t(2005).\\n1\\n2\\n3\\n4\\nth\\n5\\n6\\n7\\n8', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 213}), Document(page_content='Chapter\\t6.\\t\\nDecision\\tTrees\\nLike\\t\\nSVMs,\\t\\nDecision\\tTrees\\n\\t\\nare\\tversatile\\tMachine\\tLearning\\talgorithms\\tthat\\tcan\\tperform\\tboth\\nclassification\\tand\\tregression\\ttasks,\\tand\\teven\\tmultioutput\\ttasks.\\tThey\\tare\\tvery\\tpowerful\\talgorithms,\\ncapable\\tof\\tfitting\\tcomplex\\tdatasets.\\tFor\\texample,\\tin\\t\\nChapter\\t2\\n\\tyou\\ttrained\\ta\\t\\nDecisionTreeRegressor\\nmodel\\ton\\tthe\\tCalifornia\\thousing\\tdataset,\\tfitting\\tit\\tperfectly\\t(actually\\toverfitting\\tit).\\nDecision\\tTrees\\tare\\talso\\tthe\\tfundamental\\tcomponents\\tof\\t\\nRandom\\tForests\\t(see\\t\\nChapter\\t7\\n),\\twhich\\tare\\namong\\tthe\\tmost\\tpowerful\\tMachine\\tLearning\\talgorithms\\tavailable\\ttoday.\\nIn\\tthis\\tchapter\\twe\\twill\\tstart\\tby\\tdiscussing\\thow\\tto\\ttrain,\\tvisualize,\\tand\\tmake\\tpredictions\\twith\\tDecision\\nTrees.\\tThen\\twe\\twill\\tgo\\tthrough\\tthe\\tCART\\ttraining\\talgorithm\\tused\\tby\\tScikit-Learn,\\tand\\twe\\twill\\tdiscuss\\nhow\\tto\\tregularize\\ttrees\\tand\\tuse\\tthem\\tfor\\tregression\\ttasks.\\tFinally,\\twe\\twill\\tdiscuss\\tsome\\tof\\tthe\\tlimitations\\nof\\tDecision\\tTrees.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 214}), Document(page_content='Training\\tand\\tVisualizing\\ta\\tDecision\\tTree\\nTo\\t\\nunderstand\\tDecision\\tTrees,\\tlet’s\\tjust\\tbuild\\tone\\tand\\ttake\\ta\\tlook\\tat\\thow\\tit\\tmakes\\tpredictions.\\tThe\\nfollowing\\tcode\\ttrains\\ta\\t\\nDecisionTreeClassifier\\n\\ton\\tthe\\t\\niris\\tdataset\\t(see\\t\\nChapter\\t4\\n):\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nload_iris\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nDecisionTreeClassifier\\niris\\n\\t\\n=\\n\\t\\nload_iris\\n()\\nX\\n\\t\\n=\\n\\t\\niris\\n.\\ndata\\n[:,\\n\\t\\n2\\n:]\\n\\t\\n#\\tpetal\\tlength\\tand\\twidth\\ny\\n\\t\\n=\\n\\t\\niris\\n.\\ntarget\\ntree_clf\\n\\t\\n=\\n\\t\\nDecisionTreeClassifier\\n(\\nmax_depth\\n=\\n2\\n)\\ntree_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nYou\\tcan\\tvisualize\\tthe\\ttrained\\tDecision\\tTree\\tby\\tfirst\\tusing\\tthe\\t\\nexport_graphviz()\\n\\t\\nmethod\\t\\nto\\toutput\\ta\\ngraph\\tdefinition\\tfile\\tcalled\\t\\niris_tree.dot\\n:\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nexport_graphviz\\nexport_graphviz\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntree_clf\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nout_file\\n=\\nimage_path\\n(\\n\"iris_tree.dot\"\\n),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfeature_names\\n=\\niris\\n.\\nfeature_names\\n[\\n2\\n:],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nclass_names\\n=\\niris\\n.\\ntarget_names\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nrounded\\n=\\nTrue\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfilled\\n=\\nTrue\\n\\t\\t\\t\\t\\n)\\nThen\\tyou\\tcan\\tconvert\\tthis\\t\\n.dot\\n\\tfile\\tto\\ta\\tvariety\\tof\\tformats\\tsuch\\tas\\tPDF\\tor\\tPNG\\tusing\\tthe\\t\\ndot\\n\\tcommand-\\nline\\ttool\\tfrom\\tthe\\t\\ngraphviz\\n\\tpackage.\\n1\\n\\tThis\\tcommand\\tline\\tconverts\\tthe\\t\\n.dot\\n\\tfile\\tto\\ta\\t\\n.png\\n\\timage\\tfile:\\n$\\tdot\\t-Tpng\\tiris_tree.dot\\t-o\\tiris_tree.png\\nYour\\tfirst\\tdecision\\ttree\\tlooks\\tlike\\t\\nFigure\\t6-1\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 215}), Document(page_content='Figure\\t6-1.\\t\\nIris\\tDecision\\tTree', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 216}), Document(page_content='Making\\tPredictions\\nLet’s\\t\\nsee\\thow\\tthe\\ttree\\trepresented\\tin\\t\\nFigure\\t6-1\\n\\tmakes\\tpredictions.\\tSuppose\\tyou\\tfind\\tan\\tiris\\tflower\\tand\\nyou\\twant\\tto\\tclassify\\tit.\\tYou\\tstart\\tat\\tthe\\t\\nroot\\tnode\\n\\t(depth\\t0,\\tat\\tthe\\ttop):\\tthis\\tnode\\tasks\\twhether\\tthe\\tflower’s\\npetal\\tlength\\tis\\tsmaller\\tthan\\t2.45\\tcm.\\tIf\\tit\\tis,\\tthen\\tyou\\tmove\\tdown\\tto\\tthe\\troot’s\\tleft\\tchild\\tnode\\t(depth\\t1,\\nleft).\\tIn\\tthis\\tcase,\\tit\\tis\\ta\\t\\nleaf\\tnode\\n\\t(i.e.,\\tit\\tdoes\\tnot\\thave\\tany\\tchildren\\tnodes),\\tso\\tit\\tdoes\\tnot\\task\\tany\\nquestions:\\tyou\\tcan\\tsimply\\tlook\\tat\\tthe\\tpredicted\\tclass\\tfor\\tthat\\tnode\\tand\\tthe\\tDecision\\tTree\\tpredicts\\tthat\\nyour\\tflower\\tis\\tan\\tIris-Setosa\\t(\\nclass=setosa\\n).\\nNow\\tsuppose\\tyou\\tfind\\tanother\\tflower,\\tbut\\tthis\\ttime\\tthe\\tpetal\\tlength\\tis\\tgreater\\tthan\\t2.45\\tcm.\\tYou\\tmust\\nmove\\tdown\\tto\\tthe\\troot’s\\tright\\tchild\\tnode\\t(depth\\t1,\\tright),\\twhich\\tis\\tnot\\ta\\tleaf\\tnode,\\tso\\tit\\tasks\\tanother\\nquestion:\\tis\\tthe\\tpetal\\twidth\\tsmaller\\tthan\\t1.75\\tcm?\\tIf\\tit\\tis,\\tthen\\tyour\\tflower\\tis\\tmost\\tlikely\\tan\\tIris-\\nVersicolor\\t(depth\\t2,\\tleft).\\tIf\\tnot,\\tit\\tis\\tlikely\\tan\\tIris-Virginica\\t(depth\\t2,\\tright).\\tIt’s\\treally\\tthat\\tsimple.\\nNOTE\\nOne\\tof\\tthe\\tmany\\tqualities\\tof\\tDecision\\tTrees\\tis\\tthat\\tthey\\trequire\\tvery\\tlittle\\tdata\\tpreparation.\\tIn\\tparticular,\\tthey\\tdon’t\\trequire\\nfeature\\tscaling\\tor\\tcentering\\tat\\tall.\\nA\\tnode’s\\t\\nsamples\\n\\tattribute\\tcounts\\thow\\tmany\\ttraining\\tinstances\\tit\\tapplies\\tto.\\tFor\\texample,\\t100\\ttraining\\ninstances\\thave\\ta\\tpetal\\tlength\\tgreater\\tthan\\t2.45\\tcm\\t(depth\\t1,\\tright),\\tamong\\twhich\\t54\\thave\\ta\\tpetal\\twidth\\nsmaller\\tthan\\t1.75\\tcm\\t(depth\\t2,\\tleft).\\tA\\tnode’s\\t\\nvalue\\n\\tattribute\\ttells\\tyou\\thow\\tmany\\ttraining\\tinstances\\tof\\neach\\tclass\\tthis\\tnode\\tapplies\\tto:\\tfor\\texample,\\tthe\\tbottom-right\\tnode\\tapplies\\tto\\t0\\tIris-Setosa,\\t1\\tIris-\\nVersicolor,\\tand\\t45\\tIris-Virginica.\\tFinally,\\ta\\tnode’s\\t\\ngini\\n\\tattribute\\tmeasures\\tits\\t\\nimpurity\\n:\\ta\\tnode\\tis\\t“pure”\\n(\\ngini=0\\n)\\tif\\tall\\ttraining\\tinstances\\tit\\tapplies\\tto\\tbelong\\tto\\tthe\\tsame\\tclass.\\tFor\\texample,\\tsince\\tthe\\tdepth-1\\nleft\\tnode\\tapplies\\tonly\\tto\\tIris-Setosa\\ttraining\\tinstances,\\tit\\tis\\tpure\\tand\\tits\\t\\ngini\\n\\tscore\\tis\\t0.\\t\\nEquation\\t6-1\\nshows\\thow\\tthe\\ttraining\\talgorithm\\tcomputes\\tthe\\tgini\\tscore\\t\\nG\\ni\\n\\tof\\tthe\\ti\\nth\\n\\tnode.\\tFor\\texample,\\tthe\\tdepth-2\\tleft\\nnode\\thas\\ta\\t\\ngini\\n\\tscore\\tequal\\tto\\t1\\t–\\t(0/54)\\n2\\n\\t–\\t(49/54)\\n2\\n\\t–\\t(5/54)\\n2\\n\\t≈\\t0.168.\\tAnother\\t\\nimpurity\\tmeasure\\n\\tis\\ndiscussed\\tshortly.\\nEquation\\t6-1.\\t\\nGini\\timpurity\\np\\ni\\n,\\nk\\n\\tis\\tthe\\tratio\\tof\\tclass\\t\\nk\\n\\tinstances\\tamong\\tthe\\ttraining\\tinstances\\tin\\tthe\\t\\ni\\nth\\n\\tnode.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 217}), Document(page_content='NOTE\\nScikit-Learn\\t\\nuses\\tthe\\t\\nCART\\talgorithm,\\twhich\\tproduces\\t\\nonly\\t\\nbinary\\ttrees\\n:\\tnonleaf\\tnodes\\talways\\thave\\ttwo\\tchildren\\t(i.e.,\\nquestions\\tonly\\thave\\tyes/no\\tanswers).\\tHowever,\\tother\\talgorithms\\tsuch\\tas\\tID3\\tcan\\tproduce\\tDecision\\tTrees\\twith\\tnodes\\tthat\\thave\\nmore\\tthan\\ttwo\\tchildren.\\nFigure\\t6-2\\n\\tshows\\tthis\\t\\nDecision\\tTree’s\\tdecision\\tboundaries.\\tThe\\tthick\\tvertical\\tline\\trepresents\\tthe\\tdecision\\nboundary\\tof\\tthe\\troot\\tnode\\t(depth\\t0):\\tpetal\\tlength\\t=\\t2.45\\tcm.\\tSince\\tthe\\tleft\\tarea\\tis\\tpure\\t(only\\tIris-Setosa),\\nit\\tcannot\\tbe\\tsplit\\tany\\tfurther.\\tHowever,\\tthe\\tright\\tarea\\tis\\timpure,\\tso\\tthe\\tdepth-1\\tright\\tnode\\tsplits\\tit\\tat\\tpetal\\nwidth\\t=\\t1.75\\tcm\\t(represented\\tby\\tthe\\tdashed\\tline).\\tSince\\t\\nmax_depth\\n\\twas\\tset\\tto\\t2,\\tthe\\tDecision\\tTree\\tstops\\nright\\tthere.\\tHowever,\\tif\\tyou\\tset\\t\\nmax_depth\\n\\tto\\t3,\\tthen\\tthe\\ttwo\\tdepth-2\\tnodes\\twould\\teach\\tadd\\tanother\\ndecision\\tboundary\\t(represented\\tby\\tthe\\tdotted\\tlines).\\nFigure\\t6-2.\\t\\nDecision\\tTree\\tdecision\\tboundaries\\nMODEL\\tINTERPRETATION:\\tWHITE\\tBOX\\tVERSUS\\tBLACK\\tBOX\\nAs\\tyou\\tcan\\tsee\\tDecision\\tTrees\\tare\\tfairly\\tintuitive\\tand\\ttheir\\tdecisions\\tare\\teasy\\tto\\tinterpret.\\tSuch\\tmodels\\tare\\toften\\t\\ncalled\\t\\nwhite\\tbox\\nmodels\\n.\\tIn\\tcontrast,\\tas\\twe\\twill\\tsee,\\tRandom\\tForests\\tor\\tneural\\tnetworks\\tare\\tgenerally\\t\\nconsidered\\t\\nblack\\tbox\\tmodels\\n.\\tThey\\tmake\\tgreat\\npredictions,\\tand\\tyou\\tcan\\teasily\\tcheck\\tthe\\tcalculations\\tthat\\tthey\\tperformed\\tto\\tmake\\tthese\\tpredictions;\\tnevertheless,\\tit\\tis\\tusually\\thard\\tto\\nexplain\\tin\\tsimple\\tterms\\twhy\\tthe\\tpredictions\\twere\\tmade.\\tFor\\texample,\\tif\\ta\\tneural\\tnetwork\\tsays\\tthat\\ta\\tparticular\\tperson\\tappears\\ton\\ta\\npicture,\\tit\\tis\\thard\\tto\\tknow\\twhat\\tactually\\tcontributed\\tto\\tthis\\tprediction:\\tdid\\tthe\\tmodel\\trecognize\\tthat\\tperson’s\\teyes?\\tHer\\tmouth?\\tHer\\tnose?\\nHer\\tshoes?\\tOr\\teven\\tthe\\tcouch\\tthat\\tshe\\twas\\tsitting\\ton?\\tConversely,\\tDecision\\tTrees\\tprovide\\tnice\\tand\\tsimple\\tclassification\\trules\\tthat\\tcan\\neven\\tbe\\tapplied\\tmanually\\tif\\tneed\\tbe\\t(e.g.,\\tfor\\tflower\\tclassification).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 218}), Document(page_content='Estimating\\tClass\\tProbabilities\\nA\\t\\nDecision\\tTree\\tcan\\talso\\testimate\\tthe\\tprobability\\tthat\\tan\\tinstance\\tbelongs\\tto\\ta\\tparticular\\tclass\\t\\nk\\n:\\tfirst\\tit\\ntraverses\\tthe\\ttree\\tto\\tfind\\tthe\\tleaf\\tnode\\tfor\\tthis\\tinstance,\\tand\\tthen\\tit\\treturns\\tthe\\tratio\\tof\\ttraining\\tinstances\\tof\\nclass\\t\\nk\\n\\tin\\tthis\\tnode.\\tFor\\texample,\\tsuppose\\tyou\\thave\\tfound\\ta\\tflower\\twhose\\tpetals\\tare\\t5\\tcm\\tlong\\tand\\t1.5\\ncm\\twide.\\tThe\\tcorresponding\\tleaf\\tnode\\tis\\tthe\\tdepth-2\\tleft\\tnode,\\tso\\tthe\\tDecision\\tTree\\tshould\\toutput\\tthe\\nfollowing\\tprobabilities:\\t0%\\tfor\\tIris-Setosa\\t(0/54),\\t90.7%\\tfor\\tIris-Versicolor\\t(49/54),\\tand\\t9.3%\\tfor\\tIris-\\nVirginica\\t(5/54).\\tAnd\\tof\\tcourse\\tif\\tyou\\task\\tit\\tto\\tpredict\\tthe\\tclass,\\tit\\tshould\\toutput\\tIris-Versicolor\\t(class\\t1)\\nsince\\tit\\thas\\tthe\\thighest\\tprobability.\\tLet’s\\tcheck\\tthis:\\n>>>\\t\\ntree_clf\\n.\\npredict_proba\\n([[\\n5\\n,\\n\\t\\n1.5\\n]])\\narray([[\\t0.\\t,\\t\\t0.90740741,\\t\\t0.09259259]])\\n>>>\\t\\ntree_clf\\n.\\npredict\\n([[\\n5\\n,\\n\\t\\n1.5\\n]])\\narray([1])\\nPerfect!\\tNotice\\tthat\\tthe\\testimated\\tprobabilities\\twould\\tbe\\tidentical\\tanywhere\\telse\\tin\\tthe\\tbottom-right\\nrectangle\\tof\\t\\nFigure\\t6-2\\n\\t—\\tfor\\texample,\\tif\\tthe\\tpetals\\twere\\t6\\tcm\\tlong\\tand\\t1.5\\tcm\\twide\\t(even\\tthough\\tit\\nseems\\tobvious\\tthat\\tit\\twould\\tmost\\tlikely\\tbe\\tan\\tIris-Virginica\\tin\\tthis\\tcase).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 219}), Document(page_content='The\\tCART\\tTraining\\tAlgorithm\\nScikit-Learn\\t\\nuses\\tthe\\t\\nClassification\\tAnd\\tRegression\\tTree\\n\\t(CART)\\talgorithm\\tto\\ttrain\\tDecision\\tTrees\\t(also\\ncalled\\t“growing”\\ttrees).\\tThe\\tidea\\tis\\treally\\tquite\\tsimple:\\tthe\\talgorithm\\tfirst\\tsplits\\tthe\\ttraining\\tset\\tin\\ttwo\\nsubsets\\tusing\\ta\\tsingle\\tfeature\\t\\nk\\n\\tand\\ta\\tthreshold\\t\\nt\\nk\\n\\t(e.g.,\\t“petal\\tlength\\t≤\\t2.45\\tcm”).\\tHow\\tdoes\\tit\\tchoose\\t\\nk\\nand\\t\\nt\\nk\\n?\\tIt\\tsearches\\tfor\\tthe\\tpair\\t(\\nk\\n,\\t\\nt\\nk\\n)\\tthat\\tproduces\\tthe\\tpurest\\tsubsets\\t(weighted\\tby\\ttheir\\tsize).\\tThe\\tcost\\nfunction\\tthat\\tthe\\talgorithm\\ttries\\tto\\tminimize\\tis\\tgiven\\tby\\t\\nEquation\\t6-2\\n.\\nEquation\\t6-2.\\t\\nCART\\tcost\\tfunction\\tfor\\tclassification\\nOnce\\tit\\thas\\tsuccessfully\\tsplit\\tthe\\ttraining\\tset\\tin\\ttwo,\\tit\\tsplits\\tthe\\tsubsets\\tusing\\tthe\\tsame\\tlogic,\\tthen\\tthe\\tsub-\\nsubsets\\tand\\tso\\ton,\\trecursively.\\tIt\\tstops\\trecursing\\tonce\\tit\\treaches\\tthe\\tmaximum\\tdepth\\t(defined\\tby\\tthe\\nmax_depth\\n\\thyperparameter),\\tor\\tif\\tit\\tcannot\\tfind\\ta\\tsplit\\tthat\\twill\\treduce\\timpurity.\\tA\\tfew\\tother\\nhyperparameters\\t(described\\tin\\ta\\tmoment)\\tcontrol\\tadditional\\tstopping\\tconditions\\t(\\nmin_samples_split\\n,\\nmin_samples_leaf\\n,\\t\\nmin_weight_fraction_leaf\\n,\\tand\\t\\nmax_leaf_nodes\\n).\\nWARNING\\nAs\\tyou\\tcan\\tsee,\\tthe\\tCART\\talgorithm\\tis\\t\\na\\t\\ngreedy\\talgorithm\\n:\\tit\\tgreedily\\tsearches\\tfor\\tan\\toptimum\\tsplit\\tat\\tthe\\ttop\\tlevel,\\tthen\\nrepeats\\tthe\\tprocess\\tat\\teach\\tlevel.\\tIt\\tdoes\\tnot\\tcheck\\twhether\\tor\\tnot\\tthe\\tsplit\\twill\\tlead\\tto\\tthe\\tlowest\\tpossible\\timpurity\\tseveral\\tlevels\\ndown.\\tA\\tgreedy\\talgorithm\\toften\\tproduces\\ta\\treasonably\\tgood\\tsolution,\\tbut\\tit\\tis\\tnot\\tguaranteed\\tto\\tbe\\tthe\\toptimal\\tsolution.\\nUnfortunately,\\tfinding\\tthe\\toptimal\\ttree\\tis\\tknown\\tto\\tbe\\t\\nan\\t\\nNP-Complete\\n\\tproblem:\\n2\\n\\tit\\trequires\\t\\nO\\n(exp(\\nm\\n))\\ntime,\\tmaking\\tthe\\tproblem\\tintractable\\teven\\tfor\\tfairly\\tsmall\\ttraining\\tsets.\\tThis\\tis\\twhy\\twe\\tmust\\tsettle\\tfor\\ta\\n“reasonably\\tgood”\\tsolution.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 220}), Document(page_content='Computational\\tComplexity\\nMaking\\t\\npredictions\\trequires\\ttraversing\\tthe\\tDecision\\tTree\\tfrom\\tthe\\troot\\tto\\ta\\tleaf.\\tDecision\\tTrees\\tare\\ngenerally\\tapproximately\\tbalanced,\\tso\\ttraversing\\tthe\\tDecision\\tTree\\trequires\\tgoing\\tthrough\\troughly\\nO\\n(\\nlog\\n2\\n(\\nm\\n))\\tnodes.\\n3\\n\\tSince\\teach\\tnode\\tonly\\trequires\\tchecking\\tthe\\tvalue\\tof\\tone\\tfeature,\\tthe\\toverall\\nprediction\\tcomplexity\\tis\\tjust\\t\\nO\\n(\\nlog\\n2\\n(\\nm\\n)),\\tindependent\\tof\\tthe\\tnumber\\tof\\tfeatures.\\tSo\\tpredictions\\tare\\tvery\\nfast,\\teven\\twhen\\tdealing\\twith\\tlarge\\ttraining\\tsets.\\nHowever,\\tthe\\ttraining\\talgorithm\\tcompares\\tall\\tfeatures\\t(or\\tless\\tif\\t\\nmax_features\\n\\tis\\tset)\\ton\\tall\\tsamples\\tat\\neach\\tnode.\\tThis\\tresults\\tin\\ta\\ttraining\\tcomplexity\\tof\\t\\nO\\n(\\nn\\n\\t×\\t\\nm\\n\\t\\nlog\\n(\\nm\\n)).\\tFor\\tsmall\\ttraining\\tsets\\t(less\\tthan\\ta\\nfew\\tthousand\\tinstances),\\tScikit-Learn\\tcan\\tspeed\\tup\\ttraining\\tby\\tpresorting\\tthe\\tdata\\t(set\\t\\npresort=True\\n),\\nbut\\tthis\\tslows\\tdown\\ttraining\\tconsiderably\\tfor\\tlarger\\ttraining\\tsets.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 221}), Document(page_content='Gini\\tImpurity\\tor\\tEntropy?\\nBy\\tdefault,\\tthe\\t\\nGini\\timpurity\\tmeasure\\t\\nis\\tused,\\tbut\\tyou\\tcan\\tselect\\tthe\\t\\nentropy\\n\\t\\nimpurity\\tmeasure\\tinstead\\tby\\nsetting\\tthe\\t\\ncriterion\\n\\thyperparameter\\tto\\t\\n\"entropy\"\\n.\\tThe\\tconcept\\tof\\tentropy\\toriginated\\tin\\nthermodynamics\\tas\\ta\\tmeasure\\tof\\tmolecular\\tdisorder:\\tentropy\\tapproaches\\tzero\\twhen\\tmolecules\\tare\\tstill\\nand\\twell\\tordered.\\tIt\\tlater\\tspread\\tto\\ta\\twide\\tvariety\\tof\\tdomains,\\tincluding\\tShannon’s\\t\\ninformation\\ttheory\\n,\\nwhere\\tit\\tmeasures\\tthe\\taverage\\tinformation\\tcontent\\tof\\ta\\tmessage:\\n4\\n\\t\\nentropy\\tis\\tzero\\twhen\\tall\\tmessages\\tare\\nidentical.\\tIn\\tMachine\\tLearning,\\tit\\tis\\tfrequently\\tused\\tas\\tan\\timpurity\\tmeasure:\\ta\\tset’s\\tentropy\\tis\\tzero\\twhen\\nit\\tcontains\\tinstances\\tof\\tonly\\tone\\tclass.\\t\\nEquation\\t6-3\\n\\tshows\\tthe\\tdefinition\\tof\\tthe\\tentropy\\tof\\tthe\\ti\\nth\\n\\tnode.\\nFor\\texample,\\tthe\\tdepth-2\\tleft\\tnode\\tin\\t\\nFigure\\t6-1\\n\\thas\\tan\\tentropy\\tequal\\tto\\t\\n≈\\t0.31.\\nEquation\\t6-3.\\t\\nEntropy\\nSo\\tshould\\tyou\\tuse\\tGini\\timpurity\\tor\\tentropy?\\tThe\\ttruth\\tis,\\tmost\\tof\\tthe\\ttime\\tit\\tdoes\\tnot\\tmake\\ta\\tbig\\ndifference:\\tthey\\tlead\\tto\\tsimilar\\ttrees.\\tGini\\timpurity\\tis\\tslightly\\tfaster\\tto\\tcompute,\\tso\\tit\\tis\\ta\\tgood\\tdefault.\\nHowever,\\twhen\\tthey\\tdiffer,\\tGini\\timpurity\\ttends\\tto\\tisolate\\tthe\\tmost\\tfrequent\\tclass\\tin\\tits\\town\\tbranch\\tof\\tthe\\ntree,\\twhile\\tentropy\\ttends\\tto\\tproduce\\tslightly\\tmore\\tbalanced\\ttrees.\\n5', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 222}), Document(page_content='Regularization\\tHyperparameters\\nDecision\\tTrees\\t\\nmake\\tvery\\tfew\\tassumptions\\tabout\\tthe\\ttraining\\tdata\\t(as\\topposed\\tto\\tlinear\\tmodels,\\twhich\\nobviously\\tassume\\tthat\\tthe\\tdata\\tis\\tlinear,\\tfor\\texample).\\tIf\\tleft\\tunconstrained,\\tthe\\ttree\\tstructure\\twill\\tadapt\\nitself\\tto\\tthe\\ttraining\\tdata,\\tfitting\\tit\\tvery\\tclosely,\\tand\\tmost\\tlikely\\toverfitting\\tit.\\tSuch\\ta\\tmodel\\tis\\toften\\tcalled\\na\\t\\nnonparametric\\tmodel\\n,\\t\\nnot\\tbecause\\tit\\tdoes\\tnot\\thave\\tany\\tparameters\\t(it\\toften\\thas\\ta\\tlot)\\tbut\\tbecause\\tthe\\nnumber\\tof\\tparameters\\tis\\tnot\\tdetermined\\tprior\\tto\\ttraining,\\tso\\tthe\\tmodel\\tstructure\\tis\\tfree\\tto\\tstick\\tclosely\\tto\\nthe\\tdata.\\tIn\\tcontrast,\\t\\na\\t\\nparametric\\tmodel\\n\\tsuch\\tas\\ta\\tlinear\\tmodel\\thas\\ta\\tpredetermined\\tnumber\\tof\\nparameters,\\tso\\tits\\tdegree\\tof\\tfreedom\\tis\\tlimited,\\treducing\\tthe\\trisk\\tof\\toverfitting\\t(but\\tincreasing\\tthe\\trisk\\tof\\nunderfitting).\\nTo\\tavoid\\t\\noverfitting\\tthe\\ttraining\\tdata,\\tyou\\tneed\\tto\\trestrict\\tthe\\tDecision\\tTree’s\\tfreedom\\tduring\\ttraining.\\tAs\\nyou\\tknow\\tby\\tnow,\\tthis\\tis\\tcalled\\tregularization.\\tThe\\tregularization\\thyperparameters\\tdepend\\ton\\tthe\\nalgorithm\\tused,\\tbut\\tgenerally\\tyou\\tcan\\tat\\tleast\\trestrict\\tthe\\tmaximum\\tdepth\\tof\\tthe\\tDecision\\tTree.\\tIn\\tScikit-\\nLearn,\\tthis\\tis\\tcontrolled\\tby\\tthe\\t\\nmax_depth\\n\\thyperparameter\\t(the\\tdefault\\tvalue\\tis\\t\\nNone\\n,\\twhich\\tmeans\\nunlimited).\\tReducing\\t\\nmax_depth\\n\\twill\\t\\nregularize\\tthe\\tmodel\\tand\\tthus\\treduce\\tthe\\trisk\\tof\\toverfitting.\\nThe\\t\\nDecisionTreeClassifier\\n\\t\\nclass\\thas\\ta\\tfew\\tother\\tparameters\\tthat\\tsimilarly\\trestrict\\tthe\\tshape\\tof\\tthe\\nDecision\\tTree:\\t\\nmin_samples_split\\n\\t(the\\tminimum\\tnumber\\tof\\tsamples\\ta\\tnode\\tmust\\thave\\tbefore\\tit\\tcan\\tbe\\nsplit),\\t\\nmin_samples_leaf\\n\\t(the\\tminimum\\tnumber\\tof\\tsamples\\ta\\tleaf\\tnode\\tmust\\thave),\\nmin_weight_fraction_leaf\\n\\t(same\\tas\\t\\nmin_samples_leaf\\n\\tbut\\texpressed\\tas\\ta\\tfraction\\tof\\tthe\\ttotal\\nnumber\\tof\\tweighted\\tinstances),\\t\\nmax_leaf_nodes\\n\\t(maximum\\tnumber\\tof\\tleaf\\tnodes),\\tand\\t\\nmax_features\\n(maximum\\tnumber\\tof\\tfeatures\\tthat\\tare\\tevaluated\\tfor\\tsplitting\\tat\\teach\\tnode).\\tIncreasing\\t\\nmin_*\\nhyperparameters\\tor\\treducing\\t\\nmax_*\\n\\thyperparameters\\twill\\tregularize\\tthe\\tmodel.\\nNOTE\\nOther\\talgorithms\\twork\\tby\\tfirst\\ttraining\\tthe\\tDecision\\tTree\\twithout\\trestrictions,\\t\\nthen\\t\\npruning\\n\\t(deleting)\\tunnecessary\\tnodes.\\tA\\tnode\\nwhose\\tchildren\\tare\\tall\\tleaf\\tnodes\\tis\\tconsidered\\tunnecessary\\tif\\tthe\\tpurity\\timprovement\\tit\\tprovides\\tis\\t\\nnot\\t\\nstatistically\\tsignificant\\n.\\nStandard\\tstatistical\\ttests,\\tsuch\\tas\\tthe\\t\\nχ\\n2\\n\\t\\ntest\\n,\\t\\nare\\tused\\tto\\testimate\\tthe\\tprobability\\tthat\\tthe\\timprovement\\tis\\tpurely\\tthe\\tresult\\tof\\nchance\\t(which\\tis\\tcalled\\t\\nthe\\t\\nnull\\thypothesis\\n).\\tIf\\tthis\\tprobability,\\tcalled\\tthe\\t\\np-value\\n,\\t\\nis\\thigher\\tthan\\ta\\tgiven\\tthreshold\\t(typically\\t5%,\\ncontrolled\\tby\\ta\\thyperparameter),\\tthen\\tthe\\tnode\\tis\\tconsidered\\tunnecessary\\tand\\tits\\tchildren\\tare\\tdeleted.\\tThe\\tpruning\\tcontinues\\tuntil\\nall\\tunnecessary\\tnodes\\thave\\tbeen\\tpruned.\\nFigure\\t6-3\\n\\tshows\\ttwo\\tDecision\\tTrees\\ttrained\\ton\\tthe\\tmoons\\tdataset\\t(introduced\\tin\\t\\nChapter\\t5\\n).\\tOn\\tthe\\tleft,\\nthe\\tDecision\\tTree\\tis\\ttrained\\twith\\tthe\\tdefault\\thyperparameters\\t(i.e.,\\tno\\trestrictions),\\tand\\ton\\tthe\\tright\\tthe\\nDecision\\tTree\\tis\\ttrained\\twith\\t\\nmin_samples_leaf=4\\n.\\tIt\\tis\\tquite\\tobvious\\tthat\\tthe\\tmodel\\ton\\tthe\\tleft\\tis\\noverfitting,\\tand\\tthe\\tmodel\\ton\\tthe\\tright\\twill\\tprobably\\tgeneralize\\t\\nbetter.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 223}), Document(page_content='Figure\\t6-3.\\t\\nRegularization\\tusing\\tmin_samples_leaf', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 224}), Document(page_content='Regression\\nDecision\\tTrees\\t\\nare\\talso\\tcapable\\tof\\tperforming\\tregression\\ttasks.\\tLet’s\\tbuild\\ta\\tregression\\ttree\\tusing\\t\\nScikit-\\nLearn’s\\t\\nDecisionTreeRegressor\\n\\tclass,\\ttraining\\tit\\ton\\ta\\tnoisy\\tquadratic\\tdataset\\twith\\t\\nmax_depth=2\\n:\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nDecisionTreeRegressor\\ntree_reg\\n\\t\\n=\\n\\t\\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n)\\ntree_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nThe\\tresulting\\ttree\\tis\\trepresented\\ton\\t\\nFigure\\t6-4\\n.\\nFigure\\t6-4.\\t\\nA\\tDecision\\tTree\\tfor\\tregression\\nThis\\ttree\\tlooks\\tvery\\tsimilar\\tto\\tthe\\tclassification\\ttree\\tyou\\tbuilt\\tearlier.\\tThe\\tmain\\tdifference\\tis\\tthat\\tinstead\\nof\\tpredicting\\ta\\tclass\\tin\\teach\\tnode,\\tit\\tpredicts\\ta\\tvalue.\\tFor\\texample,\\tsuppose\\tyou\\twant\\tto\\tmake\\ta\\nprediction\\tfor\\ta\\tnew\\tinstance\\twith\\t\\nx\\n1\\n\\t=\\t0.6.\\tYou\\ttraverse\\tthe\\ttree\\tstarting\\tat\\tthe\\troot,\\tand\\tyou\\teventually\\nreach\\tthe\\tleaf\\tnode\\tthat\\tpredicts\\t\\nvalue=0.1106\\n.\\tThis\\tprediction\\tis\\tsimply\\tthe\\taverage\\ttarget\\tvalue\\tof\\tthe\\n110\\ttraining\\tinstances\\tassociated\\tto\\tthis\\tleaf\\tnode.\\tThis\\tprediction\\tresults\\tin\\ta\\tMean\\tSquared\\tError\\n(MSE)\\tequal\\tto\\t0.0151\\tover\\tthese\\t110\\tinstances.\\nThis\\tmodel’s\\tpredictions\\tare\\trepresented\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t6-5\\n.\\tIf\\tyou\\tset\\t\\nmax_depth=3\\n,\\tyou\\tget\\tthe\\npredictions\\trepresented\\ton\\tthe\\tright.\\tNotice\\thow\\tthe\\tpredicted\\tvalue\\tfor\\teach\\tregion\\tis\\talways\\tthe\\taverage\\ntarget\\tvalue\\tof\\tthe\\tinstances\\tin\\tthat\\tregion.\\tThe\\talgorithm\\tsplits\\teach\\tregion\\tin\\ta\\tway\\tthat\\tmakes\\tmost\\ntraining\\tinstances\\tas\\tclose\\tas\\tpossible\\tto\\tthat\\tpredicted\\tvalue.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 225}), Document(page_content='Figure\\t6-5.\\t\\nPredictions\\tof\\ttwo\\tDecision\\tTree\\tregression\\tmodels\\nThe\\t\\nCART\\talgorithm\\tworks\\tmostly\\tthe\\tsame\\tway\\tas\\tearlier,\\texcept\\tthat\\tinstead\\tof\\ttrying\\tto\\tsplit\\tthe\\ntraining\\tset\\tin\\ta\\tway\\tthat\\tminimizes\\timpurity,\\tit\\tnow\\ttries\\tto\\tsplit\\tthe\\ttraining\\tset\\tin\\ta\\tway\\tthat\\tminimizes\\nthe\\tMSE.\\t\\nEquation\\t6-4\\n\\tshows\\tthe\\tcost\\tfunction\\tthat\\tthe\\talgorithm\\ttries\\tto\\tminimize.\\nEquation\\t6-4.\\t\\nCART\\tcost\\tfunction\\tfor\\tregression\\nJust\\tlike\\tfor\\tclassification\\ttasks,\\tDecision\\tTrees\\tare\\tprone\\tto\\toverfitting\\t\\nwhen\\tdealing\\twith\\tregression\\ntasks.\\tWithout\\tany\\tregularization\\t(i.e.,\\tusing\\tthe\\tdefault\\thyperparameters),\\tyou\\tget\\tthe\\tpredictions\\ton\\tthe\\nleft\\tof\\t\\nFigure\\t6-6\\n.\\tIt\\tis\\tobviously\\toverfitting\\tthe\\ttraining\\tset\\tvery\\tbadly.\\tJust\\tsetting\\nmin_samples_leaf=10\\n\\tresults\\tin\\ta\\tmuch\\tmore\\treasonable\\tmodel,\\trepresented\\ton\\tthe\\t\\nright\\tof\\t\\nFigure\\t6-6\\n.\\nFigure\\t6-6.\\t\\nRegularizing\\ta\\tDecision\\tTree\\tregressor', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 226}), Document(page_content='Instability\\nHopefully\\t\\nby\\tnow\\tyou\\tare\\tconvinced\\tthat\\tDecision\\tTrees\\thave\\ta\\tlot\\tgoing\\tfor\\tthem:\\tthey\\tare\\tsimple\\tto\\nunderstand\\tand\\tinterpret,\\teasy\\tto\\tuse,\\tversatile,\\tand\\tpowerful.\\tHowever\\tthey\\tdo\\thave\\ta\\tfew\\tlimitations.\\nFirst,\\tas\\tyou\\tmay\\thave\\tnoticed,\\tDecision\\tTrees\\tlove\\torthogonal\\tdecision\\tboundaries\\t(all\\tsplits\\tare\\nperpendicular\\tto\\tan\\taxis),\\twhich\\tmakes\\tthem\\tsensitive\\tto\\ttraining\\tset\\trotation.\\tFor\\texample,\\t\\nFigure\\t6-7\\nshows\\ta\\tsimple\\tlinearly\\tseparable\\tdataset:\\ton\\tthe\\tleft,\\ta\\tDecision\\tTree\\tcan\\tsplit\\tit\\teasily,\\twhile\\ton\\tthe\\nright,\\tafter\\tthe\\tdataset\\tis\\trotated\\tby\\t45°,\\tthe\\tdecision\\tboundary\\tlooks\\tunnecessarily\\tconvoluted.\\tAlthough\\nboth\\tDecision\\tTrees\\tfit\\tthe\\ttraining\\tset\\tperfectly,\\tit\\tis\\tvery\\tlikely\\tthat\\tthe\\tmodel\\ton\\tthe\\tright\\twill\\tnot\\ngeneralize\\twell.\\tOne\\tway\\tto\\tlimit\\tthis\\tproblem\\tis\\tto\\tuse\\tPCA\\t(see\\t\\nChapter\\t8\\n),\\twhich\\toften\\tresults\\tin\\ta\\nbetter\\torientation\\tof\\tthe\\ttraining\\tdata.\\nFigure\\t6-7.\\t\\nSensitivity\\tto\\ttraining\\tset\\trotation\\nMore\\tgenerally,\\tthe\\tmain\\tissue\\twith\\tDecision\\tTrees\\tis\\tthat\\tthey\\tare\\tvery\\tsensitive\\tto\\tsmall\\tvariations\\tin\\nthe\\ttraining\\tdata.\\tFor\\texample,\\tif\\tyou\\tjust\\tremove\\tthe\\twidest\\tIris-Versicolor\\tfrom\\tthe\\tiris\\ttraining\\tset\\t(the\\none\\twith\\tpetals\\t4.8\\tcm\\tlong\\tand\\t1.8\\tcm\\twide)\\tand\\ttrain\\ta\\tnew\\tDecision\\tTree,\\tyou\\tmay\\tget\\tthe\\tmodel\\nrepresented\\tin\\t\\nFigure\\t6-8\\n.\\tAs\\tyou\\tcan\\tsee,\\tit\\tlooks\\tvery\\tdifferent\\tfrom\\tthe\\tprevious\\tDecision\\tTree\\n(\\nFigure\\t6-2\\n).\\tActually,\\tsince\\tthe\\ttraining\\talgorithm\\tused\\tby\\tScikit-Learn\\tis\\tstochastic\\n6\\n\\tyou\\tmay\\t\\nget\\tvery\\ndifferent\\tmodels\\teven\\ton\\tthe\\tsame\\ttraining\\tdata\\t(unless\\tyou\\tset\\tthe\\t\\nrandom_state\\n\\thyperparameter).\\nFigure\\t6-8.\\t\\nSensitivity\\tto\\ttraining\\tset\\tdetails', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 227}), Document(page_content='Random\\tForests\\t\\ncan\\tlimit\\tthis\\tinstability\\tby\\taveraging\\tpredictions\\tover\\tmany\\ttrees,\\tas\\twe\\twill\\tsee\\tin\\tthe\\nnext\\t\\nchapter.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 228}), Document(page_content='Exercises\\n1\\n.\\t\\nWhat\\tis\\tthe\\tapproximate\\tdepth\\tof\\ta\\tDecision\\tTree\\ttrained\\t(without\\trestrictions)\\ton\\ta\\ttraining\\tset\\twith\\n1\\tmillion\\tinstances?\\n2\\n.\\t\\nIs\\ta\\tnode’s\\tGini\\timpurity\\tgenerally\\tlower\\tor\\tgreater\\tthan\\tits\\tparent’s?\\tIs\\tit\\t\\ngenerally\\n\\tlower/greater,\\nor\\t\\nalways\\n\\tlower/greater?\\n3\\n.\\t\\nIf\\ta\\tDecision\\tTree\\tis\\toverfitting\\tthe\\ttraining\\tset,\\tis\\tit\\ta\\tgood\\tidea\\tto\\ttry\\tdecreasing\\t\\nmax_depth\\n?\\n4\\n.\\t\\nIf\\ta\\tDecision\\tTree\\tis\\tunderfitting\\tthe\\ttraining\\tset,\\tis\\tit\\ta\\tgood\\tidea\\tto\\ttry\\tscaling\\tthe\\tinput\\tfeatures?\\n5\\n.\\t\\nIf\\tit\\ttakes\\tone\\thour\\tto\\ttrain\\ta\\tDecision\\tTree\\ton\\ta\\ttraining\\tset\\tcontaining\\t1\\tmillion\\tinstances,\\troughly\\nhow\\tmuch\\ttime\\twill\\tit\\ttake\\tto\\ttrain\\tanother\\tDecision\\tTree\\ton\\ta\\ttraining\\tset\\tcontaining\\t10\\tmillion\\ninstances?\\n6\\n.\\t\\nIf\\tyour\\ttraining\\tset\\tcontains\\t100,000\\tinstances,\\twill\\tsetting\\t\\npresort=True\\n\\tspeed\\tup\\ttraining?\\n7\\n.\\t\\nTrain\\tand\\tfine-tune\\ta\\tDecision\\tTree\\tfor\\tthe\\tmoons\\tdataset.\\na\\n.\\t\\nGenerate\\ta\\tmoons\\tdataset\\t\\nusing\\t\\nmake_moons(n_samples=10000,\\tnoise=0.4)\\n.\\nb\\n.\\t\\nSplit\\tit\\tinto\\ta\\ttraining\\tset\\tand\\ta\\t\\ntest\\tset\\tusing\\t\\ntrain_test_split()\\n.\\nc\\n.\\t\\nUse\\tgrid\\tsearch\\twith\\tcross-validation\\t(with\\tthe\\thelp\\tof\\tthe\\t\\nGridSearchCV\\n\\tclass)\\tto\\tfind\\tgood\\nhyperparameter\\tvalues\\tfor\\ta\\t\\nDecisionTreeClassifier\\n.\\t\\nHint:\\ttry\\tvarious\\tvalues\\tfor\\nmax_leaf_nodes\\n.\\nd\\n.\\t\\nTrain\\tit\\ton\\tthe\\tfull\\ttraining\\tset\\tusing\\tthese\\thyperparameters,\\tand\\tmeasure\\tyour\\tmodel’s\\nperformance\\ton\\tthe\\ttest\\tset.\\tYou\\tshould\\tget\\troughly\\t85%\\tto\\t87%\\taccuracy.\\n8\\n.\\t\\nGrow\\ta\\tforest.\\na\\n.\\t\\nContinuing\\tthe\\tprevious\\texercise,\\tgenerate\\t1,000\\tsubsets\\tof\\tthe\\ttraining\\tset,\\teach\\tcontaining\\t100\\ninstances\\tselected\\trandomly.\\tHint:\\tyou\\tcan\\tuse\\tScikit-Learn’s\\t\\nShuffleSplit\\n\\tclass\\tfor\\tthis.\\nb\\n.\\t\\nTrain\\tone\\tDecision\\tTree\\ton\\teach\\tsubset,\\tusing\\tthe\\tbest\\thyperparameter\\tvalues\\tfound\\tabove.\\nEvaluate\\tthese\\t1,000\\tDecision\\tTrees\\ton\\tthe\\ttest\\tset.\\tSince\\tthey\\twere\\ttrained\\ton\\tsmaller\\tsets,\\nthese\\tDecision\\tTrees\\twill\\tlikely\\tperform\\tworse\\tthan\\tthe\\tfirst\\tDecision\\tTree,\\tachieving\\tonly\\nabout\\t80%\\taccuracy.\\nc\\n.\\t\\nNow\\tcomes\\tthe\\tmagic.\\tFor\\teach\\ttest\\tset\\tinstance,\\tgenerate\\tthe\\tpredictions\\tof\\tthe\\t1,000\\tDecision\\nTrees,\\tand\\tkeep\\tonly\\tthe\\tmost\\tfrequent\\tprediction\\t(you\\tcan\\tuse\\tSciPy’s\\t\\nmode()\\n\\tfunction\\tfor\\nthis).\\tThis\\tgives\\tyou\\t\\nmajority-vote\\tpredictions\\n\\tover\\tthe\\ttest\\tset.\\nd\\n.\\t\\nEvaluate\\tthese\\tpredictions\\ton\\tthe\\ttest\\tset:\\tyou\\tshould\\tobtain\\ta\\tslightly\\thigher\\taccuracy\\tthan\\tyour\\nfirst\\tmodel\\t(about\\t0.5\\tto\\t1.5%\\thigher).\\tCongratulations,\\tyou\\thave\\ttrained\\ta\\tRandom\\tForest\\nclassifier!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 229}), Document(page_content='Solutions\\tto\\tthese\\texercises\\tare\\t\\navailable\\tin\\t\\nAppendix\\tA\\n.\\nGraphviz\\tis\\tan\\topen\\tsource\\tgraph\\tvisualization\\tsoftware\\tpackage,\\tavailable\\tat\\t\\nhttp://www.graphviz.org/\\n.\\nP\\tis\\tthe\\tset\\tof\\tproblems\\tthat\\tcan\\tbe\\tsolved\\tin\\tpolynomial\\ttime.\\tNP\\tis\\tthe\\tset\\tof\\tproblems\\twhose\\tsolutions\\tcan\\tbe\\tverified\\tin\\tpolynomial\\ttime.\\nAn\\tNP-Hard\\tproblem\\tis\\ta\\tproblem\\tto\\twhich\\tany\\tNP\\tproblem\\tcan\\tbe\\treduced\\tin\\tpolynomial\\ttime.\\tAn\\tNP-Complete\\tproblem\\tis\\tboth\\tNP\\tand\\nNP-Hard.\\tA\\tmajor\\topen\\tmathematical\\tquestion\\tis\\twhether\\tor\\tnot\\tP\\t=\\tNP.\\tIf\\tP\\t≠\\tNP\\t(which\\tseems\\tlikely),\\tthen\\tno\\tpolynomial\\talgorithm\\twill\\never\\tbe\\tfound\\tfor\\tany\\tNP-Complete\\tproblem\\t(except\\tperhaps\\ton\\ta\\tquantum\\tcomputer).\\nlog\\n2\\n\\tis\\tthe\\tbinary\\tlogarithm.\\tIt\\tis\\tequal\\tto\\t\\nlog\\n2\\n(\\nm\\n)\\t=\\t\\nlog\\n(\\nm\\n)\\t/\\t\\nlog\\n(2).\\nA\\treduction\\tof\\tentropy\\tis\\toften\\tcalled\\tan\\t\\ninformation\\tgain\\n.\\nSee\\tSebastian\\tRaschka’s\\t\\ninteresting\\tanalysis\\tfor\\tmore\\tdetails\\n.\\nIt\\trandomly\\tselects\\tthe\\tset\\tof\\tfeatures\\tto\\tevaluate\\tat\\teach\\tnode.\\n1\\n2\\n3\\n4\\n5\\n6', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 230}), Document(page_content='Chapter\\t7.\\t\\nEnsemble\\tLearning\\tand\\tRandom\\nForests\\nSuppose\\t\\nyou\\task\\ta\\tcomplex\\tquestion\\tto\\tthousands\\tof\\trandom\\tpeople,\\tthen\\taggregate\\ttheir\\tanswers.\\tIn\\nmany\\tcases\\tyou\\twill\\tfind\\tthat\\tthis\\taggregated\\tanswer\\tis\\tbetter\\tthan\\tan\\texpert’s\\tanswer.\\tThis\\tis\\tcalled\\tthe\\nwisdom\\tof\\tthe\\tcrowd\\n.\\tSimilarly,\\tif\\tyou\\taggregate\\tthe\\tpredictions\\tof\\ta\\tgroup\\tof\\tpredictors\\t(such\\tas\\nclassifiers\\tor\\tregressors),\\tyou\\twill\\toften\\tget\\tbetter\\tpredictions\\tthan\\twith\\tthe\\tbest\\tindividual\\tpredictor.\\tA\\ngroup\\tof\\tpredictors\\tis\\tcalled\\tan\\t\\nensemble\\n;\\tthus,\\tthis\\ttechnique\\tis\\tcalled\\t\\nEnsemble\\tLearning\\n,\\tand\\tan\\nEnsemble\\tLearning\\talgorithm\\tis\\tcalled\\tan\\t\\nEnsemble\\tmethod\\n.\\nFor\\texample,\\tyou\\tcan\\ttrain\\ta\\tgroup\\tof\\t\\nDecision\\tTree\\tclassifiers,\\teach\\ton\\ta\\tdifferent\\trandom\\tsubset\\tof\\tthe\\ntraining\\tset.\\tTo\\tmake\\tpredictions,\\tyou\\tjust\\tobtain\\tthe\\tpredictions\\tof\\tall\\tindividual\\ttrees,\\tthen\\tpredict\\tthe\\nclass\\tthat\\tgets\\tthe\\tmost\\tvotes\\t(see\\tthe\\tlast\\texercise\\tin\\t\\nChapter\\t6\\n).\\tSuch\\tan\\tensemble\\tof\\tDecision\\tTrees\\tis\\ncalled\\ta\\t\\nRandom\\tForest\\n,\\t\\nand\\tdespite\\tits\\tsimplicity,\\tthis\\tis\\tone\\tof\\tthe\\tmost\\tpowerful\\tMachine\\tLearning\\nalgorithms\\tavailable\\ttoday.\\nMoreover,\\tas\\twe\\tdiscussed\\tin\\t\\nChapter\\t2\\n,\\tyou\\twill\\toften\\tuse\\tEnsemble\\tmethods\\tnear\\tthe\\tend\\tof\\ta\\tproject,\\nonce\\tyou\\thave\\talready\\tbuilt\\ta\\tfew\\tgood\\tpredictors,\\tto\\tcombine\\tthem\\tinto\\tan\\teven\\tbetter\\tpredictor.\\tIn\\tfact,\\nthe\\twinning\\tsolutions\\tin\\tMachine\\tLearning\\tcompetitions\\toften\\tinvolve\\tseveral\\tEnsemble\\tmethods\\t(most\\nfamously\\tin\\tthe\\t\\nNetflix\\tPrize\\tcompetition\\n).\\nIn\\tthis\\tchapter\\twe\\twill\\tdiscuss\\tthe\\tmost\\tpopular\\tEnsemble\\tmethods,\\tincluding\\t\\nbagging\\n,\\t\\nboosting\\n,\\nstacking\\n,\\tand\\ta\\tfew\\tothers.\\tWe\\twill\\talso\\texplore\\tRandom\\tForests.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 231}), Document(page_content='Voting\\tClassifiers\\nSuppose\\t\\nyou\\thave\\ttrained\\ta\\tfew\\tclassifiers,\\teach\\tone\\tachieving\\tabout\\t80%\\taccuracy.\\tYou\\tmay\\thave\\ta\\nLogistic\\tRegression\\tclassifier,\\tan\\tSVM\\tclassifier,\\ta\\tRandom\\tForest\\tclassifier,\\ta\\tK-Nearest\\tNeighbors\\nclassifier,\\tand\\tperhaps\\ta\\tfew\\tmore\\t(see\\t\\nFigure\\t7-1\\n).\\nFigure\\t7-1.\\t\\nTraining\\tdiverse\\tclassifiers\\nA\\tvery\\tsimple\\tway\\tto\\tcreate\\tan\\teven\\tbetter\\tclassifier\\tis\\tto\\taggregate\\tthe\\tpredictions\\tof\\teach\\tclassifier\\tand\\npredict\\tthe\\tclass\\tthat\\tgets\\tthe\\tmost\\tvotes.\\tThis\\tmajority-vote\\tclassifier\\tis\\tcalled\\ta\\t\\nhard\\tvoting\\n\\tclassifier\\n(see\\t\\nFigure\\t7-2\\n).\\nFigure\\t7-2.\\t\\nHard\\tvoting\\tclassifier\\tpredictions\\nSomewhat\\tsurprisingly,\\tthis\\tvoting\\tclassifier\\toften\\tachieves\\ta\\thigher\\taccuracy\\tthan\\tthe\\tbest\\tclassifier\\tin\\nthe\\tensemble.\\tIn\\tfact,\\teven\\tif\\teach\\tclassifier\\tis\\ta\\t\\nweak\\tlearner\\n\\t\\n(meaning\\tit\\tdoes\\tonly\\tslightly\\tbetter\\tthan\\nrandom\\tguessing),\\tthe\\tensemble\\tcan\\tstill\\tbe\\ta\\t\\nstrong\\tlearner\\n\\t(achieving\\thigh\\taccuracy),\\t\\nprovided\\tthere\\nare\\ta\\tsufficient\\tnumber\\tof\\tweak\\tlearners\\tand\\tthey\\tare\\tsufficiently\\tdiverse.\\nHow\\tis\\tthis\\tpossible?\\tThe\\tfollowing\\tanalogy\\tcan\\thelp\\tshed\\tsome\\tlight\\ton\\tthis\\tmystery.\\tSuppose\\tyou\\thave', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 232}), Document(page_content=\"a\\tslightly\\tbiased\\tcoin\\tthat\\thas\\ta\\t51%\\tchance\\tof\\tcoming\\tup\\theads,\\tand\\t49%\\tchance\\tof\\tcoming\\tup\\ttails.\\tIf\\nyou\\ttoss\\tit\\t1,000\\ttimes,\\tyou\\twill\\tgenerally\\tget\\tmore\\tor\\tless\\t510\\theads\\tand\\t490\\ttails,\\tand\\thence\\ta\\tmajority\\nof\\theads.\\tIf\\tyou\\tdo\\tthe\\tmath,\\tyou\\twill\\tfind\\tthat\\tthe\\tprobability\\tof\\tobtaining\\ta\\tmajority\\tof\\theads\\tafter\\t1,000\\ntosses\\tis\\tclose\\tto\\t75%.\\tThe\\tmore\\tyou\\ttoss\\tthe\\tcoin,\\tthe\\thigher\\tthe\\tprobability\\t(e.g.,\\twith\\t10,000\\ttosses,\\tthe\\nprobability\\tclimbs\\tover\\t97%).\\tThis\\tis\\tdue\\tto\\tthe\\t\\nlaw\\tof\\tlarge\\tnumbers\\n:\\t\\nas\\tyou\\tkeep\\ttossing\\tthe\\tcoin,\\tthe\\nratio\\tof\\theads\\tgets\\tcloser\\tand\\tcloser\\tto\\tthe\\tprobability\\tof\\theads\\t(51%).\\t\\nFigure\\t7-3\\n\\tshows\\t10\\tseries\\tof\\nbiased\\tcoin\\ttosses.\\tYou\\tcan\\tsee\\tthat\\tas\\tthe\\tnumber\\tof\\ttosses\\tincreases,\\tthe\\tratio\\tof\\theads\\tapproaches\\t51%.\\nEventually\\tall\\t10\\tseries\\tend\\tup\\tso\\tclose\\tto\\t51%\\tthat\\tthey\\tare\\tconsistently\\tabove\\t50%.\\nFigure\\t7-3.\\t\\nThe\\tlaw\\tof\\tlarge\\tnumbers\\nSimilarly,\\tsuppose\\tyou\\tbuild\\tan\\tensemble\\tcontaining\\t1,000\\tclassifiers\\tthat\\tare\\tindividually\\tcorrect\\tonly\\n51%\\tof\\tthe\\ttime\\t(barely\\tbetter\\tthan\\trandom\\tguessing).\\tIf\\tyou\\tpredict\\tthe\\tmajority\\tvoted\\tclass,\\tyou\\tcan\\nhope\\tfor\\tup\\tto\\t75%\\taccuracy!\\tHowever,\\tthis\\tis\\tonly\\ttrue\\tif\\tall\\tclassifiers\\tare\\tperfectly\\tindependent,\\nmaking\\tuncorrelated\\terrors,\\twhich\\tis\\tclearly\\tnot\\tthe\\tcase\\tsince\\tthey\\tare\\ttrained\\ton\\tthe\\tsame\\tdata.\\tThey\\tare\\nlikely\\tto\\tmake\\tthe\\tsame\\ttypes\\tof\\terrors,\\tso\\tthere\\twill\\tbe\\tmany\\tmajority\\tvotes\\tfor\\tthe\\twrong\\tclass,\\treducing\\nthe\\tensemble’s\\taccuracy.\\nTIP\\nEnsemble\\tmethods\\twork\\tbest\\twhen\\tthe\\tpredictors\\tare\\tas\\tindependent\\tfrom\\tone\\tanother\\tas\\tpossible.\\tOne\\tway\\tto\\tget\\tdiverse\\nclassifiers\\tis\\tto\\ttrain\\tthem\\tusing\\tvery\\tdifferent\\talgorithms.\\tThis\\tincreases\\tthe\\tchance\\tthat\\tthey\\twill\\tmake\\tvery\\tdifferent\\ttypes\\tof\\nerrors,\\timproving\\tthe\\tensemble’s\\taccuracy.\\nThe\\tfollowing\\tcode\\tcreates\\tand\\ttrains\\ta\\tvoting\\tclassifier\\tin\\tScikit-Learn,\\tcomposed\\tof\\tthree\\tdiverse\\nclassifiers\\t(the\\ttraining\\tset\\tis\\tthe\\tmoons\\tdataset,\\t\\nintroduced\\tin\\t\\nChapter\\t5\\n):\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nRandomForestClassifier\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nVotingClassifier\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLogisticRegression\\nfrom\\n\\t\\nsklearn.svm\\n\\t\\nimport\\n\\t\\nSVC\\nlog_clf\\n\\t\\n=\\n\\t\\nLogisticRegression\\n()\\nrnd_clf\\n\\t\\n=\\n\\t\\nRandomForestClassifier\\n()\\nsvm_clf\\n\\t\\n=\\n\\t\\nSVC\\n()\\nvoting_clf\\n\\t\\n=\\n\\t\\nVotingClassifier\\n(\\n\\t\\t\\t\\t\\nestimators\\n=\\n[(\\n'lr'\\n,\\n\\t\\nlog_clf\\n),\\n\\t\\n(\\n'rf'\\n,\\n\\t\\nrnd_clf\\n),\\n\\t\\n(\\n'svc'\\n,\\n\\t\\nsvm_clf\\n)],\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 233}), Document(page_content='\\t\\t\\t\\t\\nvoting\\n=\\n\\'hard\\'\\n)\\nvoting_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\nLet’s\\tlook\\tat\\teach\\tclassifier’s\\taccuracy\\t\\non\\tthe\\ttest\\tset:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\naccuracy_score\\n>>>\\t\\nfor\\n\\t\\nclf\\n\\t\\nin\\n\\t\\n(\\nlog_clf\\n,\\n\\t\\nrnd_clf\\n,\\n\\t\\nsvm_clf\\n,\\n\\t\\nvoting_clf\\n):\\n...\\t\\n\\t\\t\\t\\t\\nclf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n...\\t\\n\\t\\t\\t\\t\\ny_pred\\n\\t\\n=\\n\\t\\nclf\\n.\\npredict\\n(\\nX_test\\n)\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\nclf\\n.\\n__class__\\n.\\n__name__\\n,\\n\\t\\naccuracy_score\\n(\\ny_test\\n,\\n\\t\\ny_pred\\n))\\n...\\nLogisticRegression\\t0.864\\nRandomForestClassifier\\t0.872\\nSVC\\t0.888\\nVotingClassifier\\t0.896\\nThere\\tyou\\thave\\tit!\\tThe\\tvoting\\tclassifier\\tslightly\\toutperforms\\tall\\tthe\\tindividual\\tclassifiers.\\nIf\\tall\\tclassifiers\\tare\\table\\tto\\testimate\\tclass\\tprobabilities\\t(i.e.,\\tthey\\thave\\ta\\t\\npredict_proba()\\n\\tmethod),\\nthen\\tyou\\tcan\\ttell\\tScikit-Learn\\tto\\tpredict\\tthe\\tclass\\twith\\tthe\\thighest\\tclass\\tprobability,\\taveraged\\tover\\tall\\tthe\\nindividual\\tclassifiers.\\tThis\\tis\\t\\ncalled\\t\\nsoft\\tvoting\\n.\\tIt\\toften\\tachieves\\thigher\\tperformance\\tthan\\thard\\tvoting\\nbecause\\tit\\tgives\\tmore\\tweight\\tto\\thighly\\tconfident\\tvotes.\\tAll\\tyou\\tneed\\tto\\tdo\\tis\\treplace\\t\\nvoting=\"hard\"\\nwith\\t\\nvoting=\"soft\"\\n\\tand\\tensure\\tthat\\tall\\tclassifiers\\tcan\\testimate\\tclass\\tprobabilities.\\tThis\\tis\\tnot\\tthe\\tcase\\nof\\tthe\\t\\nSVC\\n\\tclass\\tby\\tdefault,\\tso\\tyou\\tneed\\tto\\tset\\tits\\t\\nprobability\\n\\thyperparameter\\tto\\t\\nTrue\\n\\t(this\\twill\\tmake\\nthe\\t\\nSVC\\n\\tclass\\tuse\\tcross-validation\\tto\\testimate\\tclass\\tprobabilities,\\tslowing\\tdown\\ttraining,\\tand\\tit\\twill\\tadd\\na\\t\\npredict_proba()\\n\\tmethod).\\tIf\\tyou\\tmodify\\tthe\\tpreceding\\tcode\\tto\\tuse\\tsoft\\tvoting,\\tyou\\twill\\tfind\\tthat\\tthe\\nvoting\\tclassifier\\tachieves\\tover\\t91%\\t\\naccuracy!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 234}), Document(page_content='Bagging\\tand\\tPasting\\nOne\\t\\nway\\tto\\tget\\ta\\tdiverse\\tset\\tof\\tclassifiers\\tis\\tto\\tuse\\tvery\\tdifferent\\ttraining\\talgorithms,\\tas\\tjust\\tdiscussed.\\nAnother\\tapproach\\tis\\tto\\tuse\\tthe\\tsame\\ttraining\\talgorithm\\tfor\\tevery\\tpredictor,\\tbut\\tto\\ttrain\\tthem\\ton\\tdifferent\\nrandom\\tsubsets\\tof\\tthe\\ttraining\\tset.\\tWhen\\tsampling\\tis\\tperformed\\t\\nwith\\n\\treplacement,\\tthis\\tmethod\\tis\\tcalled\\nbagging\\n1\\n\\t(short\\tfor\\t\\nbootstrap\\taggregating\\n2\\n).\\tWhen\\tsampling\\tis\\tperformed\\t\\nwithout\\n\\treplacement,\\tit\\tis\\ncalled\\t\\npasting\\n.\\n3\\nIn\\tother\\twords,\\tboth\\tbagging\\tand\\tpasting\\tallow\\ttraining\\tinstances\\tto\\tbe\\tsampled\\tseveral\\ttimes\\tacross\\nmultiple\\tpredictors,\\tbut\\tonly\\tbagging\\tallows\\ttraining\\tinstances\\tto\\tbe\\tsampled\\tseveral\\ttimes\\tfor\\tthe\\tsame\\npredictor.\\tThis\\tsampling\\tand\\ttraining\\tprocess\\tis\\trepresented\\tin\\t\\nFigure\\t7-4\\n.\\nFigure\\t7-4.\\t\\nPasting/bagging\\ttraining\\tset\\tsampling\\tand\\ttraining\\nOnce\\tall\\tpredictors\\tare\\ttrained,\\tthe\\tensemble\\tcan\\tmake\\ta\\tprediction\\tfor\\ta\\tnew\\tinstance\\tby\\tsimply\\naggregating\\tthe\\tpredictions\\tof\\tall\\tpredictors.\\tThe\\taggregation\\tfunction\\tis\\ttypically\\t\\nthe\\t\\nstatistical\\tmode\\n(i.e.,\\tthe\\tmost\\tfrequent\\tprediction,\\tjust\\tlike\\ta\\thard\\tvoting\\tclassifier)\\tfor\\tclassification,\\tor\\tthe\\taverage\\tfor\\nregression.\\tEach\\tindividual\\tpredictor\\thas\\ta\\thigher\\tbias\\tthan\\tif\\tit\\twere\\ttrained\\ton\\tthe\\toriginal\\ttraining\\tset,\\nbut\\taggregation\\treduces\\tboth\\tbias\\tand\\tvariance.\\n4\\n\\tGenerally,\\tthe\\tnet\\tresult\\tis\\tthat\\tthe\\tensemble\\thas\\ta\\nsimilar\\tbias\\tbut\\ta\\tlower\\tvariance\\tthan\\ta\\tsingle\\tpredictor\\ttrained\\ton\\tthe\\toriginal\\ttraining\\tset.\\nAs\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t7-4\\n,\\tpredictors\\tcan\\tall\\tbe\\ttrained\\tin\\tparallel,\\tvia\\tdifferent\\tCPU\\tcores\\tor\\teven\\ndifferent\\tservers.\\tSimilarly,\\tpredictions\\tcan\\tbe\\tmade\\tin\\tparallel.\\tThis\\tis\\tone\\tof\\tthe\\treasons\\twhy\\tbagging\\nand\\tpasting\\tare\\tsuch\\tpopular\\tmethods:\\tthey\\tscale\\tvery\\twell.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 235}), Document(page_content='Bagging\\tand\\tPasting\\tin\\tScikit-Learn\\nScikit-Learn\\t\\noffers\\ta\\tsimple\\tAPI\\tfor\\tboth\\tbagging\\tand\\tpasting\\twith\\tthe\\t\\nBaggingClassifier\\n\\tclass\\t(or\\nBaggingRegressor\\n\\tfor\\tregression).\\tThe\\tfollowing\\tcode\\ttrains\\tan\\tensemble\\tof\\t\\n500\\tDecision\\tTree\\nclassifiers,\\n5\\n\\teach\\ttrained\\ton\\t100\\ttraining\\tinstances\\trandomly\\tsampled\\tfrom\\tthe\\ttraining\\tset\\twith\\nreplacement\\t(this\\tis\\tan\\texample\\tof\\tbagging,\\tbut\\tif\\tyou\\twant\\tto\\tuse\\tpasting\\tinstead,\\tjust\\tset\\nbootstrap=False\\n).\\tThe\\t\\nn_jobs\\n\\tparameter\\ttells\\tScikit-Learn\\tthe\\tnumber\\tof\\tCPU\\tcores\\tto\\tuse\\tfor\\ttraining\\nand\\tpredictions\\t(\\n–1\\n\\ttells\\tScikit-Learn\\tto\\tuse\\tall\\tavailable\\tcores):\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nBaggingClassifier\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nDecisionTreeClassifier\\nbag_clf\\n\\t\\n=\\n\\t\\nBaggingClassifier\\n(\\n\\t\\t\\t\\t\\nDecisionTreeClassifier\\n(),\\n\\t\\nn_estimators\\n=\\n500\\n,\\n\\t\\t\\t\\t\\nmax_samples\\n=\\n100\\n,\\n\\t\\nbootstrap\\n=\\nTrue\\n,\\n\\t\\nn_jobs\\n=-\\n1\\n)\\nbag_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\ny_pred\\n\\t\\n=\\n\\t\\nbag_clf\\n.\\npredict\\n(\\nX_test\\n)\\nNOTE\\nThe\\t\\nBaggingClassifier\\n\\tautomatically\\tperforms\\tsoft\\tvoting\\tinstead\\tof\\thard\\tvoting\\tif\\tthe\\tbase\\tclassifier\\tcan\\testimate\\tclass\\nprobabilities\\t(i.e.,\\tif\\tit\\thas\\ta\\t\\npredict_proba()\\n\\tmethod),\\twhich\\tis\\tthe\\tcase\\twith\\tDecision\\tTrees\\tclassifiers.\\nFigure\\t7-5\\n\\tcompares\\tthe\\tdecision\\tboundary\\tof\\ta\\tsingle\\tDecision\\tTree\\twith\\tthe\\tdecision\\tboundary\\tof\\ta\\nbagging\\tensemble\\tof\\t500\\ttrees\\t(from\\tthe\\tpreceding\\tcode),\\tboth\\ttrained\\ton\\tthe\\tmoons\\tdataset.\\tAs\\tyou\\tcan\\nsee,\\tthe\\tensemble’s\\tpredictions\\twill\\tlikely\\tgeneralize\\tmuch\\tbetter\\tthan\\tthe\\tsingle\\tDecision\\tTree’s\\npredictions:\\tthe\\tensemble\\thas\\ta\\tcomparable\\tbias\\tbut\\ta\\tsmaller\\tvariance\\t(it\\tmakes\\troughly\\tthe\\tsame\\nnumber\\tof\\terrors\\ton\\tthe\\ttraining\\tset,\\tbut\\tthe\\tdecision\\tboundary\\tis\\tless\\tirregular).\\nFigure\\t7-5.\\t\\nA\\tsingle\\tDecision\\tTree\\tversus\\ta\\tbagging\\tensemble\\tof\\t500\\ttrees\\nBootstrapping\\tintroduces\\ta\\tbit\\tmore\\tdiversity\\tin\\tthe\\tsubsets\\tthat\\teach\\tpredictor\\tis\\ttrained\\ton,\\tso\\tbagging\\nends\\tup\\twith\\ta\\tslightly\\thigher\\tbias\\tthan\\tpasting,\\tbut\\tthis\\talso\\tmeans\\tthat\\tpredictors\\tend\\tup\\tbeing\\tless\\ncorrelated\\tso\\tthe\\tensemble’s\\tvariance\\tis\\treduced.\\tOverall,\\tbagging\\toften\\tresults\\tin\\tbetter\\tmodels,\\twhich\\nexplains\\twhy\\tit\\tis\\tgenerally\\tpreferred.\\tHowever,\\tif\\tyou\\thave\\tspare\\ttime\\tand\\tCPU\\tpower\\tyou\\tcan\\tuse\\ncross-validation\\tto\\tevaluate\\tboth\\tbagging\\tand\\tpasting\\tand\\tselect\\tthe\\tone\\tthat\\t\\nworks\\tbest.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 236}), Document(page_content='Out-of-Bag\\tEvaluation\\nWith\\t\\nbagging,\\tsome\\tinstances\\tmay\\tbe\\tsampled\\tseveral\\ttimes\\tfor\\tany\\tgiven\\tpredictor,\\twhile\\tothers\\tmay\\tnot\\nbe\\tsampled\\tat\\tall.\\tBy\\tdefault\\ta\\t\\nBaggingClassifier\\n\\tsamples\\t\\nm\\n\\ttraining\\tinstances\\twith\\treplacement\\n(\\nbootstrap=True\\n),\\twhere\\t\\nm\\n\\tis\\tthe\\tsize\\tof\\tthe\\ttraining\\tset.\\tThis\\tmeans\\tthat\\tonly\\tabout\\t63%\\tof\\tthe\\ttraining\\ninstances\\tare\\tsampled\\ton\\taverage\\tfor\\teach\\tpredictor.\\n6\\n\\tThe\\tremaining\\t37%\\tof\\tthe\\ttraining\\tinstances\\tthat\\tare\\nnot\\tsampled\\tare\\tcalled\\t\\nout-of-bag\\n\\t(oob)\\tinstances.\\tNote\\tthat\\tthey\\tare\\tnot\\tthe\\tsame\\t37%\\tfor\\tall\\tpredictors.\\nSince\\ta\\tpredictor\\tnever\\tsees\\tthe\\toob\\tinstances\\tduring\\ttraining,\\tit\\tcan\\tbe\\tevaluated\\ton\\tthese\\tinstances,\\nwithout\\tthe\\tneed\\tfor\\ta\\tseparate\\tvalidation\\tset\\tor\\tcross-validation.\\tYou\\tcan\\tevaluate\\tthe\\tensemble\\titself\\tby\\naveraging\\tout\\tthe\\toob\\tevaluations\\tof\\teach\\tpredictor.\\nIn\\tScikit-Learn,\\tyou\\tcan\\tset\\t\\noob_score=True\\n\\twhen\\tcreating\\ta\\t\\nBaggingClassifier\\n\\tto\\trequest\\tan\\nautomatic\\toob\\tevaluation\\tafter\\ttraining.\\tThe\\tfollowing\\tcode\\tdemonstrates\\tthis.\\tThe\\tresulting\\tevaluation\\nscore\\tis\\tavailable\\t\\nthrough\\tthe\\t\\noob_score_\\n\\tvariable:\\n>>>\\t\\nbag_clf\\n\\t\\n=\\n\\t\\nBaggingClassifier\\n(\\n...\\t\\n\\t\\t\\t\\t\\nDecisionTreeClassifier\\n(),\\n\\t\\nn_estimators\\n=\\n500\\n,\\n...\\t\\n\\t\\t\\t\\t\\nbootstrap\\n=\\nTrue\\n,\\n\\t\\nn_jobs\\n=-\\n1\\n,\\n\\t\\noob_score\\n=\\nTrue\\n)\\n...\\n>>>\\t\\nbag_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n>>>\\t\\nbag_clf\\n.\\noob_score_\\n0.90133333333333332\\nAccording\\tto\\tthis\\toob\\tevaluation,\\tthis\\t\\nBaggingClassifier\\n\\tis\\tlikely\\tto\\tachieve\\tabout\\t90.1%\\taccuracy\\t\\non\\nthe\\ttest\\tset.\\tLet’s\\tverify\\tthis:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\naccuracy_score\\n>>>\\t\\ny_pred\\n\\t\\n=\\n\\t\\nbag_clf\\n.\\npredict\\n(\\nX_test\\n)\\n>>>\\t\\naccuracy_score\\n(\\ny_test\\n,\\n\\t\\ny_pred\\n)\\n0.91200000000000003\\nWe\\tget\\t91.2%\\taccuracy\\ton\\tthe\\ttest\\tset\\t—\\tclose\\tenough!\\nThe\\toob\\tdecision\\tfunction\\tfor\\teach\\ttraining\\tinstance\\tis\\talso\\tavailable\\tthrough\\tthe\\noob_decision_function_\\n\\tvariable.\\tIn\\tthis\\tcase\\t(since\\tthe\\tbase\\testimator\\thas\\ta\\t\\npredict_proba()\\nmethod)\\tthe\\tdecision\\tfunction\\treturns\\tthe\\tclass\\tprobabilities\\tfor\\teach\\ttraining\\tinstance.\\tFor\\texample,\\tthe\\noob\\tevaluation\\testimates\\tthat\\tthe\\tsecond\\ttraining\\tinstance\\thas\\ta\\t60.6%\\tprobability\\tof\\tbelonging\\tto\\tthe\\npositive\\t\\nclass\\t(and\\t39.4%\\tof\\tbelonging\\tto\\tthe\\tpositive\\t\\nclass):\\n>>>\\t\\nbag_clf\\n.\\noob_decision_function_\\narray([[\\t0.31746032,\\t\\t0.68253968],\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.34117647,\\t\\t0.65882353],\\n\\t\\t\\t\\t\\t\\t\\t[\\t1.\\t\\t\\t\\t\\t\\t\\t\\t,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t],\\n\\t\\t\\t\\t\\t\\t\\t...\\n\\t\\t\\t\\t\\t\\t\\t[\\t1.\\t\\t\\t\\t\\t\\t\\t\\t,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t],\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.03108808,\\t\\t0.96891192],\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.57291667,\\t\\t0.42708333]])', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 237}), Document(page_content='Random\\tPatches\\tand\\tRandom\\tSubspaces\\nThe\\t\\nBaggingClassifier\\n\\t\\nclass\\tsupports\\tsampling\\tthe\\tfeatures\\tas\\twell.\\tThis\\tis\\tcontrolled\\tby\\ttwo\\nhyperparameters:\\t\\nmax_features\\n\\tand\\t\\nbootstrap_features\\n.\\tThey\\twork\\tthe\\tsame\\tway\\tas\\t\\nmax_samples\\nand\\t\\nbootstrap\\n,\\tbut\\tfor\\tfeature\\tsampling\\tinstead\\tof\\tinstance\\tsampling.\\tThus,\\teach\\tpredictor\\twill\\tbe\\ntrained\\ton\\ta\\trandom\\tsubset\\tof\\tthe\\tinput\\tfeatures.\\nThis\\tis\\tparticularly\\tuseful\\twhen\\tyou\\tare\\tdealing\\twith\\thigh-dimensional\\tinputs\\t(such\\tas\\timages).\\tSampling\\nboth\\ttraining\\tinstances\\tand\\tfeatures\\tis\\tcalled\\tthe\\t\\nRandom\\tPatches\\n\\tmethod\\n.\\n7\\n\\tKeeping\\tall\\ttraining\\tinstances\\n(i.e.,\\t\\nbootstrap=False\\n\\tand\\t\\nmax_samples=1.0\\n)\\tbut\\tsampling\\tfeatures\\t(i.e.,\\t\\nbootstrap_features=True\\nand/or\\t\\nmax_features\\n\\tsmaller\\tthan\\t1.0)\\tis\\tcalled\\tthe\\t\\nRandom\\tSubspaces\\n\\tmethod\\n.\\n8\\nSampling\\tfeatures\\tresults\\tin\\teven\\tmore\\tpredictor\\tdiversity,\\ttrading\\ta\\tbit\\tmore\\tbias\\tfor\\ta\\tlower\\tvariance.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 238}), Document(page_content='Random\\tForests\\nAs\\t\\nwe\\thave\\tdiscussed,\\ta\\t\\nRandom\\tForest\\n9\\n\\tis\\tan\\tensemble\\tof\\tDecision\\tTrees,\\tgenerally\\ttrained\\tvia\\tthe\\nbagging\\tmethod\\t(or\\tsometimes\\tpasting),\\ttypically\\twith\\t\\nmax_samples\\n\\tset\\tto\\tthe\\tsize\\tof\\tthe\\ttraining\\tset.\\nInstead\\tof\\tbuilding\\ta\\t\\nBaggingClassifier\\n\\tand\\tpassing\\tit\\ta\\t\\nDecisionTreeClassifier\\n,\\tyou\\tcan\\tinstead\\nuse\\tthe\\t\\nRandomForestClassifier\\n\\tclass,\\twhich\\tis\\tmore\\tconvenient\\tand\\toptimized\\tfor\\tDecision\\tTrees\\n10\\n(similarly,\\tthere\\tis\\ta\\t\\nRandomForestRegressor\\n\\tclass\\tfor\\tregression\\ttasks).\\tThe\\tfollowing\\tcode\\ttrains\\ta\\nRandom\\tForest\\tclassifier\\twith\\t500\\ttrees\\t(each\\tlimited\\tto\\tmaximum\\t16\\tnodes),\\tusing\\tall\\tavailable\\tCPU\\ncores:\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nRandomForestClassifier\\nrnd_clf\\n\\t\\n=\\n\\t\\nRandomForestClassifier\\n(\\nn_estimators\\n=\\n500\\n,\\n\\t\\nmax_leaf_nodes\\n=\\n16\\n,\\n\\t\\nn_jobs\\n=-\\n1\\n)\\nrnd_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\ny_pred_rf\\n\\t\\n=\\n\\t\\nrnd_clf\\n.\\npredict\\n(\\nX_test\\n)\\nWith\\ta\\tfew\\texceptions,\\ta\\t\\nRandomForestClassifier\\n\\thas\\tall\\tthe\\thyperparameters\\tof\\ta\\nDecisionTreeClassifier\\n\\t(to\\tcontrol\\thow\\ttrees\\tare\\tgrown),\\tplus\\tall\\tthe\\thyperparameters\\tof\\ta\\nBaggingClassifier\\n\\tto\\tcontrol\\tthe\\tensemble\\titself.\\n11\\nThe\\tRandom\\tForest\\talgorithm\\tintroduces\\textra\\trandomness\\twhen\\tgrowing\\ttrees;\\tinstead\\tof\\tsearching\\tfor\\nthe\\tvery\\tbest\\tfeature\\twhen\\tsplitting\\ta\\tnode\\t(see\\t\\nChapter\\t6\\n),\\tit\\tsearches\\tfor\\tthe\\tbest\\tfeature\\tamong\\ta\\nrandom\\tsubset\\tof\\tfeatures.\\tThis\\tresults\\tin\\ta\\tgreater\\ttree\\tdiversity,\\twhich\\t(once\\tagain)\\ttrades\\ta\\thigher\\tbias\\nfor\\ta\\tlower\\tvariance,\\tgenerally\\tyielding\\tan\\toverall\\tbetter\\tmodel.\\tThe\\tfollowing\\t\\nBaggingClassifier\\n\\tis\\nroughly\\tequivalent\\tto\\tthe\\tprevious\\t\\nRandomForestClassifier\\n:\\nbag_clf\\n\\t\\n=\\n\\t\\nBaggingClassifier\\n(\\n\\t\\t\\t\\t\\nDecisionTreeClassifier\\n(\\nsplitter\\n=\\n\"random\"\\n,\\n\\t\\nmax_leaf_nodes\\n=\\n16\\n),\\n\\t\\t\\t\\t\\nn_estimators\\n=\\n500\\n,\\n\\t\\nmax_samples\\n=\\n1.0\\n,\\n\\t\\nbootstrap\\n=\\nTrue\\n,\\n\\t\\nn_jobs\\n=-\\n1\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 239}), Document(page_content='Extra-Trees\\nWhen\\t\\nyou\\tare\\tgrowing\\ta\\ttree\\tin\\ta\\tRandom\\tForest,\\tat\\teach\\tnode\\tonly\\ta\\trandom\\tsubset\\tof\\tthe\\tfeatures\\tis\\nconsidered\\tfor\\tsplitting\\t(as\\tdiscussed\\tearlier).\\tIt\\tis\\tpossible\\tto\\tmake\\ttrees\\teven\\tmore\\trandom\\tby\\talso\\nusing\\trandom\\tthresholds\\tfor\\teach\\tfeature\\trather\\tthan\\tsearching\\tfor\\tthe\\tbest\\tpossible\\tthresholds\\t(like\\nregular\\tDecision\\tTrees\\tdo).\\nA\\tforest\\tof\\tsuch\\textremely\\trandom\\ttrees\\tis\\tsimply\\tcalled\\tan\\t\\nExtremely\\tRandomized\\tTrees\\n\\tensemble\\n12\\n\\t(or\\nExtra-Trees\\n\\tfor\\tshort).\\tOnce\\tagain,\\tthis\\ttrades\\tmore\\tbias\\tfor\\ta\\tlower\\tvariance.\\tIt\\talso\\tmakes\\tExtra-Trees\\nmuch\\tfaster\\tto\\ttrain\\tthan\\tregular\\tRandom\\tForests\\tsince\\tfinding\\tthe\\tbest\\tpossible\\tthreshold\\tfor\\teach\\tfeature\\nat\\tevery\\tnode\\tis\\tone\\tof\\tthe\\tmost\\ttime-consuming\\ttasks\\tof\\tgrowing\\ta\\ttree.\\nYou\\tcan\\tcreate\\tan\\tExtra-Trees\\tclassifier\\tusing\\tScikit-Learn’s\\t\\nExtraTreesClassifier\\n\\tclass.\\tIts\\tAPI\\tis\\nidentical\\tto\\t\\nthe\\t\\nRandomForestClassifier\\n\\tclass.\\tSimilarly,\\tthe\\t\\nExtraTreesRegressor\\n\\tclass\\thas\\tthe\\nsame\\tAPI\\tas\\tthe\\t\\nRandomForestRegressor\\n\\tclass.\\nTIP\\nIt\\tis\\thard\\tto\\ttell\\tin\\tadvance\\twhether\\ta\\t\\nRandomForestClassifier\\n\\twill\\tperform\\tbetter\\tor\\tworse\\tthan\\tan\\t\\nExtraTreesClassifier\\n.\\nGenerally,\\tthe\\tonly\\tway\\tto\\tknow\\tis\\tto\\ttry\\tboth\\tand\\tcompare\\tthem\\tusing\\tcross-validation\\t(and\\ttuning\\tthe\\thyperparameters\\tusing\\ngrid\\tsearch).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 240}), Document(page_content='Feature\\tImportance\\nYet\\tanother\\tgreat\\tquality\\tof\\tRandom\\tForests\\tis\\tthat\\tthey\\tmake\\tit\\teasy\\tto\\tmeasure\\tthe\\t\\nrelative\\timportance\\tof\\neach\\tfeature.\\tScikit-Learn\\tmeasures\\ta\\tfeature’s\\timportance\\tby\\tlooking\\tat\\thow\\tmuch\\tthe\\ttree\\tnodes\\tthat\\tuse\\nthat\\tfeature\\treduce\\timpurity\\ton\\taverage\\t(across\\tall\\ttrees\\tin\\tthe\\tforest).\\tMore\\tprecisely,\\tit\\tis\\ta\\tweighted\\naverage,\\twhere\\teach\\tnode’s\\tweight\\tis\\tequal\\tto\\tthe\\tnumber\\tof\\ttraining\\tsamples\\tthat\\tare\\tassociated\\twith\\tit\\n(see\\t\\nChapter\\t6\\n).\\nScikit-Learn\\tcomputes\\tthis\\tscore\\tautomatically\\tfor\\teach\\tfeature\\tafter\\ttraining,\\tthen\\tit\\tscales\\tthe\\tresults\\tso\\nthat\\tthe\\tsum\\tof\\tall\\timportances\\tis\\tequal\\tto\\t1.\\tYou\\tcan\\taccess\\tthe\\tresult\\tusing\\tthe\\t\\nfeature_importances_\\nvariable.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\ttrains\\ta\\t\\nRandomForestClassifier\\n\\ton\\tthe\\t\\niris\\tdataset\\n(introduced\\tin\\t\\nChapter\\t4\\n)\\tand\\toutputs\\teach\\tfeature’s\\timportance.\\tIt\\tseems\\tthat\\tthe\\tmost\\timportant\\tfeatures\\nare\\tthe\\tpetal\\tlength\\t(44%)\\tand\\twidth\\t(42%),\\twhile\\tsepal\\tlength\\tand\\twidth\\tare\\trather\\tunimportant\\tin\\ncomparison\\t(11%\\tand\\t2%,\\trespectively).\\n>>>\\t\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nload_iris\\n>>>\\t\\niris\\n\\t\\n=\\n\\t\\nload_iris\\n()\\n>>>\\t\\nrnd_clf\\n\\t\\n=\\n\\t\\nRandomForestClassifier\\n(\\nn_estimators\\n=\\n500\\n,\\n\\t\\nn_jobs\\n=-\\n1\\n)\\n>>>\\t\\nrnd_clf\\n.\\nfit\\n(\\niris\\n[\\n\"data\"\\n],\\n\\t\\niris\\n[\\n\"target\"\\n])\\n>>>\\t\\nfor\\n\\t\\nname\\n,\\n\\t\\nscore\\n\\t\\nin\\n\\t\\nzip\\n(\\niris\\n[\\n\"feature_names\"\\n],\\n\\t\\nrnd_clf\\n.\\nfeature_importances_\\n):\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\nname\\n,\\n\\t\\nscore\\n)\\n...\\nsepal\\tlength\\t(cm)\\t0.112492250999\\nsepal\\twidth\\t(cm)\\t0.0231192882825\\npetal\\tlength\\t(cm)\\t0.441030464364\\npetal\\twidth\\t(cm)\\t0.423357996355\\nSimilarly,\\tif\\tyou\\ttrain\\ta\\tRandom\\tForest\\tclassifier\\ton\\tthe\\tMNIST\\tdataset\\t(introduced\\tin\\t\\nChapter\\t3\\n)\\tand\\nplot\\teach\\tpixel’s\\timportance,\\tyou\\tget\\tthe\\timage\\trepresented\\tin\\t\\nFigure\\t7-6\\n.\\nFigure\\t7-6.\\t\\nMNIST\\tpixel\\timportance\\t(according\\tto\\ta\\tRandom\\tForest\\tclassifier)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 241}), Document(page_content='Random\\tForests\\tare\\tvery\\thandy\\tto\\tget\\ta\\tquick\\tunderstanding\\tof\\twhat\\tfeatures\\tactually\\tmatter,\\tin\\tparticular\\nif\\tyou\\tneed\\tto\\tperform\\t\\nfeature\\tselection.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 242}), Document(page_content='Boosting\\nBoosting\\n\\t\\n(originally\\tcalled\\t\\nhypothesis\\tboosting\\n)\\trefers\\tto\\tany\\tEnsemble\\tmethod\\tthat\\tcan\\tcombine\\tseveral\\nweak\\tlearners\\tinto\\ta\\tstrong\\tlearner.\\tThe\\tgeneral\\tidea\\tof\\tmost\\tboosting\\tmethods\\tis\\tto\\ttrain\\tpredictors\\nsequentially,\\teach\\ttrying\\tto\\tcorrect\\tits\\tpredecessor.\\tThere\\tare\\tmany\\tboosting\\tmethods\\tavailable,\\tbut\\tby\\tfar\\nthe\\tmost\\tpopular\\tare\\t\\nAdaBoost\\n13\\n\\t(short\\tfor\\t\\nAdaptive\\tBoosting\\n)\\tand\\t\\nGradient\\tBoosting\\n.\\tLet’s\\tstart\\twith\\nAdaBoost.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 243}), Document(page_content='AdaBoost\\nOne\\t\\nway\\tfor\\ta\\tnew\\tpredictor\\tto\\tcorrect\\tits\\tpredecessor\\tis\\tto\\tpay\\ta\\tbit\\tmore\\tattention\\tto\\tthe\\ttraining\\ninstances\\tthat\\tthe\\tpredecessor\\tunderfitted.\\tThis\\tresults\\tin\\tnew\\tpredictors\\tfocusing\\tmore\\tand\\tmore\\ton\\tthe\\nhard\\tcases.\\tThis\\tis\\tthe\\ttechnique\\tused\\tby\\tAdaBoost.\\nFor\\texample,\\tto\\tbuild\\tan\\tAdaBoost\\tclassifier,\\ta\\tfirst\\tbase\\tclassifier\\t(such\\tas\\ta\\tDecision\\tTree)\\tis\\ttrained\\nand\\tused\\tto\\tmake\\tpredictions\\ton\\tthe\\ttraining\\tset.\\tThe\\trelative\\tweight\\tof\\tmisclassified\\ttraining\\tinstances\\tis\\nthen\\tincreased.\\tA\\tsecond\\tclassifier\\tis\\ttrained\\tusing\\tthe\\tupdated\\tweights\\tand\\tagain\\tit\\tmakes\\tpredictions\\ton\\nthe\\ttraining\\tset,\\tweights\\tare\\tupdated,\\tand\\tso\\ton\\t(see\\t\\nFigure\\t7-7\\n).\\nFigure\\t7-7.\\t\\nAdaBoost\\tsequential\\ttraining\\twith\\tinstance\\tweight\\tupdates\\nFigure\\t7-8\\n\\tshows\\tthe\\tdecision\\tboundaries\\tof\\tfive\\tconsecutive\\tpredictors\\ton\\tthe\\tmoons\\tdataset\\t(in\\tthis\\nexample,\\teach\\tpredictor\\tis\\ta\\thighly\\tregularized\\tSVM\\tclassifier\\twith\\tan\\tRBF\\tkernel\\n14\\n).\\tThe\\tfirst\\tclassifier\\ngets\\tmany\\tinstances\\twrong,\\tso\\ttheir\\tweights\\tget\\tboosted.\\tThe\\tsecond\\tclassifier\\ttherefore\\tdoes\\ta\\tbetter\\tjob\\non\\tthese\\tinstances,\\tand\\tso\\ton.\\tThe\\tplot\\ton\\tthe\\tright\\trepresents\\tthe\\tsame\\tsequence\\tof\\tpredictors\\texcept\\tthat\\nthe\\tlearning\\trate\\tis\\thalved\\t(i.e.,\\tthe\\tmisclassified\\tinstance\\tweights\\tare\\tboosted\\thalf\\tas\\tmuch\\tat\\tevery\\niteration).\\tAs\\tyou\\tcan\\tsee,\\tthis\\tsequential\\tlearning\\ttechnique\\thas\\tsome\\tsimilarities\\twith\\tGradient\\tDescent,\\nexcept\\tthat\\tinstead\\tof\\ttweaking\\ta\\tsingle\\tpredictor’s\\tparameters\\tto\\tminimize\\ta\\t\\ncost\\tfunction,\\tAdaBoost\\nadds\\tpredictors\\tto\\tthe\\tensemble,\\tgradually\\tmaking\\tit\\tbetter.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 244}), Document(page_content='Figure\\t7-8.\\t\\nDecision\\tboundaries\\tof\\tconsecutive\\tpredictors\\nOnce\\tall\\tpredictors\\tare\\ttrained,\\tthe\\tensemble\\tmakes\\tpredictions\\tvery\\tmuch\\tlike\\tbagging\\tor\\tpasting,\\texcept\\nthat\\tpredictors\\thave\\tdifferent\\tweights\\tdepending\\ton\\ttheir\\toverall\\taccuracy\\ton\\tthe\\tweighted\\ttraining\\tset.\\nWARNING\\nThere\\tis\\tone\\timportant\\tdrawback\\tto\\tthis\\tsequential\\tlearning\\ttechnique:\\tit\\tcannot\\tbe\\tparallelized\\t(or\\tonly\\tpartially),\\tsince\\teach\\npredictor\\tcan\\tonly\\tbe\\ttrained\\tafter\\tthe\\tprevious\\tpredictor\\thas\\tbeen\\ttrained\\tand\\tevaluated.\\tAs\\ta\\tresult,\\tit\\tdoes\\tnot\\tscale\\tas\\twell\\tas\\nbagging\\tor\\tpasting.\\nLet’s\\ttake\\ta\\tcloser\\tlook\\tat\\tthe\\tAdaBoost\\talgorithm.\\tEach\\tinstance\\tweight\\t\\nw\\n(i)\\n\\tis\\tinitially\\tset\\tto\\t\\n.\\tA\\tfirst\\npredictor\\tis\\ttrained\\tand\\tits\\tweighted\\terror\\trate\\t\\nr\\n1\\n\\tis\\tcomputed\\ton\\tthe\\ttraining\\tset;\\tsee\\t\\nEquation\\t7-1\\n.\\nEquation\\t7-1.\\t\\nWeighted\\terror\\trate\\tof\\tthe\\tj\\nth\\n\\tpredictor\\nThe\\tpredictor’s\\tweight\\t\\nα\\nj\\n\\tis\\tthen\\tcomputed\\tusing\\t\\nEquation\\t7-2\\n,\\twhere\\t\\nη\\n\\tis\\tthe\\tlearning\\trate\\nhyperparameter\\t(defaults\\tto\\t1).\\n15\\n\\tThe\\tmore\\taccurate\\tthe\\tpredictor\\tis,\\tthe\\thigher\\tits\\tweight\\twill\\tbe.\\tIf\\tit\\tis\\njust\\tguessing\\trandomly,\\tthen\\tits\\tweight\\twill\\tbe\\tclose\\tto\\tzero.\\tHowever,\\tif\\tit\\tis\\tmost\\toften\\twrong\\t(i.e.,\\tless\\naccurate\\tthan\\trandom\\tguessing),\\tthen\\tits\\tweight\\twill\\tbe\\tnegative.\\nEquation\\t7-2.\\t\\nPredictor\\tweight\\nNext\\tthe\\tinstance\\tweights\\tare\\tupdated\\tusing\\t\\nEquation\\t7-3\\n:\\tthe\\tmisclassified\\tinstances\\tare\\tboosted.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 245}), Document(page_content='Equation\\t7-3.\\t\\nWeight\\tupdate\\trule\\nThen\\tall\\tthe\\tinstance\\tweights\\tare\\tnormalized\\t(i.e.,\\tdivided\\tby\\t\\n).\\nFinally,\\ta\\tnew\\tpredictor\\tis\\ttrained\\tusing\\tthe\\tupdated\\tweights,\\tand\\tthe\\twhole\\tprocess\\tis\\trepeated\\t(the\\tnew\\npredictor’s\\tweight\\tis\\tcomputed,\\tthe\\tinstance\\tweights\\tare\\tupdated,\\tthen\\tanother\\tpredictor\\tis\\ttrained,\\tand\\tso\\non).\\tThe\\talgorithm\\tstops\\twhen\\tthe\\tdesired\\tnumber\\tof\\tpredictors\\tis\\treached,\\tor\\twhen\\ta\\tperfect\\tpredictor\\tis\\nfound.\\nTo\\tmake\\tpredictions,\\tAdaBoost\\tsimply\\tcomputes\\tthe\\tpredictions\\tof\\tall\\tthe\\tpredictors\\tand\\tweighs\\tthem\\nusing\\tthe\\tpredictor\\tweights\\t\\nα\\nj\\n.\\tThe\\tpredicted\\tclass\\tis\\tthe\\tone\\tthat\\treceives\\tthe\\tmajority\\tof\\tweighted\\tvotes\\n(see\\t\\nEquation\\t7-4\\n).\\nEquation\\t7-4.\\t\\nAdaBoost\\tpredictions\\nScikit-Learn\\t\\nactually\\tuses\\ta\\tmulticlass\\tversion\\tof\\tAdaBoost\\tcalled\\t\\nSAMME\\n16\\n\\t(which\\tstands\\tfor\\nStagewise\\tAdditive\\tModeling\\tusing\\ta\\tMulticlass\\tExponential\\tloss\\tfunction\\n).\\tWhen\\tthere\\tare\\tjust\\ttwo\\nclasses,\\tSAMME\\tis\\tequivalent\\tto\\tAdaBoost.\\tMoreover,\\tif\\tthe\\tpredictors\\tcan\\testimate\\tclass\\tprobabilities\\n(i.e.,\\tif\\tthey\\thave\\ta\\t\\npredict_proba()\\n\\tmethod),\\tScikit-Learn\\tcan\\tuse\\ta\\tvariant\\tof\\tSAMME\\tcalled\\nSAMME.R\\n\\t(the\\t\\nR\\n\\tstands\\tfor\\t“Real”),\\twhich\\trelies\\ton\\tclass\\tprobabilities\\trather\\tthan\\tpredictions\\tand\\ngenerally\\tperforms\\tbetter.\\nThe\\tfollowing\\tcode\\ttrains\\tan\\t\\nAdaBoost\\tclassifier\\tbased\\ton\\t200\\t\\nDecision\\tStumps\\n\\tusing\\tScikit-Learn’s\\nAdaBoostClassifier\\n\\tclass\\t(as\\tyou\\tmight\\texpect,\\tthere\\tis\\talso\\tan\\t\\nAdaBoostRegressor\\n\\tclass).\\tA\\nDecision\\tStump\\tis\\ta\\t\\nDecision\\tTree\\twith\\t\\nmax_depth=1\\n\\t—\\tin\\tother\\twords,\\ta\\ttree\\tcomposed\\tof\\ta\\tsingle\\ndecision\\tnode\\tplus\\ttwo\\tleaf\\tnodes.\\tThis\\tis\\tthe\\tdefault\\tbase\\testimator\\tfor\\tthe\\t\\nAdaBoostClassifier\\n\\tclass:\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nAdaBoostClassifier\\nada_clf\\n\\t\\n=\\n\\t\\nAdaBoostClassifier\\n(\\n\\t\\t\\t\\t\\nDecisionTreeClassifier\\n(\\nmax_depth\\n=\\n1\\n),\\n\\t\\nn_estimators\\n=\\n200\\n,\\n\\t\\t\\t\\t\\nalgorithm\\n=\\n\"SAMME.R\"\\n,\\n\\t\\nlearning_rate\\n=\\n0.5\\n)\\nada_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 246}), Document(page_content='TIP\\nIf\\tyour\\tAdaBoost\\tensemble\\tis\\toverfitting\\tthe\\ttraining\\tset,\\tyou\\tcan\\ttry\\treducing\\tthe\\tnumber\\tof\\testimators\\tor\\tmore\\tstrongly\\nregularizing\\tthe\\tbase\\t\\nestimator.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 247}), Document(page_content='Gradient\\tBoosting\\nAnother\\t\\nvery\\tpopular\\tBoosting\\talgorithm\\tis\\t\\nGradient\\tBoosting\\n.\\n17\\n\\tJust\\tlike\\tAdaBoost,\\tGradient\\tBoosting\\nworks\\tby\\tsequentially\\tadding\\tpredictors\\tto\\tan\\tensemble,\\teach\\tone\\tcorrecting\\tits\\tpredecessor.\\tHowever,\\ninstead\\tof\\ttweaking\\tthe\\tinstance\\tweights\\tat\\tevery\\titeration\\tlike\\tAdaBoost\\tdoes,\\tthis\\tmethod\\ttries\\tto\\tfit\\tthe\\nnew\\tpredictor\\tto\\tthe\\t\\nresidual\\terrors\\n\\t\\nmade\\tby\\tthe\\tprevious\\tpredictor.\\nLet’s\\tgo\\tthrough\\ta\\tsimple\\tregression\\texample\\tusing\\t\\nDecision\\tTrees\\tas\\tthe\\tbase\\tpredictors\\t(of\\tcourse\\nGradient\\tBoosting\\talso\\tworks\\tgreat\\twith\\tregression\\ttasks).\\tThis\\tis\\tcalled\\n\\t\\nGradient\\tTree\\tBoosting\\n,\\tor\\nGradient\\tBoosted\\tRegression\\tTrees\\n\\t(\\nGBRT\\n).\\tFirst,\\tlet’s\\tfit\\ta\\t\\nDecisionTreeRegressor\\n\\tto\\tthe\\ttraining\\tset\\n(for\\texample,\\ta\\tnoisy\\tquadratic\\ttraining\\tset):\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nDecisionTreeRegressor\\ntree_reg1\\n\\t\\n=\\n\\t\\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n)\\ntree_reg1\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nNow\\ttrain\\ta\\tsecond\\t\\nDecisionTreeRegressor\\n\\ton\\tthe\\tresidual\\terrors\\tmade\\tby\\tthe\\tfirst\\tpredictor:\\ny2\\n\\t\\n=\\n\\t\\ny\\n\\t\\n-\\n\\t\\ntree_reg1\\n.\\npredict\\n(\\nX\\n)\\ntree_reg2\\n\\t\\n=\\n\\t\\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n)\\ntree_reg2\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny2\\n)\\nThen\\t\\nwe\\ttrain\\ta\\tthird\\tregressor\\ton\\tthe\\tresidual\\terrors\\tmade\\tby\\tthe\\tsecond\\tpredictor:\\ny3\\n\\t\\n=\\n\\t\\ny2\\n\\t\\n-\\n\\t\\ntree_reg2\\n.\\npredict\\n(\\nX\\n)\\ntree_reg3\\n\\t\\n=\\n\\t\\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n)\\ntree_reg3\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny3\\n)\\nNow\\twe\\thave\\tan\\tensemble\\tcontaining\\tthree\\ttrees.\\tIt\\tcan\\tmake\\tpredictions\\ton\\ta\\tnew\\tinstance\\tsimply\\tby\\nadding\\tup\\tthe\\tpredictions\\tof\\tall\\tthe\\ttrees:\\ny_pred\\n\\t\\n=\\n\\t\\nsum\\n(\\ntree\\n.\\npredict\\n(\\nX_new\\n)\\n\\t\\nfor\\n\\t\\ntree\\n\\t\\nin\\n\\t\\n(\\ntree_reg1\\n,\\n\\t\\ntree_reg2\\n,\\n\\t\\ntree_reg3\\n))\\nFigure\\t7-9\\n\\trepresents\\tthe\\tpredictions\\tof\\tthese\\tthree\\ttrees\\tin\\tthe\\tleft\\tcolumn,\\tand\\tthe\\tensemble’s\\npredictions\\tin\\tthe\\tright\\tcolumn.\\tIn\\tthe\\tfirst\\trow,\\tthe\\tensemble\\thas\\tjust\\tone\\ttree,\\tso\\tits\\tpredictions\\tare\\nexactly\\tthe\\tsame\\tas\\tthe\\tfirst\\ttree’s\\tpredictions.\\tIn\\tthe\\tsecond\\trow,\\ta\\tnew\\ttree\\tis\\ttrained\\ton\\tthe\\tresidual\\nerrors\\tof\\tthe\\tfirst\\ttree.\\tOn\\tthe\\tright\\tyou\\tcan\\tsee\\tthat\\tthe\\tensemble’s\\tpredictions\\tare\\tequal\\tto\\tthe\\tsum\\tof\\tthe\\npredictions\\tof\\tthe\\tfirst\\ttwo\\ttrees.\\tSimilarly,\\tin\\tthe\\tthird\\trow\\tanother\\ttree\\tis\\ttrained\\ton\\tthe\\tresidual\\terrors\\nof\\tthe\\tsecond\\ttree.\\tYou\\tcan\\tsee\\tthat\\tthe\\tensemble’s\\tpredictions\\tgradually\\tget\\tbetter\\tas\\ttrees\\tare\\tadded\\tto\\nthe\\t\\nensemble.\\nA\\tsimpler\\tway\\tto\\ttrain\\tGBRT\\tensembles\\tis\\tto\\tuse\\tScikit-Learn’s\\t\\nGradientBoostingRegressor\\n\\tclass.\\nMuch\\tlike\\tthe\\t\\nRandomForestRegressor\\n\\tclass,\\tit\\thas\\thyperparameters\\tto\\tcontrol\\tthe\\tgrowth\\tof\\tDecision\\nTrees\\t(e.g.,\\t\\nmax_depth\\n,\\t\\nmin_samples_leaf\\n,\\tand\\tso\\ton),\\tas\\twell\\tas\\thyperparameters\\tto\\tcontrol\\tthe\\nensemble\\ttraining,\\tsuch\\tas\\tthe\\tnumber\\tof\\ttrees\\t(\\nn_estimators\\n).\\tThe\\tfollowing\\tcode\\tcreates\\tthe\\tsame\\nensemble\\tas\\tthe\\tprevious\\tone:', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 248}), Document(page_content='from\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nGradientBoostingRegressor\\ngbrt\\n\\t\\n=\\n\\t\\nGradientBoostingRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n\\t\\nn_estimators\\n=\\n3\\n,\\n\\t\\nlearning_rate\\n=\\n1.0\\n)\\ngbrt\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nFigure\\t7-9.\\t\\nGradient\\tBoosting\\nThe\\t\\nlearning_rate\\n\\thyperparameter\\tscales\\tthe\\tcontribution\\tof\\teach\\ttree.\\tIf\\tyou\\tset\\tit\\tto\\ta\\tlow\\tvalue,\\tsuch\\nas\\t\\n0.1\\n,\\tyou\\twill\\tneed\\tmore\\ttrees\\tin\\tthe\\tensemble\\tto\\tfit\\tthe\\ttraining\\tset,\\tbut\\tthe\\tpredictions\\twill\\tusually\\ngeneralize\\tbetter.\\tThis\\tis\\ta\\tregularization\\ttechnique\\t\\ncalled\\t\\nshrinkage\\n.\\t\\nFigure\\t7-10\\n\\tshows\\ttwo\\tGBRT\\nensembles\\ttrained\\twith\\ta\\tlow\\tlearning\\trate:\\tthe\\tone\\ton\\tthe\\tleft\\tdoes\\tnot\\thave\\tenough\\ttrees\\tto\\tfit\\tthe\\ntraining\\tset,\\twhile\\tthe\\tone\\ton\\tthe\\tright\\thas\\ttoo\\tmany\\ttrees\\tand\\toverfits\\tthe\\ttraining\\tset.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 249}), Document(page_content='Figure\\t7-10.\\t\\nGBRT\\tensembles\\twith\\tnot\\tenough\\tpredictors\\t(left)\\tand\\ttoo\\tmany\\t(right)\\nIn\\torder\\tto\\tfind\\tthe\\toptimal\\tnumber\\tof\\ttrees,\\tyou\\tcan\\tuse\\t\\nearly\\tstopping\\t(see\\t\\nChapter\\t4\\n).\\tA\\tsimple\\tway\\tto\\nimplement\\tthis\\tis\\tto\\tuse\\tthe\\t\\nstaged_predict()\\n\\tmethod:\\tit\\treturns\\tan\\titerator\\tover\\tthe\\tpredictions\\tmade\\nby\\tthe\\tensemble\\tat\\teach\\tstage\\tof\\ttraining\\t(with\\tone\\ttree,\\ttwo\\ttrees,\\tetc.).\\tThe\\tfollowing\\tcode\\ttrains\\ta\\nGBRT\\tensemble\\twith\\t120\\ttrees,\\tthen\\tmeasures\\tthe\\tvalidation\\terror\\tat\\teach\\tstage\\tof\\ttraining\\tto\\tfind\\tthe\\noptimal\\tnumber\\tof\\ttrees,\\tand\\tfinally\\ttrains\\tanother\\tGBRT\\tensemble\\t\\nusing\\tthe\\toptimal\\tnumber\\tof\\ttrees:\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\ntrain_test_split\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nmean_squared_error\\nX_train\\n,\\n\\t\\nX_val\\n,\\n\\t\\ny_train\\n,\\n\\t\\ny_val\\n\\t\\n=\\n\\t\\ntrain_test_split\\n(\\nX\\n,\\n\\t\\ny\\n)\\ngbrt\\n\\t\\n=\\n\\t\\nGradientBoostingRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n\\t\\nn_estimators\\n=\\n120\\n)\\ngbrt\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\nerrors\\n\\t\\n=\\n\\t\\n[\\nmean_squared_error\\n(\\ny_val\\n,\\n\\t\\ny_pred\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ny_pred\\n\\t\\nin\\n\\t\\ngbrt\\n.\\nstaged_predict\\n(\\nX_val\\n)]\\nbst_n_estimators\\n\\t\\n=\\n\\t\\nnp\\n.\\nargmin\\n(\\nerrors\\n)\\ngbrt_best\\n\\t\\n=\\n\\t\\nGradientBoostingRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\nn_estimators\\n=\\nbst_n_estimators\\n)\\ngbrt_best\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\nThe\\tvalidation\\terrors\\tare\\trepresented\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t7-11\\n,\\tand\\tthe\\tbest\\tmodel’s\\tpredictions\\tare\\nrepresented\\ton\\tthe\\tright.\\nFigure\\t7-11.\\t\\nTuning\\tthe\\tnumber\\tof\\ttrees\\tusing\\tearly\\tstopping\\nIt\\tis\\talso\\tpossible\\tto\\timplement\\tearly\\tstopping\\tby\\tactually\\tstopping\\ttraining\\tearly\\t(instead\\tof\\ttraining\\ta\\nlarge\\tnumber\\tof\\ttrees\\tfirst\\tand\\tthen\\tlooking\\tback\\tto\\tfind\\tthe\\toptimal\\tnumber).\\tYou\\tcan\\tdo\\tso\\tby\\tsetting\\nwarm_start=True\\n,\\twhich\\tmakes\\tScikit-Learn\\tkeep\\texisting\\ttrees\\twhen\\tthe\\t\\nfit()\\n\\tmethod\\tis\\tcalled,\\nallowing\\tincremental\\ttraining.\\tThe\\t\\nfollowing\\tcode\\tstops\\ttraining\\twhen\\tthe\\tvalidation\\terror\\tdoes\\tnot', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 250}), Document(page_content='improve\\tfor\\tfive\\titerations\\tin\\ta\\trow:\\ngbrt\\n\\t\\n=\\n\\t\\nGradientBoostingRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n\\t\\nwarm_start\\n=\\nTrue\\n)\\nmin_val_error\\n\\t\\n=\\n\\t\\nfloat\\n(\\n\"inf\"\\n)\\nerror_going_up\\n\\t\\n=\\n\\t\\n0\\nfor\\n\\t\\nn_estimators\\n\\t\\nin\\n\\t\\nrange\\n(\\n1\\n,\\n\\t\\n120\\n):\\n\\t\\t\\t\\t\\ngbrt\\n.\\nn_estimators\\n\\t\\n=\\n\\t\\nn_estimators\\n\\t\\t\\t\\t\\ngbrt\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n\\t\\t\\t\\t\\ny_pred\\n\\t\\n=\\n\\t\\ngbrt\\n.\\npredict\\n(\\nX_val\\n)\\n\\t\\t\\t\\t\\nval_error\\n\\t\\n=\\n\\t\\nmean_squared_error\\n(\\ny_val\\n,\\n\\t\\ny_pred\\n)\\n\\t\\t\\t\\t\\nif\\n\\t\\nval_error\\n\\t\\n<\\n\\t\\nmin_val_error\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nmin_val_error\\n\\t\\n=\\n\\t\\nval_error\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nerror_going_up\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nerror_going_up\\n\\t\\n+=\\n\\t\\n1\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nerror_going_up\\n\\t\\n==\\n\\t\\n5\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbreak\\n\\t\\t\\n#\\tearly\\tstopping\\nThe\\t\\nGradientBoostingRegressor\\n\\tclass\\talso\\tsupports\\t\\na\\t\\nsubsample\\n\\thyperparameter,\\twhich\\tspecifies\\nthe\\tfraction\\tof\\ttraining\\tinstances\\tto\\tbe\\tused\\tfor\\ttraining\\teach\\ttree.\\tFor\\texample,\\tif\\t\\nsubsample=0.25\\n,\\tthen\\neach\\ttree\\tis\\ttrained\\ton\\t25%\\tof\\tthe\\ttraining\\tinstances,\\tselected\\trandomly.\\tAs\\tyou\\tcan\\tprobably\\tguess\\tby\\nnow,\\tthis\\ttrades\\ta\\thigher\\tbias\\tfor\\ta\\tlower\\tvariance.\\tIt\\talso\\tspeeds\\tup\\ttraining\\tconsiderably.\\tThis\\t\\ntechnique\\nis\\tcalled\\t\\nStochastic\\tGradient\\tBoosting\\n.\\nNOTE\\nIt\\tis\\tpossible\\tto\\tuse\\tGradient\\tBoosting\\twith\\tother\\t\\ncost\\tfunctions.\\tThis\\tis\\tcontrolled\\tby\\tthe\\t\\nloss\\n\\thyperparameter\\t(see\\tScikit-\\nLearn’s\\tdocumentation\\tfor\\tmore\\t\\ndetails).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 251}), Document(page_content='Stacking\\nThe\\t\\nlast\\tEnsemble\\tmethod\\twe\\twill\\tdiscuss\\tin\\tthis\\tchapter\\tis\\tcalled\\t\\nstacking\\n\\t\\n(short\\tfor\\t\\nstacked\\ngeneralization\\n).\\n18\\n\\tIt\\tis\\tbased\\ton\\ta\\tsimple\\tidea:\\tinstead\\tof\\tusing\\ttrivial\\tfunctions\\t(such\\tas\\thard\\tvoting)\\tto\\naggregate\\tthe\\tpredictions\\tof\\tall\\tpredictors\\tin\\tan\\tensemble,\\twhy\\tdon’t\\twe\\ttrain\\ta\\tmodel\\tto\\tperform\\tthis\\naggregation?\\t\\nFigure\\t7-12\\n\\t\\nshows\\tsuch\\tan\\tensemble\\tperforming\\ta\\tregression\\ttask\\ton\\ta\\tnew\\tinstance.\\tEach\\nof\\tthe\\tbottom\\tthree\\tpredictors\\tpredicts\\ta\\tdifferent\\tvalue\\t(3.1,\\t2.7,\\tand\\t2.9),\\tand\\tthen\\tthe\\tfinal\\tpredictor\\n(called\\ta\\t\\nblender\\n,\\tor\\ta\\t\\nmeta\\tlearner\\n)\\ttakes\\tthese\\tpredictions\\tas\\tinputs\\tand\\tmakes\\tthe\\tfinal\\tprediction\\n(3.0).\\nFigure\\t7-12.\\t\\nAggregating\\tpredictions\\tusing\\ta\\tblending\\tpredictor\\nTo\\ttrain\\tthe\\tblender,\\ta\\tcommon\\tapproach\\tis\\tto\\tuse\\ta\\t\\nhold-out\\tset.\\n19\\n\\tLet’s\\tsee\\thow\\tit\\tworks.\\tFirst,\\tthe\\ntraining\\tset\\tis\\tsplit\\tin\\ttwo\\tsubsets.\\tThe\\tfirst\\tsubset\\tis\\tused\\tto\\ttrain\\tthe\\tpredictors\\tin\\tthe\\tfirst\\tlayer\\t(see\\nFigure\\t7-13\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 252}), Document(page_content='Figure\\t7-13.\\t\\nTraining\\tthe\\tfirst\\tlayer\\nNext,\\tthe\\tfirst\\tlayer\\tpredictors\\tare\\tused\\tto\\tmake\\tpredictions\\ton\\tthe\\tsecond\\t(held-out)\\tset\\t(see\\t\\nFigure\\t7-\\n14\\n).\\tThis\\tensures\\tthat\\tthe\\tpredictions\\tare\\t“clean,”\\tsince\\tthe\\tpredictors\\tnever\\tsaw\\tthese\\tinstances\\tduring\\ntraining.\\tNow\\tfor\\teach\\tinstance\\tin\\tthe\\thold-out\\tset\\tthere\\tare\\tthree\\tpredicted\\tvalues.\\tWe\\tcan\\tcreate\\ta\\tnew\\ntraining\\tset\\tusing\\tthese\\tpredicted\\tvalues\\tas\\tinput\\tfeatures\\t(which\\tmakes\\tthis\\tnew\\ttraining\\tset\\tthree-\\ndimensional),\\tand\\tkeeping\\tthe\\ttarget\\tvalues.\\tThe\\tblender\\tis\\ttrained\\ton\\tthis\\tnew\\ttraining\\tset,\\tso\\tit\\tlearns\\tto\\npredict\\tthe\\ttarget\\tvalue\\tgiven\\tthe\\tfirst\\tlayer’s\\tpredictions.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 253}), Document(page_content='Figure\\t7-14.\\t\\nTraining\\tthe\\tblender\\nIt\\tis\\tactually\\tpossible\\tto\\ttrain\\tseveral\\tdifferent\\tblenders\\tthis\\tway\\t(e.g.,\\tone\\tusing\\tLinear\\tRegression,\\nanother\\tusing\\tRandom\\tForest\\tRegression,\\tand\\tso\\ton):\\twe\\tget\\ta\\twhole\\tlayer\\tof\\tblenders.\\tThe\\ttrick\\tis\\tto\\nsplit\\tthe\\ttraining\\tset\\tinto\\tthree\\tsubsets:\\tthe\\tfirst\\tone\\tis\\tused\\tto\\ttrain\\tthe\\tfirst\\tlayer,\\tthe\\tsecond\\tone\\tis\\tused\\tto\\ncreate\\tthe\\ttraining\\tset\\tused\\tto\\ttrain\\tthe\\tsecond\\tlayer\\t(using\\tpredictions\\tmade\\tby\\tthe\\tpredictors\\tof\\tthe\\tfirst\\nlayer),\\tand\\tthe\\tthird\\tone\\tis\\tused\\tto\\tcreate\\tthe\\ttraining\\tset\\tto\\ttrain\\tthe\\tthird\\tlayer\\t(using\\tpredictions\\tmade\\tby\\nthe\\tpredictors\\tof\\tthe\\tsecond\\tlayer).\\tOnce\\tthis\\tis\\tdone,\\twe\\tcan\\tmake\\ta\\tprediction\\tfor\\ta\\tnew\\tinstance\\tby\\ngoing\\tthrough\\teach\\tlayer\\tsequentially,\\tas\\tshown\\tin\\t\\nFigure\\t7-15\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 254}), Document(page_content='Figure\\t7-15.\\t\\nPredictions\\tin\\ta\\tmultilayer\\tstacking\\tensemble\\nUnfortunately,\\tScikit-Learn\\tdoes\\tnot\\tsupport\\tstacking\\tdirectly,\\tbut\\tit\\tis\\tnot\\ttoo\\thard\\tto\\troll\\tout\\tyour\\town\\nimplementation\\t(see\\tthe\\tfollowing\\texercises).\\tAlternatively,\\tyou\\tcan\\tuse\\tan\\topen\\tsource\\timplementation\\nsuch\\t\\nas\\t\\nbrew\\n\\t(available\\tat\\t\\nhttps://github.com/viisar/brew\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 255}), Document(page_content='Exercises\\n1\\n.\\t\\nIf\\tyou\\thave\\ttrained\\tfive\\tdifferent\\tmodels\\ton\\tthe\\texact\\tsame\\ttraining\\tdata,\\tand\\tthey\\tall\\tachieve\\t95%\\nprecision,\\tis\\tthere\\tany\\tchance\\tthat\\tyou\\tcan\\tcombine\\tthese\\tmodels\\tto\\tget\\tbetter\\tresults?\\tIf\\tso,\\thow?\\tIf\\nnot,\\twhy?\\n2\\n.\\t\\nWhat\\tis\\tthe\\tdifference\\tbetween\\thard\\tand\\tsoft\\tvoting\\tclassifiers?\\n3\\n.\\t\\nIs\\tit\\tpossible\\tto\\tspeed\\tup\\ttraining\\tof\\ta\\tbagging\\tensemble\\tby\\tdistributing\\tit\\tacross\\tmultiple\\tservers?\\nWhat\\tabout\\tpasting\\tensembles,\\tboosting\\tensembles,\\trandom\\tforests,\\tor\\tstacking\\tensembles?\\n4\\n.\\t\\nWhat\\tis\\tthe\\tbenefit\\tof\\tout-of-bag\\tevaluation?\\n5\\n.\\t\\nWhat\\tmakes\\tExtra-Trees\\tmore\\trandom\\tthan\\tregular\\tRandom\\tForests?\\tHow\\tcan\\tthis\\textra\\trandomness\\nhelp?\\tAre\\tExtra-Trees\\tslower\\tor\\tfaster\\tthan\\tregular\\tRandom\\tForests?\\n6\\n.\\t\\nIf\\tyour\\tAdaBoost\\tensemble\\tunderfits\\tthe\\ttraining\\tdata,\\twhat\\thyperparameters\\tshould\\tyou\\ttweak\\tand\\nhow?\\n7\\n.\\t\\nIf\\tyour\\tGradient\\tBoosting\\tensemble\\toverfits\\tthe\\ttraining\\tset,\\tshould\\tyou\\tincrease\\tor\\tdecrease\\tthe\\nlearning\\trate?\\n8\\n.\\t\\nLoad\\tthe\\tMNIST\\tdata\\t(introduced\\tin\\t\\nChapter\\t3\\n),\\tand\\tsplit\\tit\\tinto\\ta\\ttraining\\tset,\\ta\\tvalidation\\tset,\\tand\\ta\\ntest\\tset\\t(e.g.,\\tuse\\t40,000\\tinstances\\tfor\\ttraining,\\t10,000\\tfor\\tvalidation,\\tand\\t10,000\\tfor\\ttesting).\\tThen\\ntrain\\tvarious\\tclassifiers,\\tsuch\\tas\\ta\\tRandom\\tForest\\tclassifier,\\tan\\tExtra-Trees\\tclassifier,\\tand\\tan\\tSVM.\\nNext,\\ttry\\tto\\tcombine\\tthem\\tinto\\tan\\tensemble\\tthat\\toutperforms\\tthem\\tall\\ton\\tthe\\tvalidation\\tset,\\tusing\\ta\\nsoft\\tor\\thard\\tvoting\\tclassifier.\\tOnce\\tyou\\thave\\tfound\\tone,\\ttry\\tit\\ton\\tthe\\ttest\\tset.\\tHow\\tmuch\\tbetter\\tdoes\\tit\\nperform\\tcompared\\tto\\tthe\\tindividual\\tclassifiers?\\n9\\n.\\t\\nRun\\tthe\\tindividual\\tclassifiers\\tfrom\\tthe\\tprevious\\texercise\\tto\\tmake\\tpredictions\\ton\\tthe\\tvalidation\\tset,\\nand\\tcreate\\ta\\tnew\\ttraining\\tset\\twith\\tthe\\tresulting\\tpredictions:\\teach\\ttraining\\tinstance\\tis\\ta\\tvector\\ncontaining\\tthe\\tset\\tof\\tpredictions\\tfrom\\tall\\tyour\\tclassifiers\\tfor\\tan\\timage,\\tand\\tthe\\ttarget\\tis\\tthe\\timage’s\\nclass.\\tCongratulations,\\tyou\\thave\\tjust\\ttrained\\ta\\tblender,\\tand\\ttogether\\twith\\tthe\\tclassifiers\\tthey\\tform\\ta\\nstacking\\tensemble!\\t\\nNow\\tlet’s\\tevaluate\\tthe\\tensemble\\ton\\tthe\\ttest\\tset.\\tFor\\teach\\timage\\tin\\tthe\\ttest\\tset,\\nmake\\tpredictions\\twith\\tall\\tyour\\tclassifiers,\\tthen\\tfeed\\tthe\\tpredictions\\tto\\tthe\\tblender\\tto\\tget\\tthe\\nensemble’s\\tpredictions.\\tHow\\tdoes\\tit\\tcompare\\tto\\tthe\\tvoting\\tclassifier\\tyou\\ttrained\\t\\nearlier?\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\n“Bagging\\tPredictors,”\\tL.\\tBreiman\\t(1996).\\nIn\\tstatistics,\\tresampling\\twith\\treplacement\\tis\\tcalled\\t\\nbootstrapping\\n.\\n“Pasting\\tsmall\\tvotes\\tfor\\tclassification\\tin\\tlarge\\tdatabases\\tand\\ton-line,”\\tL.\\tBreiman\\t(1999).\\nBias\\tand\\tvariance\\twere\\tintroduced\\tin\\t\\nChapter\\t4\\n.\\nmax_samples\\n\\tcan\\talternatively\\tbe\\tset\\tto\\ta\\tfloat\\tbetween\\t0.0\\tand\\t1.0,\\tin\\twhich\\tcase\\tthe\\tmax\\tnumber\\tof\\tinstances\\tto\\tsample\\tis\\tequal\\tto\\tthe\\nsize\\tof\\tthe\\ttraining\\tset\\ttimes\\t\\nmax_samples\\n.\\n1\\n2\\n3\\n4\\n5\\n6', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 256}), Document(page_content='As\\t\\nm\\n\\tgrows,\\tthis\\tratio\\tapproaches\\t1\\t–\\texp(–1)\\t≈\\t63.212%.\\n“Ensembles\\ton\\tRandom\\tPatches,”\\tG.\\tLouppe\\tand\\tP.\\tGeurts\\t(2012).\\n“The\\trandom\\tsubspace\\tmethod\\tfor\\tconstructing\\tdecision\\tforests,”\\tTin\\tKam\\tHo\\t(1998).\\n“Random\\tDecision\\tForests,”\\tT.\\tHo\\t(1995).\\nThe\\t\\nBaggingClassifier\\n\\tclass\\tremains\\tuseful\\tif\\tyou\\twant\\ta\\tbag\\tof\\tsomething\\tother\\tthan\\tDecision\\tTrees.\\nThere\\tare\\ta\\tfew\\tnotable\\texceptions:\\t\\nsplitter\\n\\tis\\tabsent\\t(forced\\tto\\t\\n\"random\"\\n),\\t\\npresort\\n\\tis\\tabsent\\t(forced\\tto\\t\\nFalse\\n),\\t\\nmax_samples\\n\\tis\\tabsent\\n(forced\\tto\\t\\n1.0\\n),\\t\\nand\\t\\nbase_estimator\\n\\tis\\tabsent\\t(forced\\tto\\t\\nDecisionTreeClassifier\\n\\twith\\tthe\\tprovided\\thyperparameters).\\n“Extremely\\trandomized\\ttrees,”\\tP.\\tGeurts,\\tD.\\tErnst,\\tL.\\tWehenkel\\t(2005).\\n“A\\tDecision-Theoretic\\tGeneralization\\tof\\tOn-Line\\tLearning\\tand\\tan\\tApplication\\tto\\tBoosting,”\\tYoav\\tFreund,\\tRobert\\tE.\\tSchapire\\t(1997).\\nThis\\tis\\tjust\\tfor\\tillustrative\\tpurposes.\\tSVMs\\tare\\tgenerally\\tnot\\tgood\\tbase\\tpredictors\\tfor\\tAdaBoost,\\tbecause\\tthey\\tare\\tslow\\tand\\ttend\\tto\\tbe\\nunstable\\twith\\tAdaBoost.\\nThe\\toriginal\\tAdaBoost\\talgorithm\\tdoes\\tnot\\tuse\\ta\\tlearning\\trate\\thyperparameter.\\nFor\\tmore\\tdetails,\\tsee\\t“Multi-Class\\tAdaBoost,”\\tJ.\\tZhu\\tet\\tal.\\t(2006).\\nFirst\\tintroduced\\tin\\t“Arcing\\tthe\\tEdge,”\\tL.\\tBreiman\\t(1997).\\n“Stacked\\tGeneralization,”\\tD.\\tWolpert\\t(1992).\\nAlternatively,\\tit\\tis\\tpossible\\tto\\tuse\\tout-of-fold\\tpredictions.\\tIn\\tsome\\tcontexts\\tthis\\tis\\tcalled\\t\\nstacking\\n,\\twhile\\tusing\\ta\\thold-out\\tset\\tis\\tcalled\\nblending\\n.\\tHowever,\\tfor\\tmany\\tpeople\\tthese\\tterms\\tare\\tsynonymous.\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 257}), Document(page_content='Chapter\\t8.\\t\\nDimensionality\\tReduction\\nMany\\t\\nMachine\\tLearning\\tproblems\\tinvolve\\tthousands\\tor\\teven\\tmillions\\tof\\tfeatures\\tfor\\teach\\ttraining\\ninstance.\\tNot\\tonly\\tdoes\\tthis\\tmake\\ttraining\\textremely\\tslow,\\tit\\tcan\\talso\\tmake\\tit\\tmuch\\tharder\\tto\\tfind\\ta\\tgood\\nsolution,\\tas\\twe\\twill\\tsee.\\tThis\\tproblem\\tis\\toften\\treferred\\tto\\tas\\t\\nthe\\t\\ncurse\\tof\\tdimensionality\\n.\\nFortunately,\\tin\\treal-world\\tproblems,\\tit\\tis\\toften\\tpossible\\tto\\treduce\\tthe\\tnumber\\tof\\tfeatures\\tconsiderably,\\nturning\\tan\\tintractable\\tproblem\\tinto\\ta\\ttractable\\tone.\\tFor\\texample,\\tconsider\\tthe\\tMNIST\\timages\\t(introduced\\nin\\t\\nChapter\\t3\\n):\\tthe\\tpixels\\ton\\tthe\\timage\\tborders\\tare\\talmost\\talways\\twhite,\\tso\\tyou\\tcould\\tcompletely\\tdrop\\nthese\\tpixels\\tfrom\\tthe\\ttraining\\tset\\twithout\\tlosing\\tmuch\\tinformation.\\t\\nFigure\\t7-6\\n\\tconfirms\\tthat\\tthese\\tpixels\\nare\\tutterly\\tunimportant\\tfor\\tthe\\tclassification\\ttask.\\tMoreover,\\ttwo\\tneighboring\\tpixels\\tare\\toften\\thighly\\ncorrelated:\\tif\\tyou\\tmerge\\tthem\\tinto\\ta\\tsingle\\tpixel\\t(e.g.,\\tby\\ttaking\\tthe\\tmean\\tof\\tthe\\ttwo\\tpixel\\tintensities),\\tyou\\nwill\\tnot\\tlose\\tmuch\\tinformation.\\nWARNING\\nReducing\\tdimensionality\\tdoes\\tlose\\tsome\\tinformation\\t(just\\tlike\\tcompressing\\tan\\timage\\tto\\tJPEG\\tcan\\tdegrade\\tits\\tquality),\\tso\\teven\\nthough\\tit\\twill\\tspeed\\tup\\ttraining,\\tit\\tmay\\talso\\tmake\\tyour\\tsystem\\tperform\\tslightly\\tworse.\\tIt\\talso\\tmakes\\tyour\\tpipelines\\ta\\tbit\\tmore\\ncomplex\\tand\\tthus\\tharder\\tto\\tmaintain.\\tSo\\tyou\\tshould\\tfirst\\ttry\\tto\\ttrain\\tyour\\tsystem\\twith\\tthe\\toriginal\\tdata\\tbefore\\tconsidering\\tusing\\ndimensionality\\treduction\\tif\\ttraining\\tis\\ttoo\\tslow.\\tIn\\tsome\\tcases,\\thowever,\\treducing\\tthe\\tdimensionality\\tof\\tthe\\ttraining\\tdata\\tmay\\tfilter\\nout\\tsome\\tnoise\\tand\\tunnecessary\\tdetails\\tand\\tthus\\tresult\\tin\\thigher\\tperformance\\t(but\\tin\\tgeneral\\tit\\twon’t;\\tit\\twill\\tjust\\tspeed\\tup\\ntraining).\\nApart\\tfrom\\tspeeding\\tup\\ttraining,\\t\\ndimensionality\\treduction\\tis\\talso\\textremely\\tuseful\\tfor\\tdata\\tvisualization\\n(or\\t\\nDataViz\\n).\\tReducing\\tthe\\tnumber\\tof\\tdimensions\\tdown\\tto\\ttwo\\t(or\\tthree)\\tmakes\\tit\\tpossible\\tto\\tplot\\ta\\thigh-\\ndimensional\\ttraining\\tset\\ton\\ta\\tgraph\\tand\\toften\\tgain\\tsome\\timportant\\tinsights\\tby\\tvisually\\tdetecting\\tpatterns,\\nsuch\\tas\\tclusters.\\nIn\\tthis\\tchapter\\twe\\twill\\tdiscuss\\tthe\\tcurse\\tof\\tdimensionality\\tand\\tget\\ta\\tsense\\tof\\twhat\\tgoes\\ton\\tin\\thigh-\\ndimensional\\tspace.\\tThen,\\twe\\twill\\tpresent\\tthe\\ttwo\\tmain\\tapproaches\\tto\\tdimensionality\\treduction\\n(projection\\tand\\tManifold\\tLearning),\\tand\\twe\\twill\\tgo\\tthrough\\tthree\\tof\\tthe\\tmost\\tpopular\\tdimensionality\\nreduction\\ttechniques:\\tPCA,\\tKernel\\tPCA,\\tand\\tLLE.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 258}), Document(page_content='The\\tCurse\\tof\\tDimensionality\\nWe\\tare\\tso\\tused\\tto\\tliving\\tin\\tthree\\tdimensions\\n1\\n\\tthat\\tour\\tintuition\\tfails\\tus\\twhen\\twe\\ttry\\tto\\timagine\\ta\\thigh-\\ndimensional\\tspace.\\tEven\\ta\\tbasic\\t4D\\thypercube\\tis\\tincredibly\\thard\\tto\\tpicture\\tin\\tour\\tmind\\t(see\\t\\nFigure\\t8-1\\n),\\nlet\\talone\\ta\\t200-dimensional\\tellipsoid\\tbent\\tin\\ta\\t1,000-dimensional\\tspace.\\nFigure\\t8-1.\\t\\nPoint,\\tsegment,\\tsquare,\\tcube,\\tand\\ttesseract\\t(0D\\tto\\t4D\\thypercubes)\\n2\\nIt\\tturns\\tout\\tthat\\tmany\\tthings\\tbehave\\tvery\\tdifferently\\tin\\thigh-dimensional\\tspace.\\tFor\\texample,\\tif\\tyou\\tpick\\ta\\nrandom\\tpoint\\tin\\ta\\tunit\\tsquare\\t(a\\t1\\t×\\t1\\tsquare),\\tit\\twill\\thave\\tonly\\tabout\\ta\\t0.4%\\tchance\\tof\\tbeing\\tlocated\\tless\\nthan\\t0.001\\tfrom\\ta\\tborder\\t(in\\tother\\twords,\\tit\\tis\\tvery\\tunlikely\\tthat\\ta\\trandom\\tpoint\\twill\\tbe\\t“extreme”\\talong\\nany\\tdimension).\\tBut\\tin\\ta\\t10,000-dimensional\\tunit\\thypercube\\t(a\\t1\\t×\\t1\\t×\\t\\t×\\t1\\tcube,\\twith\\tten\\tthousand\\t1s),\\nthis\\tprobability\\tis\\tgreater\\tthan\\t99.999999%.\\tMost\\tpoints\\tin\\ta\\thigh-dimensional\\thypercube\\tare\\tvery\\tclose\\nto\\tthe\\tborder.\\n3\\nHere\\tis\\ta\\tmore\\ttroublesome\\tdifference:\\tif\\tyou\\tpick\\ttwo\\tpoints\\trandomly\\tin\\ta\\tunit\\tsquare,\\tthe\\tdistance\\nbetween\\tthese\\ttwo\\tpoints\\twill\\tbe,\\ton\\taverage,\\troughly\\t0.52.\\tIf\\tyou\\tpick\\ttwo\\trandom\\tpoints\\tin\\ta\\tunit\\t3D\\ncube,\\tthe\\taverage\\tdistance\\twill\\tbe\\troughly\\t0.66.\\tBut\\twhat\\tabout\\ttwo\\tpoints\\tpicked\\trandomly\\tin\\ta\\n1,000,000-dimensional\\thypercube?\\tWell,\\tthe\\taverage\\tdistance,\\tbelieve\\tit\\tor\\tnot,\\twill\\tbe\\tabout\\t408.25\\n(roughly\\t\\n)!\\tThis\\tis\\tquite\\tcounterintuitive:\\thow\\tcan\\ttwo\\tpoints\\tbe\\tso\\tfar\\tapart\\twhen\\nthey\\tboth\\tlie\\twithin\\tthe\\tsame\\tunit\\thypercube?\\tThis\\tfact\\timplies\\tthat\\thigh-dimensional\\tdatasets\\tare\\tat\\trisk\\nof\\tbeing\\tvery\\tsparse:\\tmost\\ttraining\\tinstances\\tare\\tlikely\\tto\\tbe\\tfar\\taway\\tfrom\\teach\\tother.\\tOf\\tcourse,\\tthis\\nalso\\tmeans\\tthat\\ta\\tnew\\tinstance\\twill\\tlikely\\tbe\\tfar\\taway\\tfrom\\tany\\ttraining\\tinstance,\\tmaking\\tpredictions\\nmuch\\tless\\treliable\\tthan\\tin\\tlower\\tdimensions,\\tsince\\tthey\\twill\\tbe\\tbased\\ton\\tmuch\\tlarger\\textrapolations.\\tIn\\nshort,\\tthe\\tmore\\tdimensions\\tthe\\ttraining\\tset\\thas,\\tthe\\tgreater\\tthe\\trisk\\tof\\toverfitting\\tit.\\nIn\\ttheory,\\tone\\tsolution\\tto\\tthe\\tcurse\\tof\\tdimensionality\\tcould\\tbe\\tto\\tincrease\\tthe\\tsize\\tof\\tthe\\ttraining\\tset\\tto\\nreach\\ta\\tsufficient\\tdensity\\tof\\ttraining\\tinstances.\\tUnfortunately,\\tin\\tpractice,\\tthe\\tnumber\\tof\\ttraining\\tinstances\\nrequired\\tto\\treach\\ta\\tgiven\\tdensity\\tgrows\\texponentially\\twith\\tthe\\tnumber\\tof\\tdimensions.\\tWith\\tjust\\t100\\nfeatures\\t(much\\tless\\tthan\\tin\\tthe\\tMNIST\\tproblem),\\tyou\\twould\\tneed\\tmore\\ttraining\\tinstances\\tthan\\tatoms\\tin\\nthe\\tobservable\\tuniverse\\tin\\torder\\tfor\\ttraining\\tinstances\\tto\\tbe\\twithin\\t0.1\\tof\\teach\\tother\\ton\\taverage,\\tassuming\\nthey\\twere\\tspread\\tout\\tuniformly\\t\\nacross\\tall\\tdimensions.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 259}), Document(page_content='Main\\tApproaches\\tfor\\tDimensionality\\tReduction\\nBefore\\twe\\tdive\\tinto\\tspecific\\tdimensionality\\treduction\\talgorithms,\\tlet’s\\ttake\\ta\\tlook\\tat\\tthe\\ttwo\\tmain\\napproaches\\tto\\treducing\\tdimensionality:\\tprojection\\tand\\tManifold\\tLearning.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 260}), Document(page_content='Projection\\nIn\\tmost\\t\\nreal-world\\tproblems,\\ttraining\\tinstances\\tare\\t\\nnot\\n\\tspread\\tout\\tuniformly\\tacross\\tall\\tdimensions.\\tMany\\nfeatures\\tare\\talmost\\tconstant,\\twhile\\tothers\\tare\\thighly\\tcorrelated\\t(as\\tdiscussed\\tearlier\\tfor\\tMNIST).\\tAs\\ta\\nresult,\\tall\\ttraining\\tinstances\\tactually\\tlie\\twithin\\t(or\\tclose\\tto)\\ta\\tmuch\\tlower-dimensional\\t\\nsubspace\\n\\tof\\tthe\\nhigh-dimensional\\tspace.\\tThis\\tsounds\\tvery\\tabstract,\\tso\\tlet’s\\tlook\\tat\\tan\\texample.\\tIn\\t\\nFigure\\t8-2\\n\\tyou\\tcan\\tsee\\na\\t3D\\tdataset\\trepresented\\tby\\tthe\\tcircles.\\nFigure\\t8-2.\\t\\nA\\t3D\\tdataset\\tlying\\tclose\\tto\\ta\\t2D\\tsubspace\\nNotice\\tthat\\tall\\ttraining\\tinstances\\tlie\\tclose\\tto\\ta\\tplane:\\tthis\\tis\\ta\\tlower-dimensional\\t(2D)\\tsubspace\\tof\\tthe\\nhigh-dimensional\\t(3D)\\tspace.\\tNow\\tif\\twe\\tproject\\tevery\\ttraining\\tinstance\\tperpendicularly\\tonto\\tthis\\nsubspace\\t(as\\trepresented\\tby\\tthe\\tshort\\tlines\\tconnecting\\tthe\\tinstances\\tto\\tthe\\tplane),\\twe\\tget\\tthe\\tnew\\t2D\\ndataset\\tshown\\tin\\t\\nFigure\\t8-3\\n.\\tTa-da!\\tWe\\thave\\tjust\\treduced\\tthe\\tdataset’s\\tdimensionality\\tfrom\\t3D\\tto\\t2D.\\nNote\\tthat\\tthe\\taxes\\tcorrespond\\tto\\tnew\\tfeatures\\t\\nz\\n1\\n\\tand\\t\\nz\\n2\\n\\t(the\\tcoordinates\\tof\\tthe\\tprojections\\ton\\tthe\\tplane).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 261}), Document(page_content='Figure\\t8-3.\\t\\nThe\\tnew\\t2D\\tdataset\\tafter\\tprojection\\nHowever,\\tprojection\\tis\\tnot\\talways\\tthe\\tbest\\tapproach\\tto\\tdimensionality\\treduction.\\tIn\\tmany\\tcases\\tthe\\nsubspace\\tmay\\ttwist\\tand\\tturn,\\tsuch\\tas\\tin\\tthe\\tfamous\\t\\nSwiss\\troll\\n\\ttoy\\tdataset\\trepresented\\tin\\t\\nFigure\\t8-4\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 262}), Document(page_content='Figure\\t8-4.\\t\\nSwiss\\troll\\tdataset\\nSimply\\tprojecting\\tonto\\ta\\tplane\\t(e.g.,\\tby\\tdropping\\t\\nx\\n3\\n)\\twould\\tsquash\\tdifferent\\tlayers\\tof\\tthe\\tSwiss\\troll\\ntogether,\\tas\\tshown\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t8-5\\n.\\tHowever,\\twhat\\tyou\\treally\\twant\\tis\\tto\\tunroll\\tthe\\tSwiss\\troll\\tto\\nobtain\\tthe\\t2D\\tdataset\\ton\\tthe\\t\\nright\\tof\\t\\nFigure\\t8-5\\n.\\nFigure\\t8-5.\\t\\nSquashing\\tby\\tprojecting\\tonto\\ta\\tplane\\t(left)\\tversus\\tunrolling\\tthe\\tSwiss\\troll\\t(right)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 263}), Document(page_content='Manifold\\tLearning\\nThe\\t\\nSwiss\\troll\\tis\\tan\\texample\\tof\\ta\\t2D\\t\\nmanifold\\n.\\tPut\\tsimply,\\ta\\t2D\\tmanifold\\tis\\ta\\t2D\\tshape\\tthat\\tcan\\tbe\\tbent\\nand\\ttwisted\\tin\\ta\\thigher-dimensional\\tspace.\\tMore\\tgenerally,\\ta\\t\\nd\\n-dimensional\\tmanifold\\tis\\ta\\tpart\\tof\\tan\\t\\nn\\n-\\ndimensional\\tspace\\t(where\\t\\nd\\n\\t<\\t\\nn\\n)\\tthat\\tlocally\\tresembles\\ta\\t\\nd\\n-dimensional\\t\\nhyperplane.\\tIn\\tthe\\tcase\\tof\\tthe\\nSwiss\\troll,\\t\\nd\\n\\t=\\t2\\tand\\t\\nn\\n\\t=\\t3:\\tit\\tlocally\\tresembles\\ta\\t2D\\tplane,\\tbut\\tit\\tis\\trolled\\tin\\tthe\\tthird\\tdimension.\\nMany\\tdimensionality\\treduction\\talgorithms\\twork\\tby\\tmodeling\\tthe\\t\\nmanifold\\n\\ton\\twhich\\tthe\\ttraining\\tinstances\\nlie;\\tthis\\tis\\tcalled\\t\\nManifold\\tLearning\\n.\\tIt\\trelies\\ton\\tthe\\t\\nmanifold\\tassumption\\n,\\talso\\tcalled\\tthe\\t\\nmanifold\\nhypothesis\\n,\\twhich\\t\\nholds\\tthat\\tmost\\treal-world\\thigh-dimensional\\tdatasets\\tlie\\tclose\\tto\\ta\\tmuch\\tlower-\\ndimensional\\tmanifold.\\tThis\\tassumption\\tis\\tvery\\toften\\tempirically\\tobserved.\\nOnce\\tagain,\\tthink\\tabout\\tthe\\tMNIST\\tdataset:\\tall\\thandwritten\\tdigit\\timages\\thave\\tsome\\tsimilarities.\\tThey\\tare\\nmade\\tof\\tconnected\\tlines,\\tthe\\tborders\\tare\\twhite,\\tthey\\tare\\tmore\\tor\\tless\\tcentered,\\tand\\tso\\ton.\\tIf\\tyou\\trandomly\\ngenerated\\timages,\\tonly\\ta\\tridiculously\\ttiny\\tfraction\\tof\\tthem\\twould\\tlook\\tlike\\thandwritten\\tdigits.\\tIn\\tother\\nwords,\\tthe\\tdegrees\\tof\\tfreedom\\tavailable\\tto\\tyou\\tif\\tyou\\ttry\\tto\\tcreate\\ta\\tdigit\\timage\\tare\\tdramatically\\tlower\\nthan\\tthe\\tdegrees\\tof\\tfreedom\\tyou\\twould\\thave\\tif\\tyou\\twere\\tallowed\\tto\\tgenerate\\tany\\timage\\tyou\\twanted.\\nThese\\tconstraints\\ttend\\tto\\tsqueeze\\tthe\\tdataset\\tinto\\ta\\tlower-dimensional\\tmanifold.\\nThe\\tmanifold\\tassumption\\tis\\toften\\taccompanied\\tby\\tanother\\timplicit\\tassumption:\\tthat\\tthe\\ttask\\tat\\thand\\t(e.g.,\\nclassification\\tor\\tregression)\\twill\\tbe\\tsimpler\\tif\\texpressed\\tin\\tthe\\tlower-dimensional\\tspace\\tof\\tthe\\tmanifold.\\nFor\\texample,\\tin\\tthe\\ttop\\trow\\tof\\t\\nFigure\\t8-6\\n\\tthe\\tSwiss\\troll\\tis\\tsplit\\tinto\\ttwo\\tclasses:\\tin\\tthe\\t3D\\tspace\\t(on\\tthe\\nleft),\\tthe\\tdecision\\tboundary\\twould\\tbe\\tfairly\\tcomplex,\\tbut\\tin\\tthe\\t2D\\tunrolled\\tmanifold\\tspace\\t(on\\tthe\\tright),\\nthe\\tdecision\\tboundary\\tis\\ta\\tsimple\\tstraight\\tline.\\nHowever,\\tthis\\tassumption\\tdoes\\tnot\\talways\\thold.\\tFor\\texample,\\tin\\tthe\\tbottom\\trow\\tof\\t\\nFigure\\t8-6\\n,\\tthe\\ndecision\\tboundary\\tis\\tlocated\\tat\\t\\nx\\n1\\n\\t=\\t5.\\tThis\\tdecision\\tboundary\\tlooks\\tvery\\tsimple\\tin\\tthe\\toriginal\\t3D\\tspace\\n(a\\tvertical\\tplane),\\tbut\\tit\\tlooks\\tmore\\tcomplex\\tin\\tthe\\tunrolled\\tmanifold\\t(a\\tcollection\\tof\\tfour\\tindependent\\nline\\tsegments).\\nIn\\tshort,\\tif\\tyou\\treduce\\tthe\\tdimensionality\\tof\\tyour\\ttraining\\tset\\tbefore\\ttraining\\ta\\tmodel,\\tit\\twill\\tdefinitely\\nspeed\\tup\\ttraining,\\tbut\\tit\\tmay\\tnot\\talways\\tlead\\tto\\ta\\tbetter\\tor\\tsimpler\\tsolution;\\tit\\tall\\tdepends\\ton\\tthe\\tdataset.\\nHopefully\\tyou\\tnow\\thave\\ta\\tgood\\tsense\\tof\\twhat\\tthe\\tcurse\\tof\\tdimensionality\\tis\\tand\\thow\\tdimensionality\\nreduction\\talgorithms\\tcan\\tfight\\tit,\\tespecially\\twhen\\tthe\\tmanifold\\tassumption\\tholds.\\tThe\\trest\\tof\\tthis\\tchapter\\nwill\\tgo\\tthrough\\tsome\\tof\\tthe\\tmost\\tpopular\\talgorithms.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 264}), Document(page_content='Figure\\t8-6.\\t\\nThe\\tdecision\\tboundary\\tmay\\tnot\\talways\\tbe\\tsimpler\\twith\\tlower\\tdimensions', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 265}), Document(page_content='PCA\\nPrincipal\\tComponent\\tAnalysis\\n\\t(PCA)\\t\\nis\\tby\\tfar\\tthe\\tmost\\tpopular\\tdimensionality\\treduction\\talgorithm.\\nFirst\\tit\\tidentifies\\tthe\\t\\nhyperplane\\tthat\\tlies\\tclosest\\tto\\tthe\\tdata,\\tand\\tthen\\tit\\tprojects\\tthe\\tdata\\tonto\\tit.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 266}), Document(page_content='Preserving\\tthe\\tVariance\\nBefore\\t\\nyou\\tcan\\tproject\\tthe\\ttraining\\tset\\tonto\\ta\\tlower-dimensional\\thyperplane,\\tyou\\tfirst\\tneed\\tto\\tchoose\\tthe\\nright\\thyperplane.\\tFor\\texample,\\ta\\tsimple\\t2D\\tdataset\\tis\\trepresented\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t8-7\\n,\\talong\\twith\\nthree\\tdifferent\\taxes\\t(i.e.,\\tone-dimensional\\thyperplanes).\\tOn\\tthe\\tright\\tis\\tthe\\tresult\\tof\\tthe\\tprojection\\tof\\tthe\\ndataset\\tonto\\teach\\tof\\tthese\\taxes.\\tAs\\tyou\\tcan\\tsee,\\tthe\\tprojection\\tonto\\tthe\\tsolid\\tline\\tpreserves\\tthe\\tmaximum\\nvariance,\\twhile\\tthe\\tprojection\\tonto\\tthe\\tdotted\\tline\\tpreserves\\tvery\\tlittle\\tvariance,\\tand\\tthe\\tprojection\\tonto\\nthe\\tdashed\\tline\\tpreserves\\tan\\tintermediate\\tamount\\tof\\tvariance.\\nFigure\\t8-7.\\t\\nSelecting\\tthe\\tsubspace\\tonto\\twhich\\tto\\tproject\\nIt\\tseems\\treasonable\\tto\\tselect\\tthe\\taxis\\tthat\\tpreserves\\tthe\\tmaximum\\tamount\\tof\\tvariance,\\tas\\tit\\twill\\tmost\\nlikely\\tlose\\tless\\tinformation\\tthan\\tthe\\tother\\tprojections.\\tAnother\\tway\\tto\\tjustify\\tthis\\tchoice\\tis\\tthat\\tit\\tis\\tthe\\naxis\\tthat\\tminimizes\\tthe\\tmean\\tsquared\\tdistance\\tbetween\\tthe\\toriginal\\tdataset\\tand\\tits\\tprojection\\tonto\\tthat\\naxis.\\tThis\\tis\\tthe\\trather\\tsimple\\tidea\\t\\nbehind\\t\\nPCA\\n.\\n4', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 267}), Document(page_content='Principal\\tComponents\\nPCA\\tidentifies\\t\\nthe\\taxis\\tthat\\taccounts\\tfor\\tthe\\tlargest\\tamount\\tof\\tvariance\\tin\\tthe\\ttraining\\tset.\\tIn\\t\\nFigure\\t8-7\\n,\\tit\\nis\\tthe\\tsolid\\tline.\\tIt\\talso\\tfinds\\ta\\tsecond\\taxis,\\torthogonal\\tto\\tthe\\tfirst\\tone,\\tthat\\taccounts\\tfor\\tthe\\tlargest\\tamount\\nof\\tremaining\\tvariance.\\tIn\\tthis\\t2D\\texample\\tthere\\tis\\tno\\tchoice:\\tit\\tis\\tthe\\tdotted\\tline.\\tIf\\tit\\twere\\ta\\thigher-\\ndimensional\\tdataset,\\tPCA\\twould\\talso\\tfind\\ta\\tthird\\taxis,\\torthogonal\\tto\\tboth\\tprevious\\taxes,\\tand\\ta\\tfourth,\\ta\\nfifth,\\tand\\tso\\ton\\t—\\tas\\tmany\\taxes\\tas\\tthe\\tnumber\\tof\\tdimensions\\tin\\tthe\\tdataset.\\nThe\\tunit\\tvector\\tthat\\tdefines\\tthe\\ti\\nth\\n\\taxis\\tis\\tcalled\\t\\nthe\\ti\\nth\\n\\t\\nprincipal\\tcomponent\\n\\t(PC).\\tIn\\t\\nFigure\\t8-7\\n,\\tthe\\t1\\nst\\nPC\\tis\\t\\nc\\n1\\n\\tand\\tthe\\t2\\nnd\\n\\tPC\\tis\\t\\nc\\n2\\n.\\tIn\\t\\nFigure\\t8-2\\n\\tthe\\tfirst\\ttwo\\tPCs\\tare\\trepresented\\tby\\tthe\\torthogonal\\tarrows\\tin\\nthe\\tplane,\\tand\\tthe\\tthird\\tPC\\twould\\tbe\\torthogonal\\tto\\tthe\\tplane\\t(pointing\\tup\\tor\\tdown).\\nNOTE\\nThe\\tdirection\\tof\\tthe\\tprincipal\\tcomponents\\tis\\tnot\\tstable:\\tif\\tyou\\tperturb\\tthe\\ttraining\\tset\\tslightly\\tand\\trun\\tPCA\\tagain,\\tsome\\tof\\tthe\\tnew\\nPCs\\tmay\\tpoint\\tin\\tthe\\topposite\\tdirection\\tof\\tthe\\toriginal\\tPCs.\\tHowever,\\tthey\\twill\\tgenerally\\tstill\\tlie\\ton\\tthe\\tsame\\taxes.\\tIn\\tsome\\tcases,\\na\\tpair\\tof\\tPCs\\tmay\\teven\\trotate\\tor\\tswap,\\tbut\\tthe\\tplane\\tthey\\tdefine\\twill\\tgenerally\\tremain\\tthe\\tsame.\\nSo\\thow\\tcan\\tyou\\tfind\\tthe\\tprincipal\\tcomponents\\tof\\ta\\ttraining\\tset?\\tLuckily,\\tthere\\tis\\ta\\tstandard\\tmatrix\\nfactorization\\ttechnique\\t\\ncalled\\t\\nSingular\\tValue\\tDecomposition\\n\\t(SVD)\\tthat\\tcan\\tdecompose\\tthe\\ttraining\\tset\\nmatrix\\t\\nX\\n\\tinto\\tthe\\tdot\\tproduct\\tof\\tthree\\tmatrices\\t\\nU\\n\\t·\\tΣ\\t·\\t\\nV\\nT\\n,\\twhere\\t\\nV\\nT\\n\\tcontains\\tall\\tthe\\tprincipal\\tcomponents\\nthat\\twe\\tare\\tlooking\\tfor,\\tas\\tshown\\tin\\t\\nEquation\\t8-1\\n.\\nEquation\\t8-1.\\t\\nPrincipal\\tcomponents\\tmatrix\\nThe\\tfollowing\\tPython\\tcode\\tuses\\tNumPy’s\\t\\nsvd()\\n\\tfunction\\t\\nto\\tobtain\\tall\\tthe\\tprincipal\\tcomponents\\tof\\tthe\\ntraining\\tset,\\tthen\\textracts\\tthe\\tfirst\\ttwo\\tPCs:\\nX_centered\\n\\t\\n=\\n\\t\\nX\\n\\t\\n-\\n\\t\\nX\\n.\\nmean\\n(\\naxis\\n=\\n0\\n)\\nU\\n,\\n\\t\\ns\\n,\\n\\t\\nV\\n\\t\\n=\\n\\t\\nnp\\n.\\nlinalg\\n.\\nsvd\\n(\\nX_centered\\n)\\nc1\\n\\t\\n=\\n\\t\\nV\\n.\\nT\\n[:,\\n\\t\\n0\\n]\\nc2\\n\\t\\n=\\n\\t\\nV\\n.\\nT\\n[:,\\n\\t\\n1\\n]', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 268}), Document(page_content='WARNING\\nPCA\\tassumes\\tthat\\tthe\\tdataset\\tis\\tcentered\\taround\\tthe\\torigin.\\tAs\\twe\\twill\\tsee,\\tScikit-Learn’s\\tPCA\\tclasses\\ttake\\tcare\\tof\\tcentering\\nthe\\tdata\\tfor\\tyou.\\tHowever,\\tif\\tyou\\timplement\\tPCA\\tyourself\\t(as\\tin\\tthe\\tpreceding\\texample),\\tor\\tif\\tyou\\tuse\\tother\\tlibraries,\\tdon’t\\nforget\\tto\\tcenter\\tthe\\t\\ndata\\tfirst.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 269}), Document(page_content='Projecting\\tDown\\tto\\td\\tDimensions\\nOnce\\t\\nyou\\thave\\tidentified\\tall\\tthe\\tprincipal\\tcomponents,\\tyou\\tcan\\treduce\\tthe\\tdimensionality\\tof\\tthe\\tdataset\\ndown\\tto\\t\\nd\\n\\tdimensions\\tby\\tprojecting\\tit\\tonto\\tthe\\t\\nhyperplane\\tdefined\\tby\\tthe\\tfirst\\t\\nd\\n\\tprincipal\\tcomponents.\\nSelecting\\tthis\\thyperplane\\tensures\\tthat\\tthe\\tprojection\\twill\\tpreserve\\tas\\tmuch\\tvariance\\tas\\tpossible.\\tFor\\nexample,\\tin\\t\\nFigure\\t8-2\\n\\tthe\\t3D\\tdataset\\tis\\tprojected\\tdown\\tto\\tthe\\t2D\\tplane\\tdefined\\tby\\tthe\\tfirst\\ttwo\\tprincipal\\ncomponents,\\tpreserving\\ta\\tlarge\\tpart\\tof\\tthe\\tdataset’s\\tvariance.\\tAs\\ta\\tresult,\\tthe\\t2D\\tprojection\\tlooks\\tvery\\nmuch\\tlike\\tthe\\toriginal\\t3D\\tdataset.\\nTo\\tproject\\tthe\\ttraining\\tset\\tonto\\tthe\\thyperplane,\\tyou\\tcan\\tsimply\\tcompute\\tthe\\tdot\\tproduct\\tof\\tthe\\ttraining\\tset\\nmatrix\\t\\nX\\n\\tby\\tthe\\tmatrix\\t\\nW\\nd\\n,\\tdefined\\tas\\tthe\\tmatrix\\tcontaining\\tthe\\tfirst\\t\\nd\\n\\tprincipal\\tcomponents\\t(i.e.,\\tthe\\nmatrix\\tcomposed\\tof\\tthe\\tfirst\\t\\nd\\n\\tcolumns\\tof\\t\\nV\\nT\\n),\\tas\\tshown\\tin\\t\\nEquation\\t8-2\\n.\\nEquation\\t8-2.\\t\\nProjecting\\tthe\\ttraining\\tset\\tdown\\tto\\t\\nd\\n\\tdimensions\\nThe\\tfollowing\\tPython\\tcode\\tprojects\\tthe\\ttraining\\tset\\tonto\\tthe\\tplane\\tdefined\\tby\\tthe\\tfirst\\ttwo\\tprincipal\\ncomponents:\\nW2\\n\\t\\n=\\n\\t\\nV\\n.\\nT\\n[:,\\n\\t\\n:\\n2\\n]\\nX2D\\n\\t\\n=\\n\\t\\nX_centered\\n.\\ndot\\n(\\nW2\\n)\\nThere\\tyou\\thave\\tit!\\tYou\\tnow\\tknow\\thow\\tto\\treduce\\tthe\\tdimensionality\\tof\\tany\\tdataset\\tdown\\tto\\tany\\tnumber\\tof\\ndimensions,\\twhile\\tpreserving\\tas\\tmuch\\tvariance\\tas\\tpossible.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 270}), Document(page_content='Using\\tScikit-Learn\\nScikit-Learn’s\\t\\nPCA\\n\\tclass\\t\\nimplements\\tPCA\\tusing\\tSVD\\tdecomposition\\tjust\\tlike\\twe\\tdid\\tbefore.\\tThe\\nfollowing\\tcode\\tapplies\\tPCA\\tto\\treduce\\tthe\\tdimensionality\\tof\\tthe\\tdataset\\tdown\\tto\\ttwo\\tdimensions\\t(note\\nthat\\tit\\tautomatically\\ttakes\\tcare\\tof\\tcentering\\tthe\\tdata):\\nfrom\\n\\t\\nsklearn.decomposition\\n\\t\\nimport\\n\\t\\nPCA\\npca\\n\\t\\n=\\n\\t\\nPCA\\n(\\nn_components\\n\\t\\n=\\n\\t\\n2\\n)\\nX2D\\n\\t\\n=\\n\\t\\npca\\n.\\nfit_transform\\n(\\nX\\n)\\nAfter\\tfitting\\tthe\\t\\nPCA\\n\\ttransformer\\tto\\tthe\\tdataset,\\tyou\\tcan\\taccess\\tthe\\tprincipal\\tcomponents\\tusing\\tthe\\ncomponents_\\n\\t\\nvariable\\t(note\\tthat\\tit\\tcontains\\tthe\\tPCs\\tas\\thorizontal\\tvectors,\\tso,\\tfor\\texample,\\tthe\\tfirst\\nprincipal\\tcomponent\\tis\\tequal\\tto\\t\\npca.components_.T[:,\\t0]\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 271}), Document(page_content='Explained\\tVariance\\tRatio\\nAnother\\t\\nvery\\tuseful\\tpiece\\tof\\tinformation\\tis\\tthe\\t\\nexplained\\tvariance\\tratio\\n\\tof\\teach\\tprincipal\\tcomponent,\\navailable\\tvia\\tthe\\t\\nexplained_variance_ratio_\\n\\tvariable.\\tIt\\tindicates\\tthe\\tproportion\\tof\\tthe\\tdataset’s\\nvariance\\tthat\\tlies\\talong\\tthe\\taxis\\tof\\teach\\tprincipal\\tcomponent.\\tFor\\texample,\\tlet’s\\tlook\\tat\\tthe\\texplained\\nvariance\\tratios\\tof\\tthe\\tfirst\\ttwo\\tcomponents\\tof\\tthe\\t3D\\tdataset\\trepresented\\tin\\t\\nFigure\\t8-2\\n:\\n>>>\\t\\npca\\n.\\nexplained_variance_ratio_\\narray([\\t0.84248607,\\t\\t0.14631839])\\nThis\\ttells\\tyou\\tthat\\t84.2%\\tof\\tthe\\tdataset’s\\tvariance\\tlies\\talong\\tthe\\tfirst\\taxis,\\tand\\t14.6%\\tlies\\talong\\tthe\\nsecond\\taxis.\\tThis\\tleaves\\tless\\tthan\\t1.2%\\tfor\\tthe\\tthird\\taxis,\\tso\\tit\\tis\\treasonable\\tto\\tassume\\tthat\\tit\\tprobably\\ncarries\\tlittle\\tinformation.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 272}), Document(page_content='Choosing\\tthe\\tRight\\tNumber\\tof\\tDimensions\\nInstead\\t\\nof\\tarbitrarily\\tchoosing\\tthe\\tnumber\\tof\\tdimensions\\tto\\treduce\\tdown\\tto,\\tit\\tis\\tgenerally\\tpreferable\\tto\\nchoose\\tthe\\tnumber\\tof\\tdimensions\\tthat\\tadd\\tup\\tto\\ta\\tsufficiently\\tlarge\\tportion\\tof\\tthe\\tvariance\\t(e.g.,\\t95%).\\nUnless,\\tof\\tcourse,\\tyou\\tare\\treducing\\tdimensionality\\tfor\\tdata\\tvisualization\\t—\\tin\\tthat\\tcase\\tyou\\twill\\ngenerally\\twant\\tto\\treduce\\tthe\\tdimensionality\\tdown\\tto\\t2\\tor\\t3.\\nThe\\tfollowing\\tcode\\tcomputes\\tPCA\\twithout\\treducing\\tdimensionality,\\tthen\\tcomputes\\tthe\\tminimum\\tnumber\\nof\\tdimensions\\trequired\\tto\\tpreserve\\t95%\\tof\\tthe\\ttraining\\tset’s\\tvariance:\\npca\\n\\t\\n=\\n\\t\\nPCA\\n()\\npca\\n.\\nfit\\n(\\nX_train\\n)\\ncumsum\\n\\t\\n=\\n\\t\\nnp\\n.\\ncumsum\\n(\\npca\\n.\\nexplained_variance_ratio_\\n)\\nd\\n\\t\\n=\\n\\t\\nnp\\n.\\nargmax\\n(\\ncumsum\\n\\t\\n>=\\n\\t\\n0.95\\n)\\n\\t\\n+\\n\\t\\n1\\nYou\\tcould\\tthen\\tset\\t\\nn_components=d\\n\\tand\\trun\\tPCA\\tagain.\\tHowever,\\tthere\\tis\\ta\\tmuch\\tbetter\\toption:\\tinstead\\nof\\tspecifying\\tthe\\tnumber\\tof\\tprincipal\\tcomponents\\tyou\\twant\\tto\\tpreserve,\\tyou\\t\\ncan\\tset\\t\\nn_components\\n\\tto\\tbe\\na\\tfloat\\tbetween\\t\\n0.0\\n\\tand\\t\\n1.0\\n,\\tindicating\\tthe\\tratio\\tof\\tvariance\\tyou\\twish\\tto\\tpreserve:\\npca\\n\\t\\n=\\n\\t\\nPCA\\n(\\nn_components\\n=\\n0.95\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\npca\\n.\\nfit_transform\\n(\\nX_train\\n)\\nYet\\tanother\\toption\\tis\\tto\\tplot\\tthe\\t\\nexplained\\tvariance\\tas\\ta\\tfunction\\tof\\tthe\\tnumber\\tof\\tdimensions\\t(simply\\tplot\\ncumsum\\n;\\tsee\\t\\nFigure\\t8-8\\n).\\tThere\\twill\\tusually\\tbe\\tan\\telbow\\tin\\tthe\\tcurve,\\twhere\\tthe\\texplained\\tvariance\\tstops\\ngrowing\\tfast.\\tYou\\tcan\\tthink\\tof\\tthis\\tas\\tthe\\tintrinsic\\tdimensionality\\tof\\tthe\\tdataset.\\tIn\\tthis\\tcase,\\tyou\\tcan\\tsee\\nthat\\treducing\\tthe\\tdimensionality\\tdown\\tto\\tabout\\t100\\tdimensions\\twouldn’t\\tlose\\ttoo\\tmuch\\texplained\\nvariance.\\nFigure\\t8-8.\\t\\nExplained\\tvariance\\tas\\ta\\tfunction\\tof\\tthe\\tnumber\\tof\\tdimensions', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 273}), Document(page_content='PCA\\tfor\\tCompression\\nObviously\\t\\nafter\\tdimensionality\\treduction,\\tthe\\ttraining\\tset\\ttakes\\tup\\tmuch\\tless\\tspace.\\tFor\\texample,\\ttry\\napplying\\tPCA\\tto\\tthe\\tMNIST\\tdataset\\twhile\\tpreserving\\t95%\\tof\\tits\\tvariance.\\tYou\\tshould\\tfind\\tthat\\teach\\ninstance\\twill\\thave\\tjust\\tover\\t150\\tfeatures,\\tinstead\\tof\\tthe\\toriginal\\t784\\tfeatures.\\tSo\\twhile\\tmost\\tof\\tthe\\nvariance\\tis\\tpreserved,\\tthe\\tdataset\\tis\\tnow\\tless\\tthan\\t20%\\tof\\tits\\toriginal\\tsize!\\tThis\\tis\\ta\\treasonable\\ncompression\\tratio,\\tand\\tyou\\tcan\\tsee\\thow\\tthis\\tcan\\tspeed\\tup\\ta\\tclassification\\talgorithm\\t(such\\tas\\tan\\tSVM\\nclassifier)\\ttremendously.\\nIt\\tis\\talso\\tpossible\\tto\\tdecompress\\tthe\\treduced\\tdataset\\tback\\tto\\t784\\tdimensions\\tby\\tapplying\\tthe\\tinverse\\ntransformation\\tof\\tthe\\tPCA\\tprojection.\\tOf\\tcourse\\tthis\\twon’t\\tgive\\tyou\\tback\\tthe\\toriginal\\tdata,\\tsince\\tthe\\nprojection\\tlost\\ta\\tbit\\tof\\tinformation\\t(within\\tthe\\t5%\\tvariance\\tthat\\twas\\tdropped),\\tbut\\tit\\twill\\tlikely\\tbe\\tquite\\nclose\\tto\\tthe\\toriginal\\tdata.\\tThe\\tmean\\tsquared\\tdistance\\tbetween\\tthe\\toriginal\\tdata\\tand\\tthe\\treconstructed\\tdata\\n(compressed\\tand\\tthen\\tdecompressed)\\tis\\tcalled\\tthe\\t\\nreconstruction\\terror\\n.\\t\\nFor\\texample,\\tthe\\tfollowing\\tcode\\ncompresses\\tthe\\tMNIST\\tdataset\\tdown\\tto\\t154\\tdimensions,\\tthen\\tuses\\tthe\\t\\ninverse_transform()\\n\\tmethod\\tto\\ndecompress\\tit\\tback\\tto\\t784\\tdimensions.\\t\\nFigure\\t8-9\\n\\tshows\\ta\\tfew\\tdigits\\tfrom\\tthe\\toriginal\\ttraining\\tset\\t(on\\tthe\\nleft),\\tand\\tthe\\tcorresponding\\tdigits\\tafter\\tcompression\\tand\\tdecompression.\\tYou\\tcan\\tsee\\tthat\\tthere\\tis\\ta\\tslight\\nimage\\tquality\\tloss,\\tbut\\tthe\\tdigits\\tare\\tstill\\tmostly\\tintact.\\npca\\n\\t\\n=\\n\\t\\nPCA\\n(\\nn_components\\n\\t\\n=\\n\\t\\n154\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\npca\\n.\\nfit_transform\\n(\\nX_train\\n)\\nX_recovered\\n\\t\\n=\\n\\t\\npca\\n.\\ninverse_transform\\n(\\nX_reduced\\n)\\nFigure\\t8-9.\\t\\nMNIST\\tcompression\\tpreserving\\t95%\\tof\\tthe\\tvariance\\nThe\\tequation\\tof\\tthe\\tinverse\\ttransformation\\tis\\tshown\\tin\\t\\nEquation\\t8-3\\n.\\nEquation\\t8-3.\\t\\nPCA\\tinverse\\ttransformation,\\tback\\tto\\tthe\\toriginal\\tnumber\\tof\\tdimensions\\n', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 274}), Document(page_content='Incremental\\tPCA\\nOne\\t\\nproblem\\twith\\tthe\\tpreceding\\timplementation\\tof\\tPCA\\tis\\tthat\\tit\\trequires\\tthe\\twhole\\ttraining\\tset\\tto\\tfit\\tin\\nmemory\\tin\\torder\\tfor\\tthe\\tSVD\\talgorithm\\tto\\trun.\\tFortunately,\\t\\nIncremental\\tPCA\\n\\t(IPCA)\\talgorithms\\thave\\nbeen\\tdeveloped:\\tyou\\tcan\\tsplit\\tthe\\ttraining\\tset\\tinto\\tmini-batches\\tand\\tfeed\\tan\\tIPCA\\talgorithm\\tone\\tmini-\\nbatch\\tat\\ta\\ttime.\\tThis\\tis\\tuseful\\tfor\\tlarge\\ttraining\\tsets,\\tand\\talso\\tto\\tapply\\tPCA\\tonline\\t(i.e.,\\ton\\tthe\\tfly,\\tas\\tnew\\ninstances\\tarrive).\\nThe\\tfollowing\\tcode\\tsplits\\tthe\\tMNIST\\tdataset\\tinto\\t100\\tmini-batches\\t(using\\tNumPy’s\\t\\narray_split()\\nfunction)\\t\\nand\\tfeeds\\tthem\\tto\\tScikit-Learn’s\\t\\nIncrementalPCA\\n\\tclass\\n5\\n\\tto\\t\\nreduce\\tthe\\tdimensionality\\tof\\tthe\\nMNIST\\tdataset\\tdown\\tto\\t154\\tdimensions\\t(just\\tlike\\tbefore).\\tNote\\tthat\\tyou\\tmust\\tcall\\tthe\\t\\npartial_fit()\\nmethod\\twith\\teach\\tmini-batch\\trather\\tthan\\t\\nthe\\t\\nfit()\\n\\tmethod\\twith\\tthe\\twhole\\t\\ntraining\\tset:\\nfrom\\n\\t\\nsklearn.decomposition\\n\\t\\nimport\\n\\t\\nIncrementalPCA\\nn_batches\\n\\t\\n=\\n\\t\\n100\\ninc_pca\\n\\t\\n=\\n\\t\\nIncrementalPCA\\n(\\nn_components\\n=\\n154\\n)\\nfor\\n\\t\\nX_batch\\n\\t\\nin\\n\\t\\nnp\\n.\\narray_split\\n(\\nX_train\\n,\\n\\t\\nn_batches\\n):\\n\\t\\t\\t\\t\\ninc_pca\\n.\\npartial_fit\\n(\\nX_batch\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\ninc_pca\\n.\\ntransform\\n(\\nX_train\\n)\\nAlternatively,\\tyou\\tcan\\tuse\\tNumPy’s\\t\\nmemmap\\n\\t\\nclass,\\twhich\\tallows\\tyou\\tto\\tmanipulate\\ta\\tlarge\\tarray\\tstored\\tin\\na\\tbinary\\tfile\\ton\\tdisk\\tas\\tif\\tit\\twere\\tentirely\\tin\\tmemory;\\tthe\\tclass\\tloads\\tonly\\tthe\\tdata\\tit\\tneeds\\tin\\tmemory,\\nwhen\\tit\\tneeds\\tit.\\tSince\\tthe\\t\\nIncrementalPCA\\n\\tclass\\tuses\\tonly\\ta\\tsmall\\tpart\\tof\\tthe\\tarray\\tat\\tany\\tgiven\\ttime,\\nthe\\tmemory\\tusage\\tremains\\tunder\\tcontrol.\\tThis\\tmakes\\tit\\tpossible\\tto\\tcall\\tthe\\tusual\\t\\nfit()\\n\\tmethod,\\tas\\tyou\\ncan\\tsee\\tin\\tthe\\tfollowing\\tcode:\\nX_mm\\n\\t\\n=\\n\\t\\nnp\\n.\\nmemmap\\n(\\nfilename\\n,\\n\\t\\ndtype\\n=\\n\"float32\"\\n,\\n\\t\\nmode\\n=\\n\"readonly\"\\n,\\n\\t\\nshape\\n=\\n(\\nm\\n,\\n\\t\\nn\\n))\\nbatch_size\\n\\t\\n=\\n\\t\\nm\\n\\t\\n//\\n\\t\\nn_batches\\ninc_pca\\n\\t\\n=\\n\\t\\nIncrementalPCA\\n(\\nn_components\\n=\\n154\\n,\\n\\t\\nbatch_size\\n=\\nbatch_size\\n)\\ninc_pca\\n.\\nfit\\n(\\nX_mm\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 275}), Document(page_content='Randomized\\tPCA\\nScikit-Learn\\t\\noffers\\tyet\\tanother\\toption\\tto\\tperform\\tPCA,\\tcalled\\t\\nRandomized\\tPCA\\n.\\tThis\\tis\\ta\\tstochastic\\nalgorithm\\tthat\\tquickly\\tfinds\\tan\\tapproximation\\tof\\tthe\\tfirst\\t\\nd\\n\\tprincipal\\tcomponents.\\tIts\\tcomputational\\ncomplexity\\tis\\t\\nO\\n(\\nm\\n\\t×\\t\\nd\\n2\\n)\\t+\\t\\nO\\n(\\nd\\n3\\n),\\tinstead\\tof\\t\\nO\\n(\\nm\\n\\t×\\t\\nn\\n2\\n)\\t+\\t\\nO\\n(\\nn\\n3\\n),\\tso\\tit\\tis\\tdramatically\\tfaster\\tthan\\tthe\\nprevious\\talgorithms\\twhen\\t\\nd\\n\\tis\\tmuch\\t\\nsmaller\\tthan\\t\\nn\\n.\\nrnd_pca\\n\\t\\n=\\n\\t\\nPCA\\n(\\nn_components\\n=\\n154\\n,\\n\\t\\nsvd_solver\\n=\\n\"randomized\"\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\nrnd_pca\\n.\\nfit_transform\\n(\\nX_train\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 276}), Document(page_content='Kernel\\tPCA\\nIn\\t\\nChapter\\t5\\n\\t\\nwe\\tdiscussed\\tthe\\t\\nkernel\\ttrick,\\ta\\tmathematical\\ttechnique\\tthat\\timplicitly\\tmaps\\tinstances\\tinto\\ta\\nvery\\t\\nhigh-dimensional\\tspace\\t(called\\tthe\\t\\nfeature\\tspace\\n),\\tenabling\\tnonlinear\\tclassification\\tand\\tregression\\nwith\\tSupport\\tVector\\tMachines.\\tRecall\\tthat\\ta\\tlinear\\tdecision\\tboundary\\tin\\tthe\\thigh-dimensional\\tfeature\\nspace\\tcorresponds\\tto\\ta\\tcomplex\\tnonlinear\\tdecision\\tboundary\\tin\\tthe\\t\\noriginal\\tspace\\n.\\nIt\\tturns\\tout\\tthat\\tthe\\tsame\\ttrick\\tcan\\tbe\\tapplied\\tto\\tPCA,\\tmaking\\tit\\tpossible\\tto\\tperform\\tcomplex\\tnonlinear\\nprojections\\tfor\\tdimensionality\\treduction.\\tThis\\tis\\tcalled\\t\\nKernel\\tPCA\\n\\t(kPCA)\\n.\\n6\\n\\tIt\\tis\\toften\\tgood\\tat\\npreserving\\tclusters\\tof\\tinstances\\tafter\\tprojection,\\tor\\tsometimes\\teven\\tunrolling\\tdatasets\\tthat\\tlie\\tclose\\tto\\ta\\ntwisted\\tmanifold.\\nFor\\texample,\\t\\nthe\\tfollowing\\tcode\\tuses\\tScikit-Learn’s\\t\\nKernelPCA\\n\\tclass\\tto\\tperform\\tkPCA\\twith\\tan\\tRBF\\nkernel\\t(see\\t\\nChapter\\t5\\n\\tfor\\tmore\\tdetails\\tabout\\tthe\\tRBF\\tkernel\\tand\\tthe\\tother\\tkernels):\\nfrom\\n\\t\\nsklearn.decomposition\\n\\t\\nimport\\n\\t\\nKernelPCA\\nrbf_pca\\n\\t\\n=\\n\\t\\nKernelPCA\\n(\\nn_components\\n\\t\\n=\\n\\t\\n2\\n,\\n\\t\\nkernel\\n=\\n\"rbf\"\\n,\\n\\t\\ngamma\\n=\\n0.04\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\nrbf_pca\\n.\\nfit_transform\\n(\\nX\\n)\\nFigure\\t8-10\\n\\tshows\\tthe\\tSwiss\\troll,\\treduced\\tto\\ttwo\\tdimensions\\tusing\\ta\\tlinear\\tkernel\\t(equivalent\\tto\\tsimply\\nusing\\tthe\\t\\nPCA\\n\\tclass),\\tan\\tRBF\\tkernel,\\tand\\ta\\tsigmoid\\tkernel\\t(Logistic).\\nFigure\\t8-10.\\t\\nSwiss\\troll\\treduced\\tto\\t2D\\tusing\\tkPCA\\twith\\tvarious\\tkernels', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 277}), Document(page_content='Selecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nAs\\tkPCA\\tis\\tan\\tunsupervised\\tlearning\\talgorithm,\\tthere\\tis\\tno\\tobvious\\tperformance\\tmeasure\\tto\\thelp\\tyou\\nselect\\tthe\\tbest\\tkernel\\tand\\thyperparameter\\tvalues.\\tHowever,\\tdimensionality\\treduction\\tis\\toften\\ta\\npreparation\\tstep\\tfor\\ta\\tsupervised\\tlearning\\ttask\\t(e.g.,\\tclassification),\\tso\\tyou\\tcan\\tsimply\\tuse\\tgrid\\tsearch\\tto\\nselect\\tthe\\tkernel\\tand\\thyperparameters\\tthat\\tlead\\tto\\tthe\\tbest\\tperformance\\ton\\tthat\\ttask.\\tFor\\texample,\\tthe\\nfollowing\\tcode\\tcreates\\ta\\ttwo-step\\tpipeline,\\tfirst\\treducing\\tdimensionality\\tto\\ttwo\\tdimensions\\tusing\\tkPCA,\\nthen\\tapplying\\tLogistic\\tRegression\\tfor\\tclassification.\\tThen\\tit\\tuses\\t\\nGridSearchCV\\n\\t\\nto\\tfind\\tthe\\tbest\\tkernel\\nand\\tgamma\\tvalue\\tfor\\tkPCA\\tin\\torder\\tto\\tget\\tthe\\tbest\\tclassification\\taccuracy\\tat\\tthe\\tend\\tof\\tthe\\tpipeline:\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\nGridSearchCV\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLogisticRegression\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nPipeline\\nclf\\n\\t\\n=\\n\\t\\nPipeline\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"kpca\"\\n,\\n\\t\\nKernelPCA\\n(\\nn_components\\n=\\n2\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"log_reg\"\\n,\\n\\t\\nLogisticRegression\\n())\\n\\t\\t\\t\\t\\n])\\nparam_grid\\n\\t\\n=\\n\\t\\n[{\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\"kpca__gamma\"\\n:\\n\\t\\nnp\\n.\\nlinspace\\n(\\n0.03\\n,\\n\\t\\n0.05\\n,\\n\\t\\n10\\n),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\"kpca__kernel\"\\n:\\n\\t\\n[\\n\"rbf\"\\n,\\n\\t\\n\"sigmoid\"\\n]\\n\\t\\t\\t\\t\\n}]\\ngrid_search\\n\\t\\n=\\n\\t\\nGridSearchCV\\n(\\nclf\\n,\\n\\t\\nparam_grid\\n,\\n\\t\\ncv\\n=\\n3\\n)\\ngrid_search\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nThe\\tbest\\tkernel\\tand\\thyperparameters\\tare\\tthen\\tavailable\\tthrough\\tthe\\t\\nbest_params_\\n\\tvariable:\\n>>>\\t\\nprint\\n(\\ngrid_search\\n.\\nbest_params_\\n)\\n{\\'kpca__gamma\\':\\t0.043333333333333335,\\t\\'kpca__kernel\\':\\t\\'rbf\\'}\\nAnother\\tapproach,\\tthis\\ttime\\tentirely\\tunsupervised,\\tis\\tto\\tselect\\tthe\\tkernel\\tand\\thyperparameters\\tthat\\tyield\\nthe\\tlowest\\treconstruction\\terror.\\tHowever,\\treconstruction\\tis\\tnot\\tas\\teasy\\tas\\twith\\tlinear\\tPCA.\\tHere’s\\twhy.\\nFigure\\t8-11\\n\\tshows\\tthe\\toriginal\\tSwiss\\troll\\t3D\\tdataset\\t(top\\tleft),\\tand\\tthe\\tresulting\\t2D\\tdataset\\tafter\\tkPCA\\tis\\napplied\\tusing\\tan\\tRBF\\tkernel\\t(top\\tright).\\tThanks\\tto\\tthe\\tkernel\\ttrick,\\tthis\\tis\\tmathematically\\tequivalent\\tto\\nmapping\\tthe\\ttraining\\tset\\tto\\tan\\tinfinite-dimensional\\t\\nfeature\\tspace\\t(bottom\\tright)\\tusing\\t\\nthe\\t\\nfeature\\tmap\\n\\tφ,\\nthen\\tprojecting\\tthe\\ttransformed\\ttraining\\tset\\tdown\\tto\\t2D\\tusing\\tlinear\\tPCA.\\tNotice\\tthat\\tif\\twe\\tcould\\tinvert\\nthe\\tlinear\\tPCA\\tstep\\tfor\\ta\\tgiven\\tinstance\\tin\\tthe\\treduced\\tspace,\\tthe\\treconstructed\\tpoint\\twould\\tlie\\tin\\tfeature\\nspace,\\tnot\\tin\\tthe\\toriginal\\tspace\\t(e.g.,\\tlike\\tthe\\tone\\trepresented\\tby\\tan\\tx\\tin\\tthe\\tdiagram).\\tSince\\tthe\\tfeature\\nspace\\tis\\tinfinite-dimensional,\\twe\\tcannot\\tcompute\\tthe\\treconstructed\\tpoint,\\tand\\ttherefore\\twe\\tcannot\\ncompute\\tthe\\ttrue\\treconstruction\\terror.\\tFortunately,\\tit\\tis\\tpossible\\tto\\tfind\\ta\\tpoint\\tin\\tthe\\toriginal\\tspace\\tthat\\nwould\\tmap\\tclose\\tto\\tthe\\treconstructed\\tpoint.\\tThis\\tis\\tcalled\\tthe\\t\\nreconstruction\\t\\npre-image\\n.\\tOnce\\tyou\\thave\\nthis\\tpre-image,\\tyou\\tcan\\tmeasure\\tits\\tsquared\\tdistance\\tto\\tthe\\toriginal\\tinstance.\\tYou\\tcan\\tthen\\tselect\\tthe\\nkernel\\tand\\thyperparameters\\tthat\\tminimize\\tthis\\treconstruction\\tpre-image\\terror.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 278}), Document(page_content='Figure\\t8-11.\\t\\nKernel\\tPCA\\tand\\tthe\\treconstruction\\tpre-image\\terror\\nYou\\tmay\\tbe\\twondering\\thow\\tto\\tperform\\tthis\\treconstruction.\\tOne\\tsolution\\tis\\tto\\ttrain\\ta\\tsupervised\\nregression\\tmodel,\\twith\\tthe\\tprojected\\tinstances\\tas\\tthe\\ttraining\\tset\\tand\\tthe\\toriginal\\tinstances\\tas\\tthe\\ttargets.\\nScikit-Learn\\twill\\tdo\\tthis\\tautomatically\\tif\\tyou\\t\\nset\\t\\nfit_inverse_transform=True\\n,\\tas\\tshown\\tin\\tthe\\nfollowing\\tcode:\\n7\\nrbf_pca\\n\\t\\n=\\n\\t\\nKernelPCA\\n(\\nn_components\\n\\t\\n=\\n\\t\\n2\\n,\\n\\t\\nkernel\\n=\\n\"rbf\"\\n,\\n\\t\\ngamma\\n=\\n0.0433\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfit_inverse_transform\\n=\\nTrue\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\nrbf_pca\\n.\\nfit_transform\\n(\\nX\\n)\\nX_preimage\\n\\t\\n=\\n\\t\\nrbf_pca\\n.\\ninverse_transform\\n(\\nX_reduced\\n)\\nNOTE\\nBy\\tdefault,\\t\\nfit_inverse_transform=False\\n\\tand\\t\\nKernelPCA\\n\\thas\\tno\\t\\ninverse_transform()\\n\\tmethod.\\t\\nThis\\tmethod\\tonly\\tgets\\ncreated\\twhen\\tyou\\tset\\t\\nfit_inverse_transform=True\\n.\\nYou\\tcan\\tthen\\tcompute\\tthe\\treconstruction\\t\\npre-image\\terror:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nmean_squared_error\\n>>>\\t\\nmean_squared_error\\n(\\nX\\n,\\n\\t\\nX_preimage\\n)\\n32.786308795766132\\nNow\\tyou\\tcan\\tuse\\tgrid\\tsearch\\twith\\tcross-validation\\tto\\tfind\\tthe\\tkernel\\tand\\thyperparameters\\tthat\\tminimize\\nthis\\tpre-image\\treconstruction\\t\\nerror.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 279}), Document(page_content='LLE\\nLocally\\tLinear\\tEmbedding\\n\\t(LLE)\\n8\\n\\tis\\t\\nanother\\tvery\\tpowerful\\t\\nnonlinear\\tdimensionality\\treduction\\n(NLDR)\\t\\ntechnique.\\tIt\\tis\\ta\\t\\nManifold\\tLearning\\ttechnique\\tthat\\tdoes\\tnot\\trely\\ton\\tprojections\\tlike\\tthe\\tprevious\\nalgorithms.\\tIn\\ta\\tnutshell,\\tLLE\\tworks\\tby\\tfirst\\tmeasuring\\thow\\teach\\ttraining\\tinstance\\tlinearly\\trelates\\tto\\tits\\nclosest\\tneighbors\\t(c.n.),\\tand\\tthen\\tlooking\\tfor\\ta\\tlow-dimensional\\trepresentation\\tof\\tthe\\ttraining\\tset\\twhere\\nthese\\tlocal\\trelationships\\tare\\tbest\\tpreserved\\t(more\\tdetails\\tshortly).\\tThis\\tmakes\\tit\\tparticularly\\tgood\\tat\\nunrolling\\ttwisted\\tmanifolds,\\tespecially\\twhen\\tthere\\tis\\tnot\\ttoo\\tmuch\\tnoise.\\nFor\\texample,\\tthe\\tfollowing\\tcode\\tuses\\tScikit-Learn’s\\t\\nLocallyLinearEmbedding\\n\\tclass\\t\\nto\\tunroll\\tthe\\tSwiss\\nroll.\\tThe\\tresulting\\t2D\\tdataset\\tis\\tshown\\tin\\t\\nFigure\\t8-12\\n.\\tAs\\tyou\\tcan\\tsee,\\tthe\\tSwiss\\troll\\tis\\tcompletely\\nunrolled\\tand\\tthe\\tdistances\\tbetween\\tinstances\\tare\\tlocally\\twell\\tpreserved.\\tHowever,\\tdistances\\tare\\tnot\\npreserved\\ton\\ta\\tlarger\\tscale:\\tthe\\tleft\\tpart\\tof\\tthe\\tunrolled\\tSwiss\\troll\\tis\\tsqueezed,\\twhile\\tthe\\tright\\tpart\\tis\\nstretched.\\tNevertheless,\\tLLE\\tdid\\ta\\tpretty\\tgood\\tjob\\tat\\tmodeling\\tthe\\tmanifold.\\nfrom\\n\\t\\nsklearn.manifold\\n\\t\\nimport\\n\\t\\nLocallyLinearEmbedding\\nlle\\n\\t\\n=\\n\\t\\nLocallyLinearEmbedding\\n(\\nn_components\\n=\\n2\\n,\\n\\t\\nn_neighbors\\n=\\n10\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\nlle\\n.\\nfit_transform\\n(\\nX\\n)\\nFigure\\t8-12.\\t\\nUnrolled\\tSwiss\\troll\\tusing\\tLLE\\nHere’s\\thow\\t\\nLLE\\tworks:\\tfirst,\\tfor\\teach\\ttraining\\tinstance\\t\\nx\\n(i)\\n,\\tthe\\talgorithm\\tidentifies\\tits\\t\\nk\\n\\tclosest\\nneighbors\\t(in\\tthe\\tpreceding\\tcode\\t\\nk\\n\\t=\\t10),\\tthen\\ttries\\tto\\treconstruct\\t\\nx\\n(i)\\n\\tas\\ta\\tlinear\\tfunction\\tof\\tthese\\nneighbors.\\tMore\\tspecifically,\\tit\\tfinds\\tthe\\tweights\\t\\nw\\ni,j\\n\\tsuch\\tthat\\tthe\\tsquared\\tdistance\\tbetween\\t\\nx\\n(i)\\n\\tand\\t\\n\\tis\\tas\\tsmall\\tas\\tpossible,\\tassuming\\t\\nw\\ni,j\\n\\t=\\t0\\t\\nif\\t\\nx\\n(j)\\n\\tis\\tnot\\tone\\tof\\tthe\\t\\nk\\n\\tclosest\\tneighbors\\tof\\t\\nx\\n(i)\\n.\\nThus\\tthe\\tfirst\\tstep\\tof\\tLLE\\tis\\tthe\\tconstrained\\toptimization\\tproblem\\tdescribed\\tin\\t\\nEquation\\t8-4\\n,\\twhere\\t\\nW\\n\\tis', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 280}), Document(page_content='the\\tweight\\tmatrix\\tcontaining\\tall\\tthe\\tweights\\t\\nw\\ni,j\\n.\\tThe\\tsecond\\tconstraint\\tsimply\\tnormalizes\\tthe\\tweights\\tfor\\neach\\ttraining\\tinstance\\t\\nx\\n(i)\\n.\\nEquation\\t8-4.\\t\\nLLE\\tstep\\t1:\\tlinearly\\tmodeling\\tlocal\\trelationships\\nAfter\\tthis\\tstep,\\tthe\\tweight\\tmatrix\\t\\n\\t(containing\\tthe\\tweights\\t\\n)\\tencodes\\tthe\\tlocal\\tlinear\\trelationships\\nbetween\\tthe\\ttraining\\tinstances.\\tNow\\tthe\\tsecond\\tstep\\tis\\tto\\tmap\\tthe\\ttraining\\tinstances\\tinto\\ta\\t\\nd\\n-dimensional\\nspace\\t(where\\t\\nd\\n\\t<\\t\\nn\\n)\\twhile\\tpreserving\\tthese\\tlocal\\trelationships\\tas\\tmuch\\tas\\tpossible.\\tIf\\t\\nz\\n(i)\\n\\tis\\tthe\\timage\\tof\\nx\\n(i)\\n\\tin\\tthis\\t\\nd\\n-dimensional\\tspace,\\tthen\\twe\\twant\\tthe\\tsquared\\tdistance\\tbetween\\t\\nz\\n(i)\\n\\tand\\t\\n\\tto\\tbe\\tas\\nsmall\\tas\\tpossible.\\tThis\\tidea\\tleads\\tto\\tthe\\tunconstrained\\toptimization\\tproblem\\tdescribed\\tin\\t\\nEquation\\t8-5\\n.\\tIt\\nlooks\\tvery\\tsimilar\\tto\\tthe\\tfirst\\tstep,\\tbut\\tinstead\\tof\\tkeeping\\tthe\\tinstances\\tfixed\\tand\\tfinding\\tthe\\toptimal\\nweights,\\twe\\tare\\tdoing\\tthe\\treverse:\\tkeeping\\tthe\\tweights\\tfixed\\tand\\tfinding\\tthe\\toptimal\\tposition\\tof\\tthe\\ninstances’\\timages\\tin\\tthe\\tlow-dimensional\\tspace.\\tNote\\tthat\\t\\nZ\\n\\tis\\tthe\\tmatrix\\tcontaining\\tall\\t\\nz\\n(i)\\n.\\nEquation\\t8-5.\\t\\nLLE\\tstep\\t2:\\treducing\\tdimensionality\\twhile\\tpreserving\\trelationships\\nScikit-Learn’s\\tLLE\\timplementation\\thas\\tthe\\tfollowing\\tcomputational\\tcomplexity:\\t\\nO\\n(\\nm\\n\\tlog(\\nm\\n)\\nn\\n\\tlog(\\nk\\n))\\n\\tfor\\nfinding\\tthe\\t\\nk\\n\\tnearest\\tneighbors,\\t\\nO\\n(\\nmnk\\n3\\n)\\tfor\\toptimizing\\tthe\\tweights,\\tand\\t\\nO\\n(\\ndm\\n2\\n)\\tfor\\tconstructing\\tthe\\tlow-\\ndimensional\\trepresentations.\\tUnfortunately,\\tthe\\t\\nm\\n2\\n\\tin\\tthe\\tlast\\tterm\\tmakes\\tthis\\talgorithm\\t\\nscale\\tpoorly\\tto\\nvery\\tlarge\\tdatasets.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 281}), Document(page_content='Other\\tDimensionality\\tReduction\\tTechniques\\nThere\\tare\\tmany\\tother\\tdimensionality\\treduction\\ttechniques,\\tseveral\\tof\\twhich\\tare\\tavailable\\tin\\tScikit-Learn.\\nHere\\tare\\tsome\\tof\\tthe\\tmost\\tpopular:\\nMultidimensional\\tScaling\\n\\t(MDS)\\t\\nreduces\\tdimensionality\\twhile\\ttrying\\tto\\tpreserve\\tthe\\tdistances\\nbetween\\tthe\\tinstances\\t(see\\t\\nFigure\\t8-13\\n).\\nIsomap\\n\\tcreates\\t\\na\\tgraph\\tby\\tconnecting\\teach\\tinstance\\tto\\tits\\tnearest\\tneighbors,\\tthen\\treduces\\ndimensionality\\twhile\\ttrying\\tto\\t\\npreserve\\tthe\\t\\ngeodesic\\tdistances\\n9\\n\\tbetween\\tthe\\tinstances.\\nt-Distributed\\tStochastic\\tNeighbor\\tEmbedding\\n\\t(t-SNE)\\t\\nreduces\\tdimensionality\\twhile\\ttrying\\tto\\tkeep\\nsimilar\\tinstances\\tclose\\tand\\tdissimilar\\tinstances\\tapart.\\tIt\\tis\\tmostly\\tused\\tfor\\tvisualization,\\tin\\nparticular\\tto\\tvisualize\\tclusters\\tof\\tinstances\\tin\\thigh-dimensional\\tspace\\t(e.g.,\\tto\\tvisualize\\tthe\\tMNIST\\nimages\\tin\\t2D).\\nLinear\\tDiscriminant\\tAnalysis\\n\\t(LDA)\\tis\\t\\nactually\\ta\\tclassification\\talgorithm,\\tbut\\tduring\\ttraining\\tit\\nlearns\\tthe\\tmost\\tdiscriminative\\taxes\\tbetween\\tthe\\tclasses,\\tand\\tthese\\taxes\\tcan\\tthen\\tbe\\tused\\tto\\tdefine\\ta\\nhyperplane\\tonto\\twhich\\tto\\tproject\\tthe\\tdata.\\tThe\\tbenefit\\tis\\tthat\\tthe\\tprojection\\twill\\tkeep\\tclasses\\tas\\tfar\\napart\\tas\\tpossible,\\tso\\tLDA\\tis\\ta\\tgood\\ttechnique\\tto\\treduce\\tdimensionality\\tbefore\\trunning\\tanother\\nclassification\\talgorithm\\tsuch\\tas\\tan\\tSVM\\tclassifier.\\nFigure\\t8-13.\\t\\nReducing\\tthe\\tSwiss\\troll\\tto\\t2D\\tusing\\tvarious\\ttechniques', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 282}), Document(page_content='Exercises\\n1\\n.\\t\\nWhat\\tare\\tthe\\tmain\\tmotivations\\tfor\\treducing\\ta\\tdataset’s\\tdimensionality?\\tWhat\\tare\\tthe\\tmain\\ndrawbacks?\\n2\\n.\\t\\nWhat\\tis\\tthe\\tcurse\\tof\\tdimensionality?\\n3\\n.\\t\\nOnce\\ta\\tdataset’s\\tdimensionality\\thas\\tbeen\\treduced,\\tis\\tit\\tpossible\\tto\\treverse\\tthe\\toperation?\\tIf\\tso,\\nhow?\\tIf\\tnot,\\twhy?\\n4\\n.\\t\\nCan\\tPCA\\tbe\\tused\\tto\\treduce\\tthe\\tdimensionality\\tof\\ta\\thighly\\tnonlinear\\tdataset?\\n5\\n.\\t\\nSuppose\\tyou\\tperform\\tPCA\\ton\\ta\\t1,000-dimensional\\tdataset,\\tsetting\\tthe\\texplained\\tvariance\\tratio\\tto\\n95%.\\tHow\\tmany\\tdimensions\\twill\\tthe\\tresulting\\tdataset\\thave?\\n6\\n.\\t\\nIn\\twhat\\tcases\\twould\\tyou\\tuse\\tvanilla\\tPCA,\\tIncremental\\tPCA,\\tRandomized\\tPCA,\\tor\\tKernel\\tPCA?\\n7\\n.\\t\\nHow\\tcan\\tyou\\tevaluate\\tthe\\tperformance\\tof\\ta\\tdimensionality\\treduction\\talgorithm\\ton\\tyour\\tdataset?\\n8\\n.\\t\\nDoes\\tit\\tmake\\tany\\tsense\\tto\\tchain\\ttwo\\tdifferent\\tdimensionality\\treduction\\talgorithms?\\n9\\n.\\t\\nLoad\\tthe\\tMNIST\\tdataset\\t(introduced\\tin\\t\\nChapter\\t3\\n)\\tand\\tsplit\\tit\\tinto\\ta\\ttraining\\tset\\tand\\ta\\ttest\\tset\\t(take\\nthe\\tfirst\\t60,000\\tinstances\\tfor\\ttraining,\\tand\\tthe\\tremaining\\t10,000\\tfor\\ttesting).\\tTrain\\ta\\tRandom\\tForest\\nclassifier\\ton\\tthe\\tdataset\\tand\\ttime\\thow\\tlong\\tit\\ttakes,\\tthen\\tevaluate\\tthe\\tresulting\\tmodel\\ton\\tthe\\ttest\\tset.\\nNext,\\tuse\\tPCA\\tto\\treduce\\tthe\\tdataset’s\\tdimensionality,\\twith\\tan\\texplained\\tvariance\\tratio\\tof\\t95%.\\tTrain\\na\\tnew\\tRandom\\tForest\\tclassifier\\ton\\tthe\\treduced\\tdataset\\tand\\tsee\\thow\\tlong\\tit\\ttakes.\\tWas\\ttraining\\tmuch\\nfaster?\\tNext\\tevaluate\\tthe\\tclassifier\\ton\\tthe\\ttest\\tset:\\thow\\tdoes\\tit\\tcompare\\tto\\tthe\\tprevious\\tclassifier?\\n10\\n.\\t\\nUse\\tt-SNE\\tto\\treduce\\tthe\\tMNIST\\tdataset\\tdown\\tto\\ttwo\\tdimensions\\tand\\tplot\\tthe\\tresult\\tusing\\nMatplotlib.\\tYou\\tcan\\tuse\\ta\\tscatterplot\\tusing\\t10\\tdifferent\\tcolors\\tto\\trepresent\\teach\\timage’s\\ttarget\\tclass.\\nAlternatively,\\tyou\\tcan\\twrite\\tcolored\\tdigits\\tat\\tthe\\tlocation\\tof\\teach\\tinstance,\\tor\\teven\\tplot\\tscaled-down\\nversions\\tof\\tthe\\tdigit\\timages\\tthemselves\\t(if\\tyou\\tplot\\tall\\tdigits,\\tthe\\tvisualization\\twill\\tbe\\ttoo\\tcluttered,\\nso\\tyou\\tshould\\teither\\tdraw\\ta\\trandom\\tsample\\tor\\tplot\\tan\\tinstance\\tonly\\tif\\tno\\tother\\tinstance\\thas\\talready\\nbeen\\tplotted\\tat\\ta\\tclose\\tdistance).\\tYou\\tshould\\tget\\ta\\tnice\\tvisualization\\twith\\twell-separated\\tclusters\\tof\\ndigits.\\tTry\\tusing\\tother\\tdimensionality\\treduction\\talgorithms\\tsuch\\tas\\tPCA,\\tLLE,\\tor\\tMDS\\tand\\tcompare\\nthe\\tresulting\\tvisualizations.\\nSolutions\\tto\\tthese\\texercises\\t\\nare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\nWell,\\tfour\\tdimensions\\tif\\tyou\\tcount\\ttime,\\tand\\ta\\tfew\\tmore\\tif\\tyou\\tare\\ta\\tstring\\ttheorist.\\nWatch\\ta\\trotating\\ttesseract\\tprojected\\tinto\\t3D\\tspace\\tat\\t\\nhttp://goo.gl/OM7ktJ\\n.\\tImage\\tby\\tWikipedia\\tuser\\tNerdBoy1392\\t(\\nCreative\\tCommons\\nBY-SA\\t3.0\\n).\\tReproduced\\tfrom\\t\\nhttps://en.wikipedia.org/wiki/Tesseract\\n.\\nFun\\tfact:\\tanyone\\tyou\\tknow\\tis\\tprobably\\tan\\textremist\\tin\\tat\\tleast\\tone\\tdimension\\t(e.g.,\\thow\\tmuch\\tsugar\\tthey\\tput\\tin\\ttheir\\tcoffee),\\tif\\tyou\\nconsider\\tenough\\tdimensions.\\n“On\\tLines\\tand\\tPlanes\\tof\\tClosest\\tFit\\tto\\tSystems\\tof\\tPoints\\tin\\tSpace,”\\tK.\\tPearson\\t(1901).\\nScikit-Learn\\tuses\\tthe\\talgorithm\\tdescribed\\tin\\t“Incremental\\tLearning\\tfor\\tRobust\\tVisual\\tTracking,”\\tD.\\tRoss\\tet\\tal.\\t(2007).\\n1\\n2\\n3\\n4\\n5', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 283}), Document(page_content='“Kernel\\tPrincipal\\tComponent\\tAnalysis,”\\tB.\\tSchölkopf,\\tA.\\tSmola,\\tK.\\tMüller\\t(1999).\\nScikit-Learn\\tuses\\tthe\\talgorithm\\tbased\\ton\\tKernel\\tRidge\\tRegression\\tdescribed\\tin\\tGokhan\\tH.\\tBakır,\\tJason\\tWeston,\\tand\\tBernhard\\tScholkopf,\\n“Learning\\tto\\tFind\\tPre-images”\\n\\t(Tubingen,\\tGermany:\\tMax\\tPlanck\\tInstitute\\tfor\\tBiological\\tCybernetics,\\t2004).\\n“Nonlinear\\tDimensionality\\tReduction\\tby\\tLocally\\tLinear\\tEmbedding,”\\tS.\\tRoweis,\\tL.\\tSaul\\t(2000).\\nThe\\tgeodesic\\tdistance\\tbetween\\ttwo\\tnodes\\tin\\ta\\tgraph\\tis\\tthe\\tnumber\\tof\\tnodes\\ton\\tthe\\tshortest\\tpath\\tbetween\\tthese\\tnodes.\\n6\\n7\\n8\\n9', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 284}), Document(page_content='Part\\tII.\\t\\nNeural\\tNetworks\\tand\\tDeep\\tLearning', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 285}), Document(page_content='Chapter\\t9.\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nTensorFlow\\n\\t\\nis\\ta\\tpowerful\\topen\\tsource\\tsoftware\\tlibrary\\tfor\\tnumerical\\tcomputation,\\tparticularly\\twell\\nsuited\\tand\\tfine-tuned\\tfor\\tlarge-scale\\tMachine\\tLearning.\\tIts\\tbasic\\tprinciple\\tis\\tsimple:\\tyou\\tfirst\\tdefine\\tin\\nPython\\ta\\tgraph\\tof\\tcomputations\\tto\\tperform\\t(for\\texample,\\tthe\\tone\\tin\\t\\nFigure\\t9-1\\n),\\tand\\tthen\\tTensorFlow\\ntakes\\tthat\\tgraph\\tand\\truns\\tit\\tefficiently\\tusing\\toptimized\\tC++\\tcode.\\nFigure\\t9-1.\\t\\nA\\tsimple\\tcomputation\\tgraph\\nMost\\timportantly,\\tit\\tis\\tpossible\\tto\\tbreak\\tup\\tthe\\tgraph\\tinto\\tseveral\\tchunks\\tand\\trun\\tthem\\tin\\tparallel\\tacross\\nmultiple\\tCPUs\\tor\\tGPUs\\t(as\\tshown\\tin\\t\\nFigure\\t9-2\\n).\\tTensorFlow\\talso\\tsupports\\tdistributed\\tcomputing,\\t\\nso\\nyou\\tcan\\ttrain\\tcolossal\\tneural\\tnetworks\\ton\\thumongous\\ttraining\\tsets\\tin\\ta\\treasonable\\tamount\\tof\\ttime\\tby\\nsplitting\\tthe\\tcomputations\\tacross\\thundreds\\tof\\tservers\\t(see\\t\\nChapter\\t12\\n).\\tTensorFlow\\tcan\\ttrain\\ta\\tnetwork\\nwith\\tmillions\\tof\\tparameters\\ton\\ta\\ttraining\\tset\\tcomposed\\tof\\tbillions\\tof\\tinstances\\twith\\tmillions\\tof\\tfeatures\\neach.\\tThis\\tshould\\tcome\\tas\\tno\\tsurprise,\\tsince\\tTensorFlow\\twas\\tdeveloped\\tby\\t\\nthe\\tGoogle\\tBrain\\tteam\\tand\\tit\\npowers\\tmany\\tof\\tGoogle’s\\tlarge-scale\\tservices,\\tsuch\\tas\\tGoogle\\tCloud\\tSpeech,\\tGoogle\\tPhotos,\\tand\\nGoogle\\tSearch.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 286}), Document(page_content='Figure\\t9-2.\\t\\nParallel\\tcomputation\\ton\\tmultiple\\tCPUs/GPUs/servers\\nWhen\\tTensorFlow\\twas\\topen-sourced\\tin\\tNovember\\t2015,\\tthere\\twere\\talready\\tmany\\tpopular\\topen\\tsource\\nlibraries\\tfor\\t\\nDeep\\tLearning\\t(\\nTable\\t9-1\\n\\tlists\\ta\\tfew),\\tand\\tto\\tbe\\tfair\\tmost\\tof\\tTensorFlow’s\\tfeatures\\talready\\nexisted\\tin\\tone\\tlibrary\\tor\\tanother.\\tNevertheless,\\tTensorFlow’s\\tclean\\tdesign,\\tscalability,\\tflexibility,\\n1\\n\\tand\\ngreat\\tdocumentation\\t(not\\tto\\tmention\\tGoogle’s\\tname)\\tquickly\\tboosted\\tit\\tto\\tthe\\ttop\\tof\\tthe\\tlist.\\tIn\\tshort,\\nTensorFlow\\twas\\tdesigned\\tto\\tbe\\tflexible,\\tscalable,\\tand\\tproduction-ready,\\tand\\texisting\\tframeworks\\narguably\\t\\nhit\\tonly\\ttwo\\tout\\tof\\tthe\\tthree\\tof\\tthese.\\tHere\\tare\\tsome\\tof\\tTensorFlow’s\\thighlights:\\nIt\\truns\\tnot\\tonly\\ton\\tWindows,\\tLinux,\\tand\\tmacOS,\\tbut\\talso\\ton\\tmobile\\tdevices,\\tincluding\\tboth\\tiOS\\tand\\nAndroid.\\nIt\\tprovides\\ta\\tvery\\tsimple\\tPython\\tAPI\\t\\ncalled\\t\\nTF.Learn\\n2\\n\\t(\\ntensorflow.contrib.learn\\n),\\tcompatible\\nwith\\t\\nScikit-Learn.\\tAs\\tyou\\twill\\tsee,\\tyou\\tcan\\tuse\\tit\\tto\\ttrain\\tvarious\\ttypes\\tof\\tneural\\tnetworks\\tin\\tjust\\ta\\nfew\\tlines\\tof\\tcode.\\tIt\\twas\\tpreviously\\tan\\tindependent\\tproject\\t\\ncalled\\t\\nScikit\\tFlow\\n\\t(or\\t\\nskflow\\n).\\nIt\\talso\\tprovides\\tanother\\tsimple\\tAPI\\t\\ncalled\\t\\nTF-slim\\n\\t(\\ntensorflow.contrib.slim\\n)\\tto\\tsimplify\\nbuilding,\\ttraining,\\tand\\tevaluating\\tneural\\tnetworks.\\nSeveral\\tother\\thigh-level\\tAPIs\\thave\\tbeen\\t\\nbuilt\\tindependently\\ton\\ttop\\tof\\tTensorFlow,\\tsuch\\tas\\t\\nKeras\\n(now\\tavailable\\tin\\t\\ntensorflow.contrib.keras\\n)\\tor\\t\\nPretty\\tTensor\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 287}), Document(page_content='Its\\tmain\\tPython\\tAPI\\toffers\\tmuch\\tmore\\tflexibility\\t(at\\tthe\\tcost\\tof\\thigher\\tcomplexity)\\tto\\tcreate\\tall\\tsorts\\nof\\tcomputations,\\tincluding\\tany\\tneural\\tnetwork\\tarchitecture\\tyou\\tcan\\tthink\\tof.\\nIt\\tincludes\\thighly\\tefficient\\tC++\\timplementations\\tof\\tmany\\tML\\toperations,\\tparticularly\\tthose\\tneeded\\tto\\nbuild\\tneural\\tnetworks.\\tThere\\tis\\talso\\ta\\tC++\\tAPI\\tto\\tdefine\\tyour\\town\\thigh-performance\\toperations.\\nIt\\tprovides\\tseveral\\tadvanced\\toptimization\\tnodes\\tto\\tsearch\\tfor\\tthe\\tparameters\\tthat\\tminimize\\ta\\tcost\\nfunction.\\tThese\\tare\\tvery\\teasy\\tto\\tuse\\tsince\\tTensorFlow\\tautomatically\\ttakes\\tcare\\tof\\tcomputing\\tthe\\ngradients\\tof\\tthe\\tfunctions\\tyou\\tdefine.\\tThis\\tis\\t\\ncalled\\t\\nautomatic\\tdifferentiating\\n\\t(or\\t\\nautodiff\\n).\\nIt\\talso\\tcomes\\twith\\ta\\tgreat\\tvisualization\\ttool\\t\\ncalled\\t\\nTensorBoard\\n\\tthat\\tallows\\tyou\\tto\\tbrowse\\tthrough\\nthe\\tcomputation\\tgraph,\\tview\\tlearning\\tcurves,\\tand\\tmore.\\nGoogle\\talso\\tlaunched\\ta\\t\\ncloud\\tservice\\tto\\trun\\tTensorFlow\\tgraphs\\n.\\nLast\\tbut\\tnot\\tleast,\\tit\\thas\\ta\\tdedicated\\tteam\\tof\\tpassionate\\tand\\thelpful\\tdevelopers,\\tand\\ta\\tgrowing\\ncommunity\\tcontributing\\tto\\timproving\\tit.\\tIt\\tis\\tone\\tof\\tthe\\tmost\\tpopular\\topen\\tsource\\tprojects\\ton\\nGitHub,\\tand\\tmore\\tand\\tmore\\tgreat\\tprojects\\tare\\tbeing\\tbuilt\\ton\\ttop\\tof\\tit\\t(for\\texamples,\\tcheck\\tout\\tthe\\nresources\\tpage\\ton\\t\\nhttps://www.tensorflow.org/\\n,\\tor\\t\\nhttps://github.com/jtoy/awesome-tensorflow\\n).\\nTo\\task\\ttechnical\\tquestions,\\tyou\\tshould\\tuse\\t\\nhttp://stackoverflow.com/\\n\\tand\\ttag\\tyour\\tquestion\\twith\\n\"tensorflow\"\\n.\\tYou\\tcan\\tfile\\tbugs\\tand\\tfeature\\trequests\\tthrough\\tGitHub.\\tFor\\tgeneral\\tdiscussions,\\tjoin\\nthe\\t\\nGoogle\\tgroup\\n.\\nIn\\tthis\\tchapter,\\twe\\twill\\tgo\\tthrough\\tthe\\tbasics\\tof\\tTensorFlow,\\tfrom\\tinstallation\\tto\\tcreating,\\trunning,\\tsaving,\\nand\\tvisualizing\\tsimple\\tcomputational\\tgraphs.\\tMastering\\tthese\\tbasics\\tis\\timportant\\tbefore\\tyou\\tbuild\\tyour\\nfirst\\tneural\\tnetwork\\t\\n(which\\twe\\twill\\tdo\\tin\\tthe\\t\\nnext\\tchapter).\\nTable\\t9-1.\\t\\nOpen\\tsource\\tDeep\\tLearning\\tlibraries\\t(not\\tan\\texhaustive\\tlist)\\nLibrary\\nAPI\\nPlatforms\\nStarted\\tby\\nYear\\nCaffe\\nPython,\\tC++,\\tMatlab\\nLinux,\\tmacOS,\\tWindows\\nY.\\tJia,\\tUC\\tBerkeley\\t(BVLC)\\n2013\\nDeeplearning4j\\nJava,\\tScala,\\tClojure\\nLinux,\\tmacOS,\\tWindows,\\tAndroid\\nA.\\tGibson,\\tJ.Patterson\\n2014\\nH2O\\nPython,\\tR\\nLinux,\\tmacOS,\\tWindows\\nH2O.ai\\n2014\\nMXNet\\nPython,\\tC++,\\tothers\\nLinux,\\tmacOS,\\tWindows,\\tiOS,\\tAndroid\\nDMLC\\n2015\\nTensorFlow\\nPython,\\tC++\\nLinux,\\tmacOS,\\tWindows,\\tiOS,\\tAndroid\\nGoogle\\n2015\\nTheano\\nPython\\nLinux,\\tmacOS,\\tiOS\\nUniversity\\tof\\tMontreal\\n2010\\nTorch\\nC++,\\tLua\\nLinux,\\tmacOS,\\tiOS,\\tAndroid\\nR.\\tCollobert,\\tK.\\tKavukcuoglu,\\tC.\\tFarabet\\n2002', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 288}), Document(page_content=\"Installation\\nLet’s\\t\\nget\\tstarted!\\tAssuming\\tyou\\tinstalled\\tJupyter\\tand\\tScikit-Learn\\tby\\tfollowing\\tthe\\tinstallation\\ninstructions\\tin\\t\\nChapter\\t2\\n,\\tyou\\tcan\\tsimply\\tuse\\tpip\\tto\\tinstall\\tTensorFlow.\\tIf\\tyou\\tcreated\\tan\\tisolated\\nenvironment\\tusing\\tvirtualenv,\\tyou\\tfirst\\tneed\\tto\\tactivate\\tit:\\n$\\tcd\\t$ML_PATH\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t#\\tYour\\tML\\tworking\\tdirectory\\t(e.g.,\\t$HOME/ml)\\n$\\tsource\\tenv/bin/activate\\nNext,\\tinstall\\tTensorFlow:\\n$\\tpip3\\tinstall\\t--upgrade\\ttensorflow\\nNOTE\\nFor\\tGPU\\tsupport,\\tyou\\tneed\\tto\\tinstall\\t\\ntensorflow-gpu\\n\\tinstead\\tof\\t\\ntensorflow\\n.\\tSee\\t\\nChapter\\t12\\n\\tfor\\tmore\\tdetails.\\nTo\\ttest\\tyour\\tinstallation,\\ttype\\tthe\\tfollowing\\tcommand.\\tIt\\tshould\\toutput\\tthe\\tversion\\tof\\tTensorFlow\\tyou\\ninstalled.\\n$\\tpython3\\t-c\\t'import\\ttensorflow;\\tprint(tensorflow.__version__)'\\n1.0.0\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 289}), Document(page_content='Creating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\nThe\\tfollowing\\tcode\\tcreates\\tthe\\t\\ngraph\\trepresented\\tin\\t\\nFigure\\t9-1\\n:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nx\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3\\n,\\n\\t\\nname\\n=\\n\"x\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n4\\n,\\n\\t\\nname\\n=\\n\"y\"\\n)\\nf\\n\\t\\n=\\n\\t\\nx\\n*\\nx\\n*\\ny\\n\\t\\n+\\n\\t\\ny\\n\\t\\n+\\n\\t\\n2\\nThat’s\\tall\\tthere\\tis\\tto\\tit!\\tThe\\tmost\\timportant\\tthing\\tto\\tunderstand\\tis\\tthat\\tthis\\tcode\\tdoes\\tnot\\tactually\\tperform\\nany\\tcomputation,\\teven\\tthough\\tit\\tlooks\\tlike\\tit\\tdoes\\t(especially\\tthe\\tlast\\tline).\\tIt\\tjust\\tcreates\\ta\\tcomputation\\ngraph.\\tIn\\tfact,\\teven\\tthe\\tvariables\\tare\\tnot\\tinitialized\\tyet.\\tTo\\tevaluate\\tthis\\tgraph,\\tyou\\tneed\\tto\\topen\\ta\\nTensorFlow\\t\\nsession\\n\\tand\\tuse\\tit\\tto\\tinitialize\\tthe\\tvariables\\tand\\tevaluate\\t\\nf\\n.\\tA\\tTensorFlow\\tsession\\ttakes\\tcare\\nof\\tplacing\\tthe\\toperations\\tonto\\t\\ndevices\\n\\tsuch\\tas\\tCPUs\\tand\\tGPUs\\tand\\trunning\\tthem,\\tand\\tit\\tholds\\tall\\tthe\\nvariable\\tvalues.\\n3\\n\\tThe\\tfollowing\\tcode\\tcreates\\ta\\tsession,\\tinitializes\\tthe\\tvariables,\\tand\\tevaluates,\\tand\\t\\nf\\n\\tthen\\ncloses\\tthe\\tsession\\t(which\\tfrees\\tup\\tresources):\\n>>>\\t\\nsess\\n\\t\\n=\\n\\t\\ntf\\n.\\nSession\\n()\\n>>>\\t\\nsess\\n.\\nrun\\n(\\nx\\n.\\ninitializer\\n)\\n>>>\\t\\nsess\\n.\\nrun\\n(\\ny\\n.\\ninitializer\\n)\\n>>>\\t\\nresult\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\nf\\n)\\n>>>\\t\\nprint\\n(\\nresult\\n)\\n42\\n>>>\\t\\nsess\\n.\\nclose\\n()\\nHaving\\tto\\trepeat\\t\\nsess.run()\\n\\tall\\t\\nthe\\ttime\\tis\\ta\\tbit\\tcumbersome,\\t\\nbut\\tfortunately\\tthere\\tis\\ta\\tbetter\\tway:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nx\\n.\\ninitializer\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\ny\\n.\\ninitializer\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nresult\\n\\t\\n=\\n\\t\\nf\\n.\\neval\\n()\\nInside\\tthe\\t\\nwith\\n\\tblock,\\tthe\\tsession\\tis\\tset\\tas\\tthe\\t\\ndefault\\tsession.\\tCalling\\t\\nx.initializer.run()\\n\\tis\\nequivalent\\tto\\tcalling\\t\\ntf.get_default_session().run(x.initializer)\\n,\\tand\\tsimilarly\\t\\nf.eval()\\n\\tis\\nequivalent\\tto\\tcalling\\t\\ntf.get_default_session().run(f)\\n.\\tThis\\tmakes\\tthe\\tcode\\teasier\\tto\\tread.\\nMoreover,\\tthe\\tsession\\tis\\tautomatically\\tclosed\\tat\\tthe\\tend\\tof\\tthe\\tblock.\\nInstead\\tof\\tmanually\\trunning\\tthe\\t\\ninitializer\\tfor\\tevery\\tsingle\\tvariable,\\tyou\\tcan\\tuse\\tthe\\nglobal_variables_initializer()\\n\\t\\nfunction.\\tNote\\tthat\\tit\\tdoes\\tnot\\tactually\\tperform\\tthe\\tinitialization\\nimmediately,\\tbut\\trather\\tcreates\\ta\\tnode\\tin\\tthe\\tgraph\\tthat\\twill\\tinitialize\\tall\\tvariables\\twhen\\tit\\tis\\trun:\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\n\\t\\t\\n#\\tprepare\\tan\\tinit\\tnode\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\n#\\tactually\\tinitialize\\tall\\tthe\\tvariables\\n\\t\\t\\t\\t\\nresult\\n\\t\\n=\\n\\t\\nf\\n.\\neval\\n()\\nInside\\tJupyter\\tor\\twithin\\ta\\tPython\\tshell\\tyou\\tmay\\tprefer\\tto\\t\\ncreate\\tan\\t\\nInteractiveSession\\n.\\tThe\\tonly\\ndifference\\tfrom\\ta\\tregular\\t\\nSession\\n\\tis\\tthat\\twhen\\tan\\t\\nInteractiveSession\\n\\tis\\tcreated\\tit\\tautomatically\\tsets', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 290}), Document(page_content='itself\\tas\\tthe\\tdefault\\tsession,\\tso\\tyou\\tdon’t\\tneed\\ta\\t\\nwith\\n\\tblock\\t(but\\tyou\\tdo\\tneed\\tto\\tclose\\tthe\\tsession\\nmanually\\twhen\\tyou\\tare\\tdone\\twith\\tit):\\n>>>\\t\\nsess\\n\\t\\n=\\n\\t\\ntf\\n.\\nInteractiveSession\\n()\\n>>>\\t\\ninit\\n.\\nrun\\n()\\n>>>\\t\\nresult\\n\\t\\n=\\n\\t\\nf\\n.\\neval\\n()\\n>>>\\t\\nprint\\n(\\nresult\\n)\\n42\\n>>>\\t\\nsess\\n.\\nclose\\n()\\nA\\tTensorFlow\\tprogram\\tis\\ttypically\\tsplit\\tinto\\ttwo\\tparts:\\tthe\\tfirst\\tpart\\tbuilds\\ta\\tcomputation\\tgraph\\t(this\\tis\\ncalled\\tthe\\t\\nconstruction\\tphase\\n),\\t\\nand\\tthe\\tsecond\\tpart\\truns\\tit\\t(this\\tis\\tthe\\t\\nexecution\\tphase\\n).\\tThe\\tconstruction\\nphase\\ttypically\\tbuilds\\ta\\tcomputation\\tgraph\\trepresenting\\tthe\\tML\\tmodel\\tand\\tthe\\tcomputations\\trequired\\tto\\ntrain\\tit.\\tThe\\texecution\\tphase\\tgenerally\\truns\\ta\\tloop\\tthat\\tevaluates\\ta\\ttraining\\tstep\\trepeatedly\\t(for\\texample,\\none\\tstep\\tper\\tmini-batch),\\tgradually\\timproving\\tthe\\t\\nmodel\\tparameters.\\tWe\\twill\\tgo\\tthrough\\tan\\texample\\nshortly.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 291}), Document(page_content='Managing\\tGraphs\\nAny\\t\\nnode\\tyou\\tcreate\\tis\\tautomatically\\tadded\\tto\\tthe\\tdefault\\tgraph:\\n>>>\\t\\nx1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n1\\n)\\n>>>\\t\\nx1\\n.\\ngraph\\n\\t\\nis\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\nTrue\\nIn\\tmost\\tcases\\tthis\\tis\\tfine,\\tbut\\tsometimes\\tyou\\tmay\\twant\\tto\\tmanage\\tmultiple\\tindependent\\tgraphs.\\tYou\\tcan\\tdo\\nthis\\tby\\tcreating\\ta\\t\\nnew\\t\\nGraph\\n\\tand\\ttemporarily\\tmaking\\tit\\tthe\\tdefault\\tgraph\\tinside\\ta\\t\\nwith\\n\\tblock,\\tlike\\tso:\\n>>>\\t\\ngraph\\n\\t\\n=\\n\\t\\ntf\\n.\\nGraph\\n()\\n>>>\\t\\nwith\\n\\t\\ngraph\\n.\\nas_default\\n():\\n...\\t\\n\\t\\t\\t\\t\\nx2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n2\\n)\\n...\\n>>>\\t\\nx2\\n.\\ngraph\\n\\t\\nis\\n\\t\\ngraph\\nTrue\\n>>>\\t\\nx2\\n.\\ngraph\\n\\t\\nis\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\nFalse\\nTIP\\nIn\\tJupyter\\t(or\\tin\\ta\\tPython\\tshell),\\tit\\tis\\tcommon\\tto\\trun\\tthe\\tsame\\tcommands\\tmore\\tthan\\tonce\\twhile\\tyou\\tare\\texperimenting.\\tAs\\ta\\nresult,\\tyou\\tmay\\tend\\tup\\twith\\ta\\t\\ndefault\\tgraph\\tcontaining\\tmany\\tduplicate\\tnodes.\\tOne\\tsolution\\tis\\tto\\trestart\\tthe\\tJupyter\\tkernel\\t(or\\tthe\\nPython\\tshell),\\tbut\\ta\\tmore\\tconvenient\\tsolution\\tis\\tto\\tjust\\treset\\tthe\\tdefault\\tgraph\\tby\\t\\nrunning\\t\\ntf.reset_default_graph()\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 292}), Document(page_content='Lifecycle\\tof\\ta\\tNode\\tValue\\nWhen\\t\\nyou\\tevaluate\\ta\\tnode,\\tTensorFlow\\tautomatically\\tdetermines\\tthe\\tset\\tof\\tnodes\\tthat\\tit\\tdepends\\ton\\tand\\tit\\nevaluates\\tthese\\tnodes\\tfirst.\\tFor\\texample,\\tconsider\\tthe\\t\\nfollowing\\tcode:\\nw\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n3\\n)\\nx\\n\\t\\n=\\n\\t\\nw\\n\\t\\n+\\n\\t\\n2\\ny\\n\\t\\n=\\n\\t\\nx\\n\\t\\n+\\n\\t\\n5\\nz\\n\\t\\n=\\n\\t\\nx\\n\\t\\n*\\n\\t\\n3\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nprint\\n(\\ny\\n.\\neval\\n())\\n\\t\\t\\n#\\t10\\n\\t\\t\\t\\t\\nprint\\n(\\nz\\n.\\neval\\n())\\n\\t\\t\\n#\\t15\\nFirst,\\tthis\\tcode\\tdefines\\ta\\tvery\\tsimple\\tgraph.\\tThen\\tit\\tstarts\\ta\\tsession\\tand\\truns\\tthe\\tgraph\\tto\\tevaluate\\t\\ny\\n:\\nTensorFlow\\tautomatically\\tdetects\\tthat\\t\\ny\\n\\tdepends\\ton\\t\\nx\\n,\\twhich\\tdepends\\ton\\t\\nw\\n,\\tso\\tit\\tfirst\\tevaluates\\t\\nw\\n,\\tthen\\t\\nx\\n,\\nthen\\t\\ny\\n,\\tand\\treturns\\tthe\\tvalue\\tof\\t\\ny\\n.\\tFinally,\\tthe\\tcode\\truns\\tthe\\tgraph\\tto\\tevaluate\\t\\nz\\n.\\tOnce\\tagain,\\tTensorFlow\\ndetects\\tthat\\tit\\tmust\\tfirst\\tevaluate\\t\\nw\\n\\tand\\t\\nx\\n.\\tIt\\tis\\timportant\\tto\\tnote\\tthat\\tit\\twill\\t\\nnot\\n\\treuse\\tthe\\tresult\\tof\\tthe\\nprevious\\tevaluation\\tof\\t\\nw\\n\\tand\\t\\nx\\n.\\tIn\\tshort,\\tthe\\tpreceding\\tcode\\tevaluates\\t\\nw\\n\\tand\\t\\nx\\n\\ttwice.\\nAll\\tnode\\tvalues\\tare\\tdropped\\tbetween\\tgraph\\truns,\\texcept\\tvariable\\tvalues,\\twhich\\tare\\tmaintained\\tby\\tthe\\nsession\\tacross\\tgraph\\truns\\t(queues\\tand\\treaders\\talso\\tmaintain\\tsome\\tstate,\\tas\\twe\\twill\\tsee\\tin\\t\\nChapter\\t12\\n).\\tA\\nvariable\\tstarts\\tits\\tlife\\twhen\\tits\\tinitializer\\tis\\trun,\\tand\\tit\\tends\\twhen\\tthe\\tsession\\tis\\tclosed.\\nIf\\tyou\\twant\\tto\\tevaluate\\t\\ny\\n\\tand\\t\\nz\\n\\tefficiently,\\twithout\\tevaluating\\t\\nw\\n\\tand\\t\\nx\\n\\ttwice\\tas\\tin\\tthe\\tprevious\\tcode,\\tyou\\nmust\\task\\tTensorFlow\\tto\\tevaluate\\tboth\\t\\ny\\n\\tand\\t\\nz\\n\\tin\\tjust\\tone\\tgraph\\trun,\\tas\\tshown\\tin\\tthe\\tfollowing\\tcode:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ny_val\\n,\\n\\t\\nz_val\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n([\\ny\\n,\\n\\t\\nz\\n])\\n\\t\\t\\t\\t\\nprint\\n(\\ny_val\\n)\\n\\t\\t\\n#\\t10\\n\\t\\t\\t\\t\\nprint\\n(\\nz_val\\n)\\n\\t\\t\\n#\\t15\\nWARNING\\nIn\\tsingle-process\\tTensorFlow,\\tmultiple\\tsessions\\tdo\\tnot\\tshare\\tany\\tstate,\\teven\\tif\\tthey\\treuse\\tthe\\tsame\\tgraph\\t(each\\tsession\\twould\\nhave\\tits\\town\\tcopy\\tof\\tevery\\tvariable).\\tIn\\tdistributed\\tTensorFlow\\t(see\\t\\nChapter\\t12\\n),\\tvariable\\tstate\\tis\\tstored\\ton\\tthe\\tservers,\\tnot\\tin\\nthe\\tsessions,\\tso\\tmultiple\\tsessions\\tcan\\tshare\\tthe\\tsame\\tvariables.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 293}), Document(page_content='Linear\\tRegression\\twith\\tTensorFlow\\nTensorFlow\\t\\noperations\\t(also\\tcalled\\t\\nops\\n\\tfor\\tshort)\\tcan\\ttake\\tany\\tnumber\\tof\\tinputs\\tand\\tproduce\\tany\\tnumber\\nof\\toutputs.\\tFor\\texample,\\tthe\\taddition\\tand\\tmultiplication\\tops\\teach\\ttake\\ttwo\\tinputs\\tand\\tproduce\\tone\\toutput.\\nConstants\\tand\\tvariables\\ttake\\tno\\tinput\\t(they\\tare\\tcalled\\t\\nsource\\tops\\n).\\t\\nThe\\tinputs\\tand\\toutputs\\tare\\nmultidimensional\\tarrays,\\tcalled\\t\\ntensors\\n\\t(hence\\tthe\\tname\\t“tensor\\tflow”).\\tJust\\tlike\\tNumPy\\tarrays,\\ttensors\\nhave\\ta\\ttype\\tand\\ta\\tshape.\\tIn\\tfact,\\tin\\tthe\\tPython\\tAPI\\ttensors\\tare\\tsimply\\trepresented\\tby\\tNumPy\\tndarrays.\\nThey\\ttypically\\tcontain\\tfloats,\\tbut\\tyou\\tcan\\talso\\tuse\\tthem\\tto\\tcarry\\tstrings\\t(arbitrary\\tbyte\\tarrays).\\nIn\\tthe\\texamples\\tso\\tfar,\\tthe\\ttensors\\tjust\\tcontained\\ta\\tsingle\\tscalar\\tvalue,\\tbut\\tyou\\tcan\\tof\\tcourse\\tperform\\ncomputations\\ton\\tarrays\\tof\\tany\\tshape.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tmanipulates\\t2D\\tarrays\\tto\\tperform\\nLinear\\tRegression\\ton\\tthe\\tCalifornia\\thousing\\tdataset\\t(introduced\\tin\\t\\nChapter\\t2\\n).\\tIt\\tstarts\\tby\\tfetching\\tthe\\ndataset;\\tthen\\tit\\tadds\\tan\\textra\\tbias\\tinput\\tfeature\\t(\\nx\\n0\\n\\t=\\t1)\\tto\\tall\\ttraining\\tinstances\\t(it\\tdoes\\tso\\tusing\\tNumPy\\tso\\nit\\truns\\timmediately);\\tthen\\tit\\tcreates\\ttwo\\tTensorFlow\\tconstant\\tnodes,\\t\\nX\\n\\tand\\t\\ny\\n,\\tto\\thold\\tthis\\tdata\\tand\\tthe\\ntargets,\\n4\\n\\tand\\tit\\tuses\\tsome\\tof\\tthe\\tmatrix\\toperations\\tprovided\\tby\\tTensorFlow\\tto\\tdefine\\t\\ntheta\\n.\\tThese\\tmatrix\\nfunctions\\t—\\t\\ntranspose()\\n,\\n\\t\\nmatmul()\\n,\\tand\\t\\nmatrix_inverse()\\n\\t—\\tare\\tself-explanatory,\\tbut\\tas\\tusual\\tthey\\ndo\\tnot\\tperform\\tany\\tcomputations\\timmediately;\\tinstead,\\tthey\\tcreate\\tnodes\\tin\\tthe\\tgraph\\tthat\\twill\\tperform\\nthem\\twhen\\tthe\\tgraph\\tis\\trun.\\tYou\\tmay\\trecognize\\tthat\\tthe\\tdefinition\\tof\\t\\ntheta\\n\\tcorresponds\\tto\\tthe\\tNormal\\nEquation\\t(\\n\\t=\\t(\\nX\\nT\\n\\t·\\t\\nX\\n)\\n–1\\n\\t·\\t\\nX\\nT\\n\\t·\\t\\ny\\n;\\tsee\\t\\nChapter\\t4\\n).\\tFinally,\\tthe\\tcode\\tcreates\\ta\\tsession\\tand\\tuses\\tit\\t\\nto\\nevaluate\\t\\ntheta\\n.\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nfetch_california_housing\\nhousing\\n\\t\\n=\\n\\t\\nfetch_california_housing\\n()\\nm\\n,\\n\\t\\nn\\n\\t\\n=\\n\\t\\nhousing\\n.\\ndata\\n.\\nshape\\nhousing_data_plus_bias\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\nnp\\n.\\nones\\n((\\nm\\n,\\n\\t\\n1\\n)),\\n\\t\\nhousing\\n.\\ndata\\n]\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\nhousing_data_plus_bias\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"X\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\nhousing\\n.\\ntarget\\n.\\nreshape\\n(\\n-\\n1\\n,\\n\\t\\n1\\n),\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"y\"\\n)\\nXT\\n\\t\\n=\\n\\t\\ntf\\n.\\ntranspose\\n(\\nX\\n)\\ntheta\\n\\t\\n=\\n\\t\\ntf\\n.\\nmatmul\\n(\\ntf\\n.\\nmatmul\\n(\\ntf\\n.\\nmatrix_inverse\\n(\\ntf\\n.\\nmatmul\\n(\\nXT\\n,\\n\\t\\nX\\n)),\\n\\t\\nXT\\n),\\n\\t\\ny\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ntheta_value\\n\\t\\n=\\n\\t\\ntheta\\n.\\neval\\n()\\nThe\\tmain\\tbenefit\\tof\\tthis\\tcode\\tversus\\tcomputing\\tthe\\tNormal\\tEquation\\tdirectly\\tusing\\tNumPy\\tis\\tthat\\nTensorFlow\\twill\\tautomatically\\trun\\tthis\\ton\\tyour\\tGPU\\tcard\\tif\\tyou\\thave\\tone\\t(provided\\tyou\\tinstalled\\nTensorFlow\\twith\\tGPU\\tsupport,\\tof\\tcourse;\\tsee\\t\\nChapter\\t12\\n\\tfor\\t\\nmore\\tdetails).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 294}), Document(page_content='Implementing\\tGradient\\tDescent\\nLet’s\\t\\ntry\\tusing\\tBatch\\tGradient\\tDescent\\t(introduced\\tin\\t\\nChapter\\t4\\n)\\tinstead\\tof\\tthe\\tNormal\\tEquation.\\tFirst\\twe\\nwill\\tdo\\tthis\\tby\\tmanually\\tcomputing\\tthe\\tgradients,\\tthen\\twe\\twill\\tuse\\tTensorFlow’s\\tautodiff\\tfeature\\tto\\tlet\\nTensorFlow\\tcompute\\tthe\\tgradients\\tautomatically,\\tand\\tfinally\\twe\\twill\\tuse\\ta\\tcouple\\tof\\tTensorFlow’s\\tout-\\nof-the-box\\toptimizers.\\nWARNING\\nWhen\\tusing\\tGradient\\tDescent,\\tremember\\tthat\\tit\\tis\\timportant\\tto\\tfirst\\tnormalize\\tthe\\tinput\\t\\nfeature\\tvectors,\\tor\\telse\\ttraining\\tmay\\tbe\\nmuch\\tslower.\\tYou\\tcan\\tdo\\tthis\\tusing\\tTensorFlow,\\tNumPy,\\t\\nScikit-Learn’s\\t\\nStandardScaler\\n,\\tor\\tany\\tother\\tsolution\\tyou\\tprefer.\\tThe\\nfollowing\\tcode\\tassumes\\tthat\\tthis\\tnormalization\\thas\\talready\\tbeen\\tdone.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 295}), Document(page_content='Manually\\tComputing\\tthe\\tGradients\\nThe\\t\\nfollowing\\tcode\\tshould\\tbe\\tfairly\\tself-explanatory,\\texcept\\tfor\\ta\\tfew\\tnew\\telements:\\nThe\\t\\nrandom_uniform()\\n\\t\\nfunction\\tcreates\\ta\\tnode\\tin\\tthe\\tgraph\\tthat\\twill\\tgenerate\\ta\\ttensor\\tcontaining\\nrandom\\tvalues,\\tgiven\\tits\\tshape\\tand\\tvalue\\trange,\\tmuch\\tlike\\tNumPy’s\\t\\nrand()\\n\\tfunction.\\nThe\\t\\nassign()\\n\\t\\nfunction\\tcreates\\ta\\tnode\\tthat\\twill\\tassign\\ta\\tnew\\tvalue\\tto\\ta\\tvariable.\\tIn\\tthis\\tcase,\\tit\\nimplements\\tthe\\tBatch\\tGradient\\tDescent\\tstep\\t\\nθ\\n(next\\tstep)\\n\\t=\\t\\nθ\\n\\t–\\t\\nη\\nθ\\nMSE(\\nθ\\n).\\nThe\\tmain\\tloop\\texecutes\\tthe\\ttraining\\tstep\\tover\\tand\\tover\\tagain\\t(\\nn_epochs\\n\\ttimes),\\tand\\tevery\\t100\\niterations\\tit\\tprints\\tout\\tthe\\tcurrent\\t\\nMean\\tSquared\\tError\\t(\\nmse\\n).\\tYou\\tshould\\tsee\\tthe\\tMSE\\tgo\\tdown\\tat\\nevery\\titeration.\\nn_epochs\\n\\t\\n=\\n\\t\\n1000\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\nscaled_housing_data_plus_bias\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"X\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\nhousing\\n.\\ntarget\\n.\\nreshape\\n(\\n-\\n1\\n,\\n\\t\\n1\\n),\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"y\"\\n)\\ntheta\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_uniform\\n([\\nn\\n\\t\\n+\\n\\t\\n1\\n,\\n\\t\\n1\\n],\\n\\t\\n-\\n1.0\\n,\\n\\t\\n1.0\\n),\\n\\t\\nname\\n=\\n\"theta\"\\n)\\ny_pred\\n\\t\\n=\\n\\t\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\ntheta\\n,\\n\\t\\nname\\n=\\n\"predictions\"\\n)\\nerror\\n\\t\\n=\\n\\t\\ny_pred\\n\\t\\n-\\n\\t\\ny\\nmse\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\nerror\\n),\\n\\t\\nname\\n=\\n\"mse\"\\n)\\ngradients\\n\\t\\n=\\n\\t\\n2\\n/\\nm\\n\\t\\n*\\n\\t\\ntf\\n.\\nmatmul\\n(\\ntf\\n.\\ntranspose\\n(\\nX\\n),\\n\\t\\nerror\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\ntf\\n.\\nassign\\n(\\ntheta\\n,\\n\\t\\ntheta\\n\\t\\n-\\n\\t\\nlearning_rate\\n\\t\\n*\\n\\t\\ngradients\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ninit\\n)\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nepoch\\n\\t\\n%\\n\\t\\n100\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nprint\\n(\\n\"Epoch\"\\n,\\n\\t\\nepoch\\n,\\n\\t\\n\"MSE\\t=\"\\n,\\n\\t\\nmse\\n.\\neval\\n())\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n)\\n\\t\\t\\t\\t\\nbest_theta\\n\\t\\n=\\n\\t\\ntheta\\n.\\neval\\n()', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 296}), Document(page_content='Using\\tautodiff\\nThe\\t\\npreceding\\tcode\\tworks\\tfine,\\tbut\\tit\\trequires\\tmathematically\\tderiving\\tthe\\tgradients\\tfrom\\tthe\\t\\ncost\\nfunction\\t(MSE).\\tIn\\tthe\\tcase\\tof\\tLinear\\tRegression,\\tit\\tis\\treasonably\\teasy,\\tbut\\tif\\tyou\\thad\\tto\\tdo\\tthis\\twith\\tdeep\\nneural\\tnetworks\\tyou\\twould\\tget\\tquite\\ta\\theadache:\\tit\\twould\\tbe\\ttedious\\tand\\terror-prone.\\tYou\\tcould\\tuse\\nsymbolic\\tdifferentiation\\n\\tto\\t\\nautomatically\\tfind\\tthe\\tequations\\tfor\\tthe\\tpartial\\tderivatives\\tfor\\tyou,\\tbut\\tthe\\nresulting\\tcode\\twould\\tnot\\tnecessarily\\tbe\\tvery\\tefficient.\\nTo\\tunderstand\\twhy,\\tconsider\\tthe\\tfunction\\t\\nf\\n(\\nx\\n)=\\texp(exp(exp(\\nx\\n))).\\tIf\\tyou\\tknow\\tcalculus,\\tyou\\tcan\\tfigure\\tout\\nits\\tderivative\\t\\nf′\\n(\\nx\\n)\\t=\\texp(\\nx\\n)\\t×\\texp(exp(\\nx\\n))\\t×\\texp(exp(exp(\\nx\\n))).\\tIf\\tyou\\tcode\\t\\nf\\n(\\nx\\n)\\tand\\t\\nf′\\n(\\nx\\n)\\tseparately\\tand\\nexactly\\tas\\tthey\\tappear,\\tyour\\tcode\\twill\\tnot\\tbe\\tas\\tefficient\\tas\\tit\\tcould\\tbe.\\tA\\tmore\\tefficient\\tsolution\\twould\\nbe\\tto\\twrite\\ta\\tfunction\\tthat\\tfirst\\tcomputes\\texp(\\nx\\n),\\tthen\\texp(exp(\\nx\\n)),\\tthen\\texp(exp(exp(\\nx\\n))),\\tand\\treturns\\tall\\nthree.\\tThis\\tgives\\tyou\\t\\nf\\n(\\nx\\n)\\tdirectly\\t(the\\tthird\\tterm),\\tand\\tif\\tyou\\tneed\\tthe\\tderivative\\tyou\\tcan\\tjust\\tmultiply\\tall\\nthree\\tterms\\tand\\tyou\\tare\\tdone.\\tWith\\tthe\\tnaïve\\tapproach\\tyou\\twould\\thave\\thad\\tto\\tcall\\tthe\\t\\nexp\\n\\tfunction\\tnine\\ntimes\\tto\\tcompute\\tboth\\t\\nf\\n(\\nx\\n)\\tand\\t\\nf′\\n(\\nx\\n).\\tWith\\tthis\\tapproach\\tyou\\tjust\\tneed\\tto\\tcall\\tit\\tthree\\ttimes.\\nIt\\tgets\\tworse\\twhen\\tyour\\tfunction\\tis\\tdefined\\tby\\tsome\\tarbitrary\\tcode.\\tCan\\tyou\\tfind\\tthe\\tequation\\t(or\\tthe\\ncode)\\tto\\tcompute\\tthe\\tpartial\\tderivatives\\tof\\tthe\\tfollowing\\tfunction?\\tHint:\\tdon’t\\teven\\ttry.\\ndef\\n\\t\\nmy_func\\n(\\na\\n,\\n\\t\\nb\\n):\\n\\t\\t\\t\\t\\nz\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\t\\t\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n100\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nz\\n\\t\\n=\\n\\t\\na\\n\\t\\n*\\n\\t\\nnp\\n.\\ncos\\n(\\nz\\n\\t\\n+\\n\\t\\ni\\n)\\n\\t\\n+\\n\\t\\nz\\n\\t\\n*\\n\\t\\nnp\\n.\\nsin\\n(\\nb\\n\\t\\n-\\n\\t\\ni\\n)\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nz\\nFortunately,\\tTensorFlow’s\\tautodiff\\tfeature\\tcomes\\tto\\tthe\\trescue:\\tit\\tcan\\tautomatically\\tand\\tefficiently\\ncompute\\tthe\\tgradients\\tfor\\tyou.\\tSimply\\treplace\\tthe\\t\\ngradients\\t=\\t...\\n\\tline\\tin\\tthe\\tGradient\\tDescent\\tcode\\tin\\nthe\\tprevious\\tsection\\twith\\tthe\\tfollowing\\tline,\\tand\\tthe\\tcode\\twill\\tcontinue\\tto\\twork\\tjust\\tfine:\\ngradients\\n\\t\\n=\\n\\t\\ntf\\n.\\ngradients\\n(\\nmse\\n,\\n\\t\\n[\\ntheta\\n])[\\n0\\n]\\nThe\\t\\ngradients()\\n\\t\\nfunction\\ttakes\\tan\\top\\t(in\\tthis\\tcase\\t\\nmse\\n)\\tand\\ta\\tlist\\tof\\tvariables\\t(in\\tthis\\tcase\\tjust\\t\\ntheta\\n),\\nand\\tit\\tcreates\\ta\\tlist\\tof\\tops\\t(one\\tper\\tvariable)\\tto\\tcompute\\tthe\\tgradients\\tof\\tthe\\top\\twith\\tregards\\tto\\teach\\nvariable.\\tSo\\tthe\\t\\ngradients\\n\\tnode\\twill\\tcompute\\tthe\\tgradient\\tvector\\tof\\tthe\\tMSE\\twith\\tregards\\tto\\t\\ntheta\\n.\\nThere\\tare\\tfour\\tmain\\tapproaches\\tto\\tcomputing\\tgradients\\tautomatically.\\tThey\\tare\\tsummarized\\tin\\t\\nTable\\t9-2\\n.\\nTensorFlow\\tuses\\t\\nreverse-mode\\tautodiff\\n,\\twhich\\tis\\tperfect\\t(efficient\\tand\\taccurate)\\twhen\\tthere\\tare\\tmany\\ninputs\\tand\\tfew\\toutputs,\\tas\\tis\\toften\\tthe\\tcase\\tin\\tneural\\tnetworks.\\tIt\\tcomputes\\tall\\tthe\\tpartial\\tderivatives\\tof\\nthe\\toutputs\\twith\\tregards\\tto\\tall\\tthe\\tinputs\\tin\\tjust\\t\\nn\\noutputs\\n\\t+\\t1\\tgraph\\ttraversals.\\nTable\\t9-2.\\t\\nMain\\tsolutions\\tto\\tcompute\\tgradients\\tautomatically\\nTechnique\\nNb\\tof\\tgraph\\ttraversals\\tto\\tcompute\\tall\\ngradients\\nAccuracy\\nSupports\\tarbitrary\\ncode\\nComment\\nNumerical\\ndifferentiation\\nn\\ninputs\\n\\t+\\t1\\nLow\\nYes\\nTrivial\\tto\\timplement\\nSymbolic\\tdifferentiation\\nN/A\\nHigh\\nNo\\nBuilds\\ta\\tvery\\tdifferent', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 297}), Document(page_content='graph\\nForward-mode\\tautodiff\\nn\\ninputs\\nHigh\\nYes\\nUses\\t\\ndual\\tnumbers\\nReverse-mode\\tautodiff\\nn\\noutputs\\n\\t+\\t1\\nHigh\\nYes\\nImplemented\\tby\\nTensorFlow\\nIf\\tyou\\tare\\tinterested\\tin\\thow\\tthis\\tmagic\\t\\nworks,\\tcheck\\tout\\t\\nAppendix\\tD\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 298}), Document(page_content='Using\\tan\\tOptimizer\\nSo\\t\\nTensorFlow\\tcomputes\\tthe\\tgradients\\tfor\\tyou.\\tBut\\tit\\tgets\\teven\\teasier:\\tit\\talso\\tprovides\\ta\\tnumber\\tof\\noptimizers\\tout\\tof\\tthe\\tbox,\\tincluding\\ta\\tGradient\\tDescent\\toptimizer.\\tYou\\tcan\\tsimply\\treplace\\tthe\\tpreceding\\ngradients\\t=\\t...\\n\\tand\\t\\ntraining_op\\t=\\t...\\n\\tlines\\twith\\tthe\\tfollowing\\tcode,\\tand\\tonce\\tagain\\teverything\\nwill\\tjust\\twork\\tfine:\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nGradientDescentOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nmse\\n)\\nIf\\tyou\\twant\\tto\\tuse\\ta\\tdifferent\\ttype\\tof\\toptimizer,\\tyou\\tjust\\tneed\\tto\\tchange\\tone\\tline.\\tFor\\texample,\\tyou\\tcan\\tuse\\na\\tmomentum\\toptimizer\\t(which\\toften\\tconverges\\tmuch\\tfaster\\tthan\\tGradient\\tDescent;\\tsee\\t\\nChapter\\t11\\n)\\tby\\ndefining\\tthe\\toptimizer\\t\\nlike\\tthis:\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nMomentumOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmomentum\\n=\\n0.9\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 299}), Document(page_content='Feeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\nLet’s\\t\\ntry\\tto\\tmodify\\tthe\\tprevious\\tcode\\tto\\timplement\\t\\nMini-batch\\tGradient\\tDescent.\\tFor\\tthis,\\twe\\tneed\\ta\\tway\\nto\\treplace\\t\\nX\\n\\tand\\t\\ny\\n\\tat\\tevery\\titeration\\twith\\tthe\\tnext\\tmini-batch.\\tThe\\tsimplest\\tway\\tto\\tdo\\tthis\\tis\\tto\\tuse\\nplaceholder\\tnodes.\\tThese\\tnodes\\tare\\tspecial\\tbecause\\tthey\\tdon’t\\tactually\\tperform\\tany\\tcomputation,\\tthey\\njust\\toutput\\tthe\\tdata\\tyou\\ttell\\tthem\\tto\\toutput\\tat\\truntime.\\tThey\\tare\\ttypically\\tused\\tto\\tpass\\tthe\\ttraining\\tdata\\tto\\nTensorFlow\\tduring\\ttraining.\\tIf\\tyou\\tdon’t\\tspecify\\ta\\tvalue\\tat\\truntime\\tfor\\ta\\tplaceholder,\\tyou\\tget\\tan\\nexception.\\nTo\\tcreate\\ta\\tplaceholder\\tnode,\\t\\nyou\\tmust\\tcall\\tthe\\t\\nplaceholder()\\n\\tfunction\\tand\\tspecify\\tthe\\toutput\\ttensor’s\\ndata\\ttype.\\tOptionally,\\tyou\\tcan\\talso\\tspecify\\tits\\tshape,\\tif\\tyou\\twant\\tto\\tenforce\\tit.\\tIf\\tyou\\tspecify\\t\\nNone\\n\\tfor\\ta\\ndimension,\\tit\\tmeans\\t“any\\tsize.”\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tcreates\\ta\\tplaceholder\\tnode\\t\\nA\\n,\\tand\\talso\\ta\\nnode\\t\\nB\\t=\\tA\\t+\\t5\\n.\\tWhen\\twe\\tevaluate\\t\\nB\\n,\\twe\\tpass\\ta\\t\\nfeed_dict\\n\\t\\nto\\tthe\\t\\neval()\\n\\t\\nmethod\\tthat\\tspecifies\\tthe\\nvalue\\tof\\t\\nA\\n.\\tNote\\tthat\\t\\nA\\n\\tmust\\thave\\trank\\t2\\t(i.e.,\\tit\\tmust\\tbe\\ttwo-dimensional)\\tand\\tthere\\tmust\\tbe\\tthree\\tcolumns\\n(or\\telse\\tan\\texception\\tis\\traised),\\tbut\\tit\\tcan\\thave\\tany\\tnumber\\tof\\trows.\\n>>>\\t\\nA\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\n3\\n))\\n>>>\\t\\nB\\n\\t\\n=\\n\\t\\nA\\n\\t\\n+\\n\\t\\n5\\n>>>\\t\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n...\\t\\n\\t\\t\\t\\t\\nB_val_1\\n\\t\\n=\\n\\t\\nB\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nA\\n:\\n\\t\\n[[\\n1\\n,\\n\\t\\n2\\n,\\n\\t\\n3\\n]]})\\n...\\t\\n\\t\\t\\t\\t\\nB_val_2\\n\\t\\n=\\n\\t\\nB\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nA\\n:\\n\\t\\n[[\\n4\\n,\\n\\t\\n5\\n,\\n\\t\\n6\\n],\\n\\t\\n[\\n7\\n,\\n\\t\\n8\\n,\\n\\t\\n9\\n]]})\\n...\\n>>>\\t\\nprint\\n(\\nB_val_1\\n)\\n[[\\t6.\\t\\t7.\\t\\t8.]]\\n>>>\\t\\nprint\\n(\\nB_val_2\\n)\\n[[\\t\\t9.\\t\\t10.\\t\\t11.]\\n\\t[\\t12.\\t\\t13.\\t\\t14.]]\\nNOTE\\nYou\\tcan\\tactually\\tfeed\\tthe\\toutput\\tof\\t\\nany\\n\\toperations,\\tnot\\tjust\\tplaceholders.\\tIn\\tthis\\tcase\\tTensorFlow\\tdoes\\tnot\\ttry\\tto\\tevaluate\\tthese\\noperations;\\tit\\tuses\\tthe\\tvalues\\tyou\\tfeed\\tit.\\nTo\\timplement\\tMini-batch\\tGradient\\tDescent,\\twe\\tonly\\tneed\\tto\\ttweak\\tthe\\texisting\\tcode\\tslightly.\\tFirst\\tchange\\nthe\\tdefinition\\tof\\t\\nX\\n\\tand\\t\\ny\\n\\tin\\tthe\\tconstruction\\tphase\\tto\\tmake\\tthem\\t\\nplaceholder\\tnodes:\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn\\n\\t\\n+\\n\\t\\n1\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\n1\\n),\\n\\t\\nname\\n=\\n\"y\"\\n)\\nThen\\tdefine\\tthe\\tbatch\\tsize\\tand\\tcompute\\tthe\\ttotal\\tnumber\\tof\\tbatches:\\nbatch_size\\n\\t\\n=\\n\\t\\n100\\nn_batches\\n\\t\\n=\\n\\t\\nint\\n(\\nnp\\n.\\nceil\\n(\\nm\\n\\t\\n/\\n\\t\\nbatch_size\\n))\\nFinally,\\tin\\tthe\\texecution\\tphase,\\tfetch\\tthe\\tmini-batches\\tone\\tby\\tone,\\tthen\\tprovide\\tthe\\tvalue\\tof\\t\\nX\\n\\tand\\t\\ny\\n\\tvia\\nthe\\t\\nfeed_dict\\n\\tparameter\\twhen\\tevaluating\\ta\\tnode\\tthat\\tdepends\\ton\\teither\\tof\\tthem.\\ndef\\n\\t\\nfetch_batch\\n(\\nepoch\\n,\\n\\t\\nbatch_index\\n,\\n\\t\\nbatch_size\\n):', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 300}), Document(page_content='\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tload\\tthe\\tdata\\tfrom\\tdisk\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nX_batch\\n,\\n\\t\\ny_batch\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ninit\\n)\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nbatch_index\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_batches\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nfetch_batch\\n(\\nepoch\\n,\\n\\t\\nbatch_index\\n,\\n\\t\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\nbest_theta\\n\\t\\n=\\n\\t\\ntheta\\n.\\neval\\n()\\nNOTE\\nWe\\tdon’t\\tneed\\tto\\tpass\\tthe\\tvalue\\tof\\t\\nX\\n\\tand\\t\\ny\\n\\twhen\\tevaluating\\t\\ntheta\\n\\tsince\\tit\\tdoes\\tnot\\tdepend\\ton\\t\\neither\\tof\\tthem.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 301}), Document(page_content='Saving\\tand\\tRestoring\\tModels\\nOnce\\t\\nyou\\thave\\ttrained\\tyour\\tmodel,\\tyou\\tshould\\tsave\\tits\\tparameters\\tto\\tdisk\\tso\\tyou\\tcan\\tcome\\tback\\tto\\tit\\nwhenever\\tyou\\twant,\\tuse\\tit\\tin\\tanother\\tprogram,\\tcompare\\tit\\tto\\tother\\tmodels,\\tand\\tso\\ton.\\tMoreover,\\tyou\\nprobably\\twant\\tto\\tsave\\tcheckpoints\\tat\\tregular\\tintervals\\tduring\\ttraining\\tso\\tthat\\tif\\tyour\\tcomputer\\tcrashes\\nduring\\ttraining\\tyou\\tcan\\tcontinue\\tfrom\\tthe\\tlast\\tcheckpoint\\trather\\tthan\\tstart\\tover\\tfrom\\tscratch.\\nTensorFlow\\tmakes\\tsaving\\tand\\trestoring\\ta\\tmodel\\tvery\\teasy.\\tJust\\tcreate\\ta\\t\\nSaver\\n\\tnode\\t\\nat\\tthe\\tend\\tof\\tthe\\nconstruction\\tphase\\t(after\\tall\\tvariable\\tnodes\\tare\\tcreated);\\tthen,\\tin\\tthe\\texecution\\tphase,\\tjust\\tcall\\tits\\t\\nsave()\\nmethod\\t\\nwhenever\\tyou\\twant\\tto\\tsave\\tthe\\tmodel,\\tpassing\\tit\\tthe\\tsession\\tand\\tpath\\tof\\t\\nthe\\tcheckpoint\\tfile:\\n[\\n...\\n]\\ntheta\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_uniform\\n([\\nn\\n\\t\\n+\\n\\t\\n1\\n,\\n\\t\\n1\\n],\\n\\t\\n-\\n1.0\\n,\\n\\t\\n1.0\\n),\\n\\t\\nname\\n=\\n\"theta\"\\n)\\n[\\n...\\n]\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ninit\\n)\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nepoch\\n\\t\\n%\\n\\t\\n100\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\n#\\tcheckpoint\\tevery\\t100\\tepochs\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model.ckpt\"\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n)\\n\\t\\t\\t\\t\\nbest_theta\\n\\t\\n=\\n\\t\\ntheta\\n.\\neval\\n()\\n\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model_final.ckpt\"\\n)\\nRestoring\\ta\\tmodel\\tis\\tjust\\tas\\teasy:\\tyou\\tcreate\\ta\\t\\nSaver\\n\\tat\\tthe\\tend\\tof\\tthe\\tconstruction\\tphase\\tjust\\tlike\\tbefore,\\nbut\\tthen\\tat\\tthe\\tbeginning\\tof\\tthe\\texecution\\tphase,\\tinstead\\tof\\tinitializing\\tthe\\tvariables\\t\\nusing\\tthe\\t\\ninit\\n\\tnode,\\nyou\\tcall\\tthe\\t\\nrestore()\\n\\tmethod\\t\\nof\\tthe\\t\\nSaver\\n\\tobject:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\n[\\n...\\n]\\nBy\\tdefault\\ta\\t\\nSaver\\n\\tsaves\\tand\\trestores\\tall\\tvariables\\tunder\\ttheir\\town\\tname,\\tbut\\tif\\tyou\\tneed\\tmore\\tcontrol,\\nyou\\tcan\\tspecify\\twhich\\tvariables\\tto\\tsave\\tor\\trestore,\\tand\\twhat\\tnames\\tto\\tuse.\\tFor\\texample,\\tthe\\tfollowing\\nSaver\\n\\twill\\tsave\\tor\\trestore\\tonly\\tthe\\t\\ntheta\\n\\tvariable\\tunder\\tthe\\t\\nname\\t\\nweights\\n:\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n({\\n\"weights\"\\n:\\n\\t\\ntheta\\n})\\nBy\\tdefault,\\tthe\\t\\nsave()\\n\\tmethod\\talso\\tsaves\\tthe\\tstructure\\tof\\tthe\\tgraph\\tin\\ta\\tsecond\\tfile\\twith\\tthe\\tsame\\tname\\nplus\\ta\\t\\n.meta\\n\\textension.\\tYou\\tcan\\tload\\tthis\\tgraph\\tstructure\\tusing\\t\\ntf.train.import_meta_graph()\\n.\\tThis\\nadds\\tthe\\tgraph\\tto\\tthe\\tdefault\\tgraph,\\tand\\treturns\\ta\\t\\nSaver\\n\\tinstance\\tthat\\tyou\\tcan\\tthen\\tuse\\tto\\trestore\\tthe\\ngraph’s\\tstate\\t(i.e.,\\tthe\\tvariable\\tvalues):\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nimport_meta_graph\\n(\\n\"/tmp/my_model_final.ckpt.meta\"\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\n[\\n...\\n]', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 302}), Document(page_content='This\\tallows\\tyou\\tto\\tfully\\trestore\\ta\\tsaved\\tmodel,\\tincluding\\tboth\\tthe\\tgraph\\tstructure\\tand\\tthe\\tvariable\\tvalues,\\nwithout\\thaving\\tto\\tsearch\\tfor\\tthe\\tcode\\tthat\\tbuilt\\tit.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 303}), Document(page_content='Visualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\nSo\\t\\nnow\\twe\\thave\\ta\\tcomputation\\tgraph\\tthat\\ttrains\\ta\\tLinear\\tRegression\\tmodel\\tusing\\tMini-batch\\tGradient\\nDescent,\\tand\\twe\\tare\\tsaving\\tcheckpoints\\tat\\tregular\\tintervals.\\tSounds\\tsophisticated,\\tdoesn’t\\tit?\\tHowever,\\nwe\\tare\\tstill\\trelying\\ton\\tthe\\t\\nprint()\\n\\tfunction\\tto\\tvisualize\\tprogress\\tduring\\ttraining.\\tThere\\tis\\ta\\tbetter\\tway:\\nenter\\tTensorBoard.\\tIf\\tyou\\tfeed\\tit\\tsome\\ttraining\\tstats,\\tit\\twill\\tdisplay\\tnice\\tinteractive\\tvisualizations\\tof\\nthese\\tstats\\tin\\tyour\\tweb\\tbrowser\\t(e.g.,\\tlearning\\tcurves).\\tYou\\tcan\\talso\\tprovide\\tit\\tthe\\tgraph’s\\tdefinition\\tand\\nit\\twill\\tgive\\tyou\\ta\\tgreat\\tinterface\\tto\\tbrowse\\tthrough\\tit.\\tThis\\tis\\tvery\\tuseful\\tto\\tidentify\\terrors\\tin\\tthe\\tgraph,\\tto\\nfind\\tbottlenecks,\\tand\\tso\\ton.\\nThe\\tfirst\\tstep\\tis\\tto\\ttweak\\tyour\\tprogram\\ta\\tbit\\tso\\tit\\twrites\\tthe\\tgraph\\tdefinition\\tand\\tsome\\ttraining\\tstats\\t—\\tfor\\nexample,\\tthe\\ttraining\\terror\\t(MSE)\\t—\\tto\\ta\\tlog\\tdirectory\\tthat\\tTensorBoard\\twill\\tread\\tfrom.\\tYou\\tneed\\tto\\tuse\\na\\tdifferent\\tlog\\tdirectory\\tevery\\ttime\\tyou\\trun\\tyour\\tprogram,\\tor\\telse\\tTensorBoard\\twill\\tmerge\\tstats\\tfrom\\ndifferent\\truns,\\twhich\\twill\\tmess\\tup\\tthe\\tvisualizations.\\tThe\\tsimplest\\tsolution\\tfor\\tthis\\tis\\tto\\tinclude\\ta\\ntimestamp\\tin\\tthe\\tlog\\tdirectory\\tname.\\tAdd\\tthe\\tfollowing\\tcode\\tat\\tthe\\tbeginning\\tof\\tthe\\tprogram:\\nfrom\\n\\t\\ndatetime\\n\\t\\nimport\\n\\t\\ndatetime\\nnow\\n\\t\\n=\\n\\t\\ndatetime\\n.\\nutcnow\\n()\\n.\\nstrftime\\n(\\n\"\\n%Y%m%d%H%M%S\\n\"\\n)\\nroot_logdir\\n\\t\\n=\\n\\t\\n\"tf_logs\"\\nlogdir\\n\\t\\n=\\n\\t\\n\"{}/run-{}/\"\\n.\\nformat\\n(\\nroot_logdir\\n,\\n\\t\\nnow\\n)\\nNext,\\tadd\\tthe\\tfollowing\\tcode\\tat\\tthe\\tvery\\tend\\tof\\tthe\\t\\nconstruction\\tphase:\\nmse_summary\\n\\t\\n=\\n\\t\\ntf\\n.\\nsummary\\n.\\nscalar\\n(\\n\\'MSE\\'\\n,\\n\\t\\nmse\\n)\\nfile_writer\\n\\t\\n=\\n\\t\\ntf\\n.\\nsummary\\n.\\nFileWriter\\n(\\nlogdir\\n,\\n\\t\\ntf\\n.\\nget_default_graph\\n())\\nThe\\tfirst\\tline\\tcreates\\ta\\tnode\\tin\\tthe\\tgraph\\tthat\\twill\\tevaluate\\tthe\\tMSE\\tvalue\\tand\\twrite\\tit\\tto\\ta\\tTensorBoard-\\ncompatible\\tbinary\\tlog\\tstring\\tcalled\\ta\\t\\nsummary\\n.\\tThe\\tsecond\\tline\\tcreates\\ta\\t\\nFileWriter\\n\\tthat\\tyou\\twill\\tuse\\nto\\twrite\\tsummaries\\tto\\tlogfiles\\tin\\tthe\\tlog\\tdirectory.\\tThe\\tfirst\\tparameter\\tindicates\\tthe\\tpath\\tof\\tthe\\tlog\\ndirectory\\t(in\\tthis\\tcase\\tsomething\\tlike\\t\\ntf_logs/run-20160906091959/\\n,\\trelative\\tto\\tthe\\tcurrent\\tdirectory).\\nThe\\tsecond\\t(optional)\\tparameter\\tis\\tthe\\tgraph\\tyou\\twant\\tto\\tvisualize.\\tUpon\\tcreation,\\tthe\\t\\nFileWriter\\ncreates\\tthe\\tlog\\tdirectory\\tif\\tit\\tdoes\\tnot\\talready\\texist\\t(and\\tits\\tparent\\tdirectories\\tif\\tneeded),\\tand\\twrites\\tthe\\ngraph\\tdefinition\\tin\\ta\\tbinary\\tlogfile\\tcalled\\tan\\t\\nevents\\tfile\\n.\\nNext\\tyou\\tneed\\tto\\tupdate\\tthe\\texecution\\tphase\\tto\\tevaluate\\tthe\\t\\nmse_summary\\n\\tnode\\tregularly\\tduring\\ttraining\\n(e.g.,\\tevery\\t10\\tmini-batches).\\tThis\\twill\\toutput\\ta\\tsummary\\tthat\\tyou\\tcan\\tthen\\twrite\\tto\\tthe\\tevents\\tfile\\tusing\\nthe\\t\\nfile_writer\\n.\\tHere\\tis\\tthe\\tupdated\\tcode:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\nfor\\n\\t\\nbatch_index\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_batches\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nfetch_batch\\n(\\nepoch\\n,\\n\\t\\nbatch_index\\n,\\n\\t\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nbatch_index\\n\\t\\n%\\n\\t\\n10\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsummary_str\\n\\t\\n=\\n\\t\\nmse_summary\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nstep\\n\\t\\n=\\n\\t\\nepoch\\n\\t\\n*\\n\\t\\nn_batches\\n\\t\\n+\\n\\t\\nbatch_index\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfile_writer\\n.\\nadd_summary\\n(\\nsummary_str\\n,\\n\\t\\nstep\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\n[\\n...\\n]', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 304}), Document(page_content='WARNING\\nAvoid\\tlogging\\ttraining\\tstats\\tat\\tevery\\tsingle\\ttraining\\tstep,\\tas\\tthis\\twould\\tsignificantly\\tslow\\tdown\\ttraining.\\nFinally,\\tyou\\twant\\tto\\tclose\\tthe\\t\\nFileWriter\\n\\tat\\tthe\\tend\\tof\\t\\nthe\\tprogram:\\nfile_writer\\n.\\nclose\\n()\\nNow\\trun\\tthis\\tprogram:\\tit\\twill\\tcreate\\tthe\\tlog\\tdirectory\\tand\\twrite\\tan\\tevents\\tfile\\tin\\tthis\\tdirectory,\\tcontaining\\nboth\\tthe\\tgraph\\tdefinition\\tand\\tthe\\tMSE\\tvalues.\\tOpen\\tup\\ta\\tshell\\tand\\tgo\\tto\\tyour\\tworking\\tdirectory,\\tthen\\ttype\\nls\\t-l\\ttf_logs/run*\\n\\tto\\tlist\\tthe\\tcontents\\tof\\tthe\\tlog\\tdirectory:\\n$\\tcd\\t$ML_PATH\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t#\\tYour\\tML\\tworking\\tdirectory\\t(e.g.,\\t$HOME/ml)\\n$\\tls\\t-l\\ttf_logs/run*\\ntotal\\t40\\n-rw-r--r--\\t1\\tageron\\tstaff\\t18620\\tSep\\t6\\t11:10\\tevents.out.tfevents.1472553182.mymac\\nIf\\tyou\\trun\\tthe\\tprogram\\ta\\tsecond\\ttime,\\tyou\\tshould\\tsee\\ta\\tsecond\\tdirectory\\tin\\tthe\\t\\ntf_logs/\\n\\tdirectory:\\n$\\tls\\t-l\\ttf_logs/\\ntotal\\t0\\ndrwxr-xr-x\\t\\t3\\tageron\\t\\tstaff\\t\\t102\\tSep\\t\\t6\\t10:07\\trun-20160906091959\\ndrwxr-xr-x\\t\\t3\\tageron\\t\\tstaff\\t\\t102\\tSep\\t\\t6\\t10:22\\trun-20160906092202\\nGreat!\\tNow\\tit’s\\ttime\\tto\\tfire\\tup\\tthe\\tTensorBoard\\tserver.\\tYou\\tneed\\tto\\tactivate\\tyour\\tvirtualenv\\tenvironment\\nif\\tyou\\tcreated\\tone,\\tthen\\tstart\\tthe\\tserver\\tby\\trunning\\tthe\\t\\ntensorboard\\n\\tcommand,\\tpointing\\tit\\tto\\tthe\\troot\\tlog\\ndirectory.\\tThis\\tstarts\\tthe\\tTensorBoard\\tweb\\tserver,\\tlistening\\ton\\tport\\t6006\\t(which\\tis\\t“goog”\\twritten\\tupside\\ndown):\\n$\\tsource\\tenv/bin/activate\\n$\\ttensorboard\\t--logdir\\ttf_logs/\\nStarting\\tTensorBoard\\t\\ton\\tport\\t6006\\n(You\\tcan\\tnavigate\\tto\\thttp://0.0.0.0:6006)\\nNext\\topen\\ta\\tbrowser\\tand\\tgo\\tto\\t\\nhttp://0.0.0.0:6006/\\n\\t(or\\t\\nhttp://localhost:6006/\\n).\\tWelcome\\tto\\nTensorBoard!\\tIn\\tthe\\tEvents\\ttab\\tyou\\tshould\\tsee\\tMSE\\ton\\tthe\\tright.\\tIf\\tyou\\tclick\\ton\\tit,\\tyou\\twill\\tsee\\ta\\tplot\\tof\\nthe\\tMSE\\tduring\\ttraining,\\tfor\\tboth\\truns\\t(\\nFigure\\t9-3\\n).\\tYou\\tcan\\tcheck\\tor\\tuncheck\\tthe\\truns\\tyou\\twant\\tto\\tsee,\\nzoom\\tin\\tor\\tout,\\thover\\tover\\tthe\\tcurve\\tto\\tget\\tdetails,\\tand\\tso\\ton.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 305}), Document(page_content='Figure\\t9-3.\\t\\nVisualizing\\ttraining\\tstats\\tusing\\tTensorBoard\\nNow\\tclick\\ton\\tthe\\tGraphs\\ttab.\\tYou\\tshould\\tsee\\tthe\\tgraph\\tshown\\tin\\t\\nFigure\\t9-4\\n.\\nTo\\treduce\\tclutter,\\tthe\\tnodes\\tthat\\thave\\t\\nmany\\t\\nedges\\n\\t(i.e.,\\tconnections\\tto\\tother\\tnodes)\\tare\\tseparated\\tout\\tto\\tan\\nauxiliary\\tarea\\ton\\tthe\\tright\\t(you\\tcan\\tmove\\ta\\tnode\\tback\\tand\\tforth\\tbetween\\tthe\\tmain\\tgraph\\tand\\tthe\\tauxiliary\\narea\\tby\\tright-clicking\\ton\\tit).\\tSome\\tparts\\tof\\tthe\\tgraph\\tare\\talso\\tcollapsed\\tby\\tdefault.\\tFor\\texample,\\ttry\\nhovering\\tover\\tthe\\t\\ngradients\\n\\tnode,\\tthen\\tclick\\ton\\tthe\\t\\n\\ticon\\tto\\texpand\\tthis\\tsubgraph.\\tNext,\\tin\\tthis\\nsubgraph,\\ttry\\texpanding\\tthe\\t\\nmse_grad\\n\\tsubgraph.\\nFigure\\t9-4.\\t\\nVisualizing\\tthe\\tgraph\\tusing\\tTensorBoard\\nTIP\\nIf\\tyou\\twant\\tto\\ttake\\ta\\tpeek\\tat\\tthe\\tgraph\\tdirectly\\twithin\\tJupyter,\\tyou\\tcan\\tuse\\tthe\\t\\nshow_graph()\\n\\t\\nfunction\\tavailable\\tin\\tthe\\tnotebook\\nfor\\tthis\\tchapter.\\tIt\\twas\\toriginally\\twritten\\tby\\tA.\\tMordvintsev\\tin\\this\\tgreat\\t\\ndeepdream\\ttutorial\\tnotebook\\n.\\tAnother\\toption\\tis\\tto\\tinstall\\nE.\\tJang’s\\t\\nTensorFlow\\tdebugger\\ttool\\n\\twhich\\tincludes\\ta\\tJupyter\\textension\\t\\nfor\\tgraph\\tvisualization\\t(and\\tmore).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 306}), Document(page_content='Name\\tScopes\\nWhen\\t\\ndealing\\twith\\tmore\\tcomplex\\tmodels\\tsuch\\tas\\tneural\\tnetworks,\\tthe\\tgraph\\tcan\\teasily\\tbecome\\tcluttered\\nwith\\tthousands\\tof\\tnodes.\\tTo\\tavoid\\tthis,\\tyou\\tcan\\tcreate\\t\\nname\\tscopes\\n\\tto\\tgroup\\trelated\\tnodes.\\tFor\\texample,\\nlet’s\\tmodify\\tthe\\tprevious\\tcode\\tto\\tdefine\\tthe\\t\\nerror\\n\\tand\\t\\nmse\\n\\tops\\twithin\\ta\\tname\\t\\nscope\\tcalled\\t\\n\"loss\"\\n:\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"loss\"\\n)\\n\\t\\nas\\n\\t\\nscope\\n:\\n\\t\\t\\t\\t\\nerror\\n\\t\\n=\\n\\t\\ny_pred\\n\\t\\n-\\n\\t\\ny\\n\\t\\t\\t\\t\\nmse\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\nerror\\n),\\n\\t\\nname\\n=\\n\"mse\"\\n)\\nThe\\tname\\tof\\teach\\top\\tdefined\\twithin\\tthe\\tscope\\tis\\tnow\\tprefixed\\twith\\t\\n\"loss/\"\\n:\\n>>>\\t\\nprint\\n(\\nerror\\n.\\nop\\n.\\nname\\n)\\nloss/sub\\n>>>\\t\\nprint\\n(\\nmse\\n.\\nop\\n.\\nname\\n)\\nloss/mse\\nIn\\tTensorBoard,\\tthe\\t\\nmse\\n\\tand\\t\\nerror\\n\\tnodes\\tnow\\tappear\\tinside\\tthe\\t\\nloss\\n\\tnamespace,\\twhich\\tappears\\ncollapsed\\tby\\tdefault\\t(\\nFigure\\t9-5\\n).\\nFigure\\t9-5.\\t\\nA\\tcollapsed\\tnamescope\\tin\\tTensorBoard', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 307}), Document(page_content='Modularity\\nSuppose\\t\\nyou\\twant\\tto\\tcreate\\ta\\tgraph\\tthat\\tadds\\tthe\\toutput\\tof\\t\\ntwo\\t\\nrectified\\tlinear\\tunits\\n\\t(ReLU).\\tA\\tReLU\\ncomputes\\ta\\tlinear\\tfunction\\tof\\tthe\\tinputs,\\tand\\toutputs\\tthe\\tresult\\tif\\tit\\tis\\tpositive,\\tand\\t0\\totherwise,\\tas\\tshown\\nin\\t\\nEquation\\t9-1\\n.\\nEquation\\t9-1.\\t\\nRectified\\tlinear\\tunit\\nThe\\tfollowing\\tcode\\tdoes\\tthe\\tjob,\\tbut\\tit’s\\tquite\\t\\nrepetitive:\\nn_features\\n\\t\\n=\\n\\t\\n3\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nw1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_normal\\n((\\nn_features\\n,\\n\\t\\n1\\n)),\\n\\t\\nname\\n=\\n\"weights1\"\\n)\\nw2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_normal\\n((\\nn_features\\n,\\n\\t\\n1\\n)),\\n\\t\\nname\\n=\\n\"weights2\"\\n)\\nb1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"bias1\"\\n)\\nb2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"bias2\"\\n)\\nz1\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd\\n(\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\nw1\\n),\\n\\t\\nb1\\n,\\n\\t\\nname\\n=\\n\"z1\"\\n)\\nz2\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd\\n(\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\nw2\\n),\\n\\t\\nb2\\n,\\n\\t\\nname\\n=\\n\"z2\"\\n)\\nrelu1\\n\\t\\n=\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz1\\n,\\n\\t\\n0.\\n,\\n\\t\\nname\\n=\\n\"relu1\"\\n)\\nrelu2\\n\\t\\n=\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz1\\n,\\n\\t\\n0.\\n,\\n\\t\\nname\\n=\\n\"relu2\"\\n)\\noutput\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd\\n(\\nrelu1\\n,\\n\\t\\nrelu2\\n,\\n\\t\\nname\\n=\\n\"output\"\\n)\\nSuch\\trepetitive\\tcode\\tis\\thard\\tto\\tmaintain\\tand\\terror-prone\\t(in\\tfact,\\tthis\\tcode\\tcontains\\ta\\tcut-and-paste\\terror;\\ndid\\tyou\\tspot\\tit?).\\tIt\\twould\\tbecome\\teven\\tworse\\tif\\tyou\\twanted\\tto\\tadd\\ta\\tfew\\tmore\\tReLUs.\\tFortunately,\\nTensorFlow\\tlets\\tyou\\tstay\\t\\nDRY\\t(Don’t\\tRepeat\\tYourself):\\tsimply\\tcreate\\ta\\tfunction\\tto\\tbuild\\ta\\tReLU.\\tThe\\nfollowing\\tcode\\tcreates\\tfive\\tReLUs\\tand\\t\\noutputs\\ttheir\\tsum\\t(note\\tthat\\t\\nadd_n()\\n\\tcreates\\tan\\toperation\\tthat\\twill\\ncompute\\tthe\\tsum\\tof\\ta\\tlist\\tof\\ttensors):\\ndef\\n\\t\\nrelu\\n(\\nX\\n):\\n\\t\\t\\t\\t\\nw_shape\\n\\t\\n=\\n\\t\\n(\\nint\\n(\\nX\\n.\\nget_shape\\n()[\\n1\\n]),\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\nw\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_normal\\n(\\nw_shape\\n),\\n\\t\\nname\\n=\\n\"weights\"\\n)\\n\\t\\t\\t\\t\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"bias\"\\n)\\n\\t\\t\\t\\t\\nz\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd\\n(\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\nw\\n),\\n\\t\\nb\\n,\\n\\t\\nname\\n=\\n\"z\"\\n)\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz\\n,\\n\\t\\n0.\\n,\\n\\t\\nname\\n=\\n\"relu\"\\n)\\nn_features\\n\\t\\n=\\n\\t\\n3\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nrelus\\n\\t\\n=\\n\\t\\n[\\nrelu\\n(\\nX\\n)\\n\\t\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n)]\\noutput\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n(\\nrelus\\n,\\n\\t\\nname\\n=\\n\"output\"\\n)\\nNote\\tthat\\twhen\\tyou\\tcreate\\ta\\tnode,\\tTensorFlow\\tchecks\\twhether\\tits\\tname\\talready\\texists,\\tand\\tif\\tit\\tdoes\\tit\\nappends\\tan\\tunderscore\\tfollowed\\tby\\tan\\tindex\\tto\\tmake\\tthe\\tname\\tunique.\\tSo\\tthe\\tfirst\\tReLU\\tcontains\\tnodes\\nnamed\\t\\n\"weights\"\\n,\\t\\n\"bias\"\\n,\\t\\n\"z\"\\n,\\tand\\t\\n\"relu\"\\n\\t(plus\\tmany\\tmore\\tnodes\\twith\\ttheir\\tdefault\\tname,\\tsuch\\tas\\n\"MatMul\"\\n);\\tthe\\tsecond\\tReLU\\tcontains\\tnodes\\tnamed\\t\\n\"weights_1\"\\n,\\t\\n\"bias_1\"\\n,\\tand\\tso\\ton;\\tthe\\tthird\\tReLU\\ncontains\\tnodes\\tnamed\\t\\n\"weights_2\"\\n,\\t\\n\"bias_2\"\\n,\\tand\\tso\\ton.\\tTensorBoard\\tidentifies\\tsuch\\tseries\\tand\\ncollapses\\tthem\\ttogether\\tto\\treduce\\tclutter\\t(as\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t9-6\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 308}), Document(page_content='Figure\\t9-6.\\t\\nCollapsed\\tnode\\tseries\\nUsing\\tname\\tscopes,\\tyou\\tcan\\tmake\\tthe\\tgraph\\tmuch\\tclearer.\\tSimply\\tmove\\tall\\tthe\\tcontent\\tof\\tthe\\t\\nrelu()\\nfunction\\tinside\\ta\\tname\\tscope.\\t\\nFigure\\t9-7\\n\\tshows\\tthe\\tresulting\\tgraph.\\tNotice\\tthat\\tTensorFlow\\talso\\tgives\\tthe\\nname\\tscopes\\tunique\\tnames\\tby\\tappending\\t\\n_1\\n,\\t\\n_2\\n,\\tand\\tso\\t\\non.\\ndef\\n\\t\\nrelu\\n(\\nX\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"relu\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 309}), Document(page_content='Figure\\t9-7.\\t\\nA\\tclearer\\tgraph\\tusing\\tname-scoped\\tunits', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 310}), Document(page_content='Sharing\\tVariables\\nIf\\t\\nyou\\twant\\tto\\tshare\\ta\\tvariable\\tbetween\\tvarious\\tcomponents\\tof\\tyour\\tgraph,\\tone\\tsimple\\toption\\tis\\tto\\tcreate\\nit\\tfirst,\\tthen\\tpass\\tit\\tas\\ta\\tparameter\\tto\\tthe\\tfunctions\\tthat\\tneed\\tit.\\tFor\\texample,\\tsuppose\\tyou\\twant\\tto\\tcontrol\\nthe\\tReLU\\tthreshold\\t(currently\\thardcoded\\tto\\t0)\\tusing\\ta\\tshared\\t\\nthreshold\\n\\t\\nvariable\\tfor\\tall\\tReLUs.\\tYou\\ncould\\tjust\\tcreate\\tthat\\tvariable\\tfirst,\\tand\\tthen\\tpass\\tit\\tto\\tthe\\t\\nrelu()\\n\\tfunction:\\ndef\\n\\t\\nrelu\\n(\\nX\\n,\\n\\t\\nthreshold\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"relu\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz\\n,\\n\\t\\nthreshold\\n,\\n\\t\\nname\\n=\\n\"max\"\\n)\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"threshold\"\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nrelus\\n\\t\\n=\\n\\t\\n[\\nrelu\\n(\\nX\\n,\\n\\t\\nthreshold\\n)\\n\\t\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n)]\\noutput\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n(\\nrelus\\n,\\n\\t\\nname\\n=\\n\"output\"\\n)\\nThis\\tworks\\tfine:\\t\\nnow\\tyou\\tcan\\tcontrol\\tthe\\tthreshold\\tfor\\tall\\tReLUs\\tusing\\tthe\\t\\nthreshold\\n\\tvariable.\\nHowever,\\tif\\tthere\\tare\\tmany\\tshared\\tparameters\\tsuch\\tas\\tthis\\tone,\\tit\\twill\\tbe\\tpainful\\tto\\thave\\tto\\tpass\\tthem\\naround\\tas\\tparameters\\tall\\tthe\\ttime.\\tMany\\tpeople\\tcreate\\ta\\tPython\\tdictionary\\tcontaining\\tall\\tthe\\tvariables\\tin\\ntheir\\tmodel,\\tand\\tpass\\tit\\taround\\tto\\tevery\\tfunction.\\tOthers\\tcreate\\ta\\tclass\\tfor\\teach\\tmodule\\t(e.g.,\\ta\\t\\nReLU\\n\\tclass\\nusing\\tclass\\tvariables\\tto\\thandle\\tthe\\tshared\\tparameter).\\tYet\\tanother\\toption\\tis\\tto\\tset\\tthe\\tshared\\tvariable\\tas\\nan\\tattribute\\tof\\tthe\\t\\nrelu()\\n\\tfunction\\tupon\\tthe\\tfirst\\tcall,\\tlike\\tso:\\ndef\\n\\t\\nrelu\\n(\\nX\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"relu\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nnot\\n\\t\\nhasattr\\n(\\nrelu\\n,\\n\\t\\n\"threshold\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nrelu\\n.\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"threshold\"\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz\\n,\\n\\t\\nrelu\\n.\\nthreshold\\n,\\n\\t\\nname\\n=\\n\"max\"\\n)\\nTensorFlow\\t\\noffers\\tanother\\toption,\\twhich\\tmay\\tlead\\tto\\tslightly\\tcleaner\\tand\\tmore\\tmodular\\tcode\\tthan\\tthe\\nprevious\\tsolutions.\\n5\\n\\tThis\\tsolution\\tis\\ta\\tbit\\ttricky\\tto\\tunderstand\\tat\\tfirst,\\tbut\\tsince\\tit\\tis\\tused\\ta\\tlot\\tin\\nTensorFlow\\tit\\tis\\tworth\\tgoing\\tinto\\ta\\tbit\\tof\\tdetail.\\tThe\\tidea\\tis\\tto\\tuse\\tthe\\t\\nget_variable()\\n\\t\\nfunction\\tto\\ncreate\\tthe\\tshared\\tvariable\\tif\\tit\\tdoes\\tnot\\texist\\tyet,\\tor\\treuse\\tit\\tif\\tit\\talready\\texists.\\tThe\\tdesired\\tbehavior\\n(creating\\tor\\treusing)\\tis\\tcontrolled\\tby\\tan\\tattribute\\tof\\tthe\\t\\ncurrent\\t\\nvariable_scope()\\n.\\tFor\\texample,\\tthe\\nfollowing\\tcode\\twill\\tcreate\\ta\\tvariable\\tnamed\\t\\n\"relu/threshold\"\\n\\t(as\\ta\\tscalar,\\tsince\\t\\nshape=()\\n,\\tand\\tusing\\n0.0\\n\\tas\\tthe\\tinitial\\tvalue):\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n):\\n\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ninitializer\\n=\\ntf\\n.\\nconstant_initializer\\n(\\n0.0\\n))\\nNote\\tthat\\tif\\tthe\\tvariable\\thas\\talready\\tbeen\\tcreated\\tby\\tan\\tearlier\\tcall\\tto\\t\\nget_variable()\\n,\\tthis\\tcode\\twill\\nraise\\tan\\texception.\\tThis\\tbehavior\\tprevents\\treusing\\tvariables\\tby\\tmistake.\\tIf\\tyou\\twant\\tto\\treuse\\ta\\tvariable,\\nyou\\tneed\\tto\\texplicitly\\tsay\\tso\\tby\\tsetting\\tthe\\tvariable\\tscope’s\\t\\nreuse\\n\\tattribute\\tto\\t\\nTrue\\n\\t(in\\twhich\\tcase\\tyou\\ndon’t\\thave\\tto\\tspecify\\tthe\\tshape\\tor\\tthe\\tinitializer):\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n,\\n\\t\\nreuse\\n=\\nTrue\\n):', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 311}), Document(page_content='\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n)\\nThis\\tcode\\twill\\tfetch\\tthe\\texisting\\t\\n\"relu/threshold\"\\n\\tvariable,\\tor\\traise\\tan\\texception\\tif\\tit\\tdoes\\tnot\\texist\\tor\\nif\\tit\\twas\\tnot\\tcreated\\tusing\\t\\nget_variable()\\n.\\tAlternatively,\\tyou\\tcan\\tset\\tthe\\t\\nreuse\\n\\tattribute\\tto\\t\\nTrue\\n\\tinside\\nthe\\tblock\\tby\\tcalling\\tthe\\t\\nscope’s\\t\\nreuse_variables()\\n\\tmethod:\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n)\\n\\t\\nas\\n\\t\\nscope\\n:\\n\\t\\t\\t\\t\\nscope\\n.\\nreuse_variables\\n()\\n\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n)\\nWARNING\\nOnce\\t\\nreuse\\n\\tis\\tset\\tto\\t\\nTrue\\n,\\tit\\tcannot\\tbe\\tset\\tback\\tto\\t\\nFalse\\n\\twithin\\tthe\\tblock.\\tMoreover,\\tif\\tyou\\tdefine\\tother\\tvariable\\tscopes\\tinside\\nthis\\tone,\\tthey\\twill\\tautomatically\\tinherit\\t\\nreuse=True\\n.\\tLastly,\\tonly\\tvariables\\tcreated\\tby\\t\\nget_variable()\\n\\tcan\\tbe\\treused\\tthis\\tway.\\nNow\\tyou\\thave\\tall\\tthe\\tpieces\\tyou\\tneed\\tto\\tmake\\tthe\\t\\nrelu()\\n\\tfunction\\taccess\\tthe\\t\\nthreshold\\n\\tvariable\\nwithout\\thaving\\tto\\tpass\\tit\\tas\\ta\\tparameter:\\ndef\\n\\t\\nrelu\\n(\\nX\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n,\\n\\t\\nreuse\\n=\\nTrue\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n)\\n\\t\\t\\n#\\treuse\\texisting\\tvariable\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz\\n,\\n\\t\\nthreshold\\n,\\n\\t\\nname\\n=\\n\"max\"\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n):\\n\\t\\t\\n#\\tcreate\\tthe\\tvariable\\n\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ninitializer\\n=\\ntf\\n.\\nconstant_initializer\\n(\\n0.0\\n))\\nrelus\\n\\t\\n=\\n\\t\\n[\\nrelu\\n(\\nX\\n)\\n\\t\\nfor\\n\\t\\nrelu_index\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n)]\\noutput\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n(\\nrelus\\n,\\n\\t\\nname\\n=\\n\"output\"\\n)\\nThis\\t\\ncode\\tfirst\\tdefines\\tthe\\t\\nrelu()\\n\\tfunction,\\tthen\\tcreates\\tthe\\t\\nrelu/threshold\\n\\tvariable\\t(as\\ta\\tscalar\\tthat\\nwill\\tlater\\tbe\\tinitialized\\tto\\t\\n0.0\\n)\\tand\\tbuilds\\tfive\\tReLUs\\tby\\tcalling\\tthe\\t\\nrelu()\\n\\tfunction.\\tThe\\t\\nrelu()\\nfunction\\treuses\\tthe\\t\\nrelu/threshold\\n\\tvariable,\\tand\\tcreates\\tthe\\tother\\tReLU\\tnodes.\\nNOTE\\nVariables\\tcreated\\tusing\\t\\nget_variable()\\n\\tare\\talways\\tnamed\\tusing\\tthe\\tname\\tof\\t\\ntheir\\t\\nvariable_scope\\n\\tas\\ta\\tprefix\\t(e.g.,\\n\"relu/threshold\"\\n),\\tbut\\tfor\\tall\\tother\\tnodes\\t(including\\tvariables\\tcreated\\twith\\t\\ntf.Variable()\\n)\\tthe\\tvariable\\tscope\\tacts\\tlike\\ta\\tnew\\nname\\tscope.\\tIn\\tparticular,\\tif\\ta\\tname\\tscope\\twith\\tan\\tidentical\\tname\\twas\\talready\\tcreated,\\tthen\\ta\\tsuffix\\tis\\tadded\\tto\\tmake\\tthe\\tname\\nunique.\\tFor\\texample,\\tall\\tnodes\\tcreated\\tin\\tthe\\tpreceding\\tcode\\t(except\\tthe\\t\\nthreshold\\n\\tvariable)\\thave\\ta\\tname\\t\\nprefixed\\twith\\n\"relu_1/\"\\n\\tto\\t\\n\"relu_5/\"\\n,\\tas\\tshown\\tin\\t\\nFigure\\t9-8\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 312}), Document(page_content='Figure\\t9-8.\\t\\nFive\\tReLUs\\tsharing\\tthe\\tthreshold\\tvariable\\nIt\\tis\\tsomewhat\\tunfortunate\\tthat\\tthe\\t\\nthreshold\\n\\tvariable\\tmust\\tbe\\tdefined\\toutside\\tthe\\t\\nrelu()\\n\\tfunction,\\nwhere\\tall\\tthe\\trest\\tof\\tthe\\tReLU\\tcode\\tresides.\\tTo\\tfix\\tthis,\\tthe\\tfollowing\\tcode\\tcreates\\tthe\\t\\nthreshold\\nvariable\\twithin\\tthe\\t\\nrelu()\\n\\tfunction\\tupon\\tthe\\tfirst\\tcall,\\tthen\\treuses\\tit\\tin\\tsubsequent\\tcalls.\\tNow\\tthe\\t\\nrelu()\\nfunction\\tdoes\\tnot\\thave\\tto\\tworry\\tabout\\tname\\tscopes\\tor\\tvariable\\tsharing:\\tit\\tjust\\tcalls\\t\\nget_variable()\\n,\\nwhich\\twill\\tcreate\\tor\\treuse\\tthe\\t\\nthreshold\\n\\tvariable\\t(it\\tdoes\\tnot\\tneed\\tto\\tknow\\twhich\\tis\\tthe\\tcase).\\tThe\\trest\\nof\\tthe\\tcode\\tcalls\\t\\nrelu()\\n\\tfive\\ttimes,\\tmaking\\tsure\\tto\\tset\\t\\nreuse=False\\n\\ton\\tthe\\tfirst\\tcall,\\tand\\t\\nreuse=True\\nfor\\tthe\\tother\\t\\ncalls.\\ndef\\n\\t\\nrelu\\n(\\nX\\n):\\n\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ninitializer\\n=\\ntf\\n.\\nconstant_initializer\\n(\\n0.0\\n))\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz\\n,\\n\\t\\nthreshold\\n,\\n\\t\\nname\\n=\\n\"max\"\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nrelus\\n\\t\\n=\\n\\t\\n[]\\nfor\\n\\t\\nrelu_index\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n,\\n\\t\\nreuse\\n=\\n(\\nrelu_index\\n\\t\\n>=\\n\\t\\n1\\n))\\n\\t\\nas\\n\\t\\nscope\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nrelus\\n.\\nappend\\n(\\nrelu\\n(\\nX\\n))\\noutput\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n(\\nrelus\\n,\\n\\t\\nname\\n=\\n\"output\"\\n)\\nThe\\tresulting\\tgraph\\tis\\tslightly\\tdifferent\\tthan\\tbefore,\\tsince\\tthe\\tshared\\tvariable\\tlives\\twithin\\tthe\\tfirst\\tReLU\\n(see\\t\\nFigure\\t9-9\\n).\\nFigure\\t9-9.\\t\\nFive\\tReLUs\\tsharing\\tthe\\tthreshold\\tvariable\\nThis\\tconcludes\\tthis\\tintroduction\\tto\\tTensorFlow.\\tWe\\twill\\tdiscuss\\tmore\\tadvanced\\ttopics\\tas\\twe\\tgo\\tthrough\\nthe\\tfollowing\\tchapters,\\tin\\tparticular\\tmany\\toperations\\trelated\\tto\\tdeep\\tneural\\tnetworks,\\tconvolutional\\nneural\\tnetworks,\\tand\\trecurrent\\tneural\\tnetworks\\tas\\twell\\tas\\thow\\tto\\tscale\\tup\\twith\\tTensorFlow\\tusing\\nmultithreading,\\tqueues,\\tmultiple\\tGPUs,\\tand\\t\\nmultiple\\tservers.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 313}), Document(page_content='Exercises\\n1\\n.\\t\\nWhat\\tare\\tthe\\tmain\\tbenefits\\tof\\tcreating\\ta\\tcomputation\\tgraph\\trather\\tthan\\tdirectly\\texecuting\\tthe\\ncomputations?\\tWhat\\tare\\tthe\\tmain\\tdrawbacks?\\n2\\n.\\t\\nIs\\tthe\\tstatement\\t\\na_val\\n\\t\\n=\\n\\t\\na.eval(session=sess)\\n\\tequivalent\\tto\\t\\na_val\\n\\t\\n=\\n\\t\\nsess.run(a)\\n?\\n3\\n.\\t\\nIs\\tthe\\tstatement\\t\\na_val,\\tb_val\\n\\t\\n=\\n\\t\\na.eval(session=sess),\\tb.eval(session=sess)\\n\\tequivalent\\tto\\na_val,\\tb_val\\n\\t\\n=\\n\\t\\nsess.run([a,\\tb])\\n?\\n4\\n.\\t\\nCan\\tyou\\trun\\ttwo\\tgraphs\\tin\\tthe\\tsame\\tsession?\\n5\\n.\\t\\nIf\\tyou\\tcreate\\ta\\tgraph\\t\\ng\\n\\tcontaining\\ta\\tvariable\\t\\nw\\n,\\tthen\\tstart\\ttwo\\tthreads\\tand\\topen\\ta\\tsession\\tin\\teach\\nthread,\\tboth\\tusing\\tthe\\tsame\\tgraph\\t\\ng\\n,\\twill\\teach\\tsession\\thave\\tits\\town\\tcopy\\tof\\tthe\\tvariable\\t\\nw\\n\\tor\\twill\\tit\\nbe\\tshared?\\n6\\n.\\t\\nWhen\\tis\\ta\\tvariable\\tinitialized?\\tWhen\\tis\\tit\\tdestroyed?\\n7\\n.\\t\\nWhat\\tis\\tthe\\tdifference\\tbetween\\ta\\tplaceholder\\tand\\ta\\tvariable?\\n8\\n.\\t\\nWhat\\thappens\\twhen\\tyou\\trun\\tthe\\tgraph\\tto\\tevaluate\\tan\\toperation\\tthat\\tdepends\\ton\\ta\\tplaceholder\\tbut\\tyou\\ndon’t\\tfeed\\tits\\tvalue?\\tWhat\\thappens\\tif\\tthe\\toperation\\tdoes\\tnot\\tdepend\\ton\\tthe\\tplaceholder?\\n9\\n.\\t\\nWhen\\tyou\\trun\\ta\\tgraph,\\tcan\\tyou\\tfeed\\tthe\\toutput\\tvalue\\tof\\tany\\toperation,\\tor\\tjust\\tthe\\tvalue\\tof\\nplaceholders?\\n10\\n.\\t\\nHow\\tcan\\tyou\\tset\\ta\\tvariable\\tto\\tany\\tvalue\\tyou\\twant\\t(during\\tthe\\texecution\\tphase)?\\n11\\n.\\t\\nHow\\tmany\\ttimes\\tdoes\\treverse-mode\\tautodiff\\tneed\\tto\\ttraverse\\tthe\\tgraph\\tin\\torder\\tto\\tcompute\\tthe\\ngradients\\tof\\tthe\\tcost\\tfunction\\twith\\tregards\\tto\\t10\\tvariables?\\tWhat\\tabout\\tforward-mode\\tautodiff?\\tAnd\\nsymbolic\\tdifferentiation?\\n12\\n.\\t\\nImplement\\tLogistic\\tRegression\\twith\\tMini-batch\\tGradient\\tDescent\\tusing\\tTensorFlow.\\tTrain\\tit\\tand\\nevaluate\\tit\\ton\\tthe\\tmoons\\tdataset\\t(introduced\\tin\\t\\nChapter\\t5\\n).\\tTry\\tadding\\tall\\tthe\\tbells\\tand\\twhistles:\\nDefine\\tthe\\tgraph\\twithin\\ta\\t\\nlogistic_regression()\\n\\tfunction\\tthat\\tcan\\tbe\\treused\\teasily.\\nSave\\tcheckpoints\\tusing\\ta\\t\\nSaver\\n\\tat\\tregular\\tintervals\\tduring\\ttraining,\\tand\\tsave\\tthe\\tfinal\\tmodel\\tat\\nthe\\tend\\tof\\ttraining.\\nRestore\\tthe\\tlast\\tcheckpoint\\tupon\\tstartup\\tif\\ttraining\\twas\\tinterrupted.\\nDefine\\tthe\\tgraph\\tusing\\tnice\\tscopes\\tso\\tthe\\tgraph\\tlooks\\tgood\\tin\\tTensorBoard.\\nAdd\\tsummaries\\tto\\tvisualize\\tthe\\tlearning\\tcurves\\tin\\tTensorBoard.\\nTry\\ttweaking\\tsome\\thyperparameters\\tsuch\\tas\\tthe\\tlearning\\trate\\tor\\tthe\\tmini-batch\\tsize\\tand\\tlook\\tat\\nthe\\tshape\\tof\\tthe\\tlearning\\tcurve.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 314}), Document(page_content='Solutions\\tto\\tthese\\texercises\\tare\\tavailable\\t\\nin\\t\\nAppendix\\tA\\n.\\nTensorFlow\\tis\\tnot\\tlimited\\tto\\tneural\\tnetworks\\tor\\teven\\tMachine\\tLearning;\\tyou\\tcould\\trun\\tquantum\\tphysics\\tsimulations\\tif\\tyou\\twanted.\\nNot\\tto\\tbe\\tconfused\\twith\\tthe\\tTFLearn\\tlibrary,\\twhich\\tis\\tan\\tindependent\\tproject.\\nIn\\tdistributed\\tTensorFlow,\\tvariable\\tvalues\\tare\\tstored\\ton\\tthe\\tservers\\tinstead\\tof\\tthe\\tsession,\\tas\\twe\\twill\\tsee\\tin\\t\\nChapter\\t12\\n.\\nNote\\tthat\\t\\nhousing.target\\n\\tis\\ta\\t1D\\tarray,\\tbut\\twe\\tneed\\tto\\treshape\\tit\\tto\\ta\\tcolumn\\tvector\\tto\\tcompute\\t\\ntheta\\n.\\tRecall\\tthat\\tNumPy’s\\t\\nreshape()\\nfunction\\taccepts\\t–1\\t(meaning\\t“unspecified”)\\tfor\\tone\\tof\\tthe\\tdimensions:\\tthat\\tdimension\\twill\\tbe\\tcomputed\\tbased\\ton\\tthe\\tarray’s\\tlength\\tand\\nthe\\tremaining\\tdimensions.\\nCreating\\ta\\t\\nReLU\\n\\tclass\\tis\\targuably\\tthe\\tcleanest\\toption,\\tbut\\tit\\tis\\trather\\theavyweight.\\n1\\n2\\n3\\n4\\n5', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 315}), Document(page_content='Chapter\\t10.\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\nNetworks\\nBirds\\t\\ninspired\\tus\\tto\\tfly,\\tburdock\\tplants\\tinspired\\tvelcro,\\tand\\tnature\\thas\\tinspired\\tmany\\tother\\tinventions.\\tIt\\nseems\\tonly\\tlogical,\\tthen,\\tto\\tlook\\tat\\tthe\\tbrain’s\\tarchitecture\\tfor\\tinspiration\\ton\\thow\\tto\\tbuild\\tan\\tintelligent\\nmachine.\\tThis\\tis\\tthe\\tkey\\tidea\\tthat\\tinspired\\t\\nartificial\\tneural\\tnetworks\\n\\t(ANNs).\\tHowever,\\talthough\\tplanes\\nwere\\tinspired\\tby\\tbirds,\\tthey\\tdon’t\\thave\\tto\\tflap\\ttheir\\twings.\\tSimilarly,\\tANNs\\thave\\tgradually\\tbecome\\tquite\\ndifferent\\tfrom\\ttheir\\tbiological\\tcousins.\\tSome\\tresearchers\\teven\\targue\\tthat\\twe\\tshould\\tdrop\\tthe\\tbiological\\nanalogy\\taltogether\\t(e.g.,\\tby\\tsaying\\t“units”\\trather\\tthan\\t“neurons”),\\tlest\\twe\\trestrict\\tour\\tcreativity\\tto\\nbiologically\\tplausible\\tsystems.\\n1\\nANNs\\tare\\tat\\tthe\\tvery\\tcore\\tof\\tDeep\\tLearning.\\tThey\\tare\\tversatile,\\tpowerful,\\tand\\tscalable,\\tmaking\\tthem\\nideal\\tto\\ttackle\\tlarge\\tand\\thighly\\tcomplex\\tMachine\\tLearning\\ttasks,\\tsuch\\tas\\tclassifying\\tbillions\\tof\\timages\\n(e.g.,\\tGoogle\\tImages),\\t\\npowering\\tspeech\\trecognition\\tservices\\t(e.g.,\\tApple’s\\tSiri),\\t\\nrecommending\\tthe\\tbest\\nvideos\\tto\\twatch\\tto\\thundreds\\tof\\tmillions\\tof\\tusers\\tevery\\tday\\t(e.g.,\\tYouTube),\\t\\nor\\tlearning\\tto\\tbeat\\tthe\\tworld\\nchampion\\tat\\tthe\\tgame\\tof\\t\\nGo\\n\\tby\\texamining\\tmillions\\tof\\tpast\\tgames\\tand\\tthen\\tplaying\\tagainst\\t\\nitself\\n(DeepMind’s\\tAlphaGo).\\nIn\\tthis\\tchapter,\\twe\\twill\\tintroduce\\tartificial\\tneural\\tnetworks,\\tstarting\\twith\\ta\\tquick\\ttour\\tof\\tthe\\tvery\\tfirst\\nANN\\tarchitectures.\\tThen\\twe\\twill\\tpresent\\t\\nMulti-Layer\\tPerceptrons\\n\\t(MLPs)\\t\\nand\\timplement\\tone\\tusing\\nTensorFlow\\tto\\ttackle\\tthe\\tMNIST\\tdigit\\tclassification\\tproblem\\t(introduced\\tin\\t\\nChapter\\t3\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 316}), Document(page_content='From\\tBiological\\tto\\tArtificial\\tNeurons\\nSurprisingly,\\t\\nANNs\\thave\\tbeen\\taround\\tfor\\tquite\\ta\\twhile:\\tthey\\twere\\tfirst\\tintroduced\\tback\\tin\\t1943\\tby\\tthe\\nneurophysiologist\\tWarren\\tMcCulloch\\tand\\tthe\\tmathematician\\tWalter\\tPitts.\\tIn\\ttheir\\t\\nlandmark\\tpaper\\n,\\n2\\n\\t“A\\nLogical\\tCalculus\\tof\\tIdeas\\tImmanent\\tin\\tNervous\\tActivity,”\\tMcCulloch\\tand\\tPitts\\tpresented\\ta\\tsimplified\\ncomputational\\tmodel\\tof\\thow\\tbiological\\tneurons\\tmight\\twork\\ttogether\\tin\\tanimal\\tbrains\\tto\\tperform\\tcomplex\\ncomputations\\tusing\\t\\npropositional\\tlogic\\n.\\t\\nThis\\twas\\tthe\\tfirst\\tartificial\\tneural\\tnetwork\\tarchitecture.\\tSince\\nthen\\tmany\\tother\\tarchitectures\\thave\\tbeen\\tinvented,\\tas\\twe\\twill\\tsee.\\nThe\\tearly\\tsuccesses\\tof\\tANNs\\tuntil\\tthe\\t1960s\\tled\\tto\\tthe\\twidespread\\tbelief\\tthat\\twe\\twould\\tsoon\\tbe\\nconversing\\twith\\ttruly\\tintelligent\\tmachines.\\tWhen\\tit\\tbecame\\tclear\\tthat\\tthis\\tpromise\\twould\\tgo\\tunfulfilled\\n(at\\tleast\\tfor\\tquite\\ta\\twhile),\\tfunding\\tflew\\telsewhere\\tand\\tANNs\\tentered\\ta\\tlong\\tdark\\tera.\\tIn\\tthe\\tearly\\t1980s\\nthere\\twas\\ta\\trevival\\tof\\tinterest\\tin\\tANNs\\tas\\tnew\\tnetwork\\tarchitectures\\twere\\tinvented\\tand\\tbetter\\ttraining\\ntechniques\\twere\\tdeveloped.\\tBut\\tby\\tthe\\t1990s,\\tpowerful\\talternative\\tMachine\\tLearning\\ttechniques\\tsuch\\tas\\nSupport\\tVector\\tMachines\\t(see\\t\\nChapter\\t5\\n)\\twere\\tfavored\\tby\\tmost\\tresearchers,\\tas\\tthey\\tseemed\\tto\\toffer\\nbetter\\tresults\\tand\\tstronger\\ttheoretical\\tfoundations.\\tFinally,\\twe\\tare\\tnow\\twitnessing\\tyet\\tanother\\twave\\tof\\ninterest\\tin\\tANNs.\\tWill\\tthis\\twave\\tdie\\tout\\tlike\\tthe\\tprevious\\tones\\tdid?\\tThere\\tare\\ta\\tfew\\tgood\\treasons\\tto\\nbelieve\\tthat\\tthis\\tone\\tis\\tdifferent\\tand\\twill\\thave\\ta\\tmuch\\tmore\\tprofound\\timpact\\ton\\tour\\tlives:\\nThere\\tis\\tnow\\ta\\thuge\\tquantity\\tof\\tdata\\tavailable\\tto\\ttrain\\tneural\\tnetworks,\\tand\\tANNs\\tfrequently\\noutperform\\tother\\tML\\ttechniques\\ton\\tvery\\tlarge\\tand\\tcomplex\\tproblems.\\nThe\\ttremendous\\tincrease\\tin\\tcomputing\\tpower\\tsince\\tthe\\t1990s\\tnow\\tmakes\\tit\\tpossible\\tto\\ttrain\\tlarge\\nneural\\tnetworks\\tin\\ta\\treasonable\\tamount\\tof\\ttime.\\tThis\\tis\\tin\\tpart\\tdue\\tto\\tMoore’s\\tLaw,\\tbut\\talso\\tthanks\\nto\\tthe\\tgaming\\tindustry,\\twhich\\thas\\tproduced\\tpowerful\\tGPU\\tcards\\tby\\tthe\\tmillions.\\nThe\\ttraining\\talgorithms\\thave\\tbeen\\timproved.\\tTo\\tbe\\tfair\\tthey\\tare\\tonly\\tslightly\\tdifferent\\tfrom\\tthe\\tones\\nused\\tin\\tthe\\t1990s,\\tbut\\tthese\\trelatively\\tsmall\\ttweaks\\thave\\ta\\thuge\\tpositive\\timpact.\\nSome\\ttheoretical\\tlimitations\\tof\\tANNs\\thave\\tturned\\tout\\tto\\tbe\\tbenign\\tin\\tpractice.\\tFor\\texample,\\tmany\\npeople\\tthought\\tthat\\tANN\\ttraining\\talgorithms\\twere\\tdoomed\\tbecause\\tthey\\twere\\tlikely\\tto\\tget\\tstuck\\tin\\nlocal\\toptima,\\tbut\\tit\\tturns\\tout\\tthat\\tthis\\tis\\trather\\trare\\tin\\tpractice\\t(or\\twhen\\tit\\tis\\tthe\\tcase,\\tthey\\tare\\tusually\\nfairly\\tclose\\tto\\tthe\\tglobal\\toptimum).\\nANNs\\tseem\\tto\\thave\\tentered\\ta\\tvirtuous\\tcircle\\tof\\tfunding\\tand\\tprogress.\\tAmazing\\tproducts\\tbased\\ton\\nANNs\\tregularly\\tmake\\tthe\\theadline\\tnews,\\twhich\\tpulls\\tmore\\tand\\tmore\\tattention\\tand\\tfunding\\ttoward\\nthem,\\tresulting\\tin\\tmore\\tand\\tmore\\tprogress,\\tand\\teven\\tmore\\tamazing\\t\\nproducts.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 317}), Document(page_content='Biological\\tNeurons\\nBefore\\twe\\tdiscuss\\tartificial\\tneurons,\\tlet’s\\ttake\\ta\\tquick\\tlook\\tat\\ta\\tbiological\\tneuron\\t(represented\\tin\\nFigure\\t10-1\\n).\\tIt\\tis\\tan\\tunusual-looking\\tcell\\tmostly\\tfound\\tin\\tanimal\\tcerebral\\tcortexes\\t(e.g.,\\tyour\\tbrain),\\ncomposed\\tof\\ta\\t\\ncell\\tbody\\n\\tcontaining\\tthe\\tnucleus\\tand\\tmost\\tof\\tthe\\tcell’s\\tcomplex\\tcomponents,\\tand\\tmany\\nbranching\\textensions\\tcalled\\t\\ndendrites\\n,\\tplus\\tone\\tvery\\tlong\\textension\\tcalled\\tthe\\t\\naxon\\n.\\tThe\\taxon’s\\tlength\\nmay\\tbe\\tjust\\ta\\tfew\\ttimes\\tlonger\\tthan\\tthe\\tcell\\tbody,\\tor\\tup\\tto\\ttens\\tof\\tthousands\\tof\\ttimes\\tlonger.\\tNear\\tits\\nextremity\\tthe\\taxon\\tsplits\\toff\\tinto\\tmany\\tbranches\\tcalled\\t\\ntelodendria\\n,\\tand\\tat\\tthe\\ttip\\tof\\tthese\\tbranches\\tare\\nminuscule\\tstructures\\tcalled\\t\\nsynaptic\\tterminals\\n\\t(or\\tsimply\\t\\nsynapses\\n),\\twhich\\tare\\tconnected\\tto\\tthe\\ndendrites\\t(or\\tdirectly\\tto\\tthe\\tcell\\tbody)\\tof\\tother\\tneurons.\\tBiological\\tneurons\\treceive\\tshort\\telectrical\\nimpulses\\tcalled\\t\\nsignals\\n\\tfrom\\tother\\tneurons\\tvia\\tthese\\tsynapses.\\tWhen\\ta\\tneuron\\treceives\\ta\\tsufficient\\nnumber\\tof\\tsignals\\tfrom\\tother\\tneurons\\twithin\\ta\\tfew\\tmilliseconds,\\tit\\tfires\\tits\\town\\tsignals.\\nFigure\\t10-1.\\t\\nBiological\\tneuron\\n3\\nThus,\\tindividual\\tbiological\\tneurons\\tseem\\tto\\tbehave\\tin\\ta\\trather\\tsimple\\tway,\\tbut\\tthey\\tare\\torganized\\tin\\ta\\nvast\\tnetwork\\tof\\tbillions\\tof\\tneurons,\\teach\\tneuron\\ttypically\\tconnected\\tto\\tthousands\\tof\\tother\\tneurons.\\tHighly\\ncomplex\\tcomputations\\tcan\\tbe\\tperformed\\tby\\ta\\tvast\\tnetwork\\tof\\tfairly\\tsimple\\tneurons,\\tmuch\\tlike\\ta\\tcomplex\\nanthill\\tcan\\temerge\\tfrom\\tthe\\tcombined\\tefforts\\tof\\tsimple\\tants.\\tThe\\tarchitecture\\tof\\tbiological\\tneural\\nnetworks\\t(BNN)\\n4\\n\\tis\\tstill\\tthe\\tsubject\\tof\\tactive\\tresearch,\\tbut\\tsome\\tparts\\tof\\tthe\\tbrain\\thave\\tbeen\\tmapped,\\nand\\tit\\tseems\\tthat\\tneurons\\tare\\toften\\torganized\\tin\\tconsecutive\\tlayers,\\tas\\t\\nshown\\tin\\t\\nFigure\\t10-2\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 318}), Document(page_content='Figure\\t10-2.\\t\\nMultiple\\tlayers\\tin\\ta\\tbiological\\tneural\\tnetwork\\t(human\\tcortex)\\n5', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 319}), Document(page_content='Logical\\tComputations\\twith\\tNeurons\\nWarren\\tMcCulloch\\t\\nand\\tWalter\\tPitts\\tproposed\\ta\\tvery\\tsimple\\tmodel\\tof\\tthe\\tbiological\\tneuron,\\twhich\\tlater\\nbecame\\tknown\\tas\\t\\nan\\t\\nartificial\\tneuron\\n:\\tit\\thas\\tone\\tor\\tmore\\tbinary\\t(on/off)\\tinputs\\tand\\tone\\tbinary\\toutput.\\nThe\\tartificial\\tneuron\\tsimply\\tactivates\\tits\\toutput\\twhen\\tmore\\tthan\\ta\\tcertain\\tnumber\\tof\\tits\\tinputs\\tare\\tactive.\\nMcCulloch\\tand\\tPitts\\tshowed\\tthat\\teven\\twith\\tsuch\\ta\\tsimplified\\tmodel\\tit\\tis\\tpossible\\tto\\tbuild\\ta\\tnetwork\\tof\\nartificial\\tneurons\\tthat\\tcomputes\\tany\\tlogical\\tproposition\\tyou\\twant.\\tFor\\texample,\\tlet’s\\tbuild\\ta\\tfew\\tANNs\\nthat\\tperform\\tvarious\\tlogical\\tcomputations\\t(see\\t\\nFigure\\t10-3\\n),\\tassuming\\tthat\\ta\\tneuron\\tis\\tactivated\\twhen\\tat\\nleast\\ttwo\\tof\\tits\\tinputs\\tare\\tactive.\\nFigure\\t10-3.\\t\\nANNs\\tperforming\\tsimple\\tlogical\\tcomputations\\nThe\\tfirst\\tnetwork\\ton\\tthe\\tleft\\tis\\tsimply\\tthe\\tidentity\\tfunction:\\tif\\tneuron\\tA\\tis\\tactivated,\\tthen\\tneuron\\tC\\ngets\\tactivated\\tas\\twell\\t(since\\tit\\treceives\\ttwo\\tinput\\tsignals\\tfrom\\tneuron\\tA),\\tbut\\tif\\tneuron\\tA\\tis\\toff,\\tthen\\nneuron\\tC\\tis\\toff\\tas\\twell.\\nThe\\tsecond\\tnetwork\\tperforms\\ta\\tlogical\\tAND:\\tneuron\\tC\\tis\\tactivated\\tonly\\twhen\\tboth\\tneurons\\tA\\tand\\tB\\nare\\tactivated\\t(a\\tsingle\\tinput\\tsignal\\tis\\tnot\\tenough\\tto\\tactivate\\tneuron\\tC).\\nThe\\tthird\\tnetwork\\tperforms\\ta\\tlogical\\tOR:\\tneuron\\tC\\tgets\\tactivated\\tif\\teither\\tneuron\\tA\\tor\\tneuron\\tB\\tis\\nactivated\\t(or\\tboth).\\nFinally,\\tif\\twe\\tsuppose\\tthat\\tan\\tinput\\tconnection\\tcan\\tinhibit\\tthe\\tneuron’s\\tactivity\\t(which\\tis\\tthe\\tcase\\nwith\\tbiological\\tneurons),\\tthen\\tthe\\tfourth\\tnetwork\\tcomputes\\ta\\tslightly\\tmore\\tcomplex\\tlogical\\nproposition:\\tneuron\\tC\\tis\\tactivated\\tonly\\tif\\tneuron\\tA\\tis\\tactive\\tand\\tif\\tneuron\\tB\\tis\\toff.\\tIf\\tneuron\\tA\\tis\\nactive\\tall\\tthe\\ttime,\\tthen\\tyou\\tget\\ta\\tlogical\\tNOT:\\tneuron\\tC\\tis\\tactive\\twhen\\tneuron\\tB\\tis\\toff,\\tand\\tvice\\nversa.\\nYou\\tcan\\teasily\\timagine\\thow\\tthese\\tnetworks\\tcan\\tbe\\tcombined\\tto\\tcompute\\tcomplex\\tlogical\\texpressions\\n(see\\tthe\\texercises\\tat\\tthe\\tend\\tof\\tthe\\tchapter).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 320}), Document(page_content='The\\tPerceptron\\nThe\\t\\nPerceptron\\n\\t\\nis\\tone\\tof\\tthe\\tsimplest\\tANN\\tarchitectures,\\tinvented\\tin\\t1957\\tby\\tFrank\\tRosenblatt.\\tIt\\tis\\nbased\\ton\\ta\\tslightly\\tdifferent\\tartificial\\tneuron\\t(see\\t\\nFigure\\t10-4\\n)\\tcalled\\t\\na\\t\\nlinear\\tthreshold\\tunit\\n\\t(LTU):\\tthe\\ninputs\\tand\\toutput\\tare\\tnow\\tnumbers\\t(instead\\tof\\tbinary\\ton/off\\tvalues)\\tand\\teach\\tinput\\tconnection\\tis\\nassociated\\twith\\ta\\tweight.\\tThe\\tLTU\\tcomputes\\ta\\tweighted\\tsum\\tof\\tits\\tinputs\\t(\\nz\\n\\t=\\t\\nw\\n1\\n\\t\\nx\\n1\\n\\t+\\t\\nw\\n2\\n\\t\\nx\\n2\\n\\t+\\t\\t+\\t\\nw\\nn\\n\\t\\nx\\nn\\n=\\t\\nw\\nT\\n\\t·\\t\\nx\\n),\\tthen\\tapplies\\ta\\t\\nstep\\tfunction\\n\\t\\nto\\tthat\\tsum\\tand\\toutputs\\tthe\\tresult:\\t\\nh\\nw\\n(\\nx\\n)\\t=\\tstep\\t(\\nz\\n)\\t=\\tstep\\t(\\nw\\nT\\n\\t·\\t\\nx\\n).\\nFigure\\t10-4.\\t\\nLinear\\tthreshold\\tunit\\nThe\\tmost\\tcommon\\tstep\\tfunction\\tused\\tin\\tPerceptrons\\tis\\t\\nthe\\t\\nHeaviside\\tstep\\tfunction\\n\\t(see\\t\\nEquation\\t10-1\\n).\\nSometimes\\tthe\\tsign\\tfunction\\tis\\tused\\tinstead.\\nEquation\\t10-1.\\t\\nCommon\\tstep\\tfunctions\\tused\\tin\\tPerceptrons\\nA\\tsingle\\tLTU\\tcan\\tbe\\tused\\tfor\\tsimple\\tlinear\\tbinary\\tclassification.\\tIt\\tcomputes\\ta\\tlinear\\tcombination\\tof\\tthe\\ninputs\\tand\\tif\\tthe\\tresult\\texceeds\\ta\\tthreshold,\\tit\\toutputs\\tthe\\tpositive\\tclass\\tor\\telse\\toutputs\\tthe\\tnegative\\tclass\\n(just\\tlike\\ta\\tLogistic\\tRegression\\tclassifier\\tor\\ta\\tlinear\\tSVM).\\tFor\\texample,\\tyou\\tcould\\tuse\\ta\\tsingle\\tLTU\\tto\\nclassify\\tiris\\tflowers\\tbased\\ton\\tthe\\tpetal\\tlength\\tand\\twidth\\t(also\\tadding\\tan\\textra\\tbias\\tfeature\\t\\nx\\n0\\n\\t=\\t1,\\tjust\\tlike\\nwe\\tdid\\tin\\tprevious\\tchapters).\\tTraining\\tan\\tLTU\\tmeans\\tfinding\\tthe\\tright\\tvalues\\tfor\\t\\nw\\n0\\n,\\t\\nw\\n1\\n,\\tand\\t\\nw\\n2\\n\\t(the\\ntraining\\talgorithm\\tis\\tdiscussed\\tshortly).\\nA\\tPerceptron\\tis\\tsimply\\tcomposed\\tof\\ta\\tsingle\\tlayer\\tof\\tLTUs,\\n6\\n\\twith\\teach\\tneuron\\tconnected\\tto\\tall\\tthe\\tinputs.\\nThese\\tconnections\\tare\\toften\\trepresented\\tusing\\tspecial\\tpassthrough\\tneurons\\t\\ncalled\\t\\ninput\\tneurons\\n:\\tthey\\tjust\\noutput\\twhatever\\tinput\\tthey\\tare\\tfed.\\tMoreover,\\tan\\textra\\tbias\\tfeature\\tis\\tgenerally\\tadded\\t(\\nx\\n0\\n\\t=\\t1).\\tThis\\tbias\\nfeature\\tis\\ttypically\\trepresented\\tusing\\ta\\tspecial\\ttype\\tof\\tneuron\\tcalled\\ta\\t\\nbias\\tneuron\\n,\\t\\nwhich\\tjust\\toutputs\\t1\\nall\\tthe\\ttime.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 321}), Document(page_content='A\\tPerceptron\\twith\\ttwo\\tinputs\\tand\\tthree\\toutputs\\tis\\trepresented\\tin\\t\\nFigure\\t10-5\\n.\\tThis\\tPerceptron\\tcan\\nclassify\\tinstances\\tsimultaneously\\tinto\\tthree\\tdifferent\\tbinary\\tclasses,\\twhich\\tmakes\\tit\\ta\\tmultioutput\\nclassifier.\\nFigure\\t10-5.\\t\\nPerceptron\\tdiagram\\nSo\\thow\\tis\\ta\\t\\nPerceptron\\ttrained?\\tThe\\tPerceptron\\ttraining\\talgorithm\\tproposed\\tby\\tFrank\\tRosenblatt\\twas\\nlargely\\tinspired\\t\\nby\\t\\nHebb’s\\trule\\n.\\tIn\\this\\tbook\\t\\nThe\\tOrganization\\tof\\tBehavior\\n,\\tpublished\\tin\\t1949,\\tDonald\\nHebb\\tsuggested\\tthat\\twhen\\ta\\tbiological\\tneuron\\toften\\ttriggers\\tanother\\tneuron,\\tthe\\tconnection\\tbetween\\tthese\\ntwo\\tneurons\\tgrows\\tstronger.\\tThis\\tidea\\twas\\tlater\\tsummarized\\tby\\tSiegrid\\tLöwel\\tin\\tthis\\tcatchy\\tphrase:\\n“Cells\\tthat\\tfire\\ttogether,\\twire\\ttogether.”\\tThis\\trule\\tlater\\tbecame\\tknown\\tas\\tHebb’s\\trule\\t\\n(or\\t\\nHebbian\\nlearning\\n);\\tthat\\tis,\\tthe\\tconnection\\tweight\\tbetween\\ttwo\\tneurons\\tis\\tincreased\\twhenever\\tthey\\thave\\tthe\\tsame\\noutput.\\tPerceptrons\\tare\\ttrained\\tusing\\ta\\tvariant\\tof\\tthis\\trule\\tthat\\ttakes\\tinto\\taccount\\tthe\\terror\\tmade\\tby\\tthe\\nnetwork;\\tit\\tdoes\\tnot\\treinforce\\tconnections\\tthat\\tlead\\tto\\tthe\\twrong\\toutput.\\tMore\\tspecifically,\\tthe\\tPerceptron\\nis\\tfed\\tone\\ttraining\\tinstance\\tat\\ta\\ttime,\\tand\\tfor\\teach\\tinstance\\tit\\tmakes\\tits\\tpredictions.\\tFor\\tevery\\toutput\\nneuron\\tthat\\tproduced\\ta\\twrong\\tprediction,\\tit\\treinforces\\tthe\\tconnection\\tweights\\tfrom\\tthe\\tinputs\\tthat\\twould\\nhave\\tcontributed\\tto\\tthe\\tcorrect\\tprediction.\\tThe\\trule\\tis\\tshown\\tin\\t\\nEquation\\t10-2\\n.\\nEquation\\t10-2.\\t\\nPerceptron\\tlearning\\trule\\t(weight\\tupdate)\\nw\\ni\\n,\\t\\nj\\n\\tis\\tthe\\tconnection\\tweight\\tbetween\\tthe\\ti\\nth\\n\\tinput\\tneuron\\tand\\tthe\\tj\\nth\\n\\toutput\\tneuron.\\nx\\ni\\n\\tis\\tthe\\ti\\nth\\n\\tinput\\tvalue\\tof\\tthe\\tcurrent\\ttraining\\tinstance.\\nj\\n\\tis\\tthe\\toutput\\tof\\tthe\\tj\\nth\\n\\toutput\\tneuron\\tfor\\tthe\\tcurrent\\ttraining\\tinstance.\\ny\\nj\\n\\tis\\tthe\\ttarget\\toutput\\tof\\tthe\\tj\\nth\\n\\toutput\\tneuron\\tfor\\tthe\\tcurrent\\ttraining\\tinstance.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 322}), Document(page_content='η\\n\\tis\\tthe\\tlearning\\trate.\\nThe\\tdecision\\tboundary\\tof\\teach\\toutput\\tneuron\\tis\\tlinear,\\tso\\tPerceptrons\\tare\\tincapable\\tof\\tlearning\\tcomplex\\npatterns\\t(just\\tlike\\tLogistic\\tRegression\\tclassifiers).\\tHowever,\\tif\\tthe\\ttraining\\tinstances\\tare\\tlinearly\\nseparable,\\tRosenblatt\\tdemonstrated\\tthat\\tthis\\talgorithm\\twould\\tconverge\\tto\\ta\\tsolution.\\n7\\n\\tThis\\tis\\t\\ncalled\\t\\nthe\\nPerceptron\\tconvergence\\ttheorem\\n.\\nScikit-Learn\\t\\nprovides\\ta\\t\\nPerceptron\\n\\tclass\\t\\nthat\\timplements\\ta\\tsingle\\tLTU\\tnetwork.\\tIt\\tcan\\tbe\\tused\\tpretty\\nmuch\\tas\\tyou\\twould\\texpect\\t—\\tfor\\texample,\\ton\\tthe\\tiris\\tdataset\\t(introduced\\tin\\t\\nChapter\\t4\\n):\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nload_iris\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nPerceptron\\niris\\n\\t\\n=\\n\\t\\nload_iris\\n()\\nX\\n\\t\\n=\\n\\t\\niris\\n.\\ndata\\n[:,\\n\\t\\n(\\n2\\n,\\n\\t\\n3\\n)]\\n\\t\\t\\n#\\tpetal\\tlength,\\tpetal\\twidth\\ny\\n\\t\\n=\\n\\t\\n(\\niris\\n.\\ntarget\\n\\t\\n==\\n\\t\\n0\\n)\\n.\\nastype\\n(\\nnp\\n.\\nint\\n)\\n\\t\\t\\n#\\tIris\\tSetosa?\\nper_clf\\n\\t\\n=\\n\\t\\nPerceptron\\n(\\nrandom_state\\n=\\n42\\n)\\nper_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\ny_pred\\n\\t\\n=\\n\\t\\nper_clf\\n.\\npredict\\n([[\\n2\\n,\\n\\t\\n0.5\\n]])\\nYou\\tmay\\thave\\trecognized\\tthat\\tthe\\tPerceptron\\tlearning\\talgorithm\\tstrongly\\tresembles\\t\\nStochastic\\tGradient\\nDescent.\\tIn\\tfact,\\tScikit-Learn’s\\t\\nPerceptron\\n\\tclass\\tis\\tequivalent\\tto\\tusing\\tan\\t\\nSGDClassifier\\n\\twith\\tthe\\nfollowing\\thyperparameters:\\t\\nloss=\"perceptron\"\\n,\\t\\nlearning_rate=\"constant\"\\n,\\t\\neta0=1\\n\\t(the\\tlearning\\nrate),\\tand\\t\\npenalty=None\\n\\t(no\\tregularization).\\nNote\\t\\nthat\\tcontrary\\tto\\tLogistic\\tRegression\\tclassifiers,\\tPerceptrons\\tdo\\tnot\\toutput\\ta\\tclass\\tprobability;\\trather,\\nthey\\tjust\\tmake\\tpredictions\\tbased\\ton\\ta\\thard\\tthreshold.\\tThis\\tis\\tone\\tof\\tthe\\tgood\\treasons\\tto\\tprefer\\tLogistic\\nRegression\\tover\\tPerceptrons.\\nIn\\ttheir\\t1969\\tmonograph\\ttitled\\t\\nPerceptrons\\n,\\tMarvin\\tMinsky\\tand\\tSeymour\\tPapert\\thighlighted\\ta\\tnumber\\tof\\nserious\\tweaknesses\\tof\\tPerceptrons,\\tin\\tparticular\\tthe\\tfact\\tthat\\tthey\\tare\\tincapable\\tof\\tsolving\\tsome\\ttrivial\\nproblems\\t(e.g.,\\tthe\\t\\nExclusive\\tOR\\n\\t(XOR)\\tclassification\\tproblem;\\tsee\\tthe\\tleft\\tside\\tof\\t\\nFigure\\t10-6\\n).\\tOf\\ncourse\\tthis\\tis\\ttrue\\tof\\tany\\tother\\tlinear\\tclassification\\tmodel\\tas\\twell\\t(such\\tas\\tLogistic\\tRegression\\nclassifiers),\\tbut\\tresearchers\\thad\\texpected\\tmuch\\tmore\\tfrom\\tPerceptrons,\\tand\\ttheir\\tdisappointment\\twas\\ngreat:\\tas\\ta\\tresult,\\tmany\\tresearchers\\tdropped\\t\\nconnectionism\\n\\t\\naltogether\\t(i.e.,\\tthe\\tstudy\\tof\\tneural\\tnetworks)\\nin\\tfavor\\tof\\thigher-level\\tproblems\\tsuch\\tas\\tlogic,\\tproblem\\tsolving,\\tand\\tsearch.\\nHowever,\\tit\\tturns\\tout\\tthat\\tsome\\tof\\tthe\\tlimitations\\tof\\tPerceptrons\\tcan\\tbe\\teliminated\\tby\\tstacking\\tmultiple\\nPerceptrons.\\tThe\\tresulting\\tANN\\tis\\tcalled\\ta\\t\\nMulti-Layer\\tPerceptron\\n\\t(MLP).\\t\\nIn\\tparticular,\\tan\\tMLP\\tcan\\nsolve\\tthe\\tXOR\\tproblem,\\tas\\tyou\\tcan\\tverify\\tby\\tcomputing\\tthe\\toutput\\tof\\tthe\\tMLP\\trepresented\\ton\\tthe\\tright\\tof\\nFigure\\t10-6\\n,\\tfor\\teach\\tcombination\\tof\\tinputs:\\twith\\tinputs\\t(0,\\t0)\\tor\\t(1,\\t1)\\tthe\\tnetwork\\toutputs\\t0,\\tand\\twith\\ninputs\\t(0,\\t1)\\tor\\t(1,\\t0)\\tit\\toutputs\\t1.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 323}), Document(page_content='Figure\\t10-6.\\t\\nXOR\\tclassification\\tproblem\\tand\\tan\\tMLP\\tthat\\tsolves\\tit', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 324}), Document(page_content='Multi-Layer\\tPerceptron\\tand\\tBackpropagation\\nAn\\tMLP\\tis\\tcomposed\\tof\\tone\\t(passthrough)\\tinput\\tlayer,\\tone\\tor\\tmore\\tlayers\\tof\\tLTUs,\\tcalled\\t\\nhidden\\tlayers\\n,\\nand\\tone\\tfinal\\tlayer\\tof\\tLTUs\\tcalled\\t\\nthe\\t\\noutput\\tlayer\\n\\t(see\\t\\nFigure\\t10-7\\n).\\tEvery\\tlayer\\texcept\\tthe\\toutput\\tlayer\\nincludes\\ta\\tbias\\tneuron\\tand\\tis\\tfully\\tconnected\\tto\\tthe\\tnext\\tlayer.\\tWhen\\tan\\tANN\\thas\\ttwo\\tor\\tmore\\thidden\\nlayers,\\tit\\tis\\tcalled\\t\\na\\t\\ndeep\\tneural\\tnetwork\\n\\t(DNN).\\nFigure\\t10-7.\\t\\nMulti-Layer\\tPerceptron\\nFor\\tmany\\tyears\\tresearchers\\tstruggled\\tto\\tfind\\ta\\tway\\tto\\ttrain\\tMLPs,\\twithout\\tsuccess.\\tBut\\tin\\t1986,\\tD.\\tE.\\nRumelhart\\tet\\tal.\\tpublished\\ta\\t\\ngroundbreaking\\tarticle\\n8\\n\\tintroducing\\tthe\\t\\nbackpropagation\\n\\t\\ntraining\\talgorithm.\\n9\\nToday\\twe\\twould\\tdescribe\\tit\\tas\\tGradient\\tDescent\\tusing\\treverse-mode\\tautodiff\\t(Gradient\\tDescent\\twas\\nintroduced\\tin\\t\\nChapter\\t4\\n,\\tand\\tautodiff\\twas\\tdiscussed\\tin\\t\\nChapter\\t9\\n).\\nFor\\teach\\ttraining\\tinstance,\\tthe\\talgorithm\\tfeeds\\tit\\tto\\tthe\\tnetwork\\tand\\tcomputes\\tthe\\toutput\\tof\\tevery\\tneuron\\nin\\teach\\tconsecutive\\tlayer\\t(this\\tis\\tthe\\tforward\\tpass,\\tjust\\tlike\\twhen\\tmaking\\tpredictions).\\tThen\\tit\\tmeasures\\nthe\\tnetwork’s\\toutput\\terror\\t(i.e.,\\tthe\\tdifference\\tbetween\\tthe\\tdesired\\toutput\\tand\\tthe\\tactual\\toutput\\tof\\tthe\\nnetwork),\\tand\\tit\\tcomputes\\thow\\tmuch\\teach\\tneuron\\tin\\tthe\\tlast\\thidden\\tlayer\\tcontributed\\tto\\teach\\toutput\\nneuron’s\\terror.\\tIt\\tthen\\tproceeds\\tto\\tmeasure\\thow\\tmuch\\tof\\tthese\\terror\\tcontributions\\tcame\\tfrom\\teach\\tneuron\\nin\\tthe\\tprevious\\thidden\\tlayer\\t—\\tand\\tso\\ton\\tuntil\\tthe\\talgorithm\\treaches\\tthe\\tinput\\tlayer.\\tThis\\treverse\\tpass\\nefficiently\\tmeasures\\tthe\\terror\\tgradient\\tacross\\tall\\tthe\\tconnection\\tweights\\tin\\tthe\\tnetwork\\tby\\tpropagating\\tthe\\nerror\\tgradient\\tbackward\\tin\\tthe\\tnetwork\\t(hence\\tthe\\tname\\tof\\tthe\\talgorithm).\\tIf\\tyou\\tcheck\\tout\\tthe\\treverse-\\nmode\\tautodiff\\talgorithm\\tin\\t\\nAppendix\\tD\\n,\\tyou\\twill\\tfind\\tthat\\tthe\\tforward\\tand\\treverse\\tpasses\\tof\\nbackpropagation\\tsimply\\tperform\\treverse-mode\\tautodiff.\\tThe\\tlast\\tstep\\tof\\tthe\\tbackpropagation\\talgorithm\\tis\\na\\tGradient\\tDescent\\tstep\\ton\\tall\\tthe\\tconnection\\tweights\\tin\\tthe\\tnetwork,\\tusing\\tthe\\terror\\tgradients\\tmeasured\\nearlier.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 325}), Document(page_content='Let’s\\tmake\\tthis\\teven\\tshorter:\\tfor\\teach\\ttraining\\tinstance\\tthe\\tbackpropagation\\talgorithm\\tfirst\\tmakes\\ta\\nprediction\\t(forward\\tpass),\\tmeasures\\tthe\\terror,\\tthen\\tgoes\\tthrough\\teach\\tlayer\\tin\\treverse\\tto\\tmeasure\\tthe\\nerror\\tcontribution\\tfrom\\teach\\tconnection\\t(reverse\\tpass),\\tand\\tfinally\\tslightly\\ttweaks\\tthe\\tconnection\\tweights\\nto\\treduce\\tthe\\t\\nerror\\t(Gradient\\tDescent\\tstep).\\nIn\\torder\\tfor\\tthis\\talgorithm\\tto\\twork\\tproperly,\\tthe\\tauthors\\tmade\\ta\\tkey\\tchange\\tto\\tthe\\tMLP’s\\tarchitecture:\\tthey\\nreplaced\\tthe\\tstep\\tfunction\\twith\\tthe\\tlogistic\\tfunction,\\t\\nσ\\n(\\nz\\n)\\t=\\t1\\t/\\t(1\\t+\\texp(–\\nz\\n)).\\tThis\\twas\\tessential\\tbecause\\nthe\\tstep\\tfunction\\tcontains\\tonly\\tflat\\tsegments,\\tso\\tthere\\tis\\tno\\tgradient\\tto\\twork\\twith\\t(Gradient\\tDescent\\ncannot\\tmove\\ton\\ta\\tflat\\tsurface),\\twhile\\tthe\\tlogistic\\tfunction\\thas\\ta\\twell-defined\\tnonzero\\tderivative\\neverywhere,\\tallowing\\tGradient\\tDescent\\tto\\tmake\\tsome\\tprogress\\tat\\tevery\\tstep.\\tThe\\tbackpropagation\\nalgorithm\\tmay\\tbe\\tused\\twith\\tother\\t\\nactivation\\tfunctions\\n,\\t\\ninstead\\tof\\tthe\\tlogistic\\tfunction.\\tTwo\\tother\\tpopular\\nactivation\\tfunctions\\tare:\\nThe\\t\\nhyperbolic\\ttangent\\n\\t\\nfunction\\ttanh\\t(\\nz\\n)\\t=\\t2\\nσ\\n(2\\nz\\n)\\t–\\t1\\nJust\\tlike\\tthe\\tlogistic\\tfunction\\tit\\tis\\tS-shaped,\\tcontinuous,\\tand\\tdifferentiable,\\tbut\\tits\\toutput\\tvalue\\nranges\\tfrom\\t–1\\tto\\t1\\t(instead\\tof\\t0\\tto\\t1\\tin\\tthe\\tcase\\tof\\tthe\\tlogistic\\tfunction),\\twhich\\ttends\\tto\\tmake\\teach\\nlayer’s\\toutput\\tmore\\tor\\tless\\tnormalized\\t(i.e.,\\tcentered\\taround\\t0)\\tat\\tthe\\tbeginning\\tof\\ttraining.\\tThis\\noften\\thelps\\tspeed\\tup\\tconvergence.\\nThe\\t\\nReLU\\tfunction\\t(introduced\\tin\\t\\nChapter\\t9\\n)\\nReLU\\t(\\nz\\n)\\t=\\tmax\\t(0,\\t\\nz\\n).\\tIt\\tis\\tcontinuous\\tbut\\tunfortunately\\tnot\\tdifferentiable\\tat\\t\\nz\\n\\t=\\t0\\t(the\\tslope\\tchanges\\nabruptly,\\twhich\\tcan\\tmake\\tGradient\\tDescent\\tbounce\\taround).\\tHowever,\\tin\\tpractice\\tit\\tworks\\tvery\\nwell\\tand\\thas\\tthe\\tadvantage\\tof\\tbeing\\tfast\\tto\\tcompute.\\tMost\\timportantly,\\tthe\\tfact\\tthat\\tit\\tdoes\\tnot\\thave\\ta\\nmaximum\\toutput\\tvalue\\talso\\thelps\\treduce\\tsome\\tissues\\tduring\\tGradient\\tDescent\\t(we\\twill\\tcome\\tback\\nto\\tthis\\tin\\t\\nChapter\\t11\\n).\\nThese\\tpopular\\tactivation\\tfunctions\\tand\\ttheir\\tderivatives\\tare\\trepresented\\tin\\t\\nFigure\\t10-8\\n.\\nFigure\\t10-8.\\t\\nActivation\\tfunctions\\tand\\ttheir\\tderivatives\\nAn\\tMLP\\tis\\toften\\tused\\tfor\\tclassification,\\twith\\teach\\toutput\\tcorresponding\\tto\\ta\\tdifferent\\tbinary\\tclass\\t(e.g.,\\nspam/ham,\\turgent/not-urgent,\\tand\\tso\\ton).\\tWhen\\tthe\\tclasses\\tare\\texclusive\\t(e.g.,\\tclasses\\t0\\tthrough\\t9\\tfor\\ndigit\\timage\\tclassification),\\tthe\\toutput\\tlayer\\tis\\ttypically\\tmodified\\tby\\treplacing\\tthe\\tindividual\\tactivation\\nfunctions\\tby\\ta\\tshared\\t\\nsoftmax\\n\\t\\nfunction\\t(see\\t\\nFigure\\t10-9\\n).\\tThe\\tsoftmax\\tfunction\\twas\\tintroduced\\tin\\nChapter\\t3\\n.\\tThe\\toutput\\tof\\teach\\tneuron\\tcorresponds\\tto\\tthe\\testimated\\tprobability\\tof\\tthe\\tcorresponding\\tclass.\\nNote\\tthat\\tthe\\tsignal\\tflows\\tonly\\tin\\tone\\tdirection\\t(from\\tthe\\tinputs\\tto\\tthe\\toutputs),\\tso\\tthis\\tarchitecture\\tis\\tan\\nexample\\t\\nof\\ta\\t\\nfeedforward\\tneural\\tnetwork\\n\\t(FNN).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 326}), Document(page_content='Figure\\t10-9.\\t\\nA\\tmodern\\tMLP\\t(including\\tReLU\\tand\\tsoftmax)\\tfor\\tclassification\\nNOTE\\nBiological\\tneurons\\tseem\\tto\\timplement\\ta\\troughly\\tsigmoid\\t(S-shaped)\\tactivation\\tfunction,\\tso\\tresearchers\\tstuck\\tto\\tsigmoid\\tfunctions\\nfor\\ta\\tvery\\tlong\\ttime.\\tBut\\tit\\tturns\\tout\\tthat\\tthe\\tReLU\\tactivation\\tfunction\\tgenerally\\tworks\\tbetter\\tin\\tANNs.\\tThis\\tis\\tone\\tof\\tthe\\tcases\\nwhere\\tthe\\tbiological\\tanalogy\\twas\\t\\nmisleading.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 327}), Document(page_content=\"Training\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nThe\\t\\nsimplest\\tway\\tto\\ttrain\\tan\\tMLP\\twith\\tTensorFlow\\tis\\tto\\tuse\\tthe\\thigh-level\\tAPI\\tTF.Learn,\\twhich\\toffers\\ta\\nScikit-Learn–compatible\\tAPI.\\tThe\\t\\nDNNClassifier\\n\\t\\nclass\\tmakes\\tit\\tfairly\\teasy\\tto\\ttrain\\ta\\tdeep\\tneural\\nnetwork\\twith\\tany\\tnumber\\tof\\thidden\\tlayers,\\tand\\ta\\tsoftmax\\toutput\\tlayer\\tto\\toutput\\testimated\\tclass\\nprobabilities.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\ttrains\\ta\\tDNN\\tfor\\tclassification\\twith\\ttwo\\thidden\\tlayers\\n(one\\twith\\t300\\tneurons,\\tand\\tthe\\tother\\twith\\t100\\tneurons)\\tand\\ta\\tsoftmax\\toutput\\tlayer\\twith\\t10\\tneurons:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nfeature_cols\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlearn\\n.\\ninfer_real_valued_columns_from_input\\n(\\nX_train\\n)\\ndnn_clf\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlearn\\n.\\nDNNClassifier\\n(\\nhidden_units\\n=\\n[\\n300\\n,\\n100\\n],\\n\\t\\nn_classes\\n=\\n10\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeature_columns\\n=\\nfeature_cols\\n)\\ndnn_clf\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlearn\\n.\\nSKCompat\\n(\\ndnn_clf\\n)\\n\\t\\t\\n#\\tif\\tTensorFlow\\t>=\\t1.1\\ndnn_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n,\\n\\t\\nbatch_size\\n=\\n50\\n,\\n\\t\\nsteps\\n=\\n40000\\n)\\nThe\\tcode\\tfirst\\tcreates\\ta\\tset\\tof\\treal\\tvalued\\tcolumns\\tfrom\\tthe\\ttraining\\tset\\t(other\\ttypes\\tof\\tcolumns,\\tsuch\\tas\\ncategorical\\tcolumns,\\tare\\tavailable).\\tThen\\twe\\tcreate\\tthe\\t\\nDNNClassifier\\n,\\tand\\twe\\twrap\\tit\\tin\\ta\\tScikit-\\nLearn\\tcompatibility\\thelper.\\tFinally,\\twe\\trun\\t40,000\\ttraining\\titerations\\tusing\\tbatches\\tof\\t50\\tinstances.\\nIf\\t\\nyou\\trun\\tthis\\tcode\\ton\\tthe\\tMNIST\\tdataset\\t(after\\tscaling\\tit,\\te.g.,\\tby\\tusing\\tScikit-Learn’s\\nStandardScaler\\n),\\t\\nyou\\twill\\tactually\\tget\\ta\\tmodel\\tthat\\tachieves\\taround\\t98.2%\\taccuracy\\ton\\tthe\\ttest\\tset!\\nThat’s\\tbetter\\tthan\\tthe\\tbest\\tmodel\\twe\\t\\ntrained\\tin\\t\\nChapter\\t3\\n:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\naccuracy_score\\n>>>\\t\\ny_pred\\n\\t\\n=\\n\\t\\ndnn_clf\\n.\\npredict\\n(\\nX_test\\n)\\n>>>\\t\\naccuracy_score\\n(\\ny_test\\n,\\n\\t\\ny_pred\\n[\\n'classes'\\n])\\n0.98250000000000004\\nWARNING\\nThe\\t\\ntensorflow.contrib\\n\\tpackage\\t\\ncontains\\tmany\\tuseful\\tfunctions,\\tbut\\tit\\tis\\ta\\tplace\\tfor\\texperimental\\tcode\\tthat\\thas\\tnot\\tyet\\ngraduated\\tto\\tbe\\tpart\\tof\\tthe\\tcore\\tTensorFlow\\tAPI.\\tSo\\tthe\\t\\nDNNClassifier\\n\\tclass\\t(and\\tany\\tother\\t\\ncontrib\\n\\tcode)\\tmay\\tchange\\nwithout\\tnotice\\tin\\tthe\\tfuture.\\nUnder\\tthe\\thood,\\tthe\\t\\nDNNClassifier\\n\\tclass\\tcreates\\tall\\tthe\\tneuron\\tlayers,\\tbased\\ton\\tthe\\tReLU\\tactivation\\nfunction\\t(we\\tcan\\tchange\\tthis\\tby\\tsetting\\tthe\\t\\nactivation_fn\\n\\thyperparameter).\\tThe\\toutput\\tlayer\\trelies\\ton\\nthe\\tsoftmax\\tfunction,\\t\\nand\\tthe\\t\\ncost\\tfunction\\tis\\tcross\\tentropy\\t(introduced\\tin\\t\\nChapter\\t4\\n).\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 328}), Document(page_content='Training\\ta\\tDNN\\tUsing\\tPlain\\tTensorFlow\\nIf\\t\\nyou\\twant\\tmore\\tcontrol\\tover\\tthe\\tarchitecture\\tof\\tthe\\tnetwork,\\tyou\\tmay\\tprefer\\tto\\tuse\\tTensorFlow’s\\tlower-\\nlevel\\tPython\\tAPI\\t(introduced\\tin\\t\\nChapter\\t9\\n).\\tIn\\tthis\\tsection\\twe\\twill\\tbuild\\tthe\\tsame\\tmodel\\tas\\tbefore\\tusing\\nthis\\tAPI,\\tand\\twe\\twill\\timplement\\tMini-batch\\tGradient\\tDescent\\tto\\ttrain\\tit\\ton\\tthe\\tMNIST\\tdataset.\\tThe\\tfirst\\nstep\\tis\\tthe\\tconstruction\\tphase,\\tbuilding\\tthe\\tTensorFlow\\tgraph.\\tThe\\tsecond\\tstep\\tis\\tthe\\texecution\\tphase,\\nwhere\\tyou\\tactually\\trun\\tthe\\tgraph\\tto\\ttrain\\tthe\\tmodel.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 329}), Document(page_content='Construction\\tPhase\\nLet’s\\tstart.\\t\\nFirst\\twe\\tneed\\tto\\timport\\tthe\\t\\ntensorflow\\n\\tlibrary.\\tThen\\twe\\tmust\\tspecify\\tthe\\tnumber\\tof\\tinputs\\nand\\toutputs,\\tand\\tset\\tthe\\tnumber\\tof\\thidden\\tneurons\\tin\\teach\\tlayer:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nn_inputs\\n\\t\\n=\\n\\t\\n28\\n*\\n28\\n\\t\\t\\n#\\tMNIST\\nn_hidden1\\n\\t\\n=\\n\\t\\n300\\nn_hidden2\\n\\t\\n=\\n\\t\\n100\\nn_outputs\\n\\t\\n=\\n\\t\\n10\\nNext,\\tjust\\tlike\\tyou\\tdid\\tin\\t\\nChapter\\t9\\n,\\tyou\\tcan\\tuse\\tplaceholder\\tnodes\\tto\\trepresent\\tthe\\ttraining\\tdata\\tand\\ntargets.\\tThe\\tshape\\tof\\t\\nX\\n\\tis\\tonly\\tpartially\\tdefined.\\tWe\\tknow\\tthat\\tit\\twill\\tbe\\ta\\t2D\\ttensor\\t(i.e.,\\ta\\tmatrix),\\twith\\ninstances\\talong\\tthe\\tfirst\\tdimension\\tand\\tfeatures\\talong\\tthe\\tsecond\\tdimension,\\tand\\twe\\tknow\\tthat\\tthe\\tnumber\\nof\\tfeatures\\tis\\tgoing\\tto\\tbe\\t28\\tx\\t28\\t(one\\tfeature\\tper\\tpixel),\\tbut\\twe\\tdon’t\\tknow\\tyet\\thow\\tmany\\tinstances\\teach\\ntraining\\tbatch\\twill\\tcontain.\\tSo\\tthe\\tshape\\tof\\t\\nX\\n\\tis\\t\\n(None,\\tn_inputs)\\n.\\tSimilarly,\\twe\\tknow\\tthat\\t\\ny\\n\\twill\\tbe\\ta\\n1D\\ttensor\\twith\\tone\\tentry\\tper\\tinstance,\\tbut\\tagain\\twe\\tdon’t\\tknow\\tthe\\tsize\\tof\\tthe\\ttraining\\tbatch\\tat\\tthis\\tpoint,\\nso\\tthe\\t\\nshape\\tis\\t\\n(None)\\n.\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_inputs\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint64\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n),\\n\\t\\nname\\n=\\n\"y\"\\n)\\nNow\\tlet’s\\tcreate\\tthe\\tactual\\tneural\\tnetwork.\\tThe\\tplaceholder\\t\\nX\\n\\twill\\tact\\tas\\tthe\\tinput\\tlayer;\\tduring\\tthe\\nexecution\\tphase,\\tit\\twill\\tbe\\treplaced\\twith\\tone\\ttraining\\tbatch\\tat\\ta\\ttime\\t(note\\tthat\\tall\\tthe\\tinstances\\tin\\ta\\ntraining\\tbatch\\twill\\tbe\\tprocessed\\tsimultaneously\\tby\\tthe\\tneural\\tnetwork).\\tNow\\tyou\\tneed\\tto\\tcreate\\tthe\\ttwo\\nhidden\\tlayers\\tand\\tthe\\toutput\\tlayer.\\tThe\\ttwo\\thidden\\tlayers\\tare\\talmost\\tidentical:\\tthey\\tdiffer\\tonly\\tby\\tthe\\ninputs\\tthey\\tare\\tconnected\\tto\\tand\\tby\\tthe\\tnumber\\tof\\tneurons\\tthey\\tcontain.\\tThe\\toutput\\tlayer\\tis\\talso\\tvery\\nsimilar,\\tbut\\tit\\tuses\\ta\\tsoftmax\\tactivation\\tfunction\\tinstead\\tof\\ta\\tReLU\\tactivation\\tfunction.\\tSo\\tlet’s\\tcreate\\ta\\nneuron_layer()\\n\\tfunction\\tthat\\twe\\twill\\tuse\\tto\\tcreate\\tone\\tlayer\\tat\\ta\\ttime.\\tIt\\twill\\tneed\\tparameters\\tto\\nspecify\\tthe\\tinputs,\\tthe\\tnumber\\tof\\tneurons,\\tthe\\tactivation\\tfunction,\\t\\nand\\tthe\\tname\\tof\\tthe\\tlayer:\\ndef\\n\\t\\nneuron_layer\\n(\\nX\\n,\\n\\t\\nn_neurons\\n,\\n\\t\\nname\\n,\\n\\t\\nactivation\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\nname\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nn_inputs\\n\\t\\n=\\n\\t\\nint\\n(\\nX\\n.\\nget_shape\\n()[\\n1\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nstddev\\n\\t\\n=\\n\\t\\n2\\n\\t\\n/\\n\\t\\nnp\\n.\\nsqrt\\n(\\nn_inputs\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\ntruncated_normal\\n((\\nn_inputs\\n,\\n\\t\\nn_neurons\\n),\\n\\t\\nstddev\\n=\\nstddev\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nW\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ninit\\n,\\n\\t\\nname\\n=\\n\"kernel\"\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n([\\nn_neurons\\n]),\\n\\t\\nname\\n=\\n\"bias\"\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nZ\\n\\t\\n=\\n\\t\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\nW\\n)\\n\\t\\n+\\n\\t\\nb\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nactivation\\n\\t\\nis\\n\\t\\nnot\\n\\t\\nNone\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nactivation\\n(\\nZ\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nZ\\nLet’s\\tgo\\tthrough\\tthis\\tcode\\tline\\tby\\tline:\\n1\\n.\\t\\nFirst\\twe\\tcreate\\ta\\tname\\tscope\\tusing\\tthe\\tname\\tof\\tthe\\tlayer:\\tit\\twill\\tcontain\\tall\\tthe\\tcomputation\\tnodes\\nfor\\tthis\\tneuron\\tlayer.\\tThis\\tis\\toptional,\\tbut\\tthe\\tgraph\\twill\\tlook\\tmuch\\tnicer\\tin\\tTensorBoard\\tif\\tits\\tnodes\\nare\\twell\\torganized.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 330}), Document(page_content='2\\n.\\t\\nNext,\\twe\\tget\\tthe\\tnumber\\tof\\tinputs\\tby\\tlooking\\tup\\tthe\\tinput\\tmatrix’s\\tshape\\tand\\tgetting\\tthe\\tsize\\tof\\tthe\\nsecond\\tdimension\\t(the\\tfirst\\tdimension\\tis\\tfor\\tinstances).\\n3\\n.\\t\\nThe\\tnext\\tthree\\tlines\\tcreate\\ta\\t\\nW\\n\\tvariable\\tthat\\twill\\thold\\tthe\\tweights\\tmatrix\\t(often\\tcalled\\tthe\\tlayer’s\\nkernel\\n).\\tIt\\twill\\tbe\\ta\\t2D\\ttensor\\tcontaining\\tall\\tthe\\tconnection\\tweights\\tbetween\\teach\\tinput\\tand\\teach\\nneuron;\\thence,\\tits\\tshape\\twill\\tbe\\t\\n(n_inputs,\\tn_neurons)\\n.\\tIt\\twill\\tbe\\tinitialized\\trandomly,\\tusing\\ta\\ntruncated\\n10\\n\\tnormal\\t(Gaussian)\\tdistribution\\twith\\ta\\tstandard\\tdeviation\\tof\\t\\n.\\tUsing\\tthis\\nspecific\\tstandard\\tdeviation\\thelps\\tthe\\talgorithm\\tconverge\\tmuch\\tfaster\\t(we\\twill\\tdiscuss\\tthis\\tfurther\\tin\\nChapter\\t11\\n;\\tit\\tis\\tone\\tof\\tthose\\tsmall\\ttweaks\\tto\\tneural\\tnetworks\\tthat\\thave\\thad\\ta\\ttremendous\\timpact\\ton\\ntheir\\tefficiency).\\tIt\\tis\\timportant\\tto\\tinitialize\\tconnection\\tweights\\trandomly\\tfor\\tall\\thidden\\tlayers\\tto\\navoid\\tany\\tsymmetries\\tthat\\tthe\\tGradient\\tDescent\\talgorithm\\twould\\tbe\\tunable\\tto\\tbreak.\\n11\\n4\\n.\\t\\nThe\\tnext\\tline\\tcreates\\ta\\t\\nb\\n\\tvariable\\tfor\\tbiases,\\tinitialized\\tto\\t0\\t(no\\tsymmetry\\tissue\\tin\\tthis\\tcase),\\twith\\none\\tbias\\tparameter\\tper\\tneuron.\\n5\\n.\\t\\nThen\\twe\\tcreate\\ta\\tsubgraph\\tto\\tcompute\\t\\nZ\\n\\t=\\t\\nX\\n\\t·\\t\\nW\\n\\t+\\t\\nb\\n.\\tThis\\tvectorized\\timplementation\\twill\\nefficiently\\tcompute\\tthe\\tweighted\\tsums\\tof\\tthe\\tinputs\\tplus\\tthe\\tbias\\tterm\\tfor\\teach\\tand\\tevery\\tneuron\\tin\\nthe\\tlayer,\\tfor\\tall\\tthe\\tinstances\\tin\\tthe\\tbatch\\tin\\tjust\\tone\\tshot.\\n6\\n.\\t\\nFinally,\\tif\\tan\\t\\nactivation\\n\\tparameter\\tis\\tprovided,\\tsuch\\tas\\t\\ntf.nn.relu\\n\\t\\n(i.e.,\\tmax\\t(0,\\t\\nZ\\n)),\\tthen\\tthe\\ncode\\treturns\\t\\nactivation(Z)\\n,\\tor\\telse\\tit\\tjust\\treturns\\t\\nZ\\n.\\nOkay,\\tso\\tnow\\tyou\\thave\\ta\\tnice\\tfunction\\tto\\tcreate\\ta\\tneuron\\tlayer.\\tLet’s\\tuse\\tit\\tto\\tcreate\\tthe\\tdeep\\tneural\\nnetwork!\\tThe\\tfirst\\thidden\\tlayer\\ttakes\\t\\nX\\n\\tas\\tits\\tinput.\\tThe\\tsecond\\ttakes\\tthe\\toutput\\tof\\tthe\\tfirst\\thidden\\tlayer\\tas\\nits\\tinput.\\tAnd\\tfinally,\\tthe\\toutput\\tlayer\\ttakes\\tthe\\toutput\\tof\\tthe\\tsecond\\thidden\\tlayer\\tas\\tits\\tinput.\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):\\n\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\nneuron_layer\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\nneuron_layer\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\nneuron_layer\\n(\\nhidden2\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\nNotice\\tthat\\tonce\\tagain\\twe\\tused\\ta\\t\\nname\\tscope\\tfor\\tclarity.\\tAlso\\tnote\\tthat\\t\\nlogits\\n\\tis\\tthe\\toutput\\tof\\tthe\\tneural\\nnetwork\\t\\nbefore\\n\\tgoing\\tthrough\\tthe\\tsoftmax\\tactivation\\tfunction:\\tfor\\toptimization\\treasons,\\twe\\twill\\thandle\\tthe\\nsoftmax\\tcomputation\\tlater.\\nAs\\tyou\\tmight\\texpect,\\tTensorFlow\\tcomes\\twith\\tmany\\thandy\\tfunctions\\tto\\tcreate\\t\\nstandard\\n\\tneural\\tnetwork\\nlayers,\\tso\\tthere’s\\toften\\tno\\tneed\\tto\\tdefine\\tyour\\town\\t\\nneuron_layer()\\n\\t\\nfunction\\tlike\\twe\\tjust\\tdid.\\tFor\\nexample,\\tTensorFlow’s\\t\\ntf.layers.dense()\\n\\t\\nfunction\\t(previously\\tcalled\\ntf.contrib.layers.fully_connected()\\n)\\tcreates\\ta\\tfully\\tconnected\\tlayer,\\twhere\\tall\\tthe\\tinputs\\tare\\nconnected\\tto\\tall\\tthe\\tneurons\\tin\\tthe\\t\\nlayer.\\tIt\\ttakes\\tcare\\tof\\tcreating\\tthe\\t\\nweights\\tand\\t\\nbiases\\tvariables,\\tnamed\\nkernel\\n\\tand\\t\\nbias\\n\\trespectively,\\tusing\\tthe\\tappropriate\\tinitialization\\tstrategy,\\tand\\tyou\\tcan\\tset\\tthe\\tactivation\\nfunction\\tusing\\tthe\\t\\nactivation\\n\\targument.\\tAs\\twe\\twill\\tsee\\tin\\t\\nChapter\\t11\\n,\\tit\\talso\\tsupports\\tregularization\\nparameters.\\tLet’s\\ttweak\\tthe\\tpreceding\\tcode\\tto\\tuse\\tthe\\t\\ndense()\\n\\tfunction\\tinstead\\tof\\tour\\t\\nneuron_layer()\\nfunction.\\tSimply\\treplace\\tthe\\t\\ndnn\\n\\tconstruction\\tsection\\twith\\tthe\\tfollowing\\tcode:\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 331}), Document(page_content='\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden2\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\nNow\\tthat\\twe\\thave\\tthe\\tneural\\tnetwork\\tmodel\\tready\\tto\\tgo,\\twe\\tneed\\tto\\tdefine\\tthe\\t\\ncost\\tfunction\\tthat\\twe\\twill\\nuse\\tto\\ttrain\\tit.\\tJust\\tas\\twe\\tdid\\tfor\\tSoftmax\\tRegression\\tin\\t\\nChapter\\t4\\n,\\twe\\twill\\tuse\\tcross\\tentropy.\\tAs\\twe\\ndiscussed\\tearlier,\\tcross\\tentropy\\twill\\tpenalize\\tmodels\\tthat\\testimate\\ta\\tlow\\tprobability\\tfor\\tthe\\ttarget\\tclass.\\nTensorFlow\\tprovides\\t\\nseveral\\n\\tfunctions\\tto\\tcompute\\tcross\\tentropy.\\t\\nWe\\twill\\tuse\\nsparse_softmax_cross_entropy_with_logits()\\n:\\tit\\tcomputes\\tthe\\tcross\\tentropy\\tbased\\ton\\tthe\\t“logits”\\n(i.e.,\\tthe\\toutput\\tof\\tthe\\tnetwork\\t\\nbefore\\n\\tgoing\\tthrough\\tthe\\tsoftmax\\tactivation\\tfunction),\\tand\\tit\\texpects\\tlabels\\nin\\tthe\\tform\\tof\\tintegers\\tranging\\tfrom\\t0\\tto\\tthe\\tnumber\\tof\\tclasses\\tminus\\t1\\t(in\\tour\\tcase,\\tfrom\\t0\\tto\\t9).\\tThis\\nwill\\tgive\\tus\\ta\\t1D\\ttensor\\tcontaining\\tthe\\tcross\\tentropy\\tfor\\teach\\tinstance.\\tWe\\tcan\\tthen\\tuse\\tTensorFlow’s\\nreduce_mean()\\n\\tfunction\\t\\nto\\tcompute\\tthe\\tmean\\tcross\\tentropy\\tover\\tall\\tinstances.\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"loss\"\\n):\\n\\t\\t\\t\\t\\nxentropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsparse_softmax_cross_entropy_with_logits\\n(\\nlabels\\n=\\ny\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlogits\\n=\\nlogits\\n)\\n\\t\\t\\t\\t\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\nxentropy\\n,\\n\\t\\nname\\n=\\n\"loss\"\\n)\\nNOTE\\nThe\\t\\nsparse_softmax_cross_entropy_with_logits()\\n\\tfunction\\tis\\tequivalent\\t\\nto\\tapplying\\tthe\\tsoftmax\\tactivation\\tfunction\\tand\\tthen\\ncomputing\\tthe\\tcross\\tentropy,\\tbut\\tit\\tis\\tmore\\tefficient,\\tand\\tit\\tproperly\\ttakes\\tcare\\tof\\tcorner\\tcases\\tlike\\tlogits\\tequal\\tto\\t0.\\tThis\\tis\\twhy\\nwe\\tdid\\tnot\\tapply\\tthe\\tsoftmax\\tactivation\\tfunction\\tearlier.\\tThere\\tis\\talso\\tanother\\tfunction\\tcalled\\nsoftmax_cross_entropy_with_logits()\\n,\\twhich\\ttakes\\tlabels\\tin\\tthe\\tform\\tof\\tone-hot\\tvectors\\t(instead\\tof\\tints\\tfrom\\t0\\tto\\tthe\\tnumber\\nof\\tclasses\\tminus\\t1).\\nWe\\thave\\tthe\\tneural\\tnetwork\\tmodel,\\twe\\thave\\tthe\\t\\ncost\\tfunction,\\tand\\tnow\\twe\\tneed\\tto\\tdefine\\ta\\nGradientDescentOptimizer\\n\\tthat\\t\\nwill\\ttweak\\tthe\\t\\nmodel\\tparameters\\tto\\tminimize\\tthe\\tcost\\tfunction.\\nNothing\\tnew;\\tit’s\\tjust\\tlike\\twe\\tdid\\tin\\t\\nChapter\\t9\\n:\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"train\"\\n):\\n\\t\\t\\t\\t\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nGradientDescentOptimizer\\n(\\nlearning_rate\\n)\\n\\t\\t\\t\\t\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\nThe\\tlast\\timportant\\tstep\\tin\\tthe\\tconstruction\\tphase\\tis\\tto\\tspecify\\thow\\tto\\tevaluate\\tthe\\tmodel.\\t\\nWe\\twill\\tsimply\\nuse\\taccuracy\\tas\\tour\\tperformance\\tmeasure.\\tFirst,\\tfor\\teach\\tinstance,\\tdetermine\\tif\\tthe\\tneural\\tnetwork’s\\nprediction\\tis\\tcorrect\\tby\\tchecking\\twhether\\tor\\tnot\\tthe\\thighest\\tlogit\\tcorresponds\\tto\\tthe\\ttarget\\tclass.\\tFor\\tthis\\nyou\\tcan\\tuse\\tthe\\t\\nin_top_k()\\n\\tfunction.\\t\\nThis\\treturns\\ta\\t1D\\ttensor\\tfull\\tof\\tboolean\\tvalues,\\tso\\twe\\tneed\\tto\\tcast\\nthese\\tbooleans\\tto\\tfloats\\tand\\tthen\\tcompute\\tthe\\taverage.\\tThis\\twill\\tgive\\tus\\tthe\\tnetwork’s\\toverall\\taccuracy.\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"eval\"\\n):\\n\\t\\t\\t\\t\\ncorrect\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nin_top_k\\n(\\nlogits\\n,\\n\\t\\ny\\n,\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\naccuracy\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\ncast\\n(\\ncorrect\\n,\\n\\t\\ntf\\n.\\nfloat32\\n))\\nAnd,\\tas\\tusual,\\t\\nwe\\tneed\\tto\\tcreate\\ta\\tnode\\tto\\tinitialize\\tall\\tvariables,\\tand\\twe\\twill\\talso\\tcreate\\ta\\t\\nSaver\\n\\tto', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 332}), Document(page_content='save\\tour\\ttrained\\tmodel\\tparameters\\tto\\tdisk:\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\nPhew!\\tThis\\tconcludes\\tthe\\tconstruction\\tphase.\\tThis\\twas\\tfewer\\tthan\\t40\\tlines\\tof\\tcode,\\tbut\\tit\\twas\\tpretty\\nintense:\\twe\\tcreated\\tplaceholders\\tfor\\tthe\\tinputs\\tand\\tthe\\ttargets,\\twe\\tcreated\\ta\\tfunction\\tto\\tbuild\\ta\\tneuron\\nlayer,\\twe\\tused\\tit\\tto\\tcreate\\tthe\\tDNN,\\twe\\tdefined\\tthe\\tcost\\tfunction,\\twe\\tcreated\\tan\\toptimizer,\\tand\\tfinally\\twe\\ndefined\\tthe\\tperformance\\tmeasure.\\tNow\\ton\\tto\\tthe\\t\\nexecution\\tphase.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 333}), Document(page_content='Execution\\tPhase\\nThis\\t\\npart\\tis\\tmuch\\tshorter\\tand\\tsimpler.\\tFirst,\\tlet’s\\tload\\tMNIST.\\tWe\\tcould\\tuse\\tScikit-Learn\\tfor\\tthat\\tas\\twe\\ndid\\tin\\tprevious\\tchapters,\\tbut\\tTensorFlow\\toffers\\tits\\town\\thelper\\tthat\\tfetches\\tthe\\tdata,\\tscales\\tit\\t(between\\t0\\nand\\t1),\\tshuffles\\tit,\\tand\\tprovides\\ta\\tsimple\\tfunction\\tto\\tload\\tone\\tmini-batch\\ta\\ttime.\\tSo\\tlet’s\\tuse\\tit\\tinstead:\\nfrom\\n\\t\\ntensorflow.examples.tutorials.mnist\\n\\t\\nimport\\n\\t\\ninput_data\\nmnist\\n\\t\\n=\\n\\t\\ninput_data\\n.\\nread_data_sets\\n(\\n\"/tmp/data/\"\\n)\\nNow\\twe\\tdefine\\tthe\\tnumber\\tof\\tepochs\\tthat\\twe\\twant\\tto\\trun,\\tas\\twell\\tas\\tthe\\tsize\\tof\\tthe\\tmini-batches:\\nn_epochs\\n\\t\\n=\\n\\t\\n40\\nbatch_size\\n\\t\\n=\\n\\t\\n50\\nAnd\\tnow\\twe\\tcan\\ttrain\\tthe\\tmodel:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nacc_train\\n\\t\\n=\\n\\t\\naccuracy\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nacc_test\\n\\t\\n=\\n\\t\\naccuracy\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nmnist\\n.\\ntest\\n.\\nimages\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ny\\n:\\n\\t\\nmnist\\n.\\ntest\\n.\\nlabels\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nprint\\n(\\nepoch\\n,\\n\\t\\n\"Train\\taccuracy:\"\\n,\\n\\t\\nacc_train\\n,\\n\\t\\n\"Test\\taccuracy:\"\\n,\\n\\t\\nacc_test\\n)\\n\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\nThis\\tcode\\topens\\ta\\tTensorFlow\\tsession,\\tand\\tit\\truns\\tthe\\t\\ninit\\n\\tnode\\tthat\\tinitializes\\tall\\tthe\\tvariables.\\tThen\\tit\\nruns\\tthe\\tmain\\ttraining\\tloop:\\tat\\teach\\tepoch,\\tthe\\tcode\\titerates\\tthrough\\ta\\tnumber\\tof\\tmini-batches\\tthat\\ncorresponds\\tto\\tthe\\ttraining\\tset\\tsize.\\tEach\\tmini-batch\\tis\\tfetched\\tvia\\tthe\\t\\nnext_batch()\\n\\t\\nmethod,\\tand\\tthen\\nthe\\tcode\\tsimply\\truns\\tthe\\ttraining\\toperation,\\tfeeding\\tit\\tthe\\tcurrent\\tmini-batch\\tinput\\tdata\\tand\\ttargets.\\tNext,\\nat\\tthe\\tend\\tof\\teach\\tepoch,\\tthe\\tcode\\tevaluates\\tthe\\tmodel\\ton\\tthe\\tlast\\tmini-batch\\tand\\ton\\tthe\\tfull\\ttest\\tset,\\tand\\tit\\nprints\\tout\\tthe\\tresult.\\tFinally,\\tthe\\tmodel\\tparameters\\tare\\tsaved\\tto\\tdisk.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 334}), Document(page_content='Using\\tthe\\tNeural\\tNetwork\\nNow\\t\\nthat\\tthe\\tneural\\tnetwork\\tis\\ttrained,\\tyou\\tcan\\tuse\\tit\\tto\\tmake\\tpredictions.\\tTo\\tdo\\tthat,\\tyou\\tcan\\treuse\\tthe\\nsame\\tconstruction\\tphase,\\tbut\\tchange\\tthe\\texecution\\tphase\\tlike\\tthis:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\nX_new_scaled\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\t\\n#\\tsome\\tnew\\timages\\t(scaled\\tfrom\\t0\\tto\\t1)\\n\\t\\t\\t\\t\\nZ\\n\\t\\n=\\n\\t\\nlogits\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_new_scaled\\n})\\n\\t\\t\\t\\t\\ny_pred\\n\\t\\n=\\n\\t\\nnp\\n.\\nargmax\\n(\\nZ\\n,\\n\\t\\naxis\\n=\\n1\\n)\\nFirst\\tthe\\tcode\\tloads\\tthe\\tmodel\\tparameters\\tfrom\\tdisk.\\tThen\\tit\\tloads\\tsome\\tnew\\timages\\tthat\\tyou\\twant\\tto\\nclassify.\\tRemember\\tto\\tapply\\tthe\\tsame\\tfeature\\tscaling\\tas\\tfor\\tthe\\ttraining\\tdata\\t(in\\tthis\\tcase,\\tscale\\tit\\tfrom\\t0\\nto\\t1).\\tThen\\tthe\\tcode\\tevaluates\\tthe\\t\\nlogits\\n\\tnode.\\tIf\\tyou\\twanted\\tto\\tknow\\tall\\tthe\\testimated\\tclass\\nprobabilities,\\tyou\\twould\\tneed\\tto\\tapply\\tthe\\t\\nsoftmax()\\n\\tfunction\\tto\\tthe\\tlogits,\\tbut\\tif\\tyou\\tjust\\twant\\tto\\tpredict\\na\\tclass,\\tyou\\tcan\\tsimply\\tpick\\tthe\\tclass\\tthat\\thas\\tthe\\thighest\\tlogit\\tvalue\\t\\n(using\\tthe\\t\\nargmax()\\n\\tfunction\\tdoes\\nthe\\ttrick).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 335}), Document(page_content='Fine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\nThe\\t\\nflexibility\\tof\\tneural\\tnetworks\\tis\\talso\\tone\\tof\\ttheir\\tmain\\tdrawbacks:\\tthere\\tare\\tmany\\thyperparameters\\tto\\ntweak.\\tNot\\tonly\\tcan\\tyou\\tuse\\tany\\t\\nimaginable\\t\\nnetwork\\ttopology\\n\\t(how\\tneurons\\tare\\tinterconnected),\\tbut\\teven\\nin\\ta\\tsimple\\tMLP\\tyou\\tcan\\tchange\\tthe\\tnumber\\tof\\tlayers,\\tthe\\tnumber\\tof\\tneurons\\tper\\tlayer,\\tthe\\ttype\\tof\\nactivation\\tfunction\\tto\\tuse\\tin\\teach\\tlayer,\\tthe\\tweight\\tinitialization\\tlogic,\\tand\\tmuch\\tmore.\\tHow\\tdo\\tyou\\tknow\\nwhat\\tcombination\\tof\\thyperparameters\\tis\\tthe\\tbest\\tfor\\tyour\\ttask?\\nOf\\tcourse,\\tyou\\tcan\\tuse\\tgrid\\tsearch\\twith\\tcross-validation\\tto\\tfind\\tthe\\tright\\thyperparameters,\\tlike\\tyou\\tdid\\tin\\nprevious\\tchapters,\\tbut\\tsince\\tthere\\tare\\tmany\\thyperparameters\\tto\\ttune,\\tand\\tsince\\ttraining\\ta\\tneural\\tnetwork\\non\\ta\\tlarge\\tdataset\\ttakes\\ta\\tlot\\tof\\ttime,\\tyou\\twill\\tonly\\tbe\\table\\tto\\texplore\\ta\\ttiny\\tpart\\tof\\tthe\\thyperparameter\\nspace\\tin\\ta\\treasonable\\tamount\\tof\\ttime.\\tIt\\tis\\tmuch\\tbetter\\tto\\tuse\\t\\nrandomized\\tsearch\\n,\\t\\nas\\twe\\tdiscussed\\tin\\nChapter\\t2\\n.\\tAnother\\toption\\tis\\tto\\tuse\\ta\\ttool\\tsuch\\tas\\t\\nOscar\\n,\\twhich\\timplements\\tmore\\tcomplex\\talgorithms\\tto\\nhelp\\tyou\\tfind\\ta\\tgood\\tset\\tof\\thyperparameters\\tquickly.\\nIt\\thelps\\tto\\thave\\tan\\tidea\\tof\\twhat\\tvalues\\tare\\treasonable\\tfor\\teach\\thyperparameter,\\tso\\tyou\\tcan\\trestrict\\tthe\\nsearch\\tspace.\\tLet’s\\tstart\\twith\\tthe\\tnumber\\tof\\thidden\\tlayers.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 336}), Document(page_content='Number\\tof\\tHidden\\tLayers\\nFor\\t\\nmany\\tproblems,\\tyou\\tcan\\tjust\\tbegin\\twith\\ta\\tsingle\\thidden\\tlayer\\tand\\tyou\\twill\\tget\\treasonable\\tresults.\\tIt\\nhas\\tactually\\tbeen\\tshown\\tthat\\tan\\tMLP\\twith\\tjust\\tone\\thidden\\tlayer\\tcan\\tmodel\\teven\\tthe\\tmost\\tcomplex\\nfunctions\\tprovided\\tit\\thas\\tenough\\tneurons.\\tFor\\ta\\tlong\\ttime,\\tthese\\tfacts\\tconvinced\\tresearchers\\tthat\\tthere\\nwas\\tno\\tneed\\tto\\tinvestigate\\tany\\tdeeper\\tneural\\tnetworks.\\tBut\\tthey\\toverlooked\\tthe\\tfact\\tthat\\tdeep\\tnetworks\\nhave\\ta\\tmuch\\thigher\\t\\nparameter\\tefficiency\\n\\t\\nthan\\tshallow\\tones:\\tthey\\tcan\\tmodel\\tcomplex\\tfunctions\\tusing\\nexponentially\\tfewer\\tneurons\\tthan\\tshallow\\tnets,\\tmaking\\tthem\\tmuch\\tfaster\\tto\\ttrain.\\nTo\\tunderstand\\twhy,\\tsuppose\\tyou\\tare\\tasked\\tto\\tdraw\\ta\\tforest\\tusing\\tsome\\tdrawing\\tsoftware,\\tbut\\tyou\\tare\\nforbidden\\tto\\tuse\\tcopy/paste.\\tYou\\twould\\thave\\tto\\tdraw\\teach\\ttree\\tindividually,\\tbranch\\tper\\tbranch,\\tleaf\\tper\\nleaf.\\tIf\\tyou\\tcould\\tinstead\\tdraw\\tone\\tleaf,\\tcopy/paste\\tit\\tto\\tdraw\\ta\\tbranch,\\tthen\\tcopy/paste\\tthat\\tbranch\\tto\\ncreate\\ta\\ttree,\\tand\\tfinally\\tcopy/paste\\tthis\\ttree\\tto\\tmake\\ta\\tforest,\\tyou\\twould\\tbe\\tfinished\\tin\\tno\\ttime.\\tReal-\\nworld\\tdata\\tis\\toften\\tstructured\\tin\\tsuch\\ta\\thierarchical\\tway\\tand\\tDNNs\\tautomatically\\ttake\\tadvantage\\tof\\tthis\\nfact:\\tlower\\thidden\\tlayers\\tmodel\\tlow-level\\tstructures\\t(e.g.,\\tline\\tsegments\\tof\\tvarious\\tshapes\\tand\\norientations),\\tintermediate\\thidden\\tlayers\\tcombine\\tthese\\tlow-level\\tstructures\\tto\\tmodel\\tintermediate-level\\nstructures\\t(e.g.,\\tsquares,\\tcircles),\\tand\\tthe\\thighest\\thidden\\tlayers\\tand\\tthe\\toutput\\tlayer\\tcombine\\tthese\\nintermediate\\tstructures\\tto\\tmodel\\thigh-level\\tstructures\\t(e.g.,\\tfaces).\\nNot\\tonly\\tdoes\\tthis\\thierarchical\\tarchitecture\\thelp\\tDNNs\\tconverge\\tfaster\\tto\\ta\\tgood\\tsolution,\\tit\\talso\\nimproves\\ttheir\\tability\\tto\\tgeneralize\\tto\\tnew\\tdatasets.\\tFor\\texample,\\tif\\tyou\\thave\\talready\\ttrained\\ta\\tmodel\\tto\\nrecognize\\tfaces\\tin\\tpictures,\\tand\\tyou\\tnow\\twant\\tto\\ttrain\\ta\\tnew\\tneural\\tnetwork\\tto\\trecognize\\thairstyles,\\tthen\\nyou\\tcan\\tkickstart\\ttraining\\tby\\treusing\\tthe\\tlower\\tlayers\\tof\\tthe\\tfirst\\tnetwork.\\tInstead\\tof\\trandomly\\tinitializing\\nthe\\tweights\\tand\\tbiases\\tof\\tthe\\tfirst\\tfew\\tlayers\\tof\\tthe\\tnew\\tneural\\tnetwork,\\tyou\\tcan\\tinitialize\\tthem\\tto\\tthe\\nvalue\\tof\\tthe\\tweights\\tand\\tbiases\\tof\\tthe\\tlower\\tlayers\\tof\\tthe\\tfirst\\tnetwork.\\tThis\\tway\\tthe\\tnetwork\\twill\\tnot\\nhave\\tto\\tlearn\\tfrom\\tscratch\\tall\\tthe\\tlow-level\\tstructures\\tthat\\toccur\\tin\\tmost\\tpictures;\\tit\\twill\\tonly\\thave\\tto\\nlearn\\tthe\\thigher-level\\tstructures\\t(e.g.,\\thairstyles).\\nIn\\tsummary,\\tfor\\tmany\\tproblems\\tyou\\tcan\\tstart\\twith\\tjust\\tone\\tor\\ttwo\\thidden\\tlayers\\tand\\tit\\twill\\twork\\tjust\\tfine\\n(e.g.,\\tyou\\tcan\\teasily\\treach\\tabove\\t97%\\taccuracy\\ton\\tthe\\tMNIST\\tdataset\\tusing\\tjust\\tone\\thidden\\tlayer\\twith\\ta\\nfew\\thundred\\tneurons,\\tand\\tabove\\t98%\\taccuracy\\tusing\\ttwo\\thidden\\tlayers\\twith\\tthe\\tsame\\ttotal\\tamount\\tof\\nneurons,\\tin\\troughly\\tthe\\tsame\\tamount\\tof\\ttraining\\ttime).\\tFor\\tmore\\tcomplex\\tproblems,\\tyou\\tcan\\tgradually\\nramp\\tup\\tthe\\tnumber\\tof\\thidden\\tlayers,\\tuntil\\tyou\\tstart\\toverfitting\\tthe\\ttraining\\tset.\\tVery\\tcomplex\\ttasks,\\tsuch\\nas\\tlarge\\timage\\tclassification\\tor\\tspeech\\trecognition,\\ttypically\\trequire\\tnetworks\\twith\\tdozens\\tof\\tlayers\\t(or\\neven\\thundreds,\\tbut\\tnot\\tfully\\tconnected\\tones,\\tas\\twe\\twill\\tsee\\tin\\t\\nChapter\\t13\\n),\\tand\\tthey\\tneed\\ta\\thuge\\tamount\\nof\\ttraining\\tdata.\\tHowever,\\tyou\\twill\\trarely\\thave\\tto\\ttrain\\tsuch\\tnetworks\\tfrom\\tscratch:\\tit\\tis\\tmuch\\tmore\\ncommon\\tto\\treuse\\tparts\\tof\\ta\\tpretrained\\tstate-of-the-art\\tnetwork\\tthat\\tperforms\\ta\\tsimilar\\ttask.\\tTraining\\twill\\nbe\\ta\\tlot\\tfaster\\tand\\trequire\\tmuch\\tless\\t\\ndata\\t(we\\twill\\tdiscuss\\tthis\\tin\\t\\nChapter\\t11\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 337}), Document(page_content='Number\\tof\\tNeurons\\tper\\tHidden\\tLayer\\nObviously\\t\\nthe\\tnumber\\tof\\tneurons\\tin\\tthe\\tinput\\tand\\toutput\\tlayers\\tis\\tdetermined\\tby\\tthe\\ttype\\tof\\tinput\\tand\\noutput\\tyour\\ttask\\trequires.\\tFor\\texample,\\tthe\\tMNIST\\ttask\\trequires\\t28\\tx\\t28\\t=\\t784\\tinput\\tneurons\\tand\\t10\\noutput\\tneurons.\\tAs\\tfor\\tthe\\thidden\\tlayers,\\ta\\tcommon\\tpractice\\tis\\tto\\tsize\\tthem\\tto\\tform\\ta\\tfunnel,\\twith\\tfewer\\nand\\tfewer\\tneurons\\tat\\teach\\tlayer\\t—\\tthe\\trationale\\tbeing\\tthat\\tmany\\tlow-level\\tfeatures\\tcan\\tcoalesce\\tinto\\tfar\\nfewer\\thigh-level\\tfeatures.\\tFor\\texample,\\ta\\ttypical\\tneural\\tnetwork\\tfor\\tMNIST\\tmay\\thave\\ttwo\\thidden\\tlayers,\\nthe\\tfirst\\twith\\t300\\tneurons\\tand\\tthe\\tsecond\\twith\\t100.\\tHowever,\\tthis\\tpractice\\tis\\tnot\\tas\\tcommon\\tnow,\\tand\\nyou\\tmay\\tsimply\\tuse\\tthe\\tsame\\tsize\\tfor\\tall\\thidden\\tlayers\\t—\\tfor\\texample,\\tall\\thidden\\tlayers\\twith\\t150\\nneurons:\\tthat’s\\tjust\\tone\\thyperparameter\\tto\\ttune\\tinstead\\tof\\tone\\tper\\tlayer.\\tJust\\tlike\\tfor\\tthe\\tnumber\\tof\\tlayers,\\nyou\\tcan\\ttry\\tincreasing\\tthe\\tnumber\\tof\\tneurons\\tgradually\\tuntil\\tthe\\tnetwork\\tstarts\\toverfitting.\\tIn\\tgeneral\\tyou\\nwill\\tget\\tmore\\tbang\\tfor\\tthe\\tbuck\\tby\\tincreasing\\tthe\\tnumber\\tof\\tlayers\\tthan\\tthe\\tnumber\\tof\\tneurons\\tper\\tlayer.\\nUnfortunately,\\tas\\tyou\\tcan\\tsee,\\tfinding\\tthe\\tperfect\\tamount\\tof\\tneurons\\tis\\tstill\\tsomewhat\\tof\\ta\\tblack\\tart.\\nA\\tsimpler\\tapproach\\tis\\tto\\tpick\\ta\\tmodel\\twith\\tmore\\tlayers\\tand\\tneurons\\tthan\\tyou\\tactually\\tneed,\\tthen\\tuse\\t\\nearly\\nstopping\\tto\\tprevent\\tit\\tfrom\\t\\noverfitting\\t(and\\tother\\tregularization\\ttechniques,\\tespecially\\t\\ndropout\\n,\\tas\\t\\nwe\\twill\\nsee\\tin\\t\\nChapter\\t11\\n).\\tThis\\thas\\tbeen\\tdubbed\\tthe\\t“stretch\\tpants”\\tapproach:\\n12\\n\\tinstead\\tof\\twasting\\ttime\\tlooking\\nfor\\tpants\\tthat\\tperfectly\\tmatch\\tyour\\tsize,\\tjust\\tuse\\tlarge\\tstretch\\tpants\\tthat\\twill\\tshrink\\tdown\\tto\\tthe\\tright\\tsize.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 338}), Document(page_content='Activation\\tFunctions\\nIn\\t\\nmost\\tcases\\tyou\\tcan\\tuse\\tthe\\t\\nReLU\\tactivation\\tfunction\\tin\\tthe\\thidden\\tlayers\\t(or\\tone\\tof\\tits\\tvariants,\\tas\\twe\\nwill\\tsee\\tin\\t\\nChapter\\t11\\n).\\tIt\\tis\\ta\\tbit\\tfaster\\tto\\tcompute\\tthan\\tother\\tactivation\\tfunctions,\\tand\\tGradient\\tDescent\\ndoes\\tnot\\tget\\tstuck\\tas\\tmuch\\ton\\tplateaus,\\tthanks\\tto\\tthe\\tfact\\tthat\\tit\\tdoes\\tnot\\tsaturate\\tfor\\tlarge\\tinput\\tvalues\\t(as\\nopposed\\tto\\tthe\\tlogistic\\tfunction\\tor\\tthe\\t\\nhyperbolic\\ttangent\\tfunction,\\twhich\\tsaturate\\tat\\t1).\\nFor\\tthe\\toutput\\tlayer,\\tthe\\tsoftmax\\tactivation\\tfunction\\tis\\tgenerally\\ta\\tgood\\tchoice\\tfor\\tclassification\\ttasks\\n(when\\tthe\\tclasses\\tare\\tmutually\\texclusive).\\tFor\\tregression\\ttasks,\\tyou\\tcan\\tsimply\\tuse\\tno\\tactivation\\tfunction\\nat\\tall.\\nThis\\tconcludes\\tthis\\tintroduction\\tto\\tartificial\\tneural\\tnetworks.\\tIn\\tthe\\tfollowing\\tchapters,\\twe\\twill\\tdiscuss\\ntechniques\\tto\\ttrain\\tvery\\tdeep\\tnets,\\tand\\tdistribute\\ttraining\\tacross\\tmultiple\\tservers\\tand\\tGPUs.\\tThen\\twe\\twill\\nexplore\\ta\\tfew\\tother\\tpopular\\tneural\\tnetwork\\tarchitectures:\\tconvolutional\\tneural\\tnetworks,\\trecurrent\\tneural\\nnetworks,\\tand\\t\\nautoencoders.\\n13', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 339}), Document(page_content='Exercises\\n1\\n.\\t\\nDraw\\tan\\tANN\\tusing\\tthe\\toriginal\\tartificial\\tneurons\\t(like\\tthe\\tones\\tin\\t\\nFigure\\t10-3\\n)\\tthat\\tcomputes\\t\\nA\\n\\t\\nB\\n\\t(where\\t\\n\\trepresents\\tthe\\tXOR\\toperation).\\tHint:\\t\\nA\\n\\t\\n\\t\\nB\\n\\t=\\t(\\nA\\n\\t\\n\\t¬\\t\\nB\\n)\\t\\n\\t(¬\\t\\nA\\n\\t\\n\\t\\nB\\n).\\n2\\n.\\t\\nWhy\\tis\\tit\\tgenerally\\tpreferable\\tto\\tuse\\ta\\tLogistic\\tRegression\\tclassifier\\trather\\tthan\\ta\\tclassical\\nPerceptron\\t(i.e.,\\ta\\tsingle\\tlayer\\tof\\tlinear\\tthreshold\\tunits\\ttrained\\tusing\\tthe\\tPerceptron\\ttraining\\nalgorithm)?\\tHow\\tcan\\tyou\\ttweak\\ta\\tPerceptron\\tto\\tmake\\tit\\tequivalent\\tto\\ta\\tLogistic\\tRegression\\nclassifier?\\n3\\n.\\t\\nWhy\\twas\\tthe\\tlogistic\\tactivation\\tfunction\\ta\\tkey\\tingredient\\tin\\ttraining\\tthe\\tfirst\\tMLPs?\\n4\\n.\\t\\nName\\tthree\\tpopular\\tactivation\\tfunctions.\\tCan\\tyou\\tdraw\\tthem?\\n5\\n.\\t\\nSuppose\\tyou\\thave\\tan\\tMLP\\tcomposed\\tof\\tone\\tinput\\tlayer\\twith\\t10\\tpassthrough\\tneurons,\\tfollowed\\tby\\none\\thidden\\tlayer\\twith\\t50\\tartificial\\tneurons,\\tand\\tfinally\\tone\\toutput\\tlayer\\twith\\t3\\tartificial\\tneurons.\\tAll\\nartificial\\tneurons\\tuse\\tthe\\tReLU\\tactivation\\tfunction.\\nWhat\\tis\\tthe\\tshape\\tof\\tthe\\tinput\\tmatrix\\t\\nX\\n?\\nWhat\\tabout\\tthe\\tshape\\tof\\tthe\\thidden\\tlayer’s\\tweight\\tvector\\t\\nW\\nh\\n,\\tand\\tthe\\tshape\\tof\\tits\\tbias\\tvector\\nb\\nh\\n?\\nWhat\\tis\\tthe\\tshape\\tof\\tthe\\toutput\\tlayer’s\\tweight\\tvector\\t\\nW\\no\\n,\\tand\\tits\\tbias\\tvector\\t\\nb\\no\\n?\\nWhat\\tis\\tthe\\tshape\\tof\\tthe\\tnetwork’s\\toutput\\tmatrix\\t\\nY\\n?\\nWrite\\tthe\\tequation\\tthat\\tcomputes\\tthe\\tnetwork’s\\toutput\\tmatrix\\t\\nY\\n\\tas\\ta\\tfunction\\tof\\t\\nX\\n,\\t\\nW\\nh\\n,\\t\\nb\\nh\\n,\\t\\nW\\no\\nand\\t\\nb\\no\\n.\\n6\\n.\\t\\nHow\\tmany\\tneurons\\tdo\\tyou\\tneed\\tin\\tthe\\toutput\\tlayer\\tif\\tyou\\twant\\tto\\tclassify\\temail\\tinto\\tspam\\tor\\tham?\\nWhat\\tactivation\\tfunction\\tshould\\tyou\\tuse\\tin\\tthe\\toutput\\tlayer?\\tIf\\tinstead\\tyou\\twant\\tto\\ttackle\\tMNIST,\\nhow\\tmany\\tneurons\\tdo\\tyou\\tneed\\tin\\tthe\\toutput\\tlayer,\\tusing\\twhat\\tactivation\\tfunction?\\tAnswer\\tthe\\tsame\\nquestions\\tfor\\tgetting\\tyour\\tnetwork\\tto\\tpredict\\thousing\\tprices\\tas\\tin\\t\\nChapter\\t2\\n.\\n7\\n.\\t\\nWhat\\tis\\tbackpropagation\\tand\\thow\\tdoes\\tit\\twork?\\tWhat\\tis\\tthe\\tdifference\\tbetween\\tbackpropagation\\nand\\treverse-mode\\tautodiff?\\n8\\n.\\t\\nCan\\tyou\\tlist\\tall\\tthe\\thyperparameters\\tyou\\tcan\\ttweak\\tin\\tan\\tMLP?\\tIf\\tthe\\tMLP\\toverfits\\tthe\\ttraining\\tdata,\\nhow\\tcould\\tyou\\ttweak\\tthese\\thyperparameters\\tto\\ttry\\tto\\tsolve\\tthe\\tproblem?\\n9\\n.\\t\\nTrain\\ta\\tdeep\\tMLP\\ton\\tthe\\tMNIST\\tdataset\\tand\\tsee\\tif\\tyou\\tcan\\tget\\tover\\t98%\\tprecision.\\tJust\\tlike\\tin\\tthe\\nlast\\texercise\\tof\\t\\nChapter\\t9\\n,\\ttry\\tadding\\tall\\tthe\\tbells\\tand\\twhistles\\t(i.e.,\\tsave\\tcheckpoints,\\trestore\\tthe\\nlast\\tcheckpoint\\tin\\tcase\\tof\\tan\\tinterruption,\\tadd\\tsummaries,\\tplot\\tlearning\\tcurves\\tusing\\tTensorBoard,\\nand\\tso\\ton).\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 340}), Document(page_content='You\\tcan\\tget\\tthe\\tbest\\tof\\tboth\\tworlds\\tby\\tbeing\\topen\\tto\\tbiological\\tinspirations\\twithout\\tbeing\\tafraid\\tto\\tcreate\\tbiologically\\tunrealistic\\tmodels,\\tas\\nlong\\tas\\tthey\\twork\\twell.\\n“A\\tLogical\\tCalculus\\tof\\tIdeas\\tImmanent\\tin\\tNervous\\tActivity,”\\tW.\\tMcCulloch\\tand\\tW.\\tPitts\\t(1943).\\nImage\\tby\\tBruce\\tBlaus\\t(\\nCreative\\tCommons\\t3.0\\n).\\tReproduced\\tfrom\\t\\nhttps://en.wikipedia.org/wiki/Neuron\\n.\\nIn\\tthe\\tcontext\\tof\\tMachine\\tLearning,\\tthe\\tphrase\\t“neural\\tnetworks”\\tgenerally\\trefers\\tto\\tANNs,\\tnot\\tBNNs.\\nDrawing\\tof\\ta\\tcortical\\tlamination\\tby\\tS.\\tRamon\\ty\\tCajal\\t(public\\tdomain).\\tReproduced\\tfrom\\t\\nhttps://en.wikipedia.org/wiki/Cerebral_cortex\\n.\\nThe\\tname\\t\\nPerceptron\\n\\tis\\tsometimes\\tused\\tto\\tmean\\ta\\ttiny\\tnetwork\\twith\\ta\\tsingle\\tLTU.\\nNote\\tthat\\tthis\\tsolution\\tis\\tgenerally\\tnot\\tunique:\\tin\\tgeneral\\twhen\\tthe\\tdata\\tare\\tlinearly\\tseparable,\\tthere\\tis\\tan\\tinfinity\\tof\\thyperplanes\\tthat\\tcan\\nseparate\\tthem.\\n“Learning\\tInternal\\tRepresentations\\tby\\tError\\tPropagation,”\\tD.\\tRumelhart,\\tG.\\tHinton,\\tR.\\tWilliams\\t(1986).\\nThis\\talgorithm\\twas\\tactually\\tinvented\\tseveral\\ttimes\\tby\\tvarious\\tresearchers\\tin\\tdifferent\\tfields,\\tstarting\\twith\\tP.\\tWerbos\\tin\\t1974.\\nUsing\\ta\\ttruncated\\tnormal\\tdistribution\\trather\\tthan\\ta\\tregular\\tnormal\\tdistribution\\tensures\\tthat\\tthere\\twon’t\\tbe\\tany\\tlarge\\tweights,\\twhich\\tcould\\nslow\\tdown\\ttraining.\\nFor\\texample,\\tif\\tyou\\tset\\tall\\tthe\\tweights\\tto\\t0,\\tthen\\tall\\tneurons\\twill\\toutput\\t0,\\tand\\tthe\\terror\\tgradient\\twill\\tbe\\tthe\\tsame\\tfor\\tall\\tneurons\\tin\\ta\\tgiven\\nhidden\\tlayer.\\tThe\\tGradient\\tDescent\\tstep\\twill\\tthen\\tupdate\\tall\\tthe\\tweights\\tin\\texactly\\tthe\\tsame\\tway\\tin\\teach\\tlayer,\\tso\\tthey\\twill\\tall\\tremain\\nequal.\\tIn\\tother\\twords,\\tdespite\\thaving\\thundreds\\tof\\tneurons\\tper\\tlayer,\\tyour\\tmodel\\twill\\tact\\tas\\tif\\tthere\\twere\\tonly\\tone\\tneuron\\tper\\tlayer.\\tIt\\tis\\tnot\\ngoing\\tto\\tfly.\\nBy\\tVincent\\tVanhoucke\\tin\\this\\t\\nDeep\\tLearning\\tclass\\n\\ton\\tUdacity.com.\\nA\\tfew\\textra\\tANN\\tarchitectures\\tare\\tpresented\\tin\\t\\nAppendix\\tE\\n.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 341}), Document(page_content='Chapter\\t11.\\t\\nTraining\\tDeep\\tNeural\\tNets\\nIn\\t\\nChapter\\t10\\n\\t\\nwe\\tintroduced\\tartificial\\tneural\\tnetworks\\tand\\ttrained\\tour\\tfirst\\tdeep\\tneural\\tnetwork.\\tBut\\tit\\nwas\\ta\\tvery\\tshallow\\tDNN,\\twith\\tonly\\ttwo\\thidden\\tlayers.\\tWhat\\tif\\tyou\\tneed\\tto\\ttackle\\ta\\tvery\\tcomplex\\nproblem,\\tsuch\\tas\\tdetecting\\thundreds\\tof\\ttypes\\tof\\tobjects\\tin\\thigh-resolution\\timages?\\tYou\\tmay\\tneed\\tto\\ttrain\\na\\tmuch\\tdeeper\\tDNN,\\tperhaps\\twith\\t(say)\\t10\\tlayers,\\teach\\tcontaining\\thundreds\\tof\\tneurons,\\tconnected\\tby\\nhundreds\\tof\\tthousands\\tof\\tconnections.\\tThis\\twould\\tnot\\tbe\\ta\\twalk\\tin\\tthe\\tpark:\\nFirst,\\tyou\\twould\\tbe\\tfaced\\twith\\tthe\\t\\ntricky\\t\\nvanishing\\tgradients\\n\\tproblem\\t(or\\tthe\\trelated\\t\\nexploding\\ngradients\\n\\tproblem)\\tthat\\taffects\\tdeep\\tneural\\tnetworks\\tand\\tmakes\\tlower\\tlayers\\tvery\\thard\\tto\\ttrain.\\nSecond,\\twith\\tsuch\\ta\\tlarge\\tnetwork,\\ttraining\\twould\\tbe\\textremely\\tslow.\\nThird,\\ta\\tmodel\\twith\\tmillions\\tof\\tparameters\\twould\\tseverely\\trisk\\toverfitting\\tthe\\ttraining\\tset.\\nIn\\tthis\\tchapter,\\twe\\twill\\tgo\\tthrough\\teach\\tof\\tthese\\tproblems\\tin\\tturn\\tand\\tpresent\\ttechniques\\tto\\tsolve\\tthem.\\nWe\\twill\\tstart\\tby\\texplaining\\tthe\\tvanishing\\tgradients\\tproblem\\tand\\texploring\\tsome\\tof\\tthe\\tmost\\tpopular\\nsolutions\\tto\\tthis\\tproblem.\\tNext\\twe\\twill\\tlook\\tat\\tvarious\\toptimizers\\tthat\\tcan\\tspeed\\tup\\ttraining\\tlarge\\tmodels\\ntremendously\\tcompared\\tto\\tplain\\t\\nGradient\\tDescent.\\tFinally,\\twe\\twill\\tgo\\tthrough\\ta\\tfew\\tpopular\\nregularization\\ttechniques\\tfor\\tlarge\\tneural\\tnetworks.\\nWith\\tthese\\ttools,\\tyou\\twill\\tbe\\table\\tto\\ttrain\\tvery\\tdeep\\tnets:\\twelcome\\tto\\tDeep\\tLearning!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 342}), Document(page_content='Vanishing/Exploding\\tGradients\\tProblems\\nAs\\twe\\tdiscussed\\tin\\t\\nChapter\\t10\\n,\\tthe\\t\\nbackpropagation\\talgorithm\\tworks\\tby\\tgoing\\tfrom\\tthe\\toutput\\tlayer\\tto\\tthe\\ninput\\tlayer,\\tpropagating\\tthe\\terror\\tgradient\\ton\\tthe\\tway.\\tOnce\\tthe\\talgorithm\\thas\\tcomputed\\tthe\\tgradient\\tof\\tthe\\ncost\\tfunction\\twith\\tregards\\tto\\teach\\tparameter\\tin\\tthe\\tnetwork,\\tit\\tuses\\tthese\\tgradients\\tto\\tupdate\\teach\\nparameter\\twith\\ta\\tGradient\\tDescent\\tstep.\\nUnfortunately,\\tgradients\\toften\\tget\\tsmaller\\tand\\tsmaller\\tas\\tthe\\talgorithm\\tprogresses\\tdown\\tto\\tthe\\tlower\\nlayers.\\tAs\\ta\\tresult,\\tthe\\tGradient\\tDescent\\tupdate\\tleaves\\tthe\\tlower\\tlayer\\tconnection\\tweights\\tvirtually\\nunchanged,\\tand\\ttraining\\tnever\\tconverges\\tto\\ta\\tgood\\tsolution.\\tThis\\t\\nis\\tcalled\\tthe\\t\\nvanishing\\tgradients\\nproblem.\\tIn\\tsome\\tcases,\\tthe\\topposite\\tcan\\thappen:\\tthe\\tgradients\\tcan\\tgrow\\tbigger\\tand\\tbigger,\\tso\\tmany\\nlayers\\tget\\tinsanely\\tlarge\\tweight\\tupdates\\tand\\tthe\\talgorithm\\tdiverges.\\tThis\\tis\\tthe\\t\\nexploding\\tgradients\\nproblem,\\t\\nwhich\\tis\\tmostly\\tencountered\\tin\\trecurrent\\tneural\\tnetworks\\t(see\\t\\nChapter\\t14\\n).\\tMore\\tgenerally,\\ndeep\\tneural\\tnetworks\\tsuffer\\tfrom\\tunstable\\tgradients;\\tdifferent\\tlayers\\tmay\\tlearn\\tat\\twidely\\tdifferent\\tspeeds.\\nAlthough\\tthis\\tunfortunate\\tbehavior\\thas\\tbeen\\tempirically\\tobserved\\tfor\\tquite\\ta\\twhile\\t(it\\twas\\tone\\tof\\tthe\\nreasons\\twhy\\tdeep\\tneural\\tnetworks\\twere\\tmostly\\tabandoned\\tfor\\ta\\tlong\\ttime),\\tit\\tis\\tonly\\taround\\t2010\\tthat\\nsignificant\\tprogress\\twas\\tmade\\tin\\tunderstanding\\tit.\\tA\\tpaper\\ttitled\\t\\n“Understanding\\tthe\\tDifficulty\\tof\\tTraining\\nDeep\\tFeedforward\\tNeural\\tNetworks”\\n\\tby\\tXavier\\tGlorot\\tand\\tYoshua\\tBengio\\n1\\n\\tfound\\ta\\tfew\\tsuspects,\\nincluding\\tthe\\tcombination\\tof\\tthe\\tpopular\\tlogistic\\tsigmoid\\tactivation\\tfunction\\tand\\tthe\\tweight\\tinitialization\\ntechnique\\tthat\\twas\\tmost\\tpopular\\tat\\tthe\\ttime,\\tnamely\\t\\nrandom\\tinitialization\\tusing\\ta\\tnormal\\tdistribution\\twith\\na\\tmean\\tof\\t0\\tand\\ta\\tstandard\\tdeviation\\tof\\t1.\\tIn\\tshort,\\tthey\\tshowed\\tthat\\twith\\tthis\\tactivation\\tfunction\\tand\\tthis\\ninitialization\\tscheme,\\tthe\\tvariance\\tof\\tthe\\toutputs\\tof\\teach\\tlayer\\tis\\tmuch\\tgreater\\tthan\\tthe\\tvariance\\tof\\tits\\ninputs.\\tGoing\\tforward\\tin\\tthe\\tnetwork,\\tthe\\tvariance\\tkeeps\\tincreasing\\tafter\\teach\\tlayer\\tuntil\\tthe\\tactivation\\nfunction\\tsaturates\\tat\\tthe\\ttop\\tlayers.\\tThis\\tis\\tactually\\tmade\\tworse\\tby\\tthe\\tfact\\tthat\\tthe\\tlogistic\\tfunction\\thas\\ta\\nmean\\tof\\t0.5,\\tnot\\t0\\t\\n(the\\thyperbolic\\ttangent\\tfunction\\thas\\ta\\tmean\\tof\\t0\\tand\\tbehaves\\tslightly\\tbetter\\tthan\\tthe\\nlogistic\\tfunction\\tin\\tdeep\\tnetworks).\\nLooking\\tat\\tthe\\tlogistic\\tactivation\\tfunction\\t(see\\t\\nFigure\\t11-1\\n),\\tyou\\tcan\\tsee\\tthat\\twhen\\tinputs\\tbecome\\tlarge\\n(negative\\tor\\tpositive),\\tthe\\tfunction\\tsaturates\\tat\\t0\\tor\\t1,\\twith\\ta\\tderivative\\textremely\\tclose\\tto\\t0.\\tThus\\twhen\\nbackpropagation\\tkicks\\tin,\\tit\\thas\\tvirtually\\tno\\tgradient\\tto\\tpropagate\\tback\\tthrough\\tthe\\tnetwork,\\tand\\twhat\\nlittle\\tgradient\\texists\\tkeeps\\tgetting\\tdiluted\\tas\\tbackpropagation\\tprogresses\\tdown\\tthrough\\tthe\\ttop\\tlayers,\\tso\\nthere\\tis\\treally\\tnothing\\tleft\\tfor\\tthe\\tlower\\tlayers.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 343}), Document(page_content='Figure\\t11-1.\\t\\nLogistic\\tactivation\\tfunction\\tsaturation', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 344}), Document(page_content='Xavier\\tand\\tHe\\tInitialization\\nIn\\ttheir\\tpaper,\\tGlorot\\tand\\tBengio\\tpropose\\ta\\tway\\tto\\tsignificantly\\talleviate\\tthis\\tproblem.\\tWe\\tneed\\tthe\\nsignal\\tto\\tflow\\tproperly\\tin\\tboth\\tdirections:\\tin\\tthe\\tforward\\tdirection\\twhen\\tmaking\\tpredictions,\\tand\\tin\\tthe\\nreverse\\tdirection\\twhen\\tbackpropagating\\tgradients.\\tWe\\tdon’t\\twant\\tthe\\tsignal\\tto\\tdie\\tout,\\tnor\\tdo\\twe\\twant\\tit\\nto\\texplode\\tand\\tsaturate.\\tFor\\tthe\\tsignal\\tto\\tflow\\tproperly,\\tthe\\tauthors\\targue\\tthat\\twe\\tneed\\tthe\\tvariance\\tof\\tthe\\noutputs\\tof\\teach\\tlayer\\tto\\tbe\\tequal\\tto\\tthe\\tvariance\\tof\\tits\\tinputs,\\n2\\n\\tand\\twe\\talso\\tneed\\tthe\\tgradients\\tto\\thave\\nequal\\tvariance\\tbefore\\tand\\tafter\\tflowing\\tthrough\\ta\\tlayer\\tin\\tthe\\treverse\\tdirection\\t(please\\tcheck\\tout\\tthe\\npaper\\tif\\tyou\\tare\\tinterested\\tin\\tthe\\tmathematical\\tdetails).\\tIt\\tis\\tactually\\tnot\\tpossible\\tto\\tguarantee\\tboth\\tunless\\nthe\\tlayer\\thas\\tan\\tequal\\tnumber\\tof\\tinput\\tand\\toutput\\tconnections,\\tbut\\tthey\\tproposed\\ta\\tgood\\tcompromise\\tthat\\nhas\\tproven\\tto\\twork\\tvery\\twell\\tin\\tpractice:\\tthe\\tconnection\\tweights\\tmust\\tbe\\tinitialized\\trandomly\\tas\\ndescribed\\tin\\t\\nEquation\\t11-1\\n,\\twhere\\t\\nn\\ninputs\\n\\tand\\t\\nn\\noutputs\\n\\tare\\tthe\\tnumber\\tof\\tinput\\tand\\toutput\\tconnections\\tfor\\nthe\\tlayer\\twhose\\tweights\\tare\\tbeing\\tinitialized\\t\\n(also\\tcalled\\t\\nfan-in\\n\\tand\\t\\nfan-out\\n).\\tThis\\tinitialization\\tstrategy\\nis\\toften\\tcalled\\t\\nXavier\\tinitialization\\n\\t(after\\tthe\\tauthor’s\\tfirst\\tname),\\tor\\tsometimes\\t\\nGlorot\\tinitialization\\n.\\nEquation\\t11-1.\\t\\nXavier\\tinitialization\\t(when\\tusing\\tthe\\tlogistic\\tactivation\\tfunction)\\nWhen\\tthe\\tnumber\\tof\\tinput\\tconnections\\tis\\troughly\\tequal\\tto\\tthe\\tnumber\\tof\\toutput\\tconnections,\\tyou\\tget\\nsimpler\\tequations\\t(e.g.,\\t\\n\\tor\\t\\n).\\tWe\\tused\\tthis\\tsimplified\\tstrategy\\tin\\nChapter\\t10\\n.\\n3\\nUsing\\tthe\\tXavier\\tinitialization\\tstrategy\\tcan\\tspeed\\tup\\ttraining\\tconsiderably,\\tand\\tit\\tis\\tone\\tof\\tthe\\ttricks\\tthat\\nled\\tto\\tthe\\tcurrent\\tsuccess\\tof\\tDeep\\tLearning.\\tSome\\t\\nrecent\\tpapers\\n4\\n\\thave\\tprovided\\tsimilar\\tstrategies\\tfor\\ndifferent\\tactivation\\tfunctions,\\tas\\tshown\\tin\\t\\nTable\\t11-1\\n.\\tThe\\tinitialization\\tstrategy\\tfor\\tthe\\t\\nReLU\\tactivation\\nfunction\\t(and\\tits\\tvariants,\\tincluding\\tthe\\tELU\\tactivation\\tdescribed\\tshortly)\\tis\\tsometimes\\t\\ncalled\\t\\nHe\\ninitialization\\n\\t(after\\tthe\\tlast\\tname\\tof\\tits\\tauthor).\\nTable\\t11-1.\\t\\nInitialization\\tparameters\\tfor\\teach\\ttype\\tof\\nactivation\\tfunction\\nActivation\\tfunction\\nUniform\\tdistribution\\t[–r,\\tr]\\nNormal\\tdistribution\\nLogistic\\nHyperbolic\\ttangent\\nReLU\\t(and\\tits\\tvariants)\\nBy\\tdefault,\\tthe\\t\\ntf.layers.dense()\\n\\tfunction\\t\\n(introduced\\tin\\t\\nChapter\\t10\\n)\\tuses\\tXavier\\tinitialization\\t(with\\na\\tuniform\\tdistribution).\\tYou\\tcan\\tchange\\tthis\\tto\\tHe\\tinitialization\\tby\\tusing\\tthe', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 345}), Document(page_content='variance_scaling_initializer()\\n\\t\\nfunction\\tlike\\tthis:\\nhe_init\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\nhe_init\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\nNOTE\\nHe\\tinitialization\\tconsiders\\tonly\\tthe\\tfan-in,\\t\\nnot\\tthe\\taverage\\tbetween\\tfan-in\\tand\\tfan-out\\tlike\\tin\\tXavier\\tinitialization.\\tThis\\tis\\talso\\tthe\\ndefault\\tfor\\tthe\\t\\nvariance_scaling_initializer()\\n\\tfunction,\\tbut\\tyou\\tcan\\tchange\\tthis\\tby\\tsetting\\tthe\\t\\nargument\\t\\nmode=\"FAN_AVG\"\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 346}), Document(page_content='Nonsaturating\\tActivation\\tFunctions\\nOne\\t\\nof\\tthe\\tinsights\\tin\\tthe\\t2010\\tpaper\\tby\\tGlorot\\tand\\tBengio\\twas\\tthat\\tthe\\tvanishing/exploding\\tgradients\\nproblems\\twere\\tin\\tpart\\tdue\\tto\\ta\\tpoor\\tchoice\\tof\\tactivation\\tfunction.\\tUntil\\tthen\\tmost\\tpeople\\thad\\tassumed\\nthat\\tif\\tMother\\tNature\\thad\\tchosen\\tto\\tuse\\troughly\\tsigmoid\\tactivation\\tfunctions\\tin\\tbiological\\tneurons,\\tthey\\nmust\\tbe\\tan\\texcellent\\tchoice.\\tBut\\tit\\tturns\\tout\\tthat\\tother\\tactivation\\tfunctions\\tbehave\\tmuch\\tbetter\\tin\\tdeep\\nneural\\tnetworks,\\tin\\tparticular\\tthe\\tReLU\\tactivation\\tfunction,\\tmostly\\tbecause\\tit\\tdoes\\tnot\\tsaturate\\tfor\\npositive\\tvalues\\t(and\\talso\\tbecause\\tit\\tis\\tquite\\tfast\\tto\\tcompute).\\nUnfortunately,\\tthe\\tReLU\\tactivation\\tfunction\\tis\\tnot\\tperfect.\\tIt\\tsuffers\\tfrom\\ta\\tproblem\\tknown\\tas\\t\\nthe\\t\\ndying\\nReLUs\\n:\\tduring\\ttraining,\\tsome\\tneurons\\teffectively\\tdie,\\tmeaning\\tthey\\tstop\\toutputting\\tanything\\tother\\tthan\\t0.\\nIn\\tsome\\tcases,\\tyou\\tmay\\tfind\\tthat\\thalf\\tof\\tyour\\tnetwork’s\\tneurons\\tare\\tdead,\\tespecially\\tif\\tyou\\tused\\ta\\tlarge\\nlearning\\trate.\\tDuring\\ttraining,\\tif\\ta\\tneuron’s\\tweights\\tget\\tupdated\\tsuch\\tthat\\tthe\\tweighted\\tsum\\tof\\tthe\\tneuron’s\\ninputs\\tis\\tnegative,\\tit\\twill\\tstart\\toutputting\\t0.\\tWhen\\tthis\\thappen,\\tthe\\tneuron\\tis\\tunlikely\\tto\\tcome\\tback\\tto\\tlife\\nsince\\tthe\\tgradient\\tof\\tthe\\tReLU\\tfunction\\tis\\t0\\twhen\\tits\\tinput\\tis\\tnegative.\\nTo\\tsolve\\tthis\\tproblem,\\tyou\\tmay\\twant\\tto\\tuse\\ta\\tvariant\\tof\\tthe\\tReLU\\tfunction,\\tsuch\\tas\\tthe\\t\\nleaky\\tReLU\\n.\\t\\nThis\\nfunction\\tis\\tdefined\\tas\\tLeakyReLU\\nα\\n(\\nz\\n)\\t=\\tmax(\\nαz\\n,\\t\\nz\\n)\\t(see\\t\\nFigure\\t11-2\\n).\\tThe\\thyperparameter\\t\\nα\\n\\tdefines\\thow\\nmuch\\tthe\\tfunction\\t“leaks”:\\tit\\tis\\tthe\\tslope\\tof\\tthe\\tfunction\\tfor\\t\\nz\\n\\t<\\t0,\\tand\\tis\\ttypically\\tset\\tto\\t0.01.\\tThis\\tsmall\\nslope\\tensures\\tthat\\tleaky\\tReLUs\\tnever\\tdie;\\tthey\\tcan\\tgo\\tinto\\ta\\tlong\\tcoma,\\tbut\\tthey\\thave\\ta\\tchance\\tto\\neventually\\twake\\tup.\\tA\\t\\nrecent\\tpaper\\n5\\n\\tcompared\\tseveral\\tvariants\\tof\\tthe\\tReLU\\tactivation\\tfunction\\tand\\tone\\nof\\tits\\tconclusions\\twas\\tthat\\tthe\\tleaky\\tvariants\\talways\\toutperformed\\tthe\\tstrict\\tReLU\\tactivation\\tfunction.\\tIn\\nfact,\\tsetting\\t\\nα\\n\\t=\\t0.2\\t(huge\\tleak)\\tseemed\\tto\\tresult\\tin\\tbetter\\tperformance\\tthan\\t\\nα\\n\\t=\\t0.01\\t(small\\tleak).\\tThey\\nalso\\tevaluated\\tthe\\t\\nrandomized\\tleaky\\tReLU\\n\\t(RReLU),\\twhere\\t\\nα\\n\\tis\\tpicked\\trandomly\\tin\\ta\\tgiven\\trange\\tduring\\ntraining,\\tand\\tit\\tis\\tfixed\\tto\\tan\\taverage\\tvalue\\tduring\\ttesting.\\tIt\\talso\\tperformed\\tfairly\\twell\\tand\\tseemed\\tto\\tact\\nas\\ta\\tregularizer\\t(reducing\\tthe\\trisk\\tof\\toverfitting\\t\\nthe\\ttraining\\tset).\\tFinally,\\tthey\\talso\\tevaluated\\tthe\\nparametric\\tleaky\\tReLU\\n\\t(PReLU),\\t\\nwhere\\t\\nα\\n\\tis\\tauthorized\\tto\\tbe\\tlearned\\tduring\\ttraining\\t(instead\\tof\\tbeing\\ta\\nhyperparameter,\\tit\\tbecomes\\ta\\tparameter\\tthat\\tcan\\tbe\\tmodified\\tby\\tbackpropagation\\tlike\\tany\\tother\\nparameter).\\tThis\\twas\\treported\\tto\\tstrongly\\toutperform\\tReLU\\ton\\tlarge\\timage\\tdatasets,\\tbut\\ton\\tsmaller\\ndatasets\\tit\\truns\\tthe\\trisk\\tof\\toverfitting\\tthe\\ttraining\\tset.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 347}), Document(page_content='Figure\\t11-2.\\t\\nLeaky\\tReLU\\nLast\\tbut\\tnot\\tleast,\\ta\\t\\n2015\\tpaper\\n\\tby\\tDjork-Arné\\tClevert\\tet\\tal.\\n6\\n\\tproposed\\ta\\tnew\\tactivation\\tfunction\\tcalled\\nthe\\t\\nexponential\\tlinear\\tunit\\n\\t(ELU)\\t\\nthat\\toutperformed\\tall\\tthe\\tReLU\\tvariants\\tin\\ttheir\\texperiments:\\ttraining\\ntime\\twas\\treduced\\tand\\tthe\\tneural\\tnetwork\\tperformed\\tbetter\\ton\\tthe\\ttest\\tset.\\tIt\\tis\\trepresented\\tin\\t\\nFigure\\t11-3\\n,\\nand\\t\\nEquation\\t11-2\\n\\tshows\\tits\\tdefinition.\\nEquation\\t11-2.\\t\\nELU\\tactivation\\tfunction\\n', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 348}), Document(page_content='Figure\\t11-3.\\t\\nELU\\tactivation\\tfunction\\nIt\\tlooks\\ta\\tlot\\tlike\\tthe\\tReLU\\tfunction,\\twith\\ta\\tfew\\tmajor\\tdifferences:\\nFirst\\tit\\ttakes\\ton\\tnegative\\tvalues\\twhen\\t\\nz\\n\\t<\\t0,\\twhich\\tallows\\tthe\\tunit\\tto\\thave\\tan\\taverage\\toutput\\tcloser\\nto\\t0.\\tThis\\thelps\\talleviate\\tthe\\tvanishing\\tgradients\\tproblem,\\tas\\tdiscussed\\tearlier.\\tThe\\thyperparameter\\nα\\n\\tdefines\\tthe\\tvalue\\tthat\\tthe\\tELU\\tfunction\\tapproaches\\twhen\\t\\nz\\n\\tis\\ta\\tlarge\\tnegative\\tnumber.\\tIt\\tis\\tusually\\nset\\tto\\t1,\\tbut\\tyou\\tcan\\ttweak\\tit\\tlike\\tany\\tother\\thyperparameter\\tif\\tyou\\twant.\\nSecond,\\tit\\thas\\ta\\tnonzero\\tgradient\\tfor\\t\\nz\\n\\t<\\t0,\\twhich\\tavoids\\tthe\\tdying\\tunits\\tissue.\\nThird,\\tthe\\tfunction\\tis\\tsmooth\\teverywhere,\\tincluding\\taround\\t\\nz\\n\\t=\\t0,\\twhich\\thelps\\tspeed\\tup\\tGradient\\nDescent,\\tsince\\tit\\tdoes\\tnot\\tbounce\\tas\\tmuch\\tleft\\tand\\tright\\tof\\t\\nz\\n\\t=\\t0.\\nThe\\tmain\\tdrawback\\tof\\tthe\\tELU\\tactivation\\tfunction\\tis\\tthat\\tit\\tis\\tslower\\tto\\tcompute\\tthan\\tthe\\tReLU\\tand\\tits\\nvariants\\t(due\\tto\\tthe\\tuse\\tof\\tthe\\texponential\\tfunction),\\tbut\\tduring\\ttraining\\tthis\\tis\\tcompensated\\tby\\tthe\\tfaster\\nconvergence\\trate.\\tHowever,\\tat\\ttest\\ttime\\tan\\tELU\\tnetwork\\twill\\tbe\\tslower\\tthan\\ta\\tReLU\\tnetwork.\\nTIP\\nSo\\twhich\\tactivation\\tfunction\\tshould\\tyou\\tuse\\tfor\\tthe\\thidden\\tlayers\\tof\\tyour\\tdeep\\tneural\\tnetworks?\\tAlthough\\tyour\\tmileage\\twill\\tvary,\\nin\\tgeneral\\tELU\\t>\\tleaky\\tReLU\\t(and\\tits\\tvariants)\\t>\\tReLU\\t>\\ttanh\\t>\\tlogistic.\\tIf\\tyou\\tcare\\ta\\tlot\\tabout\\truntime\\tperformance,\\tthen\\tyou\\nmay\\tprefer\\tleaky\\tReLUs\\tover\\tELUs.\\tIf\\tyou\\tdon’t\\twant\\tto\\ttweak\\tyet\\tanother\\thyperparameter,\\tyou\\tmay\\tjust\\tuse\\tthe\\tdefault\\t\\nα\\nvalues\\tsuggested\\tearlier\\t(0.01\\tfor\\tthe\\tleaky\\tReLU,\\tand\\t1\\tfor\\tELU).\\tIf\\tyou\\thave\\tspare\\ttime\\tand\\tcomputing\\tpower,\\tyou\\tcan\\tuse\\ncross-validation\\tto\\tevaluate\\tother\\tactivation\\tfunctions,\\tin\\tparticular\\tRReLU\\tif\\tyour\\tnetwork\\tis\\toverfitting,\\tor\\tPReLU\\tif\\tyou\\thave\\ta\\nhuge\\ttraining\\tset.\\nTensorFlow\\toffers\\tan\\t\\nelu()\\n\\tfunction\\tthat\\tyou\\tcan\\tuse\\tto\\tbuild\\tyour\\tneural\\tnetwork.\\tSimply\\tset\\tthe\\nactivation\\n\\targument\\twhen\\tcalling\\tthe\\t\\ndense()\\n\\tfunction,\\t\\nlike\\tthis:', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 349}), Document(page_content='hidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nelu\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\nTensorFlow\\tdoes\\tnot\\thave\\ta\\tpredefined\\tfunction\\tfor\\tleaky\\tReLUs,\\tbut\\tit\\tis\\teasy\\tenough\\tto\\t\\ndefine:\\ndef\\n\\t\\nleaky_relu\\n(\\nz\\n,\\n\\t\\nname\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\n0.01\\n\\t\\n*\\n\\t\\nz\\n,\\n\\t\\nz\\n,\\n\\t\\nname\\n=\\nname\\n)\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\nleaky_relu\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 350}), Document(page_content='Batch\\tNormalization\\nAlthough\\t\\nusing\\tHe\\tinitialization\\talong\\twith\\tELU\\t(or\\tany\\tvariant\\tof\\tReLU)\\tcan\\tsignificantly\\treduce\\tthe\\nvanishing/exploding\\tgradients\\tproblems\\tat\\tthe\\tbeginning\\tof\\ttraining,\\tit\\tdoesn’t\\tguarantee\\tthat\\tthey\\twon’t\\ncome\\tback\\tduring\\ttraining.\\nIn\\ta\\t\\n2015\\tpaper\\n,\\n7\\n\\tSergey\\tIoffe\\tand\\tChristian\\tSzegedy\\tproposed\\ta\\ttechnique\\tcalled\\t\\nBatch\\tNormalization\\n(BN)\\tto\\taddress\\tthe\\tvanishing/exploding\\tgradients\\tproblems,\\tand\\tmore\\tgenerally\\tthe\\tproblem\\tthat\\tthe\\ndistribution\\tof\\teach\\tlayer’s\\tinputs\\tchanges\\tduring\\ttraining,\\tas\\tthe\\tparameters\\tof\\tthe\\tprevious\\tlayers\\t\\nchange\\n(which\\tthey\\tcall\\tthe\\t\\nInternal\\tCovariate\\tShift\\n\\tproblem).\\nThe\\ttechnique\\tconsists\\tof\\tadding\\tan\\toperation\\tin\\tthe\\tmodel\\tjust\\tbefore\\tthe\\tactivation\\tfunction\\tof\\teach\\nlayer,\\tsimply\\tzero-centering\\tand\\tnormalizing\\tthe\\tinputs,\\tthen\\tscaling\\tand\\tshifting\\tthe\\tresult\\tusing\\ttwo\\tnew\\nparameters\\tper\\tlayer\\t(one\\tfor\\tscaling,\\tthe\\tother\\tfor\\tshifting).\\tIn\\tother\\twords,\\tthis\\toperation\\tlets\\tthe\\tmodel\\nlearn\\tthe\\toptimal\\tscale\\tand\\tmean\\tof\\tthe\\tinputs\\tfor\\teach\\tlayer.\\nIn\\torder\\tto\\tzero-center\\tand\\tnormalize\\tthe\\tinputs,\\tthe\\talgorithm\\tneeds\\tto\\testimate\\tthe\\tinputs’\\tmean\\tand\\nstandard\\tdeviation.\\tIt\\tdoes\\tso\\tby\\tevaluating\\tthe\\tmean\\tand\\tstandard\\tdeviation\\tof\\tthe\\tinputs\\tover\\tthe\\tcurrent\\nmini-batch\\t(hence\\tthe\\tname\\t“Batch\\tNormalization”).\\tThe\\twhole\\t\\noperation\\tis\\tsummarized\\tin\\t\\nEquation\\t11-\\n3\\n.\\nEquation\\t11-3.\\t\\nBatch\\tNormalization\\talgorithm\\nμ\\nB\\n\\tis\\tthe\\tempirical\\tmean,\\tevaluated\\tover\\tthe\\twhole\\tmini-batch\\t\\nB\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 351}), Document(page_content='σ\\nB\\n\\tis\\tthe\\tempirical\\tstandard\\tdeviation,\\talso\\tevaluated\\tover\\tthe\\twhole\\tmini-batch.\\nm\\nB\\n\\tis\\tthe\\tnumber\\tof\\tinstances\\tin\\tthe\\tmini-batch.\\n(i)\\n\\tis\\tthe\\tzero-centered\\tand\\tnormalized\\tinput.\\nγ\\n\\tis\\tthe\\tscaling\\tparameter\\tfor\\tthe\\tlayer.\\nβ\\n\\tis\\tthe\\tshifting\\tparameter\\t(offset)\\tfor\\tthe\\tlayer.\\nϵ\\n\\tis\\ta\\ttiny\\tnumber\\tto\\tavoid\\tdivision\\tby\\tzero\\t(typically\\t10\\n–5\\n).\\tThis\\tis\\tcalled\\t\\na\\t\\nsmoothing\\tterm\\n.\\nz\\n(i)\\n\\tis\\tthe\\toutput\\tof\\tthe\\tBN\\toperation:\\tit\\tis\\ta\\tscaled\\tand\\tshifted\\tversion\\tof\\tthe\\tinputs.\\nAt\\ttest\\ttime,\\tthere\\tis\\tno\\tmini-batch\\tto\\tcompute\\tthe\\tempirical\\tmean\\tand\\tstandard\\tdeviation,\\tso\\tinstead\\tyou\\nsimply\\tuse\\tthe\\twhole\\ttraining\\tset’s\\tmean\\tand\\tstandard\\tdeviation.\\tThese\\tare\\ttypically\\tefficiently\\tcomputed\\nduring\\ttraining\\tusing\\ta\\tmoving\\taverage.\\tSo,\\tin\\ttotal,\\tfour\\tparameters\\tare\\tlearned\\tfor\\teach\\tbatch-\\nnormalized\\tlayer:\\t\\nγ\\n\\t(scale),\\t\\nβ\\n\\t(offset),\\t\\nμ\\n\\t(mean),\\tand\\t\\nσ\\n\\t(standard\\tdeviation).\\nThe\\tauthors\\tdemonstrated\\tthat\\tthis\\ttechnique\\tconsiderably\\timproved\\tall\\tthe\\tdeep\\tneural\\tnetworks\\tthey\\nexperimented\\twith.\\tThe\\tvanishing\\tgradients\\tproblem\\twas\\tstrongly\\treduced,\\tto\\tthe\\tpoint\\tthat\\tthey\\tcould\\tuse\\nsaturating\\tactivation\\tfunctions\\tsuch\\tas\\tthe\\ttanh\\tand\\teven\\tthe\\tlogistic\\tactivation\\tfunction.\\tThe\\tnetworks\\nwere\\talso\\tmuch\\tless\\tsensitive\\tto\\tthe\\tweight\\tinitialization.\\tThey\\twere\\table\\tto\\tuse\\tmuch\\tlarger\\tlearning\\nrates,\\tsignificantly\\tspeeding\\tup\\tthe\\tlearning\\tprocess.\\tSpecifically,\\tthey\\tnote\\tthat\\t“Applied\\tto\\ta\\tstate-of-\\nthe-art\\timage\\tclassification\\tmodel,\\tBatch\\tNormalization\\tachieves\\tthe\\tsame\\taccuracy\\twith\\t14\\ttimes\\tfewer\\ntraining\\tsteps,\\tand\\tbeats\\tthe\\toriginal\\tmodel\\tby\\ta\\tsignificant\\tmargin.\\t[…]\\tUsing\\tan\\tensemble\\tof\\tbatch-\\nnormalized\\tnetworks,\\twe\\timprove\\tupon\\tthe\\tbest\\tpublished\\tresult\\ton\\tImageNet\\tclassification:\\treaching\\n4.9%\\ttop-5\\tvalidation\\terror\\t(and\\t4.8%\\ttest\\terror),\\texceeding\\tthe\\taccuracy\\tof\\thuman\\traters.”\\tFinally,\\tlike\\ta\\ngift\\tthat\\tkeeps\\ton\\tgiving,\\tBatch\\tNormalization\\talso\\tacts\\tlike\\ta\\tregularizer,\\treducing\\tthe\\tneed\\tfor\\tother\\nregularization\\ttechniques\\t(such\\tas\\tdropout,\\tdescribed\\tlater\\tin\\tthe\\tchapter).\\nBatch\\tNormalization\\tdoes,\\thowever,\\tadd\\tsome\\tcomplexity\\tto\\tthe\\tmodel\\t(although\\tit\\tremoves\\tthe\\tneed\\tfor\\nnormalizing\\tthe\\tinput\\tdata\\tsince\\tthe\\tfirst\\thidden\\tlayer\\twill\\ttake\\tcare\\tof\\tthat,\\tprovided\\tit\\tis\\tbatch-\\nnormalized).\\tMoreover,\\tthere\\tis\\ta\\truntime\\tpenalty:\\tthe\\tneural\\tnetwork\\tmakes\\tslower\\tpredictions\\tdue\\tto\\nthe\\textra\\tcomputations\\trequired\\tat\\teach\\tlayer.\\tSo\\tif\\tyou\\tneed\\tpredictions\\tto\\tbe\\tlightning-fast,\\tyou\\tmay\\nwant\\tto\\tcheck\\thow\\twell\\tplain\\tELU\\t+\\tHe\\tinitialization\\tperform\\tbefore\\tplaying\\twith\\tBatch\\tNormalization.\\nNOTE\\nYou\\tmay\\tfind\\tthat\\ttraining\\tis\\trather\\tslow\\tat\\tfirst\\twhile\\tGradient\\tDescent\\tis\\tsearching\\tfor\\tthe\\toptimal\\tscales\\tand\\toffsets\\tfor\\teach\\nlayer,\\tbut\\tit\\taccelerates\\tonce\\tit\\thas\\tfound\\treasonably\\tgood\\tvalues.\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\nTensorFlow\\t\\nprovides\\ta\\t\\ntf.nn.batch_normalization()\\n\\tfunction\\tthat\\tsimply\\tcenters\\tand\\tnormalizes\\tthe\\ninputs,\\tbut\\tyou\\tmust\\tcompute\\tthe\\tmean\\tand\\tstandard\\tdeviation\\tyourself\\t(based\\ton\\tthe\\tmini-batch\\tdata', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 352}), Document(page_content='during\\ttraining\\tor\\ton\\tthe\\tfull\\tdataset\\tduring\\ttesting,\\tas\\tjust\\tdiscussed)\\tand\\tpass\\tthem\\tas\\tparameters\\tto\\tthis\\nfunction,\\tand\\tyou\\tmust\\talso\\thandle\\tthe\\tcreation\\tof\\tthe\\tscaling\\tand\\toffset\\tparameters\\t(and\\tpass\\tthem\\tto\\tthis\\nfunction).\\tIt\\tis\\tdoable,\\tbut\\tnot\\tthe\\tmost\\tconvenient\\tapproach.\\tInstead,\\tyou\\tshould\\tuse\\tthe\\ntf.layers.batch_normalization()\\n\\tfunction,\\t\\nwhich\\thandles\\tall\\tthis\\tfor\\tyou,\\tas\\tin\\tthe\\tfollowing\\tcode:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nn_inputs\\n\\t\\n=\\n\\t\\n28\\n\\t\\n*\\n\\t\\n28\\nn_hidden1\\n\\t\\n=\\n\\t\\n300\\nn_hidden2\\n\\t\\n=\\n\\t\\n100\\nn_outputs\\n\\t\\n=\\n\\t\\n10\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_inputs\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\ntraining\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder_with_default\\n(\\nFalse\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\nname\\n=\\n\\'training\\'\\n)\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\nbn1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\nbatch_normalization\\n(\\nhidden1\\n,\\n\\t\\ntraining\\n=\\ntraining\\n,\\n\\t\\nmomentum\\n=\\n0.9\\n)\\nbn1_act\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nelu\\n(\\nbn1\\n)\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nbn1_act\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n)\\nbn2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\nbatch_normalization\\n(\\nhidden2\\n,\\n\\t\\ntraining\\n=\\ntraining\\n,\\n\\t\\nmomentum\\n=\\n0.9\\n)\\nbn2_act\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nelu\\n(\\nbn2\\n)\\nlogits_before_bn\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nbn2_act\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\nbatch_normalization\\n(\\nlogits_before_bn\\n,\\n\\t\\ntraining\\n=\\ntraining\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmomentum\\n=\\n0.9\\n)\\nLet’s\\twalk\\tthrough\\tthis\\tcode.\\tThe\\tfirst\\tlines\\tare\\tfairly\\tself-explanatory,\\tuntil\\twe\\tdefine\\tthe\\t\\ntraining\\nplaceholder:\\twe\\twill\\tset\\tit\\tto\\t\\nTrue\\n\\tduring\\ttraining,\\tbut\\totherwise\\tit\\twill\\tdefault\\tto\\t\\nFalse\\n.\\tThis\\twill\\tbe\\nused\\tto\\ttell\\tthe\\t\\ntf.layers.batch_normalization()\\n\\tfunction\\twhether\\tit\\tshould\\tuse\\tthe\\tcurrent\\tmini-\\nbatch’s\\tmean\\tand\\tstandard\\tdeviation\\t(during\\ttraining)\\tor\\tthe\\twhole\\ttraining\\tset’s\\tmean\\tand\\tstandard\\ndeviation\\t(during\\ttesting).\\nThen,\\twe\\talternate\\tfully\\tconnected\\tlayers\\tand\\tbatch\\tnormalization\\tlayers:\\tthe\\tfully\\tconnected\\tlayers\\tare\\ncreated\\tusing\\tthe\\t\\ntf.layers.dense()\\n\\t\\nfunction,\\tjust\\tlike\\twe\\tdid\\tin\\t\\nChapter\\t10\\n.\\tNote\\tthat\\twe\\tdon’t\\nspecify\\tany\\tactivation\\tfunction\\tfor\\tthe\\tfully\\tconnected\\tlayers\\tbecause\\twe\\twant\\tto\\tapply\\tthe\\tactivation\\nfunction\\tafter\\teach\\tbatch\\tnormalization\\tlayer.\\n8\\n\\tWe\\tcreate\\tthe\\tbatch\\tnormalization\\tlayers\\tusing\\tthe\\ntf.layers.batch_normalization()\\n\\t\\nfunction,\\tsetting\\tits\\t\\ntraining\\n\\tand\\t\\nmomentum\\n\\tparameters.\\tThe\\tBN\\nalgorithm\\tuses\\t\\nexponential\\tdecay\\n\\tto\\t\\ncompute\\tthe\\trunning\\taverages,\\twhich\\tis\\twhy\\tit\\trequires\\tthe\\nmomentum\\n\\tparameter:\\tgiven\\ta\\tnew\\tvalue\\t\\nv\\n,\\tthe\\trunning\\taverage\\t\\n\\tis\\tupdated\\tthrough\\tthe\\tequation:\\nA\\tgood\\tmomentum\\tvalue\\tis\\ttypically\\tclose\\tto\\t1\\t—\\tfor\\texample,\\t0.9,\\t0.99,\\tor\\t0.999\\t(you\\twant\\tmore\\t9s\\tfor\\nlarger\\tdatasets\\tand\\tsmaller\\tmini-batches).\\nYou\\tmay\\thave\\tnoticed\\tthat\\tthe\\tcode\\tis\\tquite\\trepetitive,\\twith\\tthe\\tsame\\tbatch\\tnormalization\\tparameters\\nappearing\\tover\\tand\\tover\\tagain.\\tTo\\tavoid\\tthis\\trepetition,\\tyou\\tcan\\tuse\\tthe\\t\\npartial()\\n\\tfunction\\tfrom\\tthe\\nfunctools\\n\\tmodule\\t(part\\tof\\tPython’s\\tstandard\\tlibrary).\\tIt\\tcreates\\ta\\tthin\\twrapper\\taround\\ta\\tfunction\\tand\\nallows\\tyou\\tto\\tdefine\\tdefault\\tvalues\\tfor\\tsome\\tparameters.\\tThe\\tcreation\\tof\\tthe\\tnetwork\\tlayers\\tin\\tthe\\npreceding\\tcode\\tcan\\tbe\\tmodified\\tlike\\tso:\\nfrom\\n\\t\\nfunctools\\n\\t\\nimport\\n\\t\\npartial\\nmy_batch_norm_layer\\n\\t\\n=\\n\\t\\npartial\\n(\\ntf\\n.\\nlayers\\n.\\nbatch_normalization\\n,', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 353}), Document(page_content='\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ntraining\\n=\\ntraining\\n,\\n\\t\\nmomentum\\n=\\n0.9\\n)\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\nbn1\\n\\t\\n=\\n\\t\\nmy_batch_norm_layer\\n(\\nhidden1\\n)\\nbn1_act\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nelu\\n(\\nbn1\\n)\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nbn1_act\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n)\\nbn2\\n\\t\\n=\\n\\t\\nmy_batch_norm_layer\\n(\\nhidden2\\n)\\nbn2_act\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nelu\\n(\\nbn2\\n)\\nlogits_before_bn\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nbn2_act\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\nlogits\\n\\t\\n=\\n\\t\\nmy_batch_norm_layer\\n(\\nlogits_before_bn\\n)\\nIt\\tmay\\tnot\\tlook\\tmuch\\tbetter\\tthan\\tbefore\\tin\\tthis\\tsmall\\texample,\\tbut\\tif\\tyou\\thave\\t10\\tlayers\\tand\\twant\\tto\\tuse\\nthe\\tsame\\tactivation\\tfunction,\\tinitializer,\\tregularizer,\\tand\\tso\\ton,\\tin\\tall\\tlayers,\\tthis\\ttrick\\twill\\tmake\\tyour\\tcode\\nmuch\\tmore\\treadable.\\nThe\\trest\\tof\\tthe\\tconstruction\\tphase\\tis\\tthe\\tsame\\tas\\tin\\t\\nChapter\\t10\\n:\\tdefine\\t\\nthe\\tcost\\tfunction,\\tcreate\\tan\\noptimizer,\\ttell\\tit\\tto\\tminimize\\tthe\\tcost\\tfunction,\\tdefine\\tthe\\tevaluation\\toperations,\\tcreate\\ta\\t\\nSaver\\n,\\tand\\tso\\ton.\\nThe\\texecution\\tphase\\tis\\talso\\tpretty\\tmuch\\tthe\\tsame,\\twith\\ttwo\\texceptions.\\tFirst,\\tduring\\ttraining,\\twhenever\\nyou\\trun\\tan\\toperation\\tthat\\tdepends\\ton\\tthe\\t\\nbatch_normalization()\\n\\tlayer,\\tyou\\tneed\\tto\\tset\\tthe\\t\\ntraining\\nplaceholder\\tto\\t\\nTrue\\n.\\tSecond,\\tthe\\t\\nbatch_normalization()\\n\\tfunction\\tcreates\\ta\\tfew\\toperations\\tthat\\tmust\\nbe\\tevaluated\\tat\\teach\\tstep\\tduring\\ttraining\\tin\\torder\\tto\\tupdate\\tthe\\tmoving\\taverages\\t(recall\\tthat\\tthese\\tmoving\\naverages\\tare\\tneeded\\tto\\tevaluate\\tthe\\ttraining\\tset’s\\tmean\\tand\\tstandard\\tdeviation).\\tThese\\toperations\\tare\\nautomatically\\tadded\\tto\\tthe\\t\\nUPDATE_OPS\\n\\tcollection,\\tso\\tall\\twe\\tneed\\tto\\tdo\\tis\\tget\\tthe\\tlist\\tof\\toperations\\tin\\tthat\\ncollection\\tand\\trun\\tthem\\tat\\teach\\ttraining\\titeration:\\nextra_update_ops\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nUPDATE_OPS\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n([\\ntraining_op\\n,\\n\\t\\nextra_update_ops\\n],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n=\\n{\\ntraining\\n:\\n\\t\\nTrue\\n,\\n\\t\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\naccuracy_val\\n\\t\\n=\\n\\t\\naccuracy\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nmnist\\n.\\ntest\\n.\\nimages\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ny\\n:\\n\\t\\nmnist\\n.\\ntest\\n.\\nlabels\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nprint\\n(\\nepoch\\n,\\n\\t\\n\"Test\\taccuracy:\"\\n,\\n\\t\\naccuracy_val\\n)\\n\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\nThat’s\\tall!\\tIn\\tthis\\ttiny\\texample\\twith\\tjust\\ttwo\\tlayers,\\tit’s\\tunlikely\\tthat\\tBatch\\tNormalization\\twill\\thave\\ta\\nvery\\tpositive\\timpact,\\tbut\\tfor\\tdeeper\\tnetworks\\tit\\tcan\\tmake\\ta\\ttremendous\\t\\ndifference.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 354}), Document(page_content='Gradient\\tClipping\\nA\\tpopular\\t\\ntechnique\\tto\\tlessen\\tthe\\texploding\\tgradients\\tproblem\\tis\\tto\\tsimply\\tclip\\tthe\\tgradients\\tduring\\nbackpropagation\\tso\\tthat\\tthey\\tnever\\texceed\\tsome\\tthreshold\\t(this\\tis\\tmostly\\tuseful\\tfor\\trecurrent\\tneural\\nnetworks;\\tsee\\t\\nChapter\\t14\\n).\\tThis\\tis\\tcalled\\t\\nGradient\\tClipping\\n.\\n9\\n\\tIn\\tgeneral\\tpeople\\tnow\\tprefer\\tBatch\\nNormalization,\\tbut\\tit’s\\tstill\\tuseful\\tto\\tknow\\tabout\\tGradient\\tClipping\\tand\\thow\\tto\\timplement\\tit.\\nIn\\tTensorFlow,\\tthe\\toptimizer’s\\t\\nminimize()\\n\\t\\nfunction\\ttakes\\tcare\\tof\\tboth\\tcomputing\\tthe\\tgradients\\tand\\napplying\\tthem,\\tso\\tyou\\tmust\\tinstead\\tcall\\t\\nthe\\toptimizer’s\\t\\ncompute_gradients()\\n\\t\\nmethod\\tfirst,\\tthen\\tcreate\\nan\\toperation\\tto\\tclip\\tthe\\tgradients\\tusing\\tthe\\t\\nclip_by_value()\\n\\t\\nfunction,\\tand\\tfinally\\tcreate\\tan\\toperation\\tto\\napply\\tthe\\tclipped\\tgradients\\tusing\\tthe\\toptimizer’s\\t\\napply_gradients()\\n\\t\\nmethod:\\nthreshold\\n\\t\\n=\\n\\t\\n1.0\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nGradientDescentOptimizer\\n(\\nlearning_rate\\n)\\ngrads_and_vars\\n\\t\\n=\\n\\t\\noptimizer\\n.\\ncompute_gradients\\n(\\nloss\\n)\\ncapped_gvs\\n\\t\\n=\\n\\t\\n[(\\ntf\\n.\\nclip_by_value\\n(\\ngrad\\n,\\n\\t\\n-\\nthreshold\\n,\\n\\t\\nthreshold\\n),\\n\\t\\nvar\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ngrad\\n,\\n\\t\\nvar\\n\\t\\nin\\n\\t\\ngrads_and_vars\\n]\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\napply_gradients\\n(\\ncapped_gvs\\n)\\nYou\\twould\\tthen\\trun\\tthis\\t\\ntraining_op\\n\\tat\\tevery\\ttraining\\tstep,\\tas\\tusual.\\tIt\\twill\\tcompute\\tthe\\tgradients,\\tclip\\nthem\\tbetween\\t–1.0\\tand\\t1.0,\\tand\\tapply\\tthem.\\tThe\\tthreshold\\tis\\ta\\thyperparameter\\tyou\\t\\ncan\\ttune.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 355}), Document(page_content='Reusing\\tPretrained\\tLayers\\nIt\\t\\nis\\tgenerally\\tnot\\ta\\tgood\\tidea\\tto\\ttrain\\ta\\tvery\\tlarge\\tDNN\\tfrom\\tscratch:\\tinstead,\\tyou\\tshould\\talways\\ttry\\tto\\nfind\\tan\\texisting\\tneural\\tnetwork\\tthat\\taccomplishes\\ta\\tsimilar\\ttask\\tto\\tthe\\tone\\tyou\\tare\\ttrying\\tto\\ttackle,\\tthen\\njust\\treuse\\tthe\\tlower\\tlayers\\tof\\tthis\\tnetwork:\\tthis\\tis\\tcalled\\t\\ntransfer\\tlearning\\n.\\tIt\\twill\\tnot\\tonly\\tspeed\\tup\\ntraining\\tconsiderably,\\tbut\\twill\\talso\\trequire\\tmuch\\tless\\ttraining\\tdata.\\nFor\\texample,\\tsuppose\\tthat\\tyou\\thave\\taccess\\tto\\ta\\tDNN\\tthat\\twas\\ttrained\\tto\\tclassify\\tpictures\\tinto\\t100\\ndifferent\\tcategories,\\tincluding\\tanimals,\\tplants,\\tvehicles,\\tand\\teveryday\\tobjects.\\tYou\\tnow\\twant\\tto\\ttrain\\ta\\nDNN\\tto\\tclassify\\tspecific\\ttypes\\tof\\tvehicles.\\tThese\\ttasks\\tare\\tvery\\tsimilar,\\tso\\tyou\\tshould\\ttry\\tto\\treuse\\tparts\\nof\\tthe\\tfirst\\tnetwork\\t(see\\t\\nFigure\\t11-4\\n).\\nFigure\\t11-4.\\t\\nReusing\\tpretrained\\tlayers\\nNOTE\\nIf\\tthe\\tinput\\tpictures\\tof\\tyour\\tnew\\ttask\\tdon’t\\thave\\tthe\\tsame\\tsize\\tas\\tthe\\tones\\tused\\tin\\tthe\\toriginal\\ttask,\\tyou\\twill\\thave\\tto\\tadd\\ta\\npreprocessing\\tstep\\tto\\tresize\\tthem\\tto\\tthe\\tsize\\texpected\\tby\\tthe\\toriginal\\tmodel.\\tMore\\tgenerally,\\ttransfer\\tlearning\\twill\\tonly\\twork\\twell\\nif\\tthe\\tinputs\\thave\\tsimilar\\tlow-level\\tfeatures.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 356}), Document(page_content='Reusing\\ta\\tTensorFlow\\tModel\\nIf\\t\\nthe\\toriginal\\tmodel\\twas\\ttrained\\tusing\\tTensorFlow,\\tyou\\tcan\\tsimply\\trestore\\tit\\tand\\ttrain\\tit\\ton\\tthe\\tnew\\ttask.\\nAs\\twe\\tdiscussed\\tin\\t\\nChapter\\t9\\n,\\tyou\\tcan\\tuse\\tthe\\t\\nimport_meta_graph()\\n\\tfunction\\tto\\timport\\tthe\\toperations\\ninto\\tthe\\tdefault\\tgraph.\\tThis\\treturns\\ta\\t\\nSaver\\n\\tthat\\tyou\\tcan\\tlater\\tuse\\tto\\tload\\tthe\\tmodel’s\\tstate:\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nimport_meta_graph\\n(\\n\"./my_model_final.ckpt.meta\"\\n)\\nYou\\tmust\\tthen\\tget\\ta\\thandle\\ton\\tthe\\toperations\\tand\\ttensors\\tyou\\twill\\tneed\\tfor\\ttraining.\\tFor\\tthis,\\tyou\\tcan\\tuse\\nthe\\tgraph’s\\t\\nget_operation_by_name()\\n\\tand\\t\\nget_tensor_by_name()\\n\\tmethods.\\tThe\\tname\\tof\\ta\\ttensor\\tis\\nthe\\tname\\tof\\tthe\\toperation\\tthat\\toutputs\\tit\\tfollowed\\tby\\t\\n:0\\n\\t(or\\t\\n:1\\n\\tif\\tit\\tis\\tthe\\tsecond\\toutput,\\t\\n:2\\n\\tif\\tit\\tis\\tthe\\nthird,\\tand\\tso\\ton):\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"X:0\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"y:0\"\\n)\\naccuracy\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"eval/accuracy:0\"\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_operation_by_name\\n(\\n\"GradientDescent\"\\n)\\nIf\\tthe\\tpretrained\\tmodel\\tis\\tnot\\twell\\tdocumented,\\tthen\\tyou\\twill\\thave\\tto\\texplore\\tthe\\tgraph\\tto\\tfind\\tthe\\tnames\\nof\\tthe\\toperations\\tyou\\twill\\tneed.\\tIn\\tthis\\tcase,\\t\\nyou\\tcan\\teither\\texplore\\tthe\\tgraph\\tusing\\tTensorBoard\\t(for\\tthis\\nyou\\tmust\\tfirst\\texport\\tthe\\tgraph\\tusing\\ta\\t\\nFileWriter\\n,\\tas\\tdiscussed\\tin\\t\\nChapter\\t9\\n),\\tor\\tyou\\tcan\\tuse\\tthe\\tgraph’s\\nget_operations()\\n\\tmethod\\tto\\tlist\\tall\\tthe\\toperations:\\nfor\\n\\t\\nop\\n\\t\\nin\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_operations\\n():\\n\\t\\t\\t\\t\\nprint\\n(\\nop\\n.\\nname\\n)\\nIf\\tyou\\tare\\tthe\\tauthor\\tof\\tthe\\toriginal\\tmodel,\\tyou\\tcould\\tmake\\tthings\\teasier\\tfor\\tpeople\\twho\\twill\\treuse\\tyour\\nmodel\\tby\\tgiving\\toperations\\tvery\\tclear\\tnames\\tand\\tdocumenting\\tthem.\\tAnother\\tapproach\\tis\\tto\\tcreate\\ta\\ncollection\\tcontaining\\tall\\tthe\\timportant\\toperations\\tthat\\tpeople\\twill\\twant\\tto\\tget\\ta\\thandle\\ton:\\nfor\\n\\t\\nop\\n\\t\\nin\\n\\t\\n(\\nX\\n,\\n\\t\\ny\\n,\\n\\t\\naccuracy\\n,\\n\\t\\ntraining_op\\n):\\n\\t\\t\\t\\t\\ntf\\n.\\nadd_to_collection\\n(\\n\"my_important_ops\"\\n,\\n\\t\\nop\\n)\\nThis\\tway\\tpeople\\twho\\treuse\\tyour\\tmodel\\twill\\tbe\\table\\tto\\tsimply\\twrite:\\nX\\n,\\n\\t\\ny\\n,\\n\\t\\naccuracy\\n,\\n\\t\\ntraining_op\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\n\"my_important_ops\"\\n)\\nYou\\tcan\\tthen\\trestore\\tthe\\tmodel’s\\tstate\\tusing\\tthe\\t\\nSaver\\n\\tand\\tcontinue\\ttraining\\tusing\\tyour\\town\\tdata:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\ttrain\\tthe\\tmodel\\ton\\tyour\\town\\tdata\\nAlternatively,\\tif\\tyou\\thave\\taccess\\tto\\tthe\\tPython\\tcode\\tthat\\tbuilt\\tthe\\toriginal\\tgraph,\\tyou\\tcan\\tuse\\tit\\tinstead\\tof\\nimport_meta_graph()\\n.\\nIn\\tgeneral,\\tyou\\twill\\twant\\tto\\treuse\\tonly\\tpart\\tof\\tthe\\toriginal\\tmodel,\\ttypically\\tthe\\tlower\\tlayers.\\tIf\\tyou\\tuse\\nimport_meta_graph()\\n\\tto\\trestore\\tthe\\tgraph,\\tit\\twill\\tload\\tthe\\tentire\\toriginal\\tgraph,\\tbut\\tnothing\\tprevents', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 357}), Document(page_content='you\\tfrom\\tjust\\tignoring\\tthe\\tlayers\\tyou\\tdo\\tnot\\tcare\\tabout.\\tFor\\texample,\\tas\\tshown\\tin\\t\\nFigure\\t11-4\\n,\\tyou\\tcould\\nbuild\\tnew\\tlayers\\t(e.g.,\\tone\\thidden\\tlayer\\tand\\tone\\toutput\\tlayer)\\ton\\ttop\\tof\\ta\\tpretrained\\tlayer\\t(e.g.,\\tpretrained\\nhidden\\tlayer\\t3).\\tYou\\twould\\talso\\tneed\\tto\\tcompute\\tthe\\tloss\\tfor\\tthis\\tnew\\toutput,\\tand\\tcreate\\tan\\toptimizer\\tto\\nminimize\\tthat\\tloss.\\nIf\\tyou\\thave\\taccess\\tto\\tthe\\tpretrained\\tgraph’s\\tPython\\tcode,\\tyou\\tcan\\tjust\\treuse\\tthe\\tparts\\tyou\\tneed\\tand\\tchop\\nout\\tthe\\trest.\\tHowever,\\tin\\tthis\\tcase\\tyou\\tneed\\ta\\t\\nSaver\\n\\tto\\trestore\\tthe\\tpretrained\\tmodel\\t(specifying\\twhich\\nvariables\\tyou\\twant\\tto\\trestore;\\totherwise,\\tTensorFlow\\twill\\tcomplain\\tthat\\tthe\\tgraphs\\tdo\\tnot\\tmatch),\\tand\\nanother\\t\\nSaver\\n\\tto\\tsave\\tthe\\tnew\\tmodel.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\trestores\\tonly\\thidden\\tlayers\\t1,\\t2,\\nand\\t3:\\n[\\n...\\n]\\n\\t\\n#\\tbuild\\tthe\\tnew\\tmodel\\twith\\tthe\\tsame\\thidden\\tlayers\\t1-3\\tas\\tbefore\\nreuse_vars\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nGLOBAL_VARIABLES\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscope\\n=\\n\"hidden[123]\"\\n)\\n\\t\\n#\\tregular\\texpression\\nreuse_vars_dict\\n\\t\\n=\\n\\t\\ndict\\n([(\\nvar\\n.\\nop\\n.\\nname\\n,\\n\\t\\nvar\\n)\\n\\t\\nfor\\n\\t\\nvar\\n\\t\\nin\\n\\t\\nreuse_vars\\n])\\nrestore_saver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n(\\nreuse_vars_dict\\n)\\n\\t\\n#\\tto\\trestore\\tlayers\\t1-3\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\n\\t\\n#\\tto\\tinit\\tall\\tvariables,\\told\\tand\\tnew\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\n\\t\\n#\\tto\\tsave\\tthe\\tnew\\tmodel\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nrestore_saver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\ttrain\\tthe\\tmodel\\n\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"./my_new_model_final.ckpt\"\\n)\\nFirst\\t\\nwe\\tbuild\\tthe\\tnew\\tmodel,\\tmaking\\tsure\\tto\\tcopy\\tthe\\toriginal\\tmodel’s\\thidden\\tlayers\\t1\\tto\\t3.\\tThen\\twe\\tget\\nthe\\tlist\\tof\\tall\\tvariables\\tin\\thidden\\tlayers\\t1\\tto\\t3,\\tusing\\tthe\\tregular\\texpression\\t\\n\"hidden[123]\"\\n.\\tNext,\\twe\\ncreate\\ta\\tdictionary\\tthat\\tmaps\\tthe\\tname\\tof\\teach\\tvariable\\tin\\tthe\\toriginal\\tmodel\\tto\\tits\\tname\\tin\\tthe\\tnew\\tmodel\\n(generally\\tyou\\twant\\tto\\tkeep\\tthe\\texact\\tsame\\tnames).\\tThen\\twe\\tcreate\\ta\\t\\nSaver\\n\\tthat\\twill\\trestore\\tonly\\tthese\\nvariables.\\tWe\\talso\\tcreate\\tan\\toperation\\tto\\tinitialize\\tall\\tthe\\tvariables\\t(old\\tand\\tnew)\\tand\\ta\\tsecond\\t\\nSaver\\n\\tto\\nsave\\tthe\\tentire\\tnew\\tmodel,\\tnot\\tjust\\tlayers\\t1\\tto\\t3.\\tWe\\tthen\\tstart\\ta\\tsession\\tand\\tinitialize\\tall\\tvariables\\tin\\tthe\\nmodel,\\tthen\\trestore\\tthe\\tvariable\\tvalues\\tfrom\\tthe\\toriginal\\tmodel’s\\tlayers\\t1\\tto\\t3.\\tFinally,\\twe\\ttrain\\tthe\\tmodel\\non\\tthe\\tnew\\ttask\\tand\\tsave\\tit.\\nTIP\\nThe\\tmore\\tsimilar\\tthe\\ttasks\\tare,\\tthe\\tmore\\tlayers\\tyou\\twant\\tto\\treuse\\t(starting\\twith\\tthe\\tlower\\tlayers).\\tFor\\tvery\\tsimilar\\ttasks,\\tyou\\tcan\\ntry\\tkeeping\\tall\\tthe\\thidden\\tlayers\\tand\\tjust\\treplace\\tthe\\toutput\\t\\nlayer.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 358}), Document(page_content='Reusing\\tModels\\tfrom\\tOther\\tFrameworks\\nIf\\tthe\\t\\nmodel\\twas\\ttrained\\tusing\\tanother\\tframework,\\tyou\\twill\\tneed\\tto\\tload\\tthe\\tmodel\\tparameters\\tmanually\\n(e.g.,\\tusing\\tTheano\\tcode\\tif\\tit\\twas\\ttrained\\twith\\tTheano),\\tthen\\tassign\\tthem\\tto\\tthe\\tappropriate\\tvariables.\\nThis\\tcan\\tbe\\tquite\\ttedious.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tshows\\thow\\tyou\\twould\\tcopy\\tthe\\tweight\\tand\\nbiases\\tfrom\\tthe\\tfirst\\thidden\\tlayer\\tof\\ta\\t\\nmodel\\ttrained\\tusing\\tanother\\tframework:\\noriginal_w\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\n#\\tLoad\\tthe\\tweights\\tfrom\\tthe\\tother\\tframework\\noriginal_b\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\n#\\tLoad\\tthe\\tbiases\\tfrom\\tthe\\tother\\tframework\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_inputs\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\n[\\n...\\n]\\n\\t\\n#\\tBuild\\tthe\\trest\\tof\\tthe\\tmodel\\n#\\tGet\\ta\\thandle\\ton\\tthe\\tassignment\\tnodes\\tfor\\tthe\\thidden1\\tvariables\\ngraph\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\nassign_kernel\\n\\t\\n=\\n\\t\\ngraph\\n.\\nget_operation_by_name\\n(\\n\"hidden1/kernel/Assign\"\\n)\\nassign_bias\\n\\t\\n=\\n\\t\\ngraph\\n.\\nget_operation_by_name\\n(\\n\"hidden1/bias/Assign\"\\n)\\ninit_kernel\\n\\t\\n=\\n\\t\\nassign_kernel\\n.\\ninputs\\n[\\n1\\n]\\ninit_bias\\n\\t\\n=\\n\\t\\nassign_bias\\n.\\ninputs\\n[\\n1\\n]\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ninit\\n,\\n\\t\\nfeed_dict\\n=\\n{\\ninit_kernel\\n:\\n\\t\\noriginal_w\\n,\\n\\t\\ninit_bias\\n:\\n\\t\\noriginal_b\\n})\\n\\t\\t\\t\\t\\n#\\t[...]\\tTrain\\tthe\\tmodel\\ton\\tyour\\tnew\\ttask\\nIn\\tthis\\timplementation,\\twe\\tfirst\\tload\\tthe\\tpretrained\\tmodel\\tusing\\tthe\\tother\\tframework\\t(not\\tshown\\there),\\nand\\twe\\textract\\tfrom\\tit\\tthe\\tmodel\\tparameters\\twe\\twant\\tto\\treuse.\\tNext,\\twe\\tbuild\\tour\\tTensorFlow\\tmodel\\tas\\nusual.\\tThen\\tcomes\\tthe\\ttricky\\tpart:\\tevery\\tTensorFlow\\tvariable\\thas\\tan\\tassociated\\tassignment\\toperation\\tthat\\nis\\tused\\tto\\tinitialize\\tit.\\tWe\\tstart\\tby\\tgetting\\ta\\thandle\\ton\\tthese\\tassignment\\toperations\\t(they\\thave\\tthe\\tsame\\nname\\tas\\tthe\\tvariable,\\tplus\\t\\n\"/Assign\"\\n).\\tWe\\talso\\tget\\ta\\thandle\\ton\\teach\\tassignment\\toperation’s\\tsecond\\ninput:\\tin\\tthe\\tcase\\tof\\tan\\tassignment\\toperation,\\tthe\\tsecond\\tinput\\tcorresponds\\tto\\tthe\\tvalue\\tthat\\twill\\tbe\\nassigned\\tto\\tthe\\tvariable,\\tso\\tin\\tthis\\tcase\\tit\\tis\\tthe\\tvariable’s\\tinitialization\\tvalue.\\tOnce\\twe\\tstart\\tthe\\tsession,\\nwe\\trun\\tthe\\tusual\\tinitialization\\toperation,\\tbut\\tthis\\ttime\\twe\\tfeed\\tit\\tthe\\tvalues\\twe\\twant\\tfor\\tthe\\tvariables\\twe\\nwant\\tto\\treuse.\\tAlternatively,\\twe\\tcould\\thave\\tcreated\\tnew\\tassignment\\toperations\\tand\\tplaceholders,\\tand\\nused\\tthem\\tto\\tset\\tthe\\tvalues\\tof\\tthe\\tvariables\\tafter\\tinitialization.\\tBut\\twhy\\tcreate\\tnew\\tnodes\\tin\\tthe\\tgraph\\nwhen\\teverything\\twe\\tneed\\tis\\talready\\tthere?', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 359}), Document(page_content='Freezing\\tthe\\tLower\\tLayers\\nIt\\t\\nis\\tlikely\\tthat\\tthe\\tlower\\tlayers\\tof\\tthe\\tfirst\\tDNN\\thave\\tlearned\\tto\\tdetect\\tlow-level\\tfeatures\\tin\\tpictures\\tthat\\nwill\\tbe\\tuseful\\tacross\\tboth\\timage\\tclassification\\ttasks,\\tso\\tyou\\tcan\\tjust\\treuse\\tthese\\tlayers\\tas\\tthey\\tare.\\tIt\\tis\\ngenerally\\ta\\tgood\\tidea\\tto\\t“freeze”\\ttheir\\t\\nweights\\twhen\\ttraining\\tthe\\tnew\\tDNN:\\tif\\tthe\\tlower-layer\\tweights\\nare\\tfixed,\\tthen\\tthe\\thigher-layer\\tweights\\twill\\tbe\\teasier\\tto\\ttrain\\t(because\\tthey\\twon’t\\thave\\tto\\tlearn\\ta\\tmoving\\ntarget).\\tTo\\tfreeze\\tthe\\tlower\\tlayers\\tduring\\ttraining,\\tone\\tsolution\\tis\\tto\\tgive\\tthe\\toptimizer\\tthe\\tlist\\tof\\nvariables\\tto\\ttrain,\\texcluding\\tthe\\tvariables\\t\\nfrom\\tthe\\tlower\\tlayers:\\ntrain_vars\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nTRAINABLE_VARIABLES\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscope\\n=\\n\"hidden[34]|outputs\"\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n,\\n\\t\\nvar_list\\n=\\ntrain_vars\\n)\\nThe\\tfirst\\tline\\tgets\\tthe\\tlist\\tof\\tall\\ttrainable\\tvariables\\tin\\thidden\\tlayers\\t3\\tand\\t4\\tand\\tin\\tthe\\toutput\\tlayer.\\tThis\\nleaves\\tout\\tthe\\tvariables\\tin\\tthe\\thidden\\tlayers\\t1\\tand\\t2.\\tNext\\twe\\tprovide\\tthis\\trestricted\\tlist\\tof\\ttrainable\\nvariables\\tto\\tthe\\toptimizer’s\\t\\nminimize()\\n\\t\\nfunction.\\tTa-da!\\tLayers\\t1\\tand\\t2\\tare\\tnow\\tfrozen:\\tthey\\twill\\tnot\\nbudge\\tduring\\ttraining\\t(these\\tare\\toften\\tcalled\\t\\nfrozen\\tlayers\\n).\\nAnother\\toption\\tis\\tto\\tadd\\ta\\t\\nstop_gradient()\\n\\tlayer\\tin\\tthe\\tgraph.\\tAny\\tlayer\\tbelow\\tit\\twill\\tbe\\tfrozen:\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):\\n\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden1\"\\n)\\n\\t\\n#\\treused\\tfrozen\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden2\"\\n)\\n\\t\\n#\\treused\\tfrozen\\n\\t\\t\\t\\t\\nhidden2_stop\\n\\t\\n=\\n\\t\\ntf\\n.\\nstop_gradient\\n(\\nhidden2\\n)\\n\\t\\t\\t\\t\\nhidden3\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden2_stop\\n,\\n\\t\\nn_hidden3\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden3\"\\n)\\n\\t\\n#\\treused,\\tnot\\tfrozen\\n\\t\\t\\t\\t\\nhidden4\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden3\\n,\\n\\t\\nn_hidden4\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden4\"\\n)\\n\\t\\n#\\tnew!\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden4\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\n\\t\\n#\\tnew!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 360}), Document(page_content='Caching\\tthe\\tFrozen\\tLayers\\nSince\\t\\nthe\\tfrozen\\tlayers\\twon’t\\tchange,\\tit\\tis\\tpossible\\tto\\tcache\\tthe\\toutput\\tof\\tthe\\ttopmost\\tfrozen\\tlayer\\tfor\\teach\\ntraining\\tinstance.\\tSince\\ttraining\\tgoes\\tthrough\\tthe\\twhole\\tdataset\\tmany\\ttimes,\\tthis\\twill\\tgive\\tyou\\ta\\thuge\\nspeed\\tboost\\tas\\tyou\\twill\\tonly\\tneed\\tto\\tgo\\tthrough\\tthe\\tfrozen\\tlayers\\tonce\\tper\\ttraining\\tinstance\\t(instead\\tof\\nonce\\tper\\tepoch).\\tFor\\texample,\\tyou\\tcould\\tfirst\\trun\\tthe\\twhole\\ttraining\\tset\\tthrough\\tthe\\tlower\\tlayers\\n(assuming\\tyou\\thave\\tenough\\tRAM),\\tthen\\tduring\\ttraining,\\tinstead\\tof\\tbuilding\\tbatches\\tof\\ttraining\\tinstances,\\nyou\\twould\\tbuild\\tbatches\\tof\\toutputs\\tfrom\\thidden\\tlayer\\t2\\tand\\tfeed\\tthem\\tto\\tthe\\ttraining\\toperation:\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nn_batches\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nrestore_saver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\nh2_cache\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\nhidden2\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nmnist\\n.\\ntrain\\n.\\nimages\\n})\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nshuffled_idx\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\npermutation\\n(\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nhidden2_batches\\n\\t\\n=\\n\\t\\nnp\\n.\\narray_split\\n(\\nh2_cache\\n[\\nshuffled_idx\\n],\\n\\t\\nn_batches\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ny_batches\\n\\t\\n=\\n\\t\\nnp\\n.\\narray_split\\n(\\nmnist\\n.\\ntrain\\n.\\nlabels\\n[\\nshuffled_idx\\n],\\n\\t\\nn_batches\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nhidden2_batch\\n,\\n\\t\\ny_batch\\n\\t\\nin\\n\\t\\nzip\\n(\\nhidden2_batches\\n,\\n\\t\\ny_batches\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nhidden2\\n:\\nhidden2_batch\\n,\\n\\t\\ny\\n:\\ny_batch\\n})\\n\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"./my_new_model_final.ckpt\"\\n)\\nThe\\tlast\\tline\\tof\\tthe\\ttraining\\tloop\\truns\\tthe\\ttraining\\toperation\\tdefined\\tearlier\\t(which\\tdoes\\tnot\\ttouch\\tlayers\\t1\\nand\\t2),\\tand\\tfeeds\\tit\\ta\\tbatch\\tof\\toutputs\\tfrom\\tthe\\tsecond\\thidden\\tlayer\\t(as\\twell\\tas\\tthe\\ttargets\\tfor\\tthat\\tbatch).\\nSince\\twe\\tgive\\tTensorFlow\\tthe\\toutput\\tof\\thidden\\tlayer\\t2,\\tit\\tdoes\\tnot\\ttry\\tto\\tevaluate\\t\\nit\\t(or\\tany\\tnode\\tit\\ndepends\\ton).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 361}), Document(page_content='Tweaking,\\tDropping,\\tor\\tReplacing\\tthe\\tUpper\\tLayers\\nThe\\t\\noutput\\tlayer\\tof\\tthe\\toriginal\\tmodel\\tshould\\tusually\\tbe\\treplaced\\tsince\\tit\\tis\\tmost\\tlikely\\tnot\\tuseful\\tat\\tall\\nfor\\tthe\\tnew\\ttask,\\tand\\tit\\tmay\\tnot\\teven\\thave\\tthe\\tright\\tnumber\\tof\\toutputs\\tfor\\tthe\\tnew\\ttask.\\nSimilarly,\\tthe\\tupper\\thidden\\tlayers\\tof\\tthe\\toriginal\\tmodel\\tare\\tless\\tlikely\\tto\\tbe\\tas\\tuseful\\tas\\tthe\\tlower\\tlayers,\\nsince\\tthe\\thigh-level\\tfeatures\\tthat\\tare\\tmost\\tuseful\\tfor\\tthe\\tnew\\ttask\\tmay\\tdiffer\\tsignificantly\\tfrom\\tthe\\tones\\nthat\\twere\\tmost\\tuseful\\tfor\\tthe\\toriginal\\ttask.\\tYou\\twant\\tto\\tfind\\tthe\\tright\\tnumber\\tof\\tlayers\\tto\\treuse.\\nTry\\tfreezing\\tall\\tthe\\tcopied\\tlayers\\tfirst,\\tthen\\ttrain\\tyour\\tmodel\\tand\\tsee\\thow\\tit\\tperforms.\\tThen\\ttry\\tunfreezing\\none\\tor\\ttwo\\tof\\tthe\\ttop\\thidden\\tlayers\\tto\\tlet\\tbackpropagation\\ttweak\\tthem\\tand\\tsee\\tif\\tperformance\\timproves.\\nThe\\tmore\\ttraining\\tdata\\tyou\\thave,\\tthe\\tmore\\tlayers\\tyou\\tcan\\tunfreeze.\\nIf\\tyou\\tstill\\tcannot\\tget\\tgood\\tperformance,\\tand\\tyou\\thave\\tlittle\\ttraining\\tdata,\\ttry\\tdropping\\tthe\\ttop\\thidden\\nlayer(s)\\tand\\tfreeze\\tall\\tremaining\\thidden\\tlayers\\tagain.\\tYou\\tcan\\titerate\\tuntil\\tyou\\tfind\\tthe\\tright\\tnumber\\tof\\nlayers\\tto\\treuse.\\tIf\\tyou\\thave\\tplenty\\tof\\ttraining\\tdata,\\tyou\\tmay\\ttry\\treplacing\\tthe\\ttop\\thidden\\tlayers\\tinstead\\tof\\ndropping\\tthem,\\tand\\teven\\tadd\\tmore\\thidden\\tlayers.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 362}), Document(page_content='Model\\tZoos\\nWhere\\t\\ncan\\tyou\\tfind\\ta\\tneural\\tnetwork\\ttrained\\tfor\\ta\\ttask\\tsimilar\\tto\\tthe\\tone\\tyou\\twant\\tto\\ttackle?\\tThe\\tfirst\\nplace\\tto\\tlook\\tis\\tobviously\\tin\\tyour\\town\\tcatalog\\tof\\tmodels.\\tThis\\tis\\tone\\tgood\\treason\\tto\\tsave\\tall\\tyour\\tmodels\\nand\\torganize\\tthem\\tso\\tyou\\tcan\\tretrieve\\tthem\\tlater\\teasily.\\tAnother\\toption\\tis\\tto\\tsearch\\tin\\ta\\t\\nmodel\\tzoo\\n.\\tMany\\npeople\\ttrain\\tMachine\\tLearning\\tmodels\\tfor\\tvarious\\ttasks\\tand\\tkindly\\trelease\\ttheir\\tpretrained\\tmodels\\tto\\tthe\\npublic.\\nTensorFlow\\t\\nhas\\tits\\town\\tmodel\\tzoo\\tavailable\\tat\\t\\nhttps://github.com/tensorflow/models\\n.\\tIn\\tparticular,\\tit\\ncontains\\tmost\\tof\\tthe\\tstate-of-the-art\\timage\\tclassification\\tnets\\tsuch\\tas\\tVGG,\\tInception,\\tand\\t\\nResNet\\t(see\\nChapter\\t13\\n,\\tand\\tcheck\\tout\\tthe\\t\\nmodels/slim\\n\\tdirectory),\\tincluding\\tthe\\tcode,\\tthe\\tpretrained\\tmodels,\\tand\\ttools\\nto\\tdownload\\tpopular\\timage\\tdatasets.\\nAnother\\tpopular\\tmodel\\tzoo\\tis\\t\\nCaffe’s\\t\\nModel\\tZoo\\n.\\tIt\\talso\\tcontains\\tmany\\tcomputer\\tvision\\tmodels\\t(e.g.,\\nLeNet,\\tAlexNet,\\tZFNet,\\tGoogLeNet,\\tVGGNet,\\tinception)\\ttrained\\ton\\tvarious\\tdatasets\\t(e.g.,\\tImageNet,\\nPlaces\\tDatabase,\\tCIFAR10,\\tetc.).\\tSaumitro\\tDasgupta\\twrote\\ta\\tconverter,\\twhich\\tis\\tavailable\\tat\\nhttps://github.com/ethereon/caffe-tensorflow\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 363}), Document(page_content='Unsupervised\\tPretraining\\nSuppose\\t\\nyou\\twant\\tto\\ttackle\\ta\\tcomplex\\ttask\\tfor\\twhich\\tyou\\tdon’t\\thave\\tmuch\\tlabeled\\ttraining\\tdata,\\tbut\\nunfortunately\\tyou\\tcannot\\tfind\\ta\\tmodel\\ttrained\\ton\\ta\\tsimilar\\ttask.\\tDon’t\\tlose\\tall\\thope!\\tFirst,\\tyou\\tshould\\tof\\ncourse\\ttry\\tto\\tgather\\tmore\\tlabeled\\ttraining\\tdata,\\tbut\\tif\\tthis\\tis\\ttoo\\thard\\tor\\ttoo\\texpensive,\\tyou\\tmay\\tstill\\tbe\\nable\\tto\\tperform\\t\\nunsupervised\\tpretraining\\n\\t(see\\t\\nFigure\\t11-5\\n).\\tThat\\tis,\\tif\\tyou\\thave\\tplenty\\tof\\tunlabeled\\ntraining\\tdata,\\tyou\\tcan\\ttry\\tto\\ttrain\\tthe\\tlayers\\tone\\tby\\tone,\\tstarting\\twith\\tthe\\tlowest\\tlayer\\tand\\tthen\\tgoing\\tup,\\nusing\\tan\\tunsupervised\\tfeature\\tdetector\\talgorithm\\tsuch\\t\\nas\\t\\nRestricted\\tBoltzmann\\tMachines\\n\\t(RBMs;\\tsee\\nAppendix\\tE\\n)\\tor\\tautoencoders\\t(see\\t\\nChapter\\t15\\n).\\tEach\\tlayer\\tis\\ttrained\\ton\\tthe\\toutput\\tof\\tthe\\tpreviously\\ntrained\\tlayers\\t(all\\tlayers\\texcept\\tthe\\tone\\tbeing\\ttrained\\tare\\tfrozen).\\tOnce\\tall\\tlayers\\thave\\tbeen\\ttrained\\tthis\\nway,\\tyou\\tcan\\tfine-tune\\tthe\\tnetwork\\tusing\\tsupervised\\tlearning\\t(i.e.,\\twith\\tbackpropagation).\\nThis\\tis\\ta\\trather\\tlong\\tand\\ttedious\\tprocess,\\tbut\\tit\\toften\\tworks\\twell;\\tin\\tfact,\\tit\\tis\\tthis\\ttechnique\\tthat\\tGeoffrey\\nHinton\\tand\\this\\tteam\\tused\\tin\\t2006\\tand\\twhich\\tled\\tto\\tthe\\trevival\\tof\\tneural\\tnetworks\\tand\\tthe\\tsuccess\\tof\\tDeep\\nLearning.\\tUntil\\t2010,\\tunsupervised\\tpretraining\\t(typically\\tusing\\tRBMs)\\twas\\tthe\\tnorm\\tfor\\tdeep\\tnets,\\tand\\tit\\nwas\\tonly\\tafter\\tthe\\tvanishing\\tgradients\\tproblem\\twas\\talleviated\\tthat\\tit\\tbecame\\tmuch\\tmore\\tcommon\\tto\\ttrain\\nDNNs\\tpurely\\tusing\\t\\nbackpropagation.\\tHowever,\\tunsupervised\\tpretraining\\t(today\\ttypically\\tusing\\nautoencoders\\trather\\tthan\\tRBMs)\\tis\\tstill\\ta\\tgood\\toption\\twhen\\tyou\\thave\\ta\\tcomplex\\ttask\\tto\\tsolve,\\tno\\tsimilar\\nmodel\\tyou\\tcan\\treuse,\\tand\\tlittle\\tlabeled\\ttraining\\tdata\\tbut\\tplenty\\tof\\tunlabeled\\t\\ntraining\\tdata.\\n10\\nFigure\\t11-5.\\t\\nUnsupervised\\tpretraining', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 364}), Document(page_content='Pretraining\\ton\\tan\\tAuxiliary\\tTask\\nOne\\t\\nlast\\toption\\tis\\tto\\ttrain\\ta\\tfirst\\tneural\\tnetwork\\ton\\tan\\tauxiliary\\ttask\\tfor\\twhich\\tyou\\tcan\\teasily\\tobtain\\tor\\ngenerate\\tlabeled\\ttraining\\tdata,\\tthen\\treuse\\tthe\\tlower\\tlayers\\tof\\tthat\\tnetwork\\tfor\\tyour\\tactual\\ttask.\\tThe\\tfirst\\nneural\\tnetwork’s\\tlower\\tlayers\\twill\\tlearn\\tfeature\\tdetectors\\tthat\\twill\\tlikely\\tbe\\treusable\\tby\\tthe\\tsecond\\nneural\\tnetwork.\\nFor\\texample,\\tif\\tyou\\twant\\tto\\tbuild\\ta\\tsystem\\tto\\trecognize\\tfaces,\\tyou\\tmay\\tonly\\thave\\ta\\tfew\\tpictures\\tof\\teach\\nindividual\\t—\\tclearly\\tnot\\tenough\\tto\\ttrain\\ta\\tgood\\tclassifier.\\tGathering\\thundreds\\tof\\tpictures\\tof\\teach\\tperson\\nwould\\tnot\\tbe\\tpractical.\\tHowever,\\tyou\\tcould\\tgather\\ta\\tlot\\tof\\tpictures\\tof\\trandom\\tpeople\\ton\\tthe\\tinternet\\tand\\ntrain\\ta\\tfirst\\tneural\\tnetwork\\tto\\tdetect\\twhether\\tor\\tnot\\ttwo\\tdifferent\\tpictures\\tfeature\\tthe\\tsame\\tperson.\\tSuch\\ta\\nnetwork\\twould\\tlearn\\tgood\\tfeature\\tdetectors\\tfor\\tfaces,\\tso\\treusing\\tits\\tlower\\tlayers\\twould\\tallow\\tyou\\tto\\ntrain\\ta\\tgood\\tface\\tclassifier\\tusing\\tlittle\\ttraining\\tdata.\\nIt\\tis\\toften\\trather\\tcheap\\tto\\tgather\\tunlabeled\\ttraining\\texamples,\\tbut\\tquite\\texpensive\\tto\\tlabel\\tthem.\\tIn\\tthis\\nsituation,\\ta\\tcommon\\ttechnique\\tis\\tto\\tlabel\\tall\\tyour\\ttraining\\texamples\\tas\\t“good,”\\tthen\\tgenerate\\tmany\\tnew\\ntraining\\tinstances\\tby\\tcorrupting\\tthe\\tgood\\tones,\\tand\\tlabel\\tthese\\tcorrupted\\tinstances\\tas\\t“bad.”\\tThen\\tyou\\tcan\\ntrain\\ta\\tfirst\\tneural\\tnetwork\\tto\\tclassify\\tinstances\\tas\\tgood\\tor\\tbad.\\tFor\\texample,\\tyou\\tcould\\tdownload\\nmillions\\tof\\tsentences,\\tlabel\\tthem\\tas\\t“good,”\\tthen\\trandomly\\tchange\\ta\\tword\\tin\\teach\\tsentence\\tand\\tlabel\\tthe\\nresulting\\tsentences\\tas\\t“bad.”\\tIf\\ta\\tneural\\tnetwork\\tcan\\ttell\\tthat\\t“The\\tdog\\tsleeps”\\tis\\ta\\tgood\\tsentence\\tbut\\n“The\\tdog\\tthey”\\tis\\tbad,\\tit\\tprobably\\tknows\\tquite\\ta\\tlot\\tabout\\tlanguage.\\tReusing\\tits\\tlower\\tlayers\\twill\\tlikely\\nhelp\\tin\\tmany\\tlanguage\\tprocessing\\ttasks.\\nAnother\\tapproach\\tis\\tto\\ttrain\\ta\\tfirst\\tnetwork\\tto\\toutput\\ta\\tscore\\tfor\\teach\\ttraining\\tinstance,\\tand\\tuse\\ta\\t\\ncost\\nfunction\\tthat\\tensures\\tthat\\ta\\tgood\\tinstance’s\\tscore\\tis\\tgreater\\tthan\\ta\\tbad\\tinstance’s\\tscore\\tby\\tat\\tleast\\tsome\\nmargin.\\tThis\\tis\\t\\ncalled\\t\\nmax\\tmargin\\tlearning\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 365}), Document(page_content='Faster\\tOptimizers\\nTraining\\t\\na\\tvery\\tlarge\\tdeep\\tneural\\tnetwork\\tcan\\tbe\\tpainfully\\tslow.\\tSo\\tfar\\twe\\thave\\tseen\\tfour\\tways\\tto\\tspeed\\nup\\ttraining\\t(and\\treach\\ta\\tbetter\\tsolution):\\tapplying\\ta\\tgood\\tinitialization\\tstrategy\\tfor\\tthe\\tconnection\\tweights,\\nusing\\ta\\tgood\\tactivation\\tfunction,\\tusing\\tBatch\\tNormalization,\\tand\\treusing\\tparts\\tof\\ta\\tpretrained\\tnetwork.\\nAnother\\thuge\\tspeed\\tboost\\tcomes\\tfrom\\tusing\\ta\\tfaster\\toptimizer\\tthan\\tthe\\tregular\\tGradient\\tDescent\\noptimizer.\\tIn\\tthis\\tsection\\twe\\twill\\tpresent\\tthe\\tmost\\tpopular\\tones:\\tMomentum\\toptimization,\\tNesterov\\nAccelerated\\tGradient,\\tAdaGrad,\\tRMSProp,\\tand\\tfinally\\tAdam\\toptimization.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 366}), Document(page_content='Momentum\\tOptimization\\nImagine\\t\\na\\tbowling\\tball\\trolling\\tdown\\ta\\tgentle\\tslope\\ton\\ta\\tsmooth\\tsurface:\\tit\\twill\\tstart\\tout\\tslowly,\\tbut\\tit\\nwill\\tquickly\\tpick\\tup\\tmomentum\\tuntil\\tit\\teventually\\treaches\\tterminal\\tvelocity\\t(if\\tthere\\tis\\tsome\\tfriction\\tor\\nair\\tresistance).\\tThis\\tis\\tthe\\tvery\\tsimple\\tidea\\tbehind\\t\\nMomentum\\toptimization\\n,\\t\\nproposed\\tby\\tBoris\\tPolyak\\nin\\t1964\\n.\\n11\\n\\tIn\\tcontrast,\\tregular\\tGradient\\tDescent\\twill\\tsimply\\ttake\\tsmall\\tregular\\tsteps\\tdown\\tthe\\tslope,\\tso\\tit\\nwill\\ttake\\tmuch\\tmore\\ttime\\tto\\treach\\tthe\\tbottom.\\nRecall\\tthat\\t\\nGradient\\tDescent\\tsimply\\tupdates\\tthe\\tweights\\t\\nθ\\n\\tby\\tdirectly\\tsubtracting\\tthe\\tgradient\\tof\\tthe\\tcost\\nfunction\\t\\nJ\\n(\\nθ\\n)\\twith\\tregards\\tto\\tthe\\tweights\\t(\\nθ\\nJ\\n(\\nθ\\n))\\tmultiplied\\tby\\tthe\\tlearning\\trate\\t\\nη\\n.\\tThe\\tequation\\tis:\\t\\nθ\\n\\t←\\nθ\\n\\t–\\t\\nη\\nθ\\nJ\\n(\\nθ\\n).\\tIt\\tdoes\\tnot\\tcare\\tabout\\twhat\\tthe\\tearlier\\tgradients\\twere.\\tIf\\tthe\\tlocal\\tgradient\\tis\\ttiny,\\tit\\tgoes\\nvery\\tslowly.\\nMomentum\\toptimization\\tcares\\ta\\tgreat\\tdeal\\tabout\\twhat\\tprevious\\tgradients\\twere:\\tat\\teach\\titeration,\\tit\\nsubtracts\\tthe\\tlocal\\tgradient\\tfrom\\tthe\\t\\nmomentum\\tvector\\n\\t\\nm\\n\\t(multiplied\\tby\\tthe\\tlearning\\trate\\t\\nη\\n),\\tand\\tit\\nupdates\\tthe\\tweights\\tby\\tsimply\\tadding\\tthis\\tmomentum\\tvector\\t(see\\t\\nEquation\\t11-4\\n).\\tIn\\tother\\twords,\\tthe\\ngradient\\tis\\tused\\tas\\tan\\tacceleration,\\tnot\\tas\\ta\\tspeed.\\tTo\\tsimulate\\tsome\\tsort\\tof\\tfriction\\tmechanism\\tand\\nprevent\\tthe\\tmomentum\\tfrom\\tgrowing\\ttoo\\tlarge,\\tthe\\talgorithm\\tintroduces\\ta\\tnew\\thyperparameter\\t\\nβ\\n,\\tsimply\\ncalled\\tthe\\t\\nmomentum\\n,\\twhich\\tmust\\tbe\\tset\\tbetween\\t0\\t(high\\tfriction)\\tand\\t1\\t(no\\tfriction).\\tA\\ttypical\\nmomentum\\tvalue\\tis\\t0.9.\\nEquation\\t11-4.\\t\\nMomentum\\talgorithm\\nYou\\tcan\\teasily\\tverify\\tthat\\tif\\tthe\\tgradient\\tremains\\tconstant,\\tthe\\tterminal\\tvelocity\\t(i.e.,\\tthe\\tmaximum\\tsize\\tof\\nthe\\tweight\\tupdates)\\tis\\tequal\\tto\\tthat\\tgradient\\tmultiplied\\tby\\tthe\\tlearning\\trate\\t\\nη\\n\\tmultiplied\\tby\\t\\n\\t(ignoring\\nthe\\tsign).\\tFor\\texample,\\tif\\t\\nβ\\n\\t=\\t0.9,\\tthen\\tthe\\tterminal\\tvelocity\\tis\\tequal\\tto\\t10\\ttimes\\tthe\\tgradient\\ttimes\\tthe\\nlearning\\trate,\\tso\\tMomentum\\toptimization\\tends\\tup\\tgoing\\t10\\ttimes\\tfaster\\tthan\\tGradient\\tDescent!\\tThis\\nallows\\tMomentum\\toptimization\\tto\\tescape\\tfrom\\tplateaus\\tmuch\\tfaster\\tthan\\tGradient\\tDescent.\\tIn\\tparticular,\\nwe\\tsaw\\tin\\t\\nChapter\\t4\\n\\tthat\\twhen\\tthe\\tinputs\\thave\\tvery\\tdifferent\\tscales\\tthe\\t\\ncost\\tfunction\\twill\\tlook\\tlike\\tan\\nelongated\\tbowl\\t(see\\t\\nFigure\\t4-7\\n).\\tGradient\\tDescent\\tgoes\\tdown\\tthe\\tsteep\\tslope\\tquite\\tfast,\\tbut\\tthen\\tit\\ttakes\\na\\tvery\\tlong\\ttime\\tto\\tgo\\tdown\\tthe\\tvalley.\\tIn\\tcontrast,\\tMomentum\\toptimization\\twill\\troll\\tdown\\tthe\\tbottom\\tof\\nthe\\tvalley\\tfaster\\tand\\tfaster\\tuntil\\tit\\treaches\\tthe\\tbottom\\t(the\\toptimum).\\tIn\\tdeep\\tneural\\tnetworks\\tthat\\tdon’t\\nuse\\tBatch\\tNormalization,\\tthe\\tupper\\tlayers\\twill\\toften\\tend\\tup\\thaving\\tinputs\\twith\\tvery\\tdifferent\\tscales,\\tso\\nusing\\tMomentum\\toptimization\\thelps\\ta\\tlot.\\tIt\\tcan\\talso\\thelp\\troll\\tpast\\tlocal\\toptima.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 367}), Document(page_content='NOTE\\nDue\\tto\\tthe\\tmomentum,\\tthe\\toptimizer\\tmay\\tovershoot\\ta\\tbit,\\tthen\\tcome\\tback,\\tovershoot\\tagain,\\tand\\toscillate\\tlike\\tthis\\tmany\\ttimes\\nbefore\\tstabilizing\\tat\\tthe\\tminimum.\\tThis\\tis\\tone\\tof\\tthe\\treasons\\twhy\\tit\\tis\\tgood\\tto\\thave\\ta\\tbit\\tof\\tfriction\\tin\\tthe\\tsystem:\\tit\\tgets\\trid\\tof\\nthese\\toscillations\\tand\\tthus\\tspeeds\\tup\\tconvergence.\\nImplementing\\tMomentum\\toptimization\\tin\\t\\nTensorFlow\\tis\\ta\\tno-brainer:\\tjust\\treplace\\tthe\\nGradientDescentOptimizer\\n\\twith\\tthe\\t\\nMomentumOptimizer\\n,\\tthen\\tlie\\tback\\tand\\tprofit!\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nMomentumOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmomentum\\n=\\n0.9\\n)\\nThe\\tone\\tdrawback\\tof\\tMomentum\\toptimization\\tis\\tthat\\tit\\tadds\\tyet\\tanother\\thyperparameter\\tto\\ttune.\\tHowever,\\nthe\\tmomentum\\tvalue\\tof\\t0.9\\tusually\\tworks\\twell\\tin\\tpractice\\tand\\talmost\\talways\\tgoes\\tfaster\\t\\nthan\\tGradient\\nDescent.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 368}), Document(page_content='Nesterov\\tAccelerated\\tGradient\\nOne\\t\\nsmall\\tvariant\\tto\\tMomentum\\toptimization,\\tproposed\\tby\\t\\nYurii\\tNesterov\\tin\\t1983\\n,\\n12\\n\\tis\\talmost\\talways\\nfaster\\tthan\\tvanilla\\tMomentum\\toptimization.\\tThe\\t\\nidea\\tof\\t\\nNesterov\\tMomentum\\toptimization\\n,\\tor\\t\\nNesterov\\nAccelerated\\tGradient\\n\\t(NAG),\\tis\\tto\\tmeasure\\tthe\\tgradient\\tof\\tthe\\tcost\\tfunction\\tnot\\tat\\tthe\\tlocal\\tposition\\tbut\\nslightly\\tahead\\tin\\tthe\\tdirection\\tof\\tthe\\tmomentum\\t(see\\t\\nEquation\\t11-5\\n).\\tThe\\tonly\\tdifference\\tfrom\\tvanilla\\nMomentum\\toptimization\\tis\\tthat\\tthe\\tgradient\\tis\\tmeasured\\tat\\t\\nθ\\n\\t+\\t\\nβ\\nm\\n\\trather\\tthan\\tat\\t\\nθ\\n.\\nEquation\\t11-5.\\t\\nNesterov\\tAccelerated\\tGradient\\talgorithm\\nThis\\tsmall\\ttweak\\tworks\\tbecause\\tin\\tgeneral\\tthe\\tmomentum\\tvector\\twill\\tbe\\tpointing\\tin\\tthe\\tright\\tdirection\\n(i.e.,\\ttoward\\tthe\\toptimum),\\tso\\tit\\twill\\tbe\\tslightly\\tmore\\taccurate\\tto\\tuse\\tthe\\tgradient\\tmeasured\\ta\\tbit\\tfarther\\tin\\nthat\\tdirection\\trather\\tthan\\tusing\\tthe\\tgradient\\tat\\tthe\\toriginal\\tposition,\\tas\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t11-6\\n\\t(where\\n1\\n\\trepresents\\tthe\\tgradient\\tof\\tthe\\tcost\\tfunction\\tmeasured\\t\\nat\\tthe\\tstarting\\tpoint\\t\\nθ\\n,\\tand\\t\\n2\\n\\trepresents\\tthe\\ngradient\\tat\\tthe\\tpoint\\tlocated\\tat\\t\\nθ\\n\\t+\\t\\nβ\\nm\\n).\\tAs\\tyou\\tcan\\tsee,\\tthe\\tNesterov\\tupdate\\tends\\tup\\tslightly\\tcloser\\tto\\tthe\\noptimum.\\tAfter\\ta\\twhile,\\tthese\\tsmall\\timprovements\\tadd\\tup\\tand\\tNAG\\tends\\tup\\tbeing\\tsignificantly\\tfaster\\tthan\\nregular\\tMomentum\\toptimization.\\tMoreover,\\tnote\\tthat\\twhen\\tthe\\tmomentum\\tpushes\\tthe\\tweights\\tacross\\ta\\nvalley,\\t\\n1\\n\\tcontinues\\tto\\tpush\\tfurther\\tacross\\tthe\\tvalley,\\twhile\\t\\n2\\n\\tpushes\\tback\\ttoward\\tthe\\tbottom\\tof\\tthe\\nvalley.\\tThis\\thelps\\treduce\\toscillations\\tand\\tthus\\tconverges\\tfaster.\\nNAG\\twill\\talmost\\talways\\tspeed\\tup\\ttraining\\tcompared\\tto\\t\\nregular\\tMomentum\\toptimization.\\tTo\\tuse\\tit,\\nsimply\\t\\nset\\t\\nuse_nesterov=True\\n\\twhen\\tcreating\\tthe\\t\\nMomentumOptimizer\\n:\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nMomentumOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmomentum\\n=\\n0.9\\n,\\n\\t\\nuse_nesterov\\n=\\nTrue\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 369}), Document(page_content='Figure\\t11-6.\\t\\nRegular\\tversus\\tNesterov\\tMomentum\\toptimization', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 370}), Document(page_content='AdaGrad\\nConsider\\t\\nthe\\telongated\\tbowl\\tproblem\\tagain:\\t\\nGradient\\tDescent\\tstarts\\tby\\tquickly\\tgoing\\tdown\\tthe\\tsteepest\\nslope,\\tthen\\tslowly\\tgoes\\tdown\\tthe\\tbottom\\tof\\tthe\\tvalley.\\tIt\\twould\\tbe\\tnice\\tif\\tthe\\talgorithm\\tcould\\tdetect\\tthis\\nearly\\ton\\tand\\tcorrect\\tits\\tdirection\\tto\\tpoint\\ta\\tbit\\tmore\\ttoward\\tthe\\tglobal\\toptimum.\\nThe\\t\\nAdaGrad\\n\\talgorithm\\n13\\n\\tachieves\\tthis\\tby\\tscaling\\tdown\\tthe\\tgradient\\tvector\\talong\\tthe\\tsteepest\\tdimensions\\n(see\\t\\nEquation\\t11-6\\n):\\nEquation\\t11-6.\\t\\nAdaGrad\\talgorithm\\nThe\\tfirst\\tstep\\taccumulates\\tthe\\tsquare\\tof\\tthe\\tgradients\\tinto\\tthe\\tvector\\t\\ns\\n\\t(the\\t\\n\\tsymbol\\trepresents\\tthe\\nelement-wise\\tmultiplication).\\tThis\\tvectorized\\tform\\tis\\tequivalent\\tto\\tcomputing\\t\\ns\\ni\\n\\t←\\t\\ns\\ni\\n\\t+\\t(∂\\t\\nJ\\n(\\nθ\\n)\\t/\\t∂\\t\\nθ\\ni\\n)\\n2\\nfor\\teach\\telement\\t\\ns\\ni\\n\\tof\\tthe\\tvector\\t\\ns\\n;\\tin\\tother\\twords,\\teach\\t\\ns\\ni\\n\\taccumulates\\tthe\\tsquares\\tof\\tthe\\tpartial\\nderivative\\tof\\tthe\\t\\ncost\\tfunction\\twith\\tregards\\tto\\tparameter\\t\\nθ\\ni\\n.\\tIf\\tthe\\tcost\\tfunction\\tis\\tsteep\\talong\\tthe\\ti\\nth\\ndimension,\\tthen\\t\\ns\\ni\\n\\twill\\tget\\tlarger\\tand\\tlarger\\tat\\teach\\titeration.\\nThe\\tsecond\\tstep\\tis\\talmost\\tidentical\\tto\\tGradient\\tDescent,\\tbut\\twith\\tone\\tbig\\tdifference:\\tthe\\tgradient\\tvector\\nis\\tscaled\\tdown\\tby\\ta\\tfactor\\tof\\t\\n\\t(the\\t\\tsymbol\\trepresents\\tthe\\telement-wise\\tdivision,\\tand\\tϵ\\tis\\ta\\nsmoothing\\tterm\\tto\\tavoid\\tdivision\\tby\\tzero,\\ttypically\\tset\\tto\\t10\\n–10\\n).\\tThis\\tvectorized\\tform\\tis\\tequivalent\\tto\\ncomputing\\t\\n\\tfor\\tall\\tparameters\\t\\nθ\\ni\\n\\t(simultaneously).\\nIn\\tshort,\\tthis\\talgorithm\\tdecays\\tthe\\tlearning\\trate,\\tbut\\tit\\tdoes\\tso\\tfaster\\tfor\\tsteep\\tdimensions\\tthan\\tfor\\ndimensions\\twith\\tgentler\\tslopes.\\tThis\\tis\\tcalled\\tan\\t\\nadaptive\\tlearning\\trate\\n.\\t\\nIt\\thelps\\tpoint\\tthe\\tresulting\\nupdates\\tmore\\tdirectly\\ttoward\\tthe\\tglobal\\toptimum\\t(see\\t\\nFigure\\t11-7\\n).\\tOne\\tadditional\\tbenefit\\tis\\tthat\\tit\\nrequires\\tmuch\\tless\\ttuning\\tof\\tthe\\tlearning\\trate\\thyperparameter\\t\\nη\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 371}), Document(page_content='Figure\\t11-7.\\t\\nAdaGrad\\tversus\\tGradient\\tDescent\\nAdaGrad\\toften\\tperforms\\twell\\tfor\\tsimple\\tquadratic\\tproblems,\\tbut\\tunfortunately\\tit\\toften\\tstops\\ttoo\\tearly\\nwhen\\ttraining\\tneural\\tnetworks.\\tThe\\tlearning\\trate\\tgets\\tscaled\\tdown\\tso\\tmuch\\tthat\\tthe\\talgorithm\\tends\\tup\\nstopping\\tentirely\\tbefore\\treaching\\tthe\\tglobal\\toptimum.\\tSo\\teven\\tthough\\tTensorFlow\\thas\\tan\\nAdagradOptimizer\\n,\\tyou\\tshould\\tnot\\tuse\\tit\\tto\\ttrain\\tdeep\\tneural\\tnetworks\\t(it\\tmay\\tbe\\tefficient\\tfor\\tsimpler\\ntasks\\tsuch\\tas\\tLinear\\tRegression,\\t\\nthough).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 372}), Document(page_content='RMSProp\\nAlthough\\t\\nAdaGrad\\tslows\\tdown\\ta\\tbit\\ttoo\\tfast\\tand\\tends\\tup\\tnever\\tconverging\\tto\\tthe\\tglobal\\toptimum,\\tthe\\nRMSProp\\n\\talgorithm\\n14\\n\\tfixes\\tthis\\tby\\taccumulating\\tonly\\tthe\\tgradients\\tfrom\\tthe\\tmost\\trecent\\titerations\\t(as\\nopposed\\tto\\tall\\tthe\\tgradients\\tsince\\tthe\\tbeginning\\tof\\ttraining).\\tIt\\tdoes\\tso\\tby\\tusing\\texponential\\tdecay\\tin\\tthe\\nfirst\\tstep\\t(see\\t\\nEquation\\t11-7\\n).\\nEquation\\t11-7.\\t\\nRMSProp\\talgorithm\\nThe\\tdecay\\trate\\t\\nβ\\n\\tis\\ttypically\\tset\\tto\\t0.9.\\tYes,\\tit\\tis\\tonce\\tagain\\ta\\tnew\\thyperparameter,\\tbut\\tthis\\tdefault\\tvalue\\noften\\tworks\\twell,\\tso\\tyou\\tmay\\tnot\\tneed\\tto\\ttune\\tit\\tat\\tall.\\nAs\\tyou\\tmight\\texpect,\\tTensorFlow\\thas\\t\\nan\\t\\nRMSPropOptimizer\\n\\tclass:\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nRMSPropOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmomentum\\n=\\n0.9\\n,\\n\\t\\ndecay\\n=\\n0.9\\n,\\n\\t\\nepsilon\\n=\\n1e-10\\n)\\nExcept\\ton\\tvery\\tsimple\\tproblems,\\tthis\\toptimizer\\talmost\\talways\\tperforms\\tmuch\\tbetter\\tthan\\tAdaGrad.\\tIt\\nalso\\tgenerally\\tconverges\\tfaster\\tthan\\tMomentum\\toptimization\\tand\\tNesterov\\tAccelerated\\tGradients.\\tIn\\tfact,\\nit\\twas\\tthe\\tpreferred\\toptimization\\talgorithm\\tof\\tmany\\tresearchers\\tuntil\\tAdam\\toptimization\\tcame\\taround.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 373}), Document(page_content='Adam\\tOptimization\\nAdam\\n,\\n15\\n\\twhich\\t\\nstands\\tfor\\t\\nadaptive\\tmoment\\testimation\\n,\\t\\ncombines\\tthe\\tideas\\tof\\tMomentum\\toptimization\\nand\\tRMSProp:\\tjust\\tlike\\tMomentum\\toptimization\\tit\\tkeeps\\ttrack\\tof\\tan\\texponentially\\tdecaying\\taverage\\tof\\npast\\tgradients,\\tand\\tjust\\tlike\\tRMSProp\\tit\\tkeeps\\ttrack\\tof\\tan\\texponentially\\tdecaying\\taverage\\tof\\tpast\\tsquared\\ngradients\\t(see\\t\\nEquation\\t11-8\\n).\\n16\\nEquation\\t11-8.\\t\\nAdam\\talgorithm\\nT\\n\\trepresents\\tthe\\titeration\\tnumber\\t(starting\\tat\\t1).\\nIf\\tyou\\tjust\\tlook\\tat\\tsteps\\t1,\\t2,\\tand\\t5,\\tyou\\twill\\tnotice\\tAdam’s\\tclose\\tsimilarity\\tto\\tboth\\tMomentum\\noptimization\\tand\\tRMSProp.\\tThe\\tonly\\tdifference\\tis\\tthat\\tstep\\t1\\tcomputes\\tan\\texponentially\\tdecaying\\naverage\\trather\\tthan\\tan\\texponentially\\tdecaying\\tsum,\\tbut\\tthese\\tare\\tactually\\tequivalent\\texcept\\tfor\\ta\\tconstant\\nfactor\\t(the\\tdecaying\\taverage\\tis\\tjust\\t1\\t–\\t\\nβ\\n1\\n\\ttimes\\tthe\\tdecaying\\tsum).\\tSteps\\t3\\tand\\t4\\tare\\tsomewhat\\tof\\ta\\ntechnical\\tdetail:\\tsince\\t\\nm\\n\\tand\\t\\ns\\n\\tare\\tinitialized\\tat\\t0,\\tthey\\twill\\tbe\\tbiased\\ttoward\\t0\\tat\\tthe\\tbeginning\\tof\\ntraining,\\tso\\tthese\\ttwo\\tsteps\\twill\\thelp\\tboost\\t\\nm\\n\\tand\\t\\ns\\n\\tat\\tthe\\tbeginning\\tof\\ttraining.\\nThe\\tmomentum\\tdecay\\thyperparameter\\t\\nβ\\n1\\n\\tis\\ttypically\\tinitialized\\tto\\t0.9,\\twhile\\tthe\\tscaling\\tdecay\\nhyperparameter\\t\\nβ\\n2\\n\\tis\\toften\\tinitialized\\tto\\t0.999.\\tAs\\tearlier,\\tthe\\t\\nsmoothing\\tterm\\t\\nϵ\\n\\tis\\tusually\\tinitialized\\tto\\ta\\ntiny\\tnumber\\tsuch\\tas\\t10\\n–8\\n.\\tThese\\tare\\tthe\\tdefault\\tvalues\\tfor\\tTensorFlow’s\\t\\nAdamOptimizer\\n\\tclass,\\t\\nso\\tyou\\ncan\\tsimply\\tuse:\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n)\\nIn\\tfact,\\tsince\\tAdam\\tis\\tan\\tadaptive\\tlearning\\trate\\talgorithm\\t(like\\tAdaGrad\\tand\\tRMSProp),\\tit\\trequires\\tless\\ntuning\\tof\\tthe\\tlearning\\trate\\thyperparameter\\t\\nη\\n.\\tYou\\tcan\\toften\\tuse\\tthe\\tdefault\\tvalue\\t\\nη\\n\\t=\\t0.001,\\tmaking\\tAdam\\neven\\teasier\\tto\\tuse\\tthan\\tGradient\\tDescent.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 374}), Document(page_content='WARNING\\nThis\\tbook\\tinitially\\trecommended\\tusing\\t\\nAdam\\toptimization,\\tbecause\\tit\\twas\\tgenerally\\tconsidered\\tfaster\\tand\\tbetter\\tthan\\tother\\nmethods.\\tHowever,\\ta\\t\\n2017\\tpaper\\n17\\n\\tby\\tAshia\\tC.\\tWilson\\tet\\tal.\\tshowed\\tthat\\tadaptive\\toptimization\\tmethods\\t(i.e.,\\tAdaGrad,\\nRMSProp\\tand\\tAdam\\toptimization)\\tcan\\tlead\\tto\\tsolutions\\tthat\\tgeneralize\\tpoorly\\ton\\tsome\\tdatasets.\\tSo\\tyou\\tmay\\twant\\tto\\tstick\\tto\\nMomentum\\toptimization\\tor\\tNesterov\\tAccelerated\\tGradient\\tfor\\tnow,\\tuntil\\tresearchers\\thave\\ta\\tbetter\\tunderstanding\\tof\\tthis\\tissue.\\nAll\\tthe\\toptimization\\ttechniques\\tdiscussed\\tso\\tfar\\tonly\\trely\\ton\\tthe\\t\\nfirst-order\\tpartial\\tderivatives\\n(\\nJacobians\\n).\\tThe\\t\\noptimization\\tliterature\\tcontains\\tamazing\\talgorithms\\tbased\\ton\\tthe\\t\\nsecond-order\\tpartial\\nderivatives\\n\\t(the\\t\\nHessians\\n).\\t\\nUnfortunately,\\tthese\\talgorithms\\tare\\tvery\\thard\\tto\\tapply\\tto\\tdeep\\tneural\\tnetworks\\nbecause\\tthere\\tare\\t\\nn\\n2\\n\\tHessians\\tper\\toutput\\t(where\\t\\nn\\n\\tis\\tthe\\tnumber\\tof\\tparameters),\\tas\\topposed\\tto\\tjust\\t\\nn\\nJacobians\\tper\\toutput.\\tSince\\tDNNs\\ttypically\\thave\\ttens\\tof\\tthousands\\tof\\tparameters,\\tthe\\tsecond-order\\noptimization\\talgorithms\\toften\\tdon’t\\teven\\tfit\\tin\\tmemory,\\tand\\teven\\twhen\\tthey\\tdo,\\tcomputing\\tthe\\tHessians\\tis\\njust\\ttoo\\tslow.\\nTRAINING\\tSPARSE\\tMODELS\\nAll\\tthe\\t\\noptimization\\talgorithms\\tjust\\tpresented\\tproduce\\tdense\\tmodels,\\tmeaning\\tthat\\tmost\\tparameters\\twill\\tbe\\tnonzero.\\tIf\\tyou\\tneed\\ta\\nblazingly\\tfast\\tmodel\\tat\\truntime,\\tor\\tif\\tyou\\tneed\\tit\\tto\\ttake\\tup\\tless\\tmemory,\\tyou\\tmay\\tprefer\\tto\\tend\\tup\\twith\\ta\\tsparse\\tmodel\\tinstead.\\nOne\\ttrivial\\tway\\tto\\tachieve\\tthis\\tis\\tto\\ttrain\\tthe\\tmodel\\tas\\tusual,\\tthen\\tget\\trid\\tof\\tthe\\ttiny\\tweights\\t(set\\tthem\\tto\\t0).\\nAnother\\toption\\tis\\tto\\tapply\\tstrong\\tℓ\\n1\\n\\t\\nregularization\\tduring\\ttraining,\\tas\\tit\\tpushes\\tthe\\toptimizer\\tto\\tzero\\tout\\tas\\tmany\\tweights\\tas\\tit\\tcan\\t(as\\ndiscussed\\tin\\t\\nChapter\\t4\\n\\tabout\\tLasso\\tRegression).\\nHowever,\\tin\\tsome\\tcases\\tthese\\ttechniques\\tmay\\tremain\\tinsufficient.\\tOne\\tlast\\toption\\tis\\tto\\tapply\\t\\nDual\\tAveraging\\n,\\t\\noften\\tcalled\\t\\nFollow\\tThe\\nRegularized\\tLeader\\n\\t(FTRL),\\ta\\t\\ntechnique\\tproposed\\tby\\tYurii\\tNesterov\\n.\\n18\\n\\tWhen\\tused\\twith\\tℓ\\n1\\n\\tregularization,\\tthis\\ttechnique\\toften\\tleads\\tto\\nvery\\tsparse\\tmodels.\\tTensorFlow\\timplements\\ta\\tvariant\\tof\\tFTRL\\tcalled\\t\\nFTRL-Proximal\\n19\\n\\tin\\tthe\\t\\nFTRLOptimizer\\n\\tclass.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 375}), Document(page_content='Learning\\tRate\\tScheduling\\nFinding\\t\\na\\tgood\\tlearning\\trate\\tcan\\tbe\\ttricky.\\tIf\\tyou\\tset\\tit\\tway\\ttoo\\thigh,\\ttraining\\tmay\\tactually\\tdiverge\\t(as\\twe\\ndiscussed\\tin\\t\\nChapter\\t4\\n).\\tIf\\tyou\\tset\\tit\\ttoo\\tlow,\\ttraining\\twill\\teventually\\tconverge\\tto\\tthe\\toptimum,\\tbut\\tit\\twill\\ntake\\ta\\tvery\\tlong\\ttime.\\tIf\\tyou\\tset\\tit\\tslightly\\ttoo\\thigh,\\tit\\twill\\tmake\\tprogress\\tvery\\tquickly\\tat\\tfirst,\\tbut\\tit\\twill\\nend\\tup\\tdancing\\taround\\tthe\\toptimum,\\tnever\\tsettling\\tdown\\t(unless\\tyou\\tuse\\tan\\tadaptive\\tlearning\\trate\\noptimization\\talgorithm\\tsuch\\tas\\tAdaGrad,\\tRMSProp,\\tor\\tAdam,\\tbut\\teven\\tthen\\tit\\tmay\\ttake\\ttime\\tto\\tsettle).\\tIf\\nyou\\thave\\ta\\tlimited\\tcomputing\\tbudget,\\tyou\\tmay\\thave\\tto\\tinterrupt\\ttraining\\tbefore\\tit\\thas\\tconverged\\tproperly,\\nyielding\\ta\\tsuboptimal\\tsolution\\t(see\\t\\nFigure\\t11-8\\n).\\nFigure\\t11-8.\\t\\nLearning\\tcurves\\tfor\\tvarious\\tlearning\\trates\\tη\\nYou\\tmay\\tbe\\table\\tto\\tfind\\ta\\tfairly\\tgood\\tlearning\\trate\\tby\\ttraining\\tyour\\tnetwork\\tseveral\\ttimes\\tduring\\tjust\\ta\\nfew\\tepochs\\tusing\\tvarious\\tlearning\\trates\\tand\\tcomparing\\tthe\\tlearning\\tcurves.\\tThe\\tideal\\tlearning\\trate\\twill\\nlearn\\tquickly\\tand\\tconverge\\tto\\tgood\\tsolution.\\nHowever,\\tyou\\tcan\\tdo\\tbetter\\tthan\\ta\\tconstant\\tlearning\\trate:\\tif\\tyou\\tstart\\twith\\ta\\thigh\\tlearning\\trate\\tand\\tthen\\nreduce\\tit\\tonce\\tit\\tstops\\tmaking\\tfast\\tprogress,\\tyou\\tcan\\treach\\ta\\tgood\\tsolution\\tfaster\\tthan\\twith\\tthe\\toptimal\\nconstant\\tlearning\\trate.\\tThere\\tare\\tmany\\tdifferent\\tstrategies\\tto\\treduce\\tthe\\tlearning\\trate\\tduring\\ttraining.\\nThese\\tstrategies\\tare\\tcalled\\t\\nlearning\\tschedules\\n\\t(we\\tbriefly\\tintroduced\\tthis\\tconcept\\tin\\t\\nChapter\\t4\\n),\\tthe\\tmost\\ncommon\\tof\\twhich\\tare:\\nPredetermined\\tpiecewise\\tconstant\\tlearning\\trate\\nFor\\texample,\\tset\\tthe\\tlearning\\trate\\tto\\t\\nη\\n0\\n\\t=\\t0.1\\tat\\tfirst,\\tthen\\tto\\t\\nη\\n1\\n\\t=\\t0.001\\tafter\\t50\\tepochs.\\tAlthough\\nthis\\tsolution\\tcan\\twork\\tvery\\twell,\\tit\\toften\\trequires\\tfiddling\\taround\\tto\\tfigure\\tout\\tthe\\tright\\tlearning\\nrates\\tand\\twhen\\tto\\tuse\\tthem.\\nPerformance\\tscheduling\\nMeasure\\tthe\\tvalidation\\terror\\tevery\\t\\nN\\n\\tsteps\\t(just\\tlike\\tfor\\tearly\\tstopping)\\tand\\treduce\\tthe\\tlearning\\trate\\nby\\ta\\tfactor\\tof\\t\\nλ\\n\\twhen\\tthe\\terror\\tstops\\tdropping.\\nExponential\\tscheduling', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 376}), Document(page_content='Set\\tthe\\tlearning\\trate\\tto\\ta\\tfunction\\tof\\tthe\\titeration\\tnumber\\t\\nt\\n:\\t\\nη\\n(\\nt\\n)\\t=\\t\\nη\\n0\\n\\t10\\n–t/r\\n.\\tThis\\tworks\\tgreat,\\tbut\\tit\\nrequires\\ttuning\\t\\nη\\n0\\n\\tand\\t\\nr\\n.\\tThe\\tlearning\\trate\\twill\\tdrop\\tby\\ta\\tfactor\\tof\\t10\\tevery\\t\\nr\\n\\tsteps.\\nPower\\tscheduling\\nSet\\tthe\\tlearning\\trate\\tto\\t\\nη\\n(\\nt\\n)\\t=\\t\\nη\\n0\\n\\t(1\\t+\\t\\nt\\n/\\nr\\n)\\n–c\\n.\\tThe\\thyperparameter\\t\\nc\\n\\tis\\ttypically\\tset\\tto\\t1.\\tThis\\tis\\nsimilar\\tto\\texponential\\tscheduling,\\tbut\\tthe\\tlearning\\trate\\tdrops\\tmuch\\tmore\\tslowly.\\nA\\t\\n2013\\tpaper\\n20\\n\\tby\\tAndrew\\tSenior\\tet\\tal.\\tcompared\\tthe\\tperformance\\tof\\tsome\\tof\\tthe\\tmost\\tpopular\\tlearning\\nschedules\\twhen\\ttraining\\tdeep\\tneural\\tnetworks\\tfor\\tspeech\\trecognition\\tusing\\tMomentum\\toptimization.\\tThe\\nauthors\\tconcluded\\tthat,\\tin\\tthis\\tsetting,\\tboth\\tperformance\\tscheduling\\tand\\texponential\\tscheduling\\tperformed\\nwell,\\tbut\\tthey\\tfavored\\texponential\\tscheduling\\tbecause\\tit\\tis\\tsimpler\\tto\\timplement,\\tis\\teasy\\tto\\ttune,\\tand\\nconverged\\tslightly\\tfaster\\tto\\tthe\\toptimal\\tsolution.\\nImplementing\\ta\\t\\nlearning\\tschedule\\twith\\tTensorFlow\\tis\\tfairly\\tstraightforward:\\ninitial_learning_rate\\n\\t\\n=\\n\\t\\n0.1\\ndecay_steps\\n\\t\\n=\\n\\t\\n10000\\ndecay_rate\\n\\t\\n=\\n\\t\\n1\\n/\\n10\\nglobal_step\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0\\n,\\n\\t\\ntrainable\\n=\\nFalse\\n,\\n\\t\\nname\\n=\\n\"global_step\"\\n)\\nlearning_rate\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nexponential_decay\\n(\\ninitial_learning_rate\\n,\\n\\t\\nglobal_step\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ndecay_steps\\n,\\n\\t\\ndecay_rate\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nMomentumOptimizer\\n(\\nlearning_rate\\n,\\n\\t\\nmomentum\\n=\\n0.9\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n,\\n\\t\\nglobal_step\\n=\\nglobal_step\\n)\\nAfter\\tsetting\\tthe\\thyperparameter\\tvalues,\\twe\\tcreate\\ta\\tnontrainable\\tvariable\\t\\nglobal_step\\n\\t(initialized\\tto\\t0)\\nto\\tkeep\\ttrack\\tof\\tthe\\tcurrent\\ttraining\\t\\niteration\\tnumber.\\tThen\\twe\\tdefine\\tan\\texponentially\\tdecaying\\tlearning\\nrate\\t(with\\t\\nη\\n0\\n\\t=\\t0.1\\tand\\t\\nr\\n\\t=\\t10,000)\\tusing\\tTensorFlow’s\\t\\nexponential_decay()\\n\\tfunction.\\tNext,\\twe\\tcreate\\nan\\toptimizer\\t(in\\tthis\\texample,\\ta\\t\\nMomentumOptimizer\\n)\\tusing\\tthis\\tdecaying\\tlearning\\trate.\\tFinally,\\twe\\ncreate\\tthe\\ttraining\\toperation\\tby\\tcalling\\tthe\\toptimizer’s\\t\\nminimize()\\n\\tmethod;\\tsince\\twe\\tpass\\tit\\tthe\\nglobal_step\\n\\tvariable,\\tit\\twill\\tkindly\\ttake\\tcare\\tof\\t\\nincrementing\\tit.\\tThat’s\\tit!\\nSince\\tAdaGrad,\\tRMSProp,\\tand\\tAdam\\toptimization\\tautomatically\\treduce\\tthe\\tlearning\\trate\\tduring\\ttraining,\\nit\\tis\\tnot\\tnecessary\\tto\\tadd\\tan\\textra\\tlearning\\tschedule.\\tFor\\tother\\toptimization\\talgorithms,\\tusing\\texponential\\ndecay\\tor\\tperformance\\tscheduling\\tcan\\tconsiderably\\tspeed\\t\\nup\\tconvergence.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 377}), Document(page_content='Avoiding\\tOverfitting\\tThrough\\tRegularization\\nWith\\tfour\\tparameters\\tI\\tcan\\tfit\\tan\\telephant\\tand\\twith\\tfive\\tI\\tcan\\tmake\\thim\\twiggle\\this\\ttrunk.\\nJohn\\tvon\\tNeumann,\\t\\ncited\\tby\\tEnrico\\tFermi\\tin\\tNature\\t427\\nDeep\\tneural\\tnetworks\\t\\ntypically\\thave\\ttens\\tof\\tthousands\\tof\\tparameters,\\tsometimes\\teven\\tmillions.\\tWith\\tso\\nmany\\tparameters,\\tthe\\tnetwork\\thas\\tan\\tincredible\\tamount\\tof\\tfreedom\\tand\\tcan\\tfit\\ta\\thuge\\tvariety\\tof\\tcomplex\\ndatasets.\\tBut\\tthis\\tgreat\\tflexibility\\talso\\tmeans\\tthat\\tit\\tis\\tprone\\tto\\toverfitting\\tthe\\ttraining\\tset.\\nWith\\tmillions\\tof\\tparameters\\tyou\\tcan\\tfit\\tthe\\twhole\\tzoo.\\tIn\\tthis\\tsection\\twe\\twill\\tpresent\\tsome\\tof\\tthe\\tmost\\npopular\\tregularization\\ttechniques\\tfor\\tneural\\tnetworks,\\tand\\thow\\tto\\timplement\\tthem\\twith\\tTensorFlow:\\nearly\\tstopping,\\tℓ\\n1\\n\\tand\\tℓ\\n2\\n\\t\\nregularization,\\tdropout,\\tmax-norm\\tregularization,\\tand\\tdata\\taugmentation.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 378}), Document(page_content='Early\\tStopping\\nTo\\t\\navoid\\toverfitting\\tthe\\ttraining\\tset,\\ta\\tgreat\\tsolution\\tis\\tearly\\tstopping\\t(introduced\\tin\\t\\nChapter\\t4\\n):\\tjust\\ninterrupt\\ttraining\\twhen\\tits\\tperformance\\ton\\tthe\\tvalidation\\tset\\tstarts\\tdropping.\\nOne\\tway\\tto\\timplement\\tthis\\twith\\tTensorFlow\\tis\\tto\\tevaluate\\tthe\\tmodel\\ton\\ta\\tvalidation\\tset\\tat\\tregular\\nintervals\\t(e.g.,\\tevery\\t50\\tsteps),\\tand\\tsave\\ta\\t“winner”\\tsnapshot\\tif\\tit\\toutperforms\\tprevious\\t“winner”\\nsnapshots.\\tCount\\tthe\\tnumber\\tof\\tsteps\\tsince\\tthe\\tlast\\t“winner”\\tsnapshot\\twas\\tsaved,\\tand\\tinterrupt\\ttraining\\nwhen\\tthis\\tnumber\\treaches\\tsome\\tlimit\\t(e.g.,\\t2,000\\tsteps).\\tThen\\trestore\\tthe\\tlast\\t“winner”\\tsnapshot.\\nAlthough\\tearly\\tstopping\\tworks\\tvery\\twell\\tin\\tpractice,\\tyou\\tcan\\tusually\\tget\\tmuch\\thigher\\tperformance\\tout\\tof\\nyour\\tnetwork\\tby\\tcombining\\tit\\twith\\tother\\tregularization\\ttechniques.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 379}), Document(page_content='ℓ\\n1\\n\\tand\\tℓ\\n2\\n\\tRegularization\\nJust\\t\\nlike\\tyou\\tdid\\tin\\t\\nChapter\\t4\\n\\tfor\\tsimple\\tlinear\\tmodels,\\tyou\\tcan\\tuse\\tℓ\\n1\\n\\tand\\tℓ\\n2\\n\\tregularization\\tto\\tconstrain\\ta\\nneural\\tnetwork’s\\tconnection\\tweights\\t(but\\ttypically\\tnot\\tits\\tbiases).\\nOne\\tway\\tto\\tdo\\tthis\\tusing\\tTensorFlow\\t\\nis\\tto\\tsimply\\tadd\\tthe\\tappropriate\\tregularization\\tterms\\tto\\tyour\\tcost\\nfunction.\\tFor\\texample,\\tassuming\\tyou\\thave\\tjust\\tone\\thidden\\tlayer\\twith\\tweights\\t\\nW1\\n\\tand\\tone\\toutput\\tlayer\\twith\\nweights\\t\\nW2\\n,\\tthen\\tyou\\tcan\\tapply\\tℓ\\n1\\n\\tregularization\\t\\nlike\\tthis:\\n[\\n...\\n]\\n\\t\\n#\\tconstruct\\tthe\\tneural\\tnetwork\\nW1\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"hidden1/kernel:0\"\\n)\\nW2\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"outputs/kernel:0\"\\n)\\nscale\\n\\t\\n=\\n\\t\\n0.001\\n\\t\\n#\\tl1\\tregularization\\thyperparameter\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"loss\"\\n):\\n\\t\\t\\t\\t\\nxentropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsparse_softmax_cross_entropy_with_logits\\n(\\nlabels\\n=\\ny\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlogits\\n=\\nlogits\\n)\\n\\t\\t\\t\\t\\nbase_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\nxentropy\\n,\\n\\t\\nname\\n=\\n\"avg_xentropy\"\\n)\\n\\t\\t\\t\\t\\nreg_losses\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\ntf\\n.\\nabs\\n(\\nW1\\n))\\n\\t\\n+\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\ntf\\n.\\nabs\\n(\\nW2\\n))\\n\\t\\t\\t\\t\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd\\n(\\nbase_loss\\n,\\n\\t\\nscale\\n\\t\\n*\\n\\t\\nreg_losses\\n,\\n\\t\\nname\\n=\\n\"loss\"\\n)\\nHowever,\\tif\\tthere\\tare\\tmany\\tlayers,\\tthis\\tapproach\\tis\\tnot\\tvery\\tconvenient.\\tFortunately,\\tTensorFlow\\nprovides\\ta\\tbetter\\toption.\\tMany\\tfunctions\\tthat\\tcreate\\tvariables\\t(such\\tas\\t\\nget_variable()\\n\\tor\\ntf.layers.dense()\\n)\\taccept\\ta\\t\\n*_regularizer\\n\\targument\\tfor\\teach\\tcreated\\tvariable\\t(e.g.,\\nkernel_regularizer\\n).\\tYou\\tcan\\tpass\\tany\\tfunction\\tthat\\ttakes\\tweights\\tas\\tan\\targument\\tand\\treturns\\tthe\\ncorresponding\\tregularization\\tloss.\\tThe\\t\\nl1_regularizer()\\n,\\t\\nl2_regularizer()\\n,\\tand\\nl1_l2_regularizer()\\n\\tfunctions\\treturn\\t\\nsuch\\tfunctions.\\tThe\\tfollowing\\tcode\\tputs\\tall\\tthis\\ttogether:\\nmy_dense_layer\\n\\t\\n=\\n\\t\\npartial\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\nkernel_regularizer\\n=\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nl1_regularizer\\n(\\nscale\\n))\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):\\n\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n)\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden2\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nactivation\\n=\\nNone\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"outputs\"\\n)\\nThis\\tcode\\tcreates\\ta\\tneural\\tnetwork\\twith\\ttwo\\thidden\\tlayers\\tand\\tone\\toutput\\tlayer,\\tand\\tit\\talso\\tcreates\\tnodes\\nin\\tthe\\tgraph\\tto\\tcompute\\tthe\\tℓ\\n1\\n\\tregularization\\tloss\\tcorresponding\\tto\\teach\\tlayer’s\\tweights.\\tTensorFlow\\nautomatically\\tadds\\tthese\\tnodes\\tto\\ta\\tspecial\\tcollection\\tcontaining\\tall\\tthe\\tregularization\\t\\nlosses.\\tYou\\tjust\\nneed\\tto\\tadd\\tthese\\tregularization\\tlosses\\tto\\tyour\\toverall\\tloss,\\tlike\\tthis:\\nreg_losses\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nREGULARIZATION_LOSSES\\n)\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n([\\nbase_loss\\n]\\n\\t\\n+\\n\\t\\nreg_losses\\n,\\n\\t\\nname\\n=\\n\"loss\"\\n)\\nWARNING\\nDon’t\\tforget\\tto\\tadd\\tthe\\tregularization\\tlosses\\tto\\tyour\\toverall\\tloss,\\tor\\telse\\tthey\\twill\\tsimply\\tbe\\t\\nignored.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 380}), Document(page_content='Dropout\\nThe\\t\\nmost\\tpopular\\tregularization\\ttechnique\\tfor\\tdeep\\tneural\\tnetworks\\tis\\targuably\\t\\ndropout\\n.\\tIt\\twas\\nproposed\\n21\\n\\tby\\tG.\\tE.\\tHinton\\tin\\t2012\\tand\\tfurther\\tdetailed\\tin\\ta\\t\\npaper\\n22\\n\\tby\\tNitish\\tSrivastava\\tet\\tal.,\\tand\\tit\\thas\\nproven\\tto\\tbe\\thighly\\tsuccessful:\\teven\\tthe\\tstate-of-the-art\\tneural\\tnetworks\\tgot\\ta\\t1–2%\\taccuracy\\tboost\\nsimply\\tby\\tadding\\tdropout.\\tThis\\tmay\\tnot\\tsound\\tlike\\ta\\tlot,\\tbut\\twhen\\ta\\tmodel\\talready\\thas\\t95%\\taccuracy,\\ngetting\\ta\\t2%\\taccuracy\\tboost\\tmeans\\tdropping\\tthe\\terror\\trate\\tby\\talmost\\t40%\\t(going\\tfrom\\t5%\\terror\\tto\\nroughly\\t3%).\\nIt\\tis\\ta\\tfairly\\tsimple\\talgorithm:\\tat\\tevery\\ttraining\\tstep,\\tevery\\tneuron\\t(including\\tthe\\tinput\\tneurons\\tbut\\nexcluding\\tthe\\toutput\\tneurons)\\thas\\ta\\tprobability\\t\\np\\n\\tof\\tbeing\\ttemporarily\\t“dropped\\tout,”\\tmeaning\\tit\\twill\\tbe\\nentirely\\tignored\\tduring\\tthis\\ttraining\\tstep,\\tbut\\tit\\tmay\\tbe\\tactive\\tduring\\tthe\\tnext\\tstep\\t(see\\t\\nFigure\\t11-9\\n).\\tThe\\nhyperparameter\\t\\np\\n\\tis\\tcalled\\tthe\\t\\ndropout\\trate\\n,\\t\\nand\\tit\\tis\\ttypically\\tset\\tto\\t50%.\\tAfter\\ttraining,\\tneurons\\tdon’t\\nget\\tdropped\\tanymore.\\tAnd\\tthat’s\\tall\\t(except\\tfor\\ta\\ttechnical\\tdetail\\twe\\twill\\tdiscuss\\tmomentarily).\\nFigure\\t11-9.\\t\\nDropout\\tregularization\\nIt\\tis\\tquite\\tsurprising\\tat\\tfirst\\tthat\\tthis\\trather\\tbrutal\\ttechnique\\tworks\\tat\\tall.\\tWould\\ta\\tcompany\\tperform\\tbetter\\nif\\tits\\temployees\\twere\\ttold\\tto\\ttoss\\ta\\tcoin\\tevery\\tmorning\\tto\\tdecide\\twhether\\tor\\tnot\\tto\\tgo\\tto\\twork?\\tWell,\\twho\\nknows;\\tperhaps\\tit\\twould!\\tThe\\tcompany\\twould\\tobviously\\tbe\\tforced\\tto\\tadapt\\tits\\torganization;\\tit\\tcould\\tnot\\nrely\\ton\\tany\\tsingle\\tperson\\tto\\tfill\\tin\\tthe\\tcoffee\\tmachine\\tor\\tperform\\tany\\tother\\tcritical\\ttasks,\\tso\\tthis\\texpertise\\nwould\\thave\\tto\\tbe\\tspread\\tacross\\tseveral\\tpeople.\\tEmployees\\twould\\thave\\tto\\tlearn\\tto\\tcooperate\\twith\\tmany\\nof\\ttheir\\tcoworkers,\\tnot\\tjust\\ta\\thandful\\tof\\tthem.\\tThe\\tcompany\\twould\\tbecome\\tmuch\\tmore\\tresilient.\\tIf\\tone\\nperson\\tquit,\\tit\\twouldn’t\\tmake\\tmuch\\tof\\ta\\tdifference.\\tIt’s\\tunclear\\twhether\\tthis\\tidea\\twould\\tactually\\twork\\tfor\\ncompanies,\\tbut\\tit\\tcertainly\\tdoes\\tfor\\tneural\\tnetworks.\\tNeurons\\ttrained\\twith\\tdropout\\tcannot\\tco-adapt\\twith\\ntheir\\tneighboring\\tneurons;\\tthey\\thave\\tto\\tbe\\tas\\tuseful\\tas\\tpossible\\ton\\ttheir\\town.\\tThey\\talso\\tcannot\\trely', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 381}), Document(page_content='excessively\\ton\\tjust\\ta\\tfew\\tinput\\tneurons;\\tthey\\tmust\\tpay\\tattention\\tto\\teach\\tof\\ttheir\\tinput\\tneurons.\\tThey\\tend\\tup\\nbeing\\tless\\tsensitive\\tto\\tslight\\tchanges\\tin\\tthe\\tinputs.\\tIn\\tthe\\tend\\tyou\\tget\\ta\\tmore\\trobust\\tnetwork\\tthat\\ngeneralizes\\tbetter.\\nAnother\\tway\\tto\\tunderstand\\tthe\\tpower\\tof\\tdropout\\tis\\tto\\trealize\\tthat\\ta\\tunique\\tneural\\tnetwork\\tis\\tgenerated\\tat\\neach\\ttraining\\tstep.\\tSince\\teach\\tneuron\\tcan\\tbe\\teither\\tpresent\\tor\\tabsent,\\tthere\\tis\\ta\\ttotal\\tof\\t2\\nN\\n\\tpossible\\nnetworks\\t(where\\t\\nN\\n\\tis\\tthe\\ttotal\\tnumber\\tof\\tdroppable\\tneurons).\\tThis\\tis\\tsuch\\ta\\thuge\\tnumber\\tthat\\tit\\tis\\nvirtually\\timpossible\\tfor\\tthe\\tsame\\tneural\\tnetwork\\tto\\tbe\\tsampled\\ttwice.\\tOnce\\tyou\\thave\\trun\\ta\\t10,000\\ntraining\\tsteps,\\tyou\\thave\\tessentially\\ttrained\\t10,000\\tdifferent\\tneural\\tnetworks\\t(each\\twith\\tjust\\tone\\ttraining\\ninstance).\\tThese\\tneural\\tnetworks\\tare\\tobviously\\tnot\\tindependent\\tsince\\tthey\\tshare\\tmany\\tof\\ttheir\\tweights,\\nbut\\tthey\\tare\\tnevertheless\\tall\\tdifferent.\\tThe\\tresulting\\tneural\\tnetwork\\tcan\\tbe\\tseen\\tas\\tan\\taveraging\\tensemble\\nof\\tall\\tthese\\tsmaller\\tneural\\tnetworks.\\nThere\\tis\\tone\\tsmall\\tbut\\timportant\\ttechnical\\tdetail.\\tSuppose\\t\\np\\n\\t=\\t50%,\\tin\\twhich\\tcase\\tduring\\ttesting\\ta\\tneuron\\nwill\\tbe\\tconnected\\tto\\ttwice\\tas\\tmany\\tinput\\tneurons\\tas\\tit\\twas\\t(on\\taverage)\\tduring\\ttraining.\\tTo\\tcompensate\\nfor\\tthis\\tfact,\\twe\\tneed\\tto\\tmultiply\\teach\\tneuron’s\\tinput\\tconnection\\tweights\\tby\\t0.5\\tafter\\ttraining.\\tIf\\twe\\tdon’t,\\neach\\tneuron\\twill\\tget\\ta\\ttotal\\tinput\\tsignal\\troughly\\ttwice\\tas\\tlarge\\tas\\twhat\\tthe\\tnetwork\\twas\\ttrained\\ton,\\tand\\tit\\nis\\tunlikely\\tto\\tperform\\twell.\\tMore\\tgenerally,\\twe\\tneed\\tto\\tmultiply\\teach\\tinput\\tconnection\\tweight\\tby\\t\\nthe\\t\\nkeep\\nprobability\\n\\t(1\\t–\\t\\np\\n)\\tafter\\ttraining.\\tAlternatively,\\twe\\tcan\\tdivide\\teach\\tneuron’s\\toutput\\tby\\tthe\\tkeep\\nprobability\\tduring\\ttraining\\t(these\\talternatives\\tare\\tnot\\tperfectly\\tequivalent,\\tbut\\tthey\\twork\\tequally\\twell).\\nTo\\timplement\\tdropout\\tusing\\t\\nTensorFlow,\\tyou\\tcan\\tsimply\\tapply\\tthe\\t\\ntf.layers.dropout()\\n\\t\\nfunction\\tto\\tthe\\ninput\\tlayer\\tand/or\\tto\\tthe\\toutput\\tof\\tany\\thidden\\tlayer\\tyou\\twant.\\tDuring\\ttraining,\\tthis\\tfunction\\trandomly\\ndrops\\tsome\\titems\\t(setting\\tthem\\tto\\t0)\\tand\\tdivides\\tthe\\tremaining\\titems\\tby\\tthe\\tkeep\\tprobability.\\tAfter\\ntraining,\\tthis\\tfunction\\tdoes\\tnothing\\tat\\tall.\\tThe\\tfollowing\\tcode\\tapplies\\tdropout\\tregularization\\tto\\tour\\tthree-\\nlayer\\t\\nneural\\tnetwork:\\n[\\n...\\n]\\ntraining\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder_with_default\\n(\\nFalse\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\nname\\n=\\n\\'training\\'\\n)\\ndropout_rate\\n\\t\\n=\\n\\t\\n0.5\\n\\t\\t\\n#\\t==\\t1\\t-\\tkeep_prob\\nX_drop\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndropout\\n(\\nX\\n,\\n\\t\\ndropout_rate\\n,\\n\\t\\ntraining\\n=\\ntraining\\n)\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):\\n\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX_drop\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden1\"\\n)\\n\\t\\t\\t\\t\\nhidden1_drop\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndropout\\n(\\nhidden1\\n,\\n\\t\\ndropout_rate\\n,\\n\\t\\ntraining\\n=\\ntraining\\n)\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden1_drop\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden2\"\\n)\\n\\t\\t\\t\\t\\nhidden2_drop\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndropout\\n(\\nhidden2\\n,\\n\\t\\ndropout_rate\\n,\\n\\t\\ntraining\\n=\\ntraining\\n)\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden2_drop\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\nWARNING\\nYou\\twant\\tto\\tuse\\tthe\\t\\ntf.layers.dropout()\\n\\tfunction,\\tnot\\t\\ntf.nn.dropout()\\n.\\tThe\\tfirst\\tone\\tturns\\toff\\t(no-op)\\twhen\\tnot\\ttraining,\\nwhich\\tis\\twhat\\tyou\\twant,\\twhile\\tthe\\tsecond\\tone\\tdoes\\tnot.\\nOf\\tcourse,\\tjust\\tlike\\tyou\\tdid\\tearlier\\tfor\\tBatch\\tNormalization,\\tyou\\tneed\\tto\\tset\\t\\ntraining\\n\\tto\\t\\nTrue\\n\\twhen\\ntraining,\\tand\\tleave\\tthe\\tdefault\\t\\nFalse\\n\\tvalue\\twhen\\ttesting.\\nIf\\tyou\\tobserve\\tthat\\tthe\\tmodel\\tis\\toverfitting,\\tyou\\tcan\\tincrease\\tthe\\tdropout\\trate.\\tConversely,\\tyou\\tshould\\ttry', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 382}), Document(page_content='decreasing\\tthe\\tdropout\\trate\\tif\\tthe\\tmodel\\tunderfits\\tthe\\ttraining\\tset.\\tIt\\tcan\\talso\\thelp\\tto\\tincrease\\tthe\\tdropout\\nrate\\tfor\\tlarge\\tlayers,\\tand\\treduce\\tit\\tfor\\tsmall\\tones.\\nDropout\\tdoes\\ttend\\tto\\tsignificantly\\tslow\\tdown\\tconvergence,\\tbut\\tit\\tusually\\tresults\\tin\\ta\\tmuch\\tbetter\\tmodel\\nwhen\\ttuned\\tproperly.\\tSo,\\tit\\tis\\tgenerally\\twell\\tworth\\tthe\\textra\\ttime\\tand\\teffort.\\nNOTE\\nDropconnect\\n\\t\\nis\\ta\\tvariant\\tof\\tdropout\\twhere\\tindividual\\tconnections\\tare\\tdropped\\trandomly\\trather\\tthan\\twhole\\tneurons.\\tIn\\tgeneral\\ndropout\\tperforms\\t\\nbetter.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 383}), Document(page_content='Max-Norm\\tRegularization\\nAnother\\t\\nregularization\\ttechnique\\tthat\\tis\\tquite\\tpopular\\tfor\\tneural\\tnetworks\\tis\\tcalled\\t\\nmax-norm\\nregularization\\n:\\tfor\\teach\\tneuron,\\tit\\tconstrains\\tthe\\tweights\\t\\nw\\n\\tof\\tthe\\tincoming\\tconnections\\tsuch\\tthat\\t\\t\\nw\\n\\t\\n2\\n\\t≤\\nr\\n,\\twhere\\t\\nr\\n\\tis\\tthe\\tmax-norm\\thyperparameter\\tand\\t\\n\\t·\\t\\n2\\n\\tis\\tthe\\n\\tℓ\\n2\\n\\tnorm.\\nWe\\ttypically\\timplement\\tthis\\tconstraint\\tby\\tcomputing\\t\\nw\\n2\\n\\tafter\\teach\\ttraining\\tstep\\tand\\tclipping\\t\\nw\\n\\tif\\nneeded\\t(\\n).\\nReducing\\t\\nr\\n\\tincreases\\tthe\\tamount\\tof\\tregularization\\tand\\thelps\\treduce\\toverfitting.\\tMax-norm\\tregularization\\ncan\\talso\\thelp\\talleviate\\tthe\\tvanishing/exploding\\tgradients\\tproblems\\t(if\\tyou\\tare\\tnot\\tusing\\tBatch\\nNormalization).\\nTensorFlow\\t\\ndoes\\tnot\\tprovide\\tan\\toff-the-shelf\\tmax-norm\\tregularizer,\\tbut\\tit\\tis\\tnot\\ttoo\\thard\\tto\\timplement.\\nThe\\tfollowing\\tcode\\tgets\\ta\\thandle\\ton\\tthe\\tweights\\tof\\tthe\\tfirst\\thidden\\tlayer,\\tthen\\tit\\tuses\\tthe\\nclip_by_norm()\\n\\tfunction\\t\\nto\\tcreate\\tan\\toperation\\tthat\\twill\\tclip\\tthe\\tweights\\talong\\tthe\\tsecond\\taxis\\tso\\tthat\\neach\\trow\\tvector\\tends\\tup\\twith\\ta\\tmaximum\\tnorm\\tof\\t1.0.\\tThe\\tlast\\tline\\tcreates\\tan\\tassignment\\toperation\\tthat\\nwill\\tassign\\tthe\\tclipped\\tweights\\tto\\tthe\\tweights\\tvariable:\\nthreshold\\n\\t\\n=\\n\\t\\n1.0\\nweights\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"hidden1/kernel:0\"\\n)\\nclipped_weights\\n\\t\\n=\\n\\t\\ntf\\n.\\nclip_by_norm\\n(\\nweights\\n,\\n\\t\\nclip_norm\\n=\\nthreshold\\n,\\n\\t\\naxes\\n=\\n1\\n)\\nclip_weights\\n\\t\\n=\\n\\t\\ntf\\n.\\nassign\\n(\\nweights\\n,\\n\\t\\nclipped_weights\\n)\\nThen\\tyou\\tjust\\tapply\\tthis\\toperation\\tafter\\teach\\ttraining\\tstep,\\tlike\\tso:\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\nclip_weights\\n.\\neval\\n()\\nIn\\tgeneral,\\tyou\\twould\\tdo\\tthis\\tfor\\tevery\\thidden\\tlayer.\\tAlthough\\tthis\\tsolution\\tshould\\twork\\tfine,\\tit\\tis\\ta\\tbit\\nmessy.\\tA\\tcleaner\\tsolution\\tis\\tto\\t\\ncreate\\ta\\t\\nmax_norm_regularizer()\\n\\tfunction\\t\\nand\\tuse\\tit\\tjust\\tlike\\tthe\\tearlier\\nl1_regularizer()\\n\\tfunction:\\ndef\\n\\t\\nmax_norm_regularizer\\n(\\nthreshold\\n,\\n\\t\\naxes\\n=\\n1\\n,\\n\\t\\nname\\n=\\n\"max_norm\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncollection\\n=\\n\"max_norm\"\\n):\\n\\t\\t\\t\\t\\ndef\\n\\t\\nmax_norm\\n(\\nweights\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nclipped\\n\\t\\n=\\n\\t\\ntf\\n.\\nclip_by_norm\\n(\\nweights\\n,\\n\\t\\nclip_norm\\n=\\nthreshold\\n,\\n\\t\\naxes\\n=\\naxes\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nclip_weights\\n\\t\\n=\\n\\t\\ntf\\n.\\nassign\\n(\\nweights\\n,\\n\\t\\nclipped\\n,\\n\\t\\nname\\n=\\nname\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntf\\n.\\nadd_to_collection\\n(\\ncollection\\n,\\n\\t\\nclip_weights\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nNone\\n\\t\\t\\n#\\tthere\\tis\\tno\\tregularization\\tloss\\tterm\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nmax_norm\\nThis\\tfunction\\treturns\\ta\\tparametrized\\t\\nmax_norm()\\n\\t\\nfunction\\tthat\\tyou\\tcan\\tuse\\tlike\\tany\\tother\\tregularizer:\\nmax_norm_reg\\n\\t\\n=\\n\\t\\nmax_norm_regularizer\\n(\\nthreshold\\n=\\n1.0\\n)\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):\\n\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_regularizer\\n=\\nmax_norm_reg\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_regularizer\\n=\\nmax_norm_reg\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n)\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden2\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 384}), Document(page_content='Note\\tthat\\tmax-norm\\tregularization\\tdoes\\tnot\\trequire\\tadding\\ta\\tregularization\\tloss\\tterm\\tto\\tyour\\toverall\\tloss\\nfunction,\\twhich\\tis\\twhy\\tthe\\t\\nmax_norm()\\n\\tfunction\\treturns\\t\\nNone\\n.\\tBut\\tyou\\tstill\\tneed\\tto\\tbe\\table\\tto\\trun\\tthe\\nclip_weights\\n\\toperations\\tafter\\teach\\ttraining\\tstep,\\tso\\tyou\\tneed\\tto\\tbe\\table\\tto\\tget\\ta\\thandle\\ton\\tthem.\\tThis\\tis\\nwhy\\tthe\\t\\nmax_norm()\\n\\tfunction\\tadds\\tthe\\t\\nclip_weights\\n\\toperation\\tto\\ta\\tcollection\\tof\\tmax-norm\\tclipping\\noperations.\\tYou\\tneed\\tto\\tfetch\\tthese\\tclipping\\toperations\\tand\\trun\\tthem\\tafter\\t\\neach\\ttraining\\tstep:\\nclip_all_weights\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\n\"max_norm\"\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nclip_all_weights\\n)\\nMuch\\n\\tcleaner\\tcode,\\tisn’t\\t\\nit?', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 385}), Document(page_content='Data\\tAugmentation\\nOne\\t\\nlast\\tregularization\\ttechnique,\\tdata\\taugmentation,\\tconsists\\tof\\tgenerating\\tnew\\ttraining\\tinstances\\tfrom\\nexisting\\tones,\\tartificially\\tboosting\\tthe\\tsize\\tof\\tthe\\ttraining\\tset.\\tThis\\twill\\treduce\\toverfitting,\\tmaking\\tthis\\ta\\nregularization\\ttechnique.\\tThe\\ttrick\\tis\\tto\\tgenerate\\trealistic\\ttraining\\tinstances;\\tideally,\\ta\\thuman\\tshould\\tnot\\nbe\\table\\tto\\ttell\\twhich\\tinstances\\twere\\tgenerated\\tand\\twhich\\tones\\twere\\tnot.\\tMoreover,\\tsimply\\tadding\\twhite\\nnoise\\twill\\tnot\\thelp;\\tthe\\tmodifications\\tyou\\tapply\\tshould\\tbe\\tlearnable\\t(white\\tnoise\\tis\\tnot).\\nFor\\texample,\\tif\\tyour\\tmodel\\tis\\tmeant\\tto\\tclassify\\tpictures\\tof\\tmushrooms,\\tyou\\tcan\\tslightly\\tshift,\\trotate,\\tand\\nresize\\tevery\\tpicture\\tin\\tthe\\ttraining\\tset\\tby\\tvarious\\tamounts\\tand\\tadd\\tthe\\tresulting\\tpictures\\tto\\tthe\\ttraining\\tset\\n(see\\t\\nFigure\\t11-10\\n).\\tThis\\tforces\\tthe\\tmodel\\tto\\tbe\\tmore\\ttolerant\\tto\\tthe\\tposition,\\torientation,\\tand\\tsize\\tof\\tthe\\nmushrooms\\tin\\tthe\\tpicture.\\tIf\\tyou\\twant\\tthe\\tmodel\\tto\\tbe\\tmore\\ttolerant\\tto\\tlighting\\tconditions,\\tyou\\tcan\\nsimilarly\\tgenerate\\tmany\\timages\\twith\\tvarious\\tcontrasts.\\tAssuming\\tthe\\tmushrooms\\tare\\tsymmetrical,\\tyou\\ncan\\talso\\tflip\\tthe\\tpictures\\thorizontally.\\tBy\\tcombining\\tthese\\ttransformations\\tyou\\tcan\\tgreatly\\tincrease\\tthe\\nsize\\tof\\tyour\\ttraining\\tset.\\nFigure\\t11-10.\\t\\nGenerating\\tnew\\ttraining\\tinstances\\tfrom\\texisting\\tones\\nIt\\tis\\toften\\tpreferable\\tto\\tgenerate\\ttraining\\tinstances\\ton\\tthe\\tfly\\tduring\\ttraining\\trather\\tthan\\twasting\\tstorage\\nspace\\tand\\tnetwork\\tbandwidth.\\tTensorFlow\\toffers\\tseveral\\timage\\tmanipulation\\toperations\\tsuch\\tas\\ntransposing\\t(shifting),\\trotating,\\tresizing,\\tflipping,\\tand\\tcropping,\\tas\\twell\\tas\\tadjusting\\tthe\\tbrightness,\\ncontrast,\\tsaturation,\\tand\\thue\\t(see\\tthe\\tAPI\\tdocumentation\\tfor\\tmore\\tdetails).\\tThis\\tmakes\\tit\\teasy\\tto\\nimplement\\tdata\\taugmentation\\tfor\\timage\\tdatasets.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 386}), Document(page_content='NOTE\\nAnother\\tpowerful\\ttechnique\\tto\\ttrain\\tvery\\tdeep\\tneural\\tnetworks\\tis\\tto\\tadd\\n\\t\\nskip\\tconnections\\n\\t(a\\tskip\\tconnection\\tis\\twhen\\tyou\\tadd\\nthe\\tinput\\tof\\ta\\tlayer\\tto\\tthe\\toutput\\tof\\ta\\thigher\\tlayer).\\tWe\\twill\\texplore\\tthis\\tidea\\tin\\t\\nChapter\\t13\\n\\twhen\\twe\\ttalk\\tabout\\tdeep\\tresidual\\nnetworks.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 387}), Document(page_content='Practical\\tGuidelines\\nIn\\t\\nthis\\tchapter,\\twe\\thave\\tcovered\\ta\\twide\\trange\\tof\\ttechniques\\tand\\tyou\\tmay\\tbe\\twondering\\twhich\\tones\\tyou\\nshould\\tuse.\\tThe\\tconfiguration\\tin\\t\\nTable\\t11-2\\n\\twill\\twork\\tfine\\tin\\tmost\\tcases.\\nTable\\t11-2.\\t\\nDefault\\tDNN\\tconfiguration\\nInitialization\\nHe\\tinitialization\\nActivation\\tfunction\\nELU\\nNormalization\\nBatch\\tNormalization\\nRegularization\\nDropout\\nOptimizer\\nNesterov\\tAccelerated\\tGradient\\nLearning\\trate\\tschedule\\nNone\\nOf\\tcourse,\\tyou\\tshould\\ttry\\tto\\treuse\\tparts\\tof\\ta\\tpretrained\\tneural\\tnetwork\\tif\\tyou\\tcan\\tfind\\tone\\tthat\\tsolves\\ta\\nsimilar\\tproblem.\\nThis\\tdefault\\tconfiguration\\tmay\\tneed\\tto\\tbe\\ttweaked:\\nIf\\tyou\\tcan’t\\tfind\\ta\\tgood\\tlearning\\trate\\t(convergence\\twas\\ttoo\\tslow,\\tso\\tyou\\tincreased\\tthe\\ttraining\\trate,\\nand\\tnow\\tconvergence\\tis\\tfast\\tbut\\tthe\\tnetwork’s\\taccuracy\\tis\\tsuboptimal),\\tthen\\tyou\\tcan\\ttry\\tadding\\ta\\nlearning\\tschedule\\tsuch\\tas\\texponential\\tdecay.\\nIf\\tyour\\ttraining\\tset\\tis\\ta\\tbit\\ttoo\\tsmall,\\tyou\\tcan\\timplement\\tdata\\taugmentation.\\nIf\\tyou\\tneed\\ta\\tsparse\\tmodel,\\tyou\\tcan\\tadd\\tsome\\tℓ\\n1\\n\\tregularization\\tto\\tthe\\tmix\\t(and\\toptionally\\tzero\\tout\\nthe\\ttiny\\tweights\\tafter\\ttraining).\\tIf\\tyou\\tneed\\tan\\teven\\tsparser\\tmodel,\\tyou\\tcan\\ttry\\tusing\\tFTRL\\tinstead\\tof\\nAdam\\toptimization,\\talong\\twith\\tℓ\\n1\\n\\tregularization.\\nIf\\tyou\\tneed\\ta\\tlightning-fast\\tmodel\\tat\\truntime,\\tyou\\tmay\\twant\\tto\\tdrop\\tBatch\\tNormalization,\\tand\\npossibly\\treplace\\tthe\\tELU\\tactivation\\tfunction\\twith\\tthe\\tleaky\\tReLU.\\tHaving\\ta\\tsparse\\tmodel\\twill\\talso\\nhelp.\\nWith\\tthese\\tguidelines,\\tyou\\tare\\tnow\\tready\\tto\\ttrain\\tvery\\tdeep\\tnets\\t—\\twell,\\tif\\tyou\\tare\\tvery\\tpatient,\\tthat\\tis!\\tIf\\nyou\\tuse\\ta\\tsingle\\tmachine,\\tyou\\tmay\\thave\\tto\\twait\\tfor\\tdays\\tor\\teven\\tmonths\\tfor\\ttraining\\tto\\tcomplete.\\tIn\\tthe\\nnext\\tchapter\\twe\\twill\\tdiscuss\\thow\\tto\\tuse\\tdistributed\\tTensorFlow\\tto\\ttrain\\tand\\trun\\tmodels\\tacross\\tmany\\nservers\\tand\\tGPUs.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 388}), Document(page_content='Exercises\\n1\\n.\\t\\nIs\\tit\\tokay\\tto\\tinitialize\\tall\\tthe\\tweights\\tto\\tthe\\tsame\\tvalue\\tas\\tlong\\tas\\tthat\\tvalue\\tis\\tselected\\trandomly\\nusing\\tHe\\tinitialization?\\n2\\n.\\t\\nIs\\tit\\tokay\\tto\\tinitialize\\tthe\\tbias\\tterms\\tto\\t0?\\n3\\n.\\t\\nName\\tthree\\tadvantages\\tof\\tthe\\tELU\\tactivation\\tfunction\\tover\\tReLU.\\n4\\n.\\t\\nIn\\twhich\\tcases\\twould\\tyou\\twant\\tto\\tuse\\teach\\tof\\tthe\\tfollowing\\tactivation\\tfunctions:\\tELU,\\tleaky\\tReLU\\n(and\\tits\\tvariants),\\tReLU,\\ttanh,\\tlogistic,\\tand\\tsoftmax?\\n5\\n.\\t\\nWhat\\tmay\\thappen\\tif\\tyou\\tset\\tthe\\t\\nmomentum\\n\\thyperparameter\\ttoo\\tclose\\tto\\t1\\t(e.g.,\\t0.99999)\\twhen\\t\\nusing\\na\\t\\nMomentumOptimizer\\n?\\n6\\n.\\t\\nName\\tthree\\tways\\tyou\\tcan\\tproduce\\ta\\tsparse\\tmodel.\\n7\\n.\\t\\nDoes\\tdropout\\tslow\\tdown\\ttraining?\\tDoes\\tit\\tslow\\tdown\\t\\ninference\\t(i.e.,\\tmaking\\tpredictions\\ton\\tnew\\ninstances)?\\n8\\n.\\t\\nDeep\\tLearning.\\na\\n.\\t\\nBuild\\ta\\tDNN\\twith\\tfive\\thidden\\tlayers\\tof\\t100\\tneurons\\teach,\\tHe\\tinitialization,\\tand\\tthe\\tELU\\nactivation\\tfunction.\\nb\\n.\\t\\nUsing\\tAdam\\toptimization\\tand\\tearly\\tstopping,\\ttry\\ttraining\\tit\\ton\\tMNIST\\tbut\\tonly\\ton\\tdigits\\t0\\tto\\t4,\\nas\\twe\\twill\\tuse\\ttransfer\\tlearning\\tfor\\tdigits\\t5\\tto\\t9\\tin\\tthe\\tnext\\texercise.\\tYou\\twill\\tneed\\ta\\tsoftmax\\noutput\\tlayer\\twith\\tfive\\tneurons,\\tand\\tas\\talways\\tmake\\tsure\\tto\\tsave\\tcheckpoints\\tat\\tregular\\tintervals\\nand\\tsave\\tthe\\tfinal\\tmodel\\tso\\tyou\\tcan\\treuse\\tit\\tlater.\\nc\\n.\\t\\nTune\\tthe\\thyperparameters\\tusing\\tcross-validation\\tand\\tsee\\twhat\\tprecision\\tyou\\tcan\\tachieve.\\nd\\n.\\t\\nNow\\ttry\\tadding\\tBatch\\tNormalization\\tand\\tcompare\\tthe\\tlearning\\tcurves:\\tis\\tit\\tconverging\\tfaster\\nthan\\tbefore?\\tDoes\\tit\\tproduce\\ta\\tbetter\\tmodel?\\ne\\n.\\t\\nIs\\tthe\\tmodel\\toverfitting\\tthe\\ttraining\\tset?\\tTry\\tadding\\tdropout\\tto\\tevery\\tlayer\\tand\\ttry\\tagain.\\tDoes\\tit\\nhelp?\\n9\\n.\\t\\nTransfer\\tlearning.\\na\\n.\\t\\nCreate\\ta\\tnew\\tDNN\\tthat\\treuses\\tall\\tthe\\tpretrained\\thidden\\tlayers\\tof\\tthe\\tprevious\\tmodel,\\tfreezes\\nthem,\\tand\\treplaces\\tthe\\tsoftmax\\toutput\\tlayer\\twith\\ta\\tnew\\tone.\\nb\\n.\\t\\nTrain\\tthis\\tnew\\tDNN\\ton\\tdigits\\t5\\tto\\t9,\\tusing\\tonly\\t100\\timages\\tper\\tdigit,\\tand\\ttime\\thow\\tlong\\tit\\ntakes.\\tDespite\\tthis\\tsmall\\tnumber\\tof\\texamples,\\tcan\\tyou\\tachieve\\thigh\\tprecision?\\nc\\n.\\t\\nTry\\tcaching\\tthe\\tfrozen\\tlayers,\\tand\\ttrain\\tthe\\tmodel\\tagain:\\thow\\tmuch\\tfaster\\tis\\tit\\tnow?\\nd\\n.\\t\\nTry\\tagain\\treusing\\tjust\\tfour\\thidden\\tlayers\\tinstead\\tof\\tfive.\\tCan\\tyou\\tachieve\\ta\\thigher\\tprecision?', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 389}), Document(page_content='e\\n.\\t\\nNow\\tunfreeze\\tthe\\ttop\\ttwo\\thidden\\tlayers\\tand\\tcontinue\\ttraining:\\tcan\\tyou\\tget\\tthe\\tmodel\\tto\\tperform\\neven\\tbetter?\\n10\\n.\\t\\nPretraining\\ton\\tan\\tauxiliary\\ttask.\\na\\n.\\t\\nIn\\tthis\\texercise\\tyou\\twill\\tbuild\\ta\\tDNN\\tthat\\tcompares\\ttwo\\tMNIST\\tdigit\\timages\\tand\\tpredicts\\nwhether\\tthey\\trepresent\\tthe\\tsame\\tdigit\\tor\\tnot.\\tThen\\tyou\\twill\\treuse\\tthe\\tlower\\tlayers\\tof\\tthis\\nnetwork\\tto\\ttrain\\tan\\tMNIST\\tclassifier\\tusing\\tvery\\tlittle\\ttraining\\tdata.\\tStart\\tby\\tbuilding\\ttwo\\tDNNs\\n(let’s\\tcall\\tthem\\tDNN\\tA\\tand\\tB),\\tboth\\tsimilar\\tto\\tthe\\tone\\tyou\\tbuilt\\tearlier\\tbut\\twithout\\tthe\\toutput\\nlayer:\\teach\\tDNN\\tshould\\thave\\tfive\\thidden\\tlayers\\tof\\t100\\tneurons\\teach,\\tHe\\tinitialization,\\tand\\nELU\\tactivation.\\tNext,\\tadd\\tone\\tmore\\thidden\\tlayer\\twith\\t10\\tunits\\ton\\ttop\\tof\\tboth\\tDNNs.\\tTo\\tdo\\tthis,\\nyou\\tshould\\tuse\\tTensorFlow’s\\t\\nconcat()\\n\\t\\nfunction\\twith\\t\\naxis=1\\n\\tto\\tconcatenate\\tthe\\toutputs\\tof\\tboth\\nDNNs\\tfor\\teach\\tinstance,\\tthen\\tfeed\\tthe\\tresult\\tto\\tthe\\thidden\\tlayer.\\tFinally,\\tadd\\tan\\toutput\\tlayer\\nwith\\ta\\tsingle\\tneuron\\tusing\\tthe\\tlogistic\\tactivation\\tfunction.\\nb\\n.\\t\\nSplit\\tthe\\tMNIST\\ttraining\\tset\\tin\\ttwo\\tsets:\\tsplit\\t#1\\tshould\\tcontaining\\t55,000\\timages,\\tand\\tsplit\\t#2\\nshould\\tcontain\\tcontain\\t5,000\\timages.\\tCreate\\ta\\tfunction\\tthat\\tgenerates\\ta\\ttraining\\tbatch\\twhere\\neach\\tinstance\\tis\\ta\\tpair\\tof\\tMNIST\\timages\\tpicked\\tfrom\\tsplit\\t#1.\\tHalf\\tof\\tthe\\ttraining\\tinstances\\nshould\\tbe\\tpairs\\tof\\timages\\tthat\\tbelong\\tto\\tthe\\tsame\\tclass,\\twhile\\tthe\\tother\\thalf\\tshould\\tbe\\timages\\nfrom\\tdifferent\\tclasses.\\tFor\\teach\\tpair,\\tthe\\ttraining\\tlabel\\tshould\\tbe\\t0\\tif\\tthe\\timages\\tare\\tfrom\\tthe\\nsame\\tclass,\\tor\\t1\\tif\\tthey\\tare\\tfrom\\tdifferent\\tclasses.\\nc\\n.\\t\\nTrain\\tthe\\tDNN\\ton\\tthis\\ttraining\\tset.\\tFor\\teach\\timage\\tpair,\\tyou\\tcan\\tsimultaneously\\tfeed\\tthe\\tfirst\\nimage\\tto\\tDNN\\tA\\tand\\tthe\\tsecond\\timage\\tto\\tDNN\\tB.\\tThe\\twhole\\tnetwork\\twill\\tgradually\\tlearn\\tto\\ntell\\twhether\\ttwo\\timages\\tbelong\\tto\\tthe\\tsame\\tclass\\tor\\tnot.\\nd\\n.\\t\\nNow\\tcreate\\ta\\tnew\\tDNN\\tby\\treusing\\tand\\tfreezing\\tthe\\thidden\\tlayers\\tof\\tDNN\\tA\\tand\\tadding\\ta\\nsoftmax\\toutput\\tlayer\\ton\\ttop\\twith\\t10\\tneurons.\\tTrain\\tthis\\tnetwork\\ton\\tsplit\\t#2\\tand\\tsee\\tif\\tyou\\tcan\\nachieve\\thigh\\tperformance\\tdespite\\thaving\\tonly\\t500\\timages\\tper\\tclass.\\nSolutions\\tto\\tthese\\texercises\\tare\\t\\navailable\\tin\\t\\nAppendix\\tA\\n.\\n“Understanding\\tthe\\tDifficulty\\tof\\tTraining\\tDeep\\tFeedforward\\tNeural\\tNetworks,”\\tX.\\tGlorot,\\tY\\tBengio\\t(2010).\\nHere’s\\tan\\tanalogy:\\tif\\tyou\\tset\\ta\\tmicrophone\\tamplifier’s\\tknob\\ttoo\\tclose\\tto\\tzero,\\tpeople\\twon’t\\thear\\tyour\\tvoice,\\tbut\\tif\\tyou\\tset\\tit\\ttoo\\tclose\\tto\\nthe\\tmax,\\tyour\\tvoice\\twill\\tbe\\tsaturated\\tand\\tpeople\\twon’t\\tunderstand\\twhat\\tyou\\tare\\tsaying.\\tNow\\timagine\\ta\\tchain\\tof\\tsuch\\tamplifiers:\\tthey\\tall\\nneed\\tto\\tbe\\tset\\tproperly\\tin\\torder\\tfor\\tyour\\tvoice\\tto\\tcome\\tout\\tloud\\tand\\tclear\\tat\\tthe\\tend\\tof\\tthe\\tchain.\\tYour\\tvoice\\thas\\tto\\tcome\\tout\\tof\\teach\\namplifier\\tat\\tthe\\tsame\\tamplitude\\tas\\tit\\tcame\\tin.\\nThis\\tsimplified\\tstrategy\\twas\\tactually\\talready\\tproposed\\tmuch\\tearlier\\t—\\tfor\\texample,\\tin\\tthe\\t1998\\tbook\\t\\nNeural\\tNetworks:\\tTricks\\tof\\tthe\\nTrade\\n\\tby\\tGenevieve\\tOrr\\tand\\tKlaus-Robert\\tMüller\\t(Springer).\\nSuch\\tas\\t“Delving\\tDeep\\tinto\\tRectifiers:\\tSurpassing\\tHuman-Level\\tPerformance\\ton\\tImageNet\\tClassification,”\\tK.\\tHe\\tet\\tal.\\t(2015).\\n“Empirical\\tEvaluation\\tof\\tRectified\\tActivations\\tin\\tConvolution\\tNetwork,”\\tB.\\tXu\\tet\\tal.\\t(2015).\\n“Fast\\tand\\tAccurate\\tDeep\\tNetwork\\tLearning\\tby\\tExponential\\tLinear\\tUnits\\t(ELUs),”\\tD.\\tClevert,\\tT.\\tUnterthiner,\\tS.\\tHochreiter\\t(2015).\\n“Batch\\tNormalization:\\tAccelerating\\tDeep\\tNetwork\\tTraining\\tby\\tReducing\\tInternal\\tCovariate\\tShift,”\\tS.\\tIoffe\\tand\\tC.\\tSzegedy\\t(2015).\\nMany\\tresearchers\\targue\\tthat\\tit\\tis\\tjust\\tas\\tgood,\\tor\\teven\\tbetter,\\tto\\tplace\\tthe\\tbatch\\tnormalization\\tlayers\\tafter\\t(rather\\tthan\\tbefore)\\tthe\\nactivations.\\n“On\\tthe\\tdifficulty\\tof\\ttraining\\trecurrent\\tneural\\tnetworks,”\\tR.\\tPascanu\\tet\\tal.\\t(2013).\\nAnother\\toption\\tis\\tto\\tcome\\tup\\twith\\ta\\tsupervised\\ttask\\tfor\\twhich\\tyou\\tcan\\teasily\\tgather\\ta\\tlot\\tof\\tlabeled\\ttraining\\tdata,\\tthen\\tuse\\ttransfer\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 390}), Document(page_content='learning,\\tas\\texplained\\tearlier.\\tFor\\texample,\\tif\\tyou\\twant\\tto\\ttrain\\ta\\tmodel\\tto\\tidentify\\tyour\\tfriends\\tin\\tpictures,\\tyou\\tcould\\tdownload\\tmillions\\tof\\nfaces\\ton\\tthe\\tinternet\\tand\\ttrain\\ta\\tclassifier\\tto\\tdetect\\twhether\\ttwo\\tfaces\\tare\\tidentical\\tor\\tnot,\\tthen\\tuse\\tthis\\tclassifier\\tto\\tcompare\\ta\\tnew\\npicture\\twith\\teach\\tpicture\\tof\\tyour\\tfriends.\\n“Some\\tmethods\\tof\\tspeeding\\tup\\tthe\\tconvergence\\tof\\titeration\\tmethods,”\\tB.\\tPolyak\\t(1964).\\n“A\\tMethod\\tfor\\tUnconstrained\\tConvex\\tMinimization\\tProblem\\twith\\tthe\\tRate\\tof\\tConvergence\\tO(1/k\\n),”\\tYurii\\tNesterov\\t(1983).\\n“Adaptive\\tSubgradient\\tMethods\\tfor\\tOnline\\tLearning\\tand\\tStochastic\\tOptimization,”\\tJ.\\tDuchi\\tet\\tal.\\t(2011).\\nThis\\talgorithm\\twas\\tcreated\\tby\\tTijmen\\tTieleman\\tand\\tGeoffrey\\tHinton\\tin\\t2012,\\tand\\tpresented\\tby\\tGeoffrey\\tHinton\\tin\\this\\tCoursera\\tclass\\ton\\nneural\\tnetworks\\t(slides:\\t\\nhttp://goo.gl/RsQeis\\n;\\tvideo:\\t\\nhttps://goo.gl/XUbIyJ\\n).\\tAmusingly,\\tsince\\tthe\\tauthors\\thave\\tnot\\twritten\\ta\\tpaper\\tto\\ndescribe\\tit,\\tresearchers\\toften\\tcite\\t“slide\\t29\\tin\\tlecture\\t6”\\tin\\ttheir\\tpapers.\\n“Adam:\\tA\\tMethod\\tfor\\tStochastic\\tOptimization,”\\tD.\\tKingma,\\tJ.\\tBa\\t(2015).\\nThese\\tare\\testimations\\tof\\tthe\\tmean\\tand\\t(uncentered)\\tvariance\\tof\\tthe\\tgradients.\\tThe\\tmean\\tis\\toften\\tcalled\\tthe\\t\\nfirst\\tmoment\\n,\\twhile\\tthe\\nvariance\\tis\\toften\\tcalled\\tthe\\t\\nsecond\\tmoment\\n,\\thence\\tthe\\tname\\tof\\tthe\\talgorithm.\\n“The\\tMarginal\\tValue\\tof\\tAdaptive\\tGradient\\tMethods\\tin\\tMachine\\tLearning,”\\tA.\\tC.\\tWilson\\tet\\tal.\\t(2017).\\n“Primal-Dual\\tSubgradient\\tMethods\\tfor\\tConvex\\tProblems,”\\tYurii\\tNesterov\\t(2005).\\n“Ad\\tClick\\tPrediction:\\ta\\tView\\tfrom\\tthe\\tTrenches,”\\tH.\\tMcMahan\\tet\\tal.\\t(2013).\\n“An\\tEmpirical\\tStudy\\tof\\tLearning\\tRates\\tin\\tDeep\\tNeural\\tNetworks\\tfor\\tSpeech\\tRecognition,”\\tA.\\tSenior\\tet\\tal.\\t(2013).\\n“Improving\\tneural\\tnetworks\\tby\\tpreventing\\tco-adaptation\\tof\\tfeature\\tdetectors,”\\tG.\\tHinton\\tet\\tal.\\t(2012).\\n“Dropout:\\tA\\tSimple\\tWay\\tto\\tPrevent\\tNeural\\tNetworks\\tfrom\\tOverfitting,”\\tN.\\tSrivastava\\tet\\tal.\\t(2014).\\n11\\n12\\n2\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 391}), Document(page_content='Chapter\\t12.\\t\\nDistributing\\tTensorFlow\\tAcross\\nDevices\\tand\\tServers\\nIn\\t\\nChapter\\t11\\n\\twe\\t\\ndiscussed\\tseveral\\ttechniques\\tthat\\tcan\\tconsiderably\\tspeed\\tup\\ttraining:\\tbetter\\tweight\\ninitialization,\\tBatch\\tNormalization,\\tsophisticated\\toptimizers,\\tand\\tso\\ton.\\tHowever,\\teven\\twith\\tall\\tof\\tthese\\ntechniques,\\ttraining\\ta\\tlarge\\tneural\\tnetwork\\ton\\ta\\tsingle\\tmachine\\twith\\ta\\tsingle\\tCPU\\tcan\\ttake\\tdays\\tor\\teven\\nweeks.\\nIn\\tthis\\tchapter\\twe\\twill\\tsee\\thow\\tto\\tuse\\tTensorFlow\\tto\\tdistribute\\tcomputations\\tacross\\tmultiple\\tdevices\\n(CPUs\\tand\\tGPUs)\\tand\\trun\\tthem\\tin\\tparallel\\t(see\\t\\nFigure\\t12-1\\n).\\tFirst\\twe\\twill\\tdistribute\\tcomputations\\nacross\\tmultiple\\tdevices\\ton\\tjust\\tone\\tmachine,\\tthen\\ton\\tmultiple\\tdevices\\tacross\\tmultiple\\tmachines.\\nFigure\\t12-1.\\t\\nExecuting\\ta\\tTensorFlow\\tgraph\\tacross\\tmultiple\\tdevices\\tin\\tparallel\\nTensorFlow’s\\tsupport\\tof\\tdistributed\\tcomputing\\tis\\tone\\tof\\tits\\tmain\\thighlights\\tcompared\\tto\\tother\\tneural\\nnetwork\\tframeworks.\\tIt\\tgives\\tyou\\tfull\\tcontrol\\tover\\thow\\tto\\tsplit\\t(or\\treplicate)\\tyour\\tcomputation\\tgraph\\nacross\\tdevices\\tand\\tservers,\\tand\\tit\\tlets\\tyou\\tparallelize\\tand\\tsynchronize\\toperations\\tin\\tflexible\\tways\\tso\\tyou\\ncan\\tchoose\\tbetween\\tall\\tsorts\\tof\\tparallelization\\tapproaches.\\nWe\\twill\\tlook\\tat\\tsome\\tof\\tthe\\tmost\\tpopular\\tapproaches\\tto\\tparallelizing\\tthe\\texecution\\tand\\ttraining\\tof\\ta\\nneural\\tnetwork.\\tInstead\\tof\\twaiting\\tfor\\tweeks\\tfor\\ta\\ttraining\\talgorithm\\tto\\tcomplete,\\tyou\\tmay\\tend\\tup\\twaiting\\nfor\\tjust\\ta\\tfew\\thours.\\tNot\\tonly\\tdoes\\tthis\\tsave\\tan\\tenormous\\tamount\\tof\\ttime,\\tit\\talso\\tmeans\\tthat\\tyou\\tcan\\nexperiment\\twith\\tvarious\\tmodels\\tmuch\\tmore\\teasily,\\tand\\tfrequently\\tretrain\\tyour\\tmodels\\ton\\tfresh\\tdata.\\nOther\\tgreat\\tuse\\tcases\\tof\\tparallelization\\tinclude\\texploring\\ta\\tmuch\\tlarger\\thyperparameter\\tspace\\twhen\\tfine-\\ntuning\\tyour\\tmodel,\\tand\\trunning\\tlarge\\tensembles\\tof\\tneural\\tnetworks\\tefficiently.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 392}), Document(page_content='But\\twe\\tmust\\tlearn\\tto\\twalk\\tbefore\\twe\\tcan\\trun.\\tLet’s\\tstart\\tby\\tparallelizing\\tsimple\\tgraphs\\tacross\\tseveral\\nGPUs\\ton\\ta\\tsingle\\tmachine.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 393}), Document(page_content='Multiple\\tDevices\\ton\\ta\\tSingle\\tMachine\\nYou\\t\\ncan\\toften\\tget\\ta\\tmajor\\tperformance\\tboost\\tsimply\\tby\\tadding\\tGPU\\tcards\\tto\\ta\\tsingle\\tmachine.\\tIn\\tfact,\\tin\\nmany\\tcases\\tthis\\twill\\tsuffice;\\tyou\\twon’t\\tneed\\tto\\tuse\\tmultiple\\tmachines\\tat\\tall.\\tFor\\texample,\\tyou\\tcan\\ntypically\\ttrain\\ta\\tneural\\tnetwork\\tjust\\tas\\tfast\\tusing\\t8\\tGPUs\\ton\\ta\\tsingle\\tmachine\\trather\\tthan\\t16\\tGPUs\\tacross\\nmultiple\\tmachines\\t(due\\tto\\tthe\\textra\\tdelay\\timposed\\tby\\tnetwork\\tcommunications\\tin\\ta\\tmultimachine\\tsetup).\\nIn\\tthis\\tsection\\twe\\twill\\tlook\\tat\\thow\\tto\\tset\\tup\\tyour\\tenvironment\\tso\\tthat\\tTensorFlow\\tcan\\tuse\\tmultiple\\tGPU\\ncards\\ton\\tone\\tmachine.\\tThen\\twe\\twill\\tlook\\tat\\thow\\tyou\\tcan\\tdistribute\\toperations\\tacross\\tavailable\\tdevices\\nand\\texecute\\tthem\\tin\\tparallel.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 394}), Document(page_content='Installation\\nIn\\t\\norder\\tto\\trun\\tTensorFlow\\ton\\tmultiple\\tGPU\\tcards,\\tyou\\tfirst\\tneed\\tto\\tmake\\tsure\\tyour\\tGPU\\tcards\\thave\\nNVidia\\tCompute\\tCapability\\t\\n(greater\\tor\\tequal\\tto\\t3.0).\\tThis\\tincludes\\tNvidia’s\\tTitan,\\tTitan\\tX,\\tK20,\\tand\\nK40\\tcards\\t(if\\tyou\\town\\tanother\\tcard,\\tyou\\tcan\\tcheck\\tits\\tcompatibility\\tat\\nhttps://developer.nvidia.com/cuda-gpus\\n).\\nTIP\\nIf\\tyou\\tdon’t\\town\\tany\\tGPU\\tcards,\\tyou\\tcan\\tuse\\ta\\thosting\\tservice\\twith\\tGPU\\tcapability\\tsuch\\tas\\tAmazon\\tAWS.\\tDetailed\\tinstructions\\nto\\tset\\tup\\tTensorFlow\\t0.9\\twith\\tPython\\t3.5\\ton\\tan\\tAmazon\\tAWS\\tGPU\\tinstance\\tare\\tavailable\\tin\\tŽiga\\tAvsec’s\\t\\nhelpful\\tblog\\tpost\\n.\\tIt\\nshould\\tnot\\tbe\\ttoo\\thard\\tto\\tupdate\\tit\\tto\\tthe\\tlatest\\tversion\\tof\\tTensorFlow.\\tGoogle\\talso\\treleased\\ta\\tcloud\\tservice\\tcalled\\t\\nCloud\\nMachine\\tLearning\\n\\tto\\trun\\tTensorFlow\\tgraphs.\\tIn\\tMay\\t2016,\\tthey\\tannounced\\tthat\\ttheir\\tplatform\\tnow\\tincludes\\tservers\\tequipped\\nwith\\t\\ntensor\\tprocessing\\tunits\\n\\t(TPUs),\\t\\nprocessors\\tspecialized\\tfor\\tMachine\\tLearning\\tthat\\tare\\tmuch\\tfaster\\tthan\\tGPUs\\tfor\\tmany\\nML\\ttasks.\\tOf\\tcourse,\\tanother\\toption\\tis\\tsimply\\tto\\tbuy\\tyour\\town\\tGPU\\tcard.\\tTim\\tDettmers\\twrote\\ta\\t\\ngreat\\tblog\\tpost\\n\\tto\\thelp\\tyou\\nchoose,\\tand\\the\\tupdates\\tit\\tfairly\\tregularly.\\nYou\\tmust\\tthen\\tdownload\\tand\\tinstall\\tthe\\tappropriate\\tversion\\tof\\tthe\\t\\nCUDA\\tand\\tcuDNN\\t\\nlibraries\\t(CUDA\\n8.0\\tand\\tcuDNN\\t5.1\\tif\\tyou\\tare\\tusing\\tthe\\tbinary\\tinstallation\\tof\\tTensorFlow\\t1.0.0),\\tand\\tset\\ta\\tfew\\nenvironment\\tvariables\\tso\\tTensorFlow\\tknows\\twhere\\tto\\tfind\\tCUDA\\tand\\tcuDNN.\\tThe\\tdetailed\\tinstallation\\ninstructions\\tare\\tlikely\\tto\\tchange\\tfairly\\tquickly,\\tso\\tit\\tis\\tbest\\tthat\\tyou\\tfollow\\tthe\\tinstructions\\ton\\nTensorFlow’s\\twebsite.\\nNvidia’s\\t\\nCompute\\tUnified\\tDevice\\tArchitecture\\n\\tlibrary\\t(CUDA)\\tallows\\tdevelopers\\tto\\tuse\\tCUDA-\\nenabled\\tGPUs\\tfor\\tall\\tsorts\\tof\\tcomputations\\t(not\\tjust\\tgraphics\\tacceleration).\\tNvidia’s\\t\\nCUDA\\tDeep\\tNeural\\nNetwork\\n\\tlibrary\\t(cuDNN)\\tis\\ta\\tGPU-accelerated\\tlibrary\\tof\\tprimitives\\tfor\\tDNNs.\\tIt\\tprovides\\toptimized\\nimplementations\\tof\\tcommon\\tDNN\\tcomputations\\tsuch\\tas\\tactivation\\tlayers,\\tnormalization,\\tforward\\tand\\nbackward\\tconvolutions,\\tand\\tpooling\\t(see\\t\\nChapter\\t13\\n).\\tIt\\tis\\tpart\\tof\\tNvidia’s\\tDeep\\tLearning\\tSDK\\t(note\\nthat\\tit\\trequires\\tcreating\\tan\\tNvidia\\tdeveloper\\taccount\\tin\\torder\\tto\\tdownload\\tit).\\tTensorFlow\\tuses\\tCUDA\\nand\\tcuDNN\\tto\\tcontrol\\tthe\\tGPU\\tcards\\tand\\taccelerate\\tcomputations\\t(see\\t\\nFigure\\t12-2\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 395}), Document(page_content='Figure\\t12-2.\\t\\nTensorFlow\\tuses\\tCUDA\\tand\\tcuDNN\\tto\\tcontrol\\tGPUs\\tand\\tboost\\tDNNs\\nYou\\tcan\\tuse\\tthe\\t\\nnvidia-smi\\n\\tcommand\\tto\\tcheck\\tthat\\tCUDA\\tis\\tproperly\\tinstalled.\\tIt\\tlists\\tthe\\tavailable\\nGPU\\tcards,\\tas\\twell\\tas\\tprocesses\\trunning\\ton\\teach\\tcard:\\n$\\tnvidia-smi\\nWed\\tSep\\t16\\t09:50:03\\t2016\\n+------------------------------------------------------+\\n|\\tNVIDIA-SMI\\t352.63\\t\\t\\t\\t\\tDriver\\tVersion:\\t352.63\\t\\t\\t\\t\\t\\t\\t\\t\\t|\\n|-------------------------------+----------------------+----------------------+\\n|\\tGPU\\t\\tName\\t\\t\\t\\t\\t\\t\\t\\tPersistence-M|\\tBus-Id\\t\\t\\t\\t\\t\\t\\t\\tDisp.A\\t|\\tVolatile\\tUncorr.\\tECC\\t|\\n|\\tFan\\t\\tTemp\\t\\tPerf\\t\\tPwr:Usage/Cap|\\t\\t\\t\\t\\t\\t\\t\\t\\tMemory-Usage\\t|\\tGPU-Util\\t\\tCompute\\tM.\\t|\\n|===============================+======================+======================|\\n|\\t\\t\\t0\\t\\tGRID\\tK520\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOff\\t\\t|\\t0000:00:03.0\\t\\t\\t\\t\\tOff\\t|\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tN/A\\t|\\n|\\tN/A\\t\\t\\t27C\\t\\t\\t\\tP8\\t\\t\\t\\t17W\\t/\\t125W\\t|\\t\\t\\t\\t\\t11MiB\\t/\\t\\t4095MiB\\t|\\t\\t\\t\\t\\t\\t0%\\t\\t\\t\\t\\t\\tDefault\\t|\\n+-------------------------------+----------------------+----------------------+\\n+-----------------------------------------------------------------------------+\\n|\\tProcesses:\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGPU\\tMemory\\t|\\n|\\t\\tGPU\\t\\t\\t\\t\\t\\t\\tPID\\t\\tType\\t\\tProcess\\tname\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tUsage\\t\\t\\t\\t\\t\\t|\\n|=============================================================================|\\n|\\t\\tNo\\trunning\\tprocesses\\tfound\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t|\\n+-----------------------------------------------------------------------------+\\nFinally,\\tyou\\tmust\\tinstall\\tTensorFlow\\twith\\tGPU\\tsupport.\\tIf\\tyou\\tcreated\\tan\\tisolated\\tenvironment\\tusing\\nvirtualenv,\\tyou\\tfirst\\tneed\\tto\\tactivate\\tit:\\n$\\tcd\\t$ML_PATH\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t#\\tYour\\tML\\tworking\\tdirectory\\t(e.g.,\\t$HOME/ml)\\n$\\tsource\\tenv/bin/activate\\nThen\\tinstall\\tthe\\tappropriate\\tGPU-enabled\\tversion\\tof\\tTensorFlow:\\n$\\tpip3\\tinstall\\t--upgrade\\ttensorflow-gpu', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 396}), Document(page_content='Now\\tyou\\tcan\\topen\\tup\\ta\\tPython\\tshell\\tand\\tcheck\\tthat\\tTensorFlow\\tdetects\\tand\\tuses\\tCUDA\\tand\\tcuDNN\\nproperly\\tby\\timporting\\tTensorFlow\\tand\\tcreating\\ta\\tsession:\\n>>>\\t\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nI\\t[...]/dso_loader.cc:108]\\tsuccessfully\\topened\\tCUDA\\tlibrary\\tlibcublas.so\\tlocally\\nI\\t[...]/dso_loader.cc:108]\\tsuccessfully\\topened\\tCUDA\\tlibrary\\tlibcudnn.so\\tlocally\\nI\\t[...]/dso_loader.cc:108]\\tsuccessfully\\topened\\tCUDA\\tlibrary\\tlibcufft.so\\tlocally\\nI\\t[...]/dso_loader.cc:108]\\tsuccessfully\\topened\\tCUDA\\tlibrary\\tlibcuda.so.1\\tlocally\\nI\\t[...]/dso_loader.cc:108]\\tsuccessfully\\topened\\tCUDA\\tlibrary\\tlibcurand.so\\tlocally\\n>>>\\t\\nsess\\n\\t\\n=\\n\\t\\ntf\\n.\\nSession\\n()\\n[...]\\nI\\t[...]/gpu_init.cc:102]\\tFound\\tdevice\\t0\\twith\\tproperties:\\nname:\\tGRID\\tK520\\nmajor:\\t3\\tminor:\\t0\\tmemoryClockRate\\t(GHz)\\t0.797\\npciBusID\\t0000:00:03.0\\nTotal\\tmemory:\\t4.00GiB\\nFree\\tmemory:\\t3.95GiB\\nI\\t[...]/gpu_init.cc:126]\\tDMA:\\t0\\nI\\t[...]/gpu_init.cc:136]\\t0:\\t\\t\\tY\\nI\\t[...]/gpu_device.cc:839]\\tCreating\\tTensorFlow\\tdevice\\n(/gpu:0)\\t->\\t(device:\\t0,\\tname:\\tGRID\\tK520,\\tpci\\tbus\\tid:\\t0000:00:03.0)\\nLooks\\tgood!\\tTensorFlow\\tdetected\\tthe\\tCUDA\\tand\\tcuDNN\\tlibraries,\\tand\\tit\\tused\\tthe\\tCUDA\\tlibrary\\tto\\ndetect\\tthe\\tGPU\\t\\ncard\\t(in\\tthis\\tcase\\tan\\tNvidia\\tGrid\\tK520\\tcard).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 397}), Document(page_content='Managing\\tthe\\tGPU\\tRAM\\nBy\\t\\ndefault\\tTensorFlow\\tautomatically\\tgrabs\\tall\\tthe\\tRAM\\tin\\tall\\tavailable\\tGPUs\\tthe\\tfirst\\ttime\\tyou\\trun\\ta\\ngraph,\\tso\\tyou\\twill\\tnot\\tbe\\table\\tto\\tstart\\ta\\tsecond\\tTensorFlow\\tprogram\\twhile\\tthe\\tfirst\\tone\\tis\\tstill\\trunning.\\tIf\\nyou\\ttry,\\tyou\\twill\\tget\\tthe\\tfollowing\\terror:\\nE\\n\\t\\n[\\n...\\n]\\n/\\ncuda_driver\\n.\\ncc\\n:\\n965\\n]\\n\\t\\nfailed\\n\\t\\nto\\n\\t\\nallocate\\n\\t\\n3.66\\nG\\n\\t\\n(\\n3928915968\\n\\t\\nbytes\\n)\\n\\t\\nfrom\\ndevice\\n:\\n\\t\\nCUDA_ERROR_OUT_OF_MEMORY\\nOne\\tsolution\\tis\\tto\\trun\\teach\\tprocess\\ton\\tdifferent\\tGPU\\tcards.\\tTo\\tdo\\tthis,\\tthe\\tsimplest\\toption\\tis\\tto\\tset\\tthe\\nCUDA_VISIBLE_DEVICES\\n\\tenvironment\\tvariable\\tso\\tthat\\teach\\tprocess\\tonly\\tsees\\tthe\\tappropriate\\tGPU\\tcards.\\nFor\\texample,\\tyou\\tcould\\tstart\\ttwo\\tprograms\\tlike\\tthis:\\n$\\tCUDA_VISIBLE_DEVICES=0,1\\tpython3\\tprogram_1.py\\n#\\tand\\tin\\tanother\\tterminal:\\n$\\tCUDA_VISIBLE_DEVICES=3,2\\tpython3\\tprogram_2.py\\nProgram\\t#1\\twill\\tonly\\tsee\\tGPU\\tcards\\t0\\tand\\t1\\t(numbered\\t0\\tand\\t1,\\trespectively),\\tand\\tprogram\\t#2\\twill\\tonly\\nsee\\tGPU\\tcards\\t2\\tand\\t3\\t(numbered\\t1\\tand\\t0,\\trespectively).\\tEverything\\twill\\twork\\tfine\\t(see\\t\\nFigure\\t12-3\\n).\\nFigure\\t12-3.\\t\\nEach\\tprogram\\tgets\\ttwo\\tGPUs\\tfor\\titself\\nAnother\\toption\\tis\\tto\\ttell\\t\\nTensorFlow\\tto\\tgrab\\tonly\\ta\\tfraction\\tof\\tthe\\tmemory.\\tFor\\texample,\\tto\\tmake\\nTensorFlow\\tgrab\\tonly\\t40%\\tof\\teach\\tGPU’s\\tmemory,\\tyou\\tmust\\tcreate\\ta\\t\\nConfigProto\\n\\t\\nobject,\\tset\\tits\\ngpu_options.per_process_gpu_memory_fraction\\n\\t\\noption\\tto\\t\\n0.4\\n,\\tand\\tcreate\\tthe\\tsession\\tusing\\tthis\\nconfiguration:\\nconfig\\n\\t\\n=\\n\\t\\ntf\\n.\\nConfigProto\\n()\\nconfig\\n.\\ngpu_options\\n.\\nper_process_gpu_memory_fraction\\n\\t\\n=\\n\\t\\n0.4\\nsession\\n\\t\\n=\\n\\t\\ntf\\n.\\nSession\\n(\\nconfig\\n=\\nconfig\\n)\\nNow\\ttwo\\tprograms\\tlike\\tthis\\tone\\tcan\\trun\\tin\\tparallel\\tusing\\tthe\\tsame\\tGPU\\tcards\\t(but\\tnot\\tthree,\\tsince\\t3\\t×\\n0.4\\n\\t>\\t1).\\tSee\\t\\nFigure\\t12-4\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 398}), Document(page_content='Figure\\t12-4.\\t\\nEach\\tprogram\\tgets\\tall\\tfour\\tGPUs,\\tbut\\twith\\tonly\\t40%\\tof\\tthe\\tRAM\\teach\\nIf\\tyou\\trun\\tthe\\t\\nnvidia-smi\\n\\t\\ncommand\\twhile\\tboth\\tprograms\\tare\\trunning,\\tyou\\tshould\\tsee\\tthat\\teach\\tprocess\\nholds\\troughly\\t40%\\tof\\tthe\\ttotal\\tRAM\\tof\\teach\\tcard:\\n$\\tnvidia-smi\\n[...]\\n+-----------------------------------------------------------------------------+\\n|\\tProcesses:\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGPU\\tMemory\\t|\\n|\\t\\tGPU\\t\\t\\t\\t\\t\\t\\tPID\\t\\tType\\t\\tProcess\\tname\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tUsage\\t\\t\\t\\t\\t\\t|\\n|=============================================================================|\\n|\\t\\t\\t\\t0\\t\\t\\t\\t\\t\\t5231\\t\\t\\t\\tC\\t\\t\\tpython\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1677MiB\\t|\\n|\\t\\t\\t\\t0\\t\\t\\t\\t\\t\\t5262\\t\\t\\t\\tC\\t\\t\\tpython\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1677MiB\\t|\\n|\\t\\t\\t\\t1\\t\\t\\t\\t\\t\\t5231\\t\\t\\t\\tC\\t\\t\\tpython\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1677MiB\\t|\\n|\\t\\t\\t\\t1\\t\\t\\t\\t\\t\\t5262\\t\\t\\t\\tC\\t\\t\\tpython\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1677MiB\\t|\\n[...]\\nYet\\tanother\\toption\\tis\\tto\\ttell\\tTensorFlow\\tto\\tgrab\\tmemory\\tonly\\twhen\\tit\\tneeds\\tit.\\tTo\\tdo\\tthis\\tyou\\tmust\\t\\nset\\nconfig.gpu_options.allow_growth\\n\\tto\\t\\nTrue\\n.\\tHowever,\\tTensorFlow\\tnever\\treleases\\tmemory\\tonce\\tit\\nhas\\tgrabbed\\tit\\t(to\\tavoid\\tmemory\\tfragmentation)\\tso\\tyou\\tmay\\tstill\\trun\\tout\\tof\\tmemory\\tafter\\ta\\twhile.\\tIt\\tmay\\nbe\\tharder\\tto\\tguarantee\\ta\\tdeterministic\\tbehavior\\tusing\\tthis\\toption,\\tso\\tin\\tgeneral\\tyou\\tprobably\\twant\\tto\\tstick\\nwith\\tone\\tof\\tthe\\tprevious\\toptions.\\nOkay,\\tnow\\tyou\\thave\\ta\\tworking\\tGPU-enabled\\tTensorFlow\\tinstallation.\\tLet’s\\tsee\\thow\\tto\\t\\nuse\\tit!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 399}), Document(page_content='Placing\\tOperations\\ton\\tDevices\\nThe\\t\\nTensorFlow\\t\\nwhitepaper\\n1\\n\\tpresents\\ta\\tfriendly\\t\\ndynamic\\tplacer\\n\\t\\nalgorithm\\tthat\\tautomagically\\tdistributes\\noperations\\tacross\\tall\\tavailable\\tdevices,\\ttaking\\tinto\\taccount\\tthings\\tlike\\tthe\\tmeasured\\tcomputation\\ttime\\tin\\nprevious\\truns\\tof\\tthe\\tgraph,\\testimations\\tof\\tthe\\tsize\\tof\\tthe\\tinput\\tand\\toutput\\ttensors\\tto\\teach\\toperation,\\tthe\\namount\\tof\\tRAM\\tavailable\\tin\\teach\\tdevice,\\tcommunication\\tdelay\\twhen\\ttransferring\\tdata\\tin\\tand\\tout\\tof\\ndevices,\\thints\\tand\\tconstraints\\tfrom\\tthe\\tuser,\\tand\\tmore.\\tUnfortunately,\\tthis\\tsophisticated\\talgorithm\\tis\\ninternal\\tto\\tGoogle;\\tit\\twas\\tnot\\treleased\\tin\\tthe\\topen\\tsource\\tversion\\tof\\tTensorFlow.\\tThe\\treason\\tit\\twas\\tleft\\nout\\tseems\\tto\\tbe\\tthat\\tin\\tpractice\\ta\\tsmall\\tset\\tof\\tplacement\\trules\\tspecified\\tby\\tthe\\tuser\\tactually\\tresults\\tin\\nmore\\tefficient\\tplacement\\tthan\\twhat\\tthe\\tdynamic\\tplacer\\tis\\tcapable\\tof.\\tHowever,\\tthe\\tTensorFlow\\tteam\\tis\\nworking\\ton\\timproving\\tthe\\tdynamic\\tplacer,\\tand\\tperhaps\\tit\\twill\\teventually\\tbe\\tgood\\tenough\\tto\\tbe\\treleased.\\nUntil\\tthen\\tTensorFlow\\trelies\\ton\\tthe\\t\\nsimple\\tplacer\\n,\\twhich\\t(as\\tits\\tname\\tsuggests)\\tis\\tvery\\tbasic.\\nSimple\\tplacement\\nWhenever\\tyou\\trun\\ta\\tgraph,\\tif\\tTensorFlow\\tneeds\\tto\\tevaluate\\ta\\tnode\\tthat\\tis\\tnot\\tplaced\\ton\\ta\\tdevice\\tyet,\\tit\\nuses\\tthe\\tsimple\\tplacer\\tto\\tplace\\tit,\\talong\\twith\\tall\\tother\\tnodes\\tthat\\tare\\tnot\\tplaced\\tyet.\\tThe\\tsimple\\tplacer\\nrespects\\tthe\\tfollowing\\trules:\\nIf\\ta\\tnode\\twas\\talready\\tplaced\\ton\\ta\\tdevice\\tin\\ta\\tprevious\\trun\\tof\\tthe\\tgraph,\\tit\\tis\\tleft\\ton\\tthat\\tdevice.\\nElse,\\tif\\tthe\\tuser\\t\\npinned\\n\\ta\\tnode\\tto\\ta\\tdevice\\t(described\\tnext),\\tthe\\tplacer\\tplaces\\tit\\ton\\tthat\\tdevice.\\nElse,\\tit\\tdefaults\\tto\\tGPU\\t#0,\\tor\\tthe\\tCPU\\tif\\tthere\\tis\\tno\\tGPU.\\nAs\\tyou\\tcan\\tsee,\\tplacing\\toperations\\ton\\tthe\\tappropriate\\tdevice\\tis\\tmostly\\tup\\tto\\tyou.\\tIf\\tyou\\tdon’t\\tdo\\tanything,\\nthe\\twhole\\tgraph\\twill\\tbe\\tplaced\\ton\\tthe\\tdefault\\tdevice.\\tTo\\tpin\\tnodes\\tonto\\ta\\tdevice,\\tyou\\tmust\\tcreate\\ta\\ndevice\\tblock\\tusing\\tthe\\t\\ndevice()\\n\\t\\nfunction.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tpins\\tthe\\tvariable\\t\\na\\n\\tand\\tthe\\nconstant\\t\\nb\\n\\ton\\tthe\\tCPU,\\tbut\\tthe\\tmultiplication\\tnode\\t\\nc\\n\\tis\\tnot\\tpinned\\ton\\tany\\tdevice,\\tso\\tit\\twill\\tbe\\tplaced\\ton\\nthe\\t\\ndefault\\tdevice:\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/cpu:0\"\\n):\\n\\t\\t\\t\\t\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3.0\\n)\\n\\t\\t\\t\\t\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n4.0\\n)\\nc\\n\\t\\n=\\n\\t\\na\\n\\t\\n*\\n\\t\\nb\\nNOTE\\nThe\\t\\n\"/cpu:0\"\\n\\tdevice\\taggregates\\tall\\tCPUs\\ton\\ta\\tmulti-CPU\\tsystem.\\tThere\\tis\\tcurrently\\tno\\tway\\tto\\tpin\\tnodes\\ton\\tspecific\\tCPUs\\tor\\nto\\tuse\\tjust\\ta\\tsubset\\tof\\tall\\tCPUs.\\nLogging\\tplacements\\nLet’s\\t\\ncheck\\tthat\\tthe\\tsimple\\tplacer\\trespects\\tthe\\tplacement\\tconstraints\\twe\\thave\\tjust\\tdefined.\\tFor\\tthis\\tyou\\ncan\\tset\\t\\nthe\\t\\nlog_device_placement\\n\\toption\\tto\\t\\nTrue\\n;\\tthis\\ttells\\tthe\\tplacer\\tto\\tlog\\ta\\tmessage\\twhenever\\tit', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 400}), Document(page_content='places\\ta\\tnode.\\tFor\\texample:\\n>>>\\t\\nconfig\\n\\t\\n=\\n\\t\\ntf\\n.\\nConfigProto\\n()\\n>>>\\t\\nconfig\\n.\\nlog_device_placement\\n\\t\\n=\\n\\t\\nTrue\\n>>>\\t\\nsess\\n\\t\\n=\\n\\t\\ntf\\n.\\nSession\\n(\\nconfig\\n=\\nconfig\\n)\\nI\\t[...]\\tCreating\\tTensorFlow\\tdevice\\t(/gpu:0)\\t->\\t(device:\\t0,\\tname:\\tGRID\\tK520,\\npci\\tbus\\tid:\\t0000:00:03.0)\\n[...]\\n>>>\\t\\nx\\n.\\ninitializer\\n.\\nrun\\n(\\nsession\\n=\\nsess\\n)\\nI\\t[...]\\ta:\\t/job:localhost/replica:0/task:0/cpu:0\\nI\\t[...]\\ta/read:\\t/job:localhost/replica:0/task:0/cpu:0\\nI\\t[...]\\tmul:\\t/job:localhost/replica:0/task:0/gpu:0\\nI\\t[...]\\ta/Assign:\\t/job:localhost/replica:0/task:0/cpu:0\\nI\\t[...]\\tb:\\t/job:localhost/replica:0/task:0/cpu:0\\nI\\t[...]\\ta/initial_value:\\t/job:localhost/replica:0/task:0/cpu:0\\n>>>\\t\\nsess\\n.\\nrun\\n(\\nc\\n)\\n12\\nThe\\tlines\\tstarting\\twith\\t\\n\"I\"\\n\\tfor\\tInfo\\tare\\tthe\\tlog\\tmessages.\\tWhen\\twe\\tcreate\\ta\\tsession,\\tTensorFlow\\tlogs\\ta\\nmessage\\tto\\ttell\\tus\\tthat\\tit\\thas\\tfound\\ta\\tGPU\\tcard\\t(in\\tthis\\tcase\\tthe\\tGrid\\tK520\\tcard).\\tThen\\tthe\\tfirst\\ttime\\twe\\nrun\\tthe\\tgraph\\t(in\\tthis\\tcase\\twhen\\tinitializing\\tthe\\tvariable\\t\\na\\n),\\tthe\\tsimple\\tplacer\\tis\\trun\\tand\\tplaces\\teach\\tnode\\non\\tthe\\tdevice\\tit\\twas\\tassigned\\tto.\\tAs\\texpected,\\tthe\\tlog\\tmessages\\tshow\\tthat\\tall\\tnodes\\tare\\tplaced\\ton\\n\"/cpu:0\"\\n\\texcept\\tthe\\tmultiplication\\tnode,\\twhich\\tends\\tup\\ton\\tthe\\tdefault\\tdevice\\t\\n\"/gpu:0\"\\n\\t(you\\tcan\\tsafely\\nignore\\tthe\\tprefix\\t\\n/job:localhost/replica:0/task:0\\n\\tfor\\tnow;\\twe\\twill\\ttalk\\tabout\\tit\\tin\\ta\\tmoment).\\nNotice\\tthat\\tthe\\tsecond\\ttime\\twe\\trun\\tthe\\tgraph\\t(to\\tcompute\\t\\nc\\n),\\tthe\\tplacer\\tis\\tnot\\tused\\tsince\\tall\\tthe\\tnodes\\nTensorFlow\\t\\nneeds\\tto\\tcompute\\t\\nc\\n\\tare\\talready\\tplaced.\\nDynamic\\tplacement\\tfunction\\nWhen\\tyou\\t\\ncreate\\ta\\tdevice\\tblock,\\tyou\\tcan\\tspecify\\ta\\tfunction\\tinstead\\tof\\ta\\tdevice\\tname.\\tTensorFlow\\twill\\ncall\\tthis\\tfunction\\tfor\\teach\\toperation\\tit\\tneeds\\tto\\tplace\\tin\\tthe\\tdevice\\tblock,\\tand\\tthe\\tfunction\\tmust\\treturn\\tthe\\nname\\tof\\tthe\\tdevice\\tto\\tpin\\tthe\\toperation\\ton.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tpins\\tall\\tthe\\t\\nvariable\\tnodes\\tto\\n\"/cpu:0\"\\n\\t(in\\tthis\\tcase\\tjust\\tthe\\tvariable\\t\\na\\n)\\tand\\tall\\tother\\tnodes\\tto\\t\\n\"/gpu:0\"\\n:\\ndef\\n\\t\\nvariables_on_cpu\\n(\\nop\\n):\\n\\t\\t\\t\\t\\nif\\n\\t\\nop\\n.\\ntype\\n\\t\\n==\\n\\t\\n\"Variable\"\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\n\"/cpu:0\"\\n\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\n\"/gpu:0\"\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\nvariables_on_cpu\\n):\\n\\t\\t\\t\\t\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3.0\\n)\\n\\t\\t\\t\\t\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n4.0\\n)\\n\\t\\t\\t\\t\\nc\\n\\t\\n=\\n\\t\\na\\n\\t\\n*\\n\\t\\nb\\nYou\\tcan\\teasily\\timplement\\tmore\\tcomplex\\talgorithms,\\tsuch\\tas\\tpinning\\tvariables\\tacross\\tGPUs\\tin\\ta\\tround-\\nrobin\\tfashion.\\nOperations\\tand\\tkernels\\nFor\\ta\\tTensorFlow\\toperation\\tto\\trun\\ton\\ta\\tdevice,\\tit\\tneeds\\tto\\thave\\tan\\timplementation\\tfor\\tthat\\tdevice;\\tthis\\tis\\ncalled\\t\\na\\t\\nkernel\\n.\\tMany\\toperations\\thave\\tkernels\\tfor\\tboth\\tCPUs\\tand\\tGPUs,\\tbut\\tnot\\tall\\tof\\tthem.\\tFor\\texample,\\nTensorFlow\\tdoes\\tnot\\thave\\ta\\tGPU\\tkernel\\tfor\\tinteger\\tvariables,\\tso\\tthe\\tfollowing\\tcode\\twill\\tfail\\twhen\\nTensorFlow\\ttries\\tto\\tplace\\tthe\\tvariable\\t\\ni\\n\\ton\\tGPU\\t#0:', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 401}), Document(page_content='>>>\\t\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/gpu:0\"\\n):\\n...\\t\\n\\t\\t\\t\\t\\ni\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3\\n)\\n[...]\\n>>>\\t\\nsess\\n.\\nrun\\n(\\ni\\n.\\ninitializer\\n)\\nTraceback\\t(most\\trecent\\tcall\\tlast):\\n[...]\\ntensorflow.python.framework.errors.InvalidArgumentError:\\tCannot\\tassign\\ta\\tdevice\\nto\\tnode\\t\\'Variable\\':\\tCould\\tnot\\tsatisfy\\texplicit\\tdevice\\tspecification\\nNote\\tthat\\tTensorFlow\\tinfers\\tthat\\tthe\\tvariable\\tmust\\tbe\\tof\\ttype\\t\\nint32\\n\\t\\nsince\\tthe\\tinitialization\\tvalue\\tis\\tan\\ninteger.\\tIf\\tyou\\tchange\\tthe\\tinitialization\\tvalue\\tto\\t\\n3.0\\n\\tinstead\\tof\\t\\n3\\n,\\tor\\tif\\tyou\\texplicitly\\tset\\ndtype=tf.float32\\n\\twhen\\tcreating\\tthe\\tvariable,\\teverything\\twill\\twork\\tfine.\\nSoft\\tplacement\\nBy\\t\\ndefault,\\tif\\tyou\\ttry\\tto\\tpin\\tan\\toperation\\ton\\ta\\tdevice\\tfor\\twhich\\tthe\\toperation\\thas\\tno\\tkernel,\\tyou\\tget\\tthe\\nexception\\tshown\\tearlier\\twhen\\tTensorFlow\\ttries\\tto\\tplace\\tthe\\toperation\\ton\\tthe\\tdevice.\\tIf\\tyou\\tprefer\\nTensorFlow\\tto\\tfall\\tback\\tto\\tthe\\tCPU\\tinstead,\\tyou\\tcan\\tset\\tthe\\t\\nallow_soft_placement\\n\\tconfiguration\\t\\noption\\nto\\t\\nTrue\\n:\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/gpu:0\"\\n):\\n\\t\\t\\t\\t\\ni\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3\\n)\\nconfig\\n\\t\\n=\\n\\t\\ntf\\n.\\nConfigProto\\n()\\nconfig\\n.\\nallow_soft_placement\\n\\t\\n=\\n\\t\\nTrue\\nsess\\n\\t\\n=\\n\\t\\ntf\\n.\\nSession\\n(\\nconfig\\n=\\nconfig\\n)\\nsess\\n.\\nrun\\n(\\ni\\n.\\ninitializer\\n)\\n\\t\\t\\n#\\tthe\\tplacer\\truns\\tand\\tfalls\\tback\\tto\\t/cpu:0\\nSo\\tfar\\twe\\thave\\tdiscussed\\thow\\tto\\tplace\\tnodes\\ton\\tdifferent\\tdevices.\\tNow\\tlet’s\\tsee\\thow\\tTensorFlow\\twill\\nrun\\tthese\\tnodes\\t\\nin\\tparallel.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 402}), Document(page_content='Parallel\\tExecution\\nWhen\\t\\nTensorFlow\\truns\\ta\\tgraph,\\tit\\tstarts\\tby\\tfinding\\tout\\tthe\\tlist\\tof\\tnodes\\tthat\\tneed\\tto\\tbe\\tevaluated,\\tand\\tit\\ncounts\\thow\\tmany\\tdependencies\\teach\\tof\\tthem\\thas.\\tTensorFlow\\tthen\\tstarts\\tevaluating\\tthe\\tnodes\\twith\\tzero\\ndependencies\\t(i.e.,\\tsource\\tnodes).\\tIf\\tthese\\tnodes\\tare\\tplaced\\ton\\tseparate\\tdevices,\\tthey\\tobviously\\tget\\nevaluated\\tin\\tparallel.\\tIf\\tthey\\tare\\tplaced\\ton\\tthe\\tsame\\tdevice,\\tthey\\tget\\tevaluated\\tin\\tdifferent\\tthreads,\\tso\\tthey\\nmay\\trun\\tin\\tparallel\\ttoo\\t(in\\tseparate\\tGPU\\tthreads\\tor\\tCPU\\tcores).\\nTensorFlow\\tmanages\\ta\\t\\nthread\\tpool\\ton\\teach\\tdevice\\tto\\tparallelize\\toperations\\t(see\\t\\nFigure\\t12-5\\n).\\tThese\\tare\\ncalled\\tthe\\t\\ninter-op\\tthread\\tpools\\n.\\tSome\\toperations\\thave\\tmultithreaded\\tkernels:\\tthey\\tcan\\tuse\\tother\\tthread\\npools\\t(one\\tper\\tdevice)\\tcalled\\tthe\\t\\nintra-op\\tthread\\tpools\\n.\\nFigure\\t12-5.\\t\\nParallelized\\texecution\\tof\\ta\\tTensorFlow\\tgraph\\nFor\\texample,\\tin\\t\\nFigure\\t12-5\\n,\\toperations\\tA,\\tB,\\tand\\tC\\tare\\tsource\\tops,\\t\\nso\\tthey\\tcan\\timmediately\\tbe\\nevaluated.\\tOperations\\tA\\tand\\tB\\tare\\tplaced\\ton\\tGPU\\t#0,\\tso\\tthey\\tare\\tsent\\tto\\tthis\\tdevice’s\\tinter-op\\tthread\\npool,\\tand\\timmediately\\tevaluated\\tin\\tparallel.\\tOperation\\tA\\thappens\\tto\\thave\\ta\\tmultithreaded\\tkernel;\\tits\\ncomputations\\tare\\tsplit\\tin\\tthree\\tparts,\\twhich\\tare\\texecuted\\tin\\tparallel\\tby\\tthe\\tintra-op\\tthread\\tpool.\\tOperation\\nC\\tgoes\\tto\\tGPU\\t#1’s\\tinter-op\\tthread\\tpool.\\nAs\\tsoon\\tas\\toperation\\tC\\tfinishes,\\tthe\\tdependency\\tcounters\\tof\\toperations\\tD\\tand\\tE\\twill\\tbe\\tdecremented\\tand\\nwill\\tboth\\treach\\t0,\\tso\\tboth\\toperations\\twill\\tbe\\tsent\\tto\\tthe\\tinter-op\\tthread\\tpool\\tto\\tbe\\texecuted.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 403}), Document(page_content='TIP\\nYou\\tcan\\tcontrol\\tthe\\tnumber\\tof\\tthreads\\tper\\tinter-op\\tpool\\tby\\tsetting\\tthe\\t\\ninter_op_parallelism_threads\\n\\toption.\\t\\nNote\\tthat\\tthe\\nfirst\\tsession\\tyou\\tstart\\tcreates\\tthe\\tinter-op\\tthread\\tpools.\\tAll\\tother\\tsessions\\twill\\tjust\\treuse\\tthem\\tunless\\tyou\\tset\\tthe\\nuse_per_session_threads\\n\\toption\\tto\\t\\nTrue\\n.\\tYou\\tcan\\tcontrol\\tthe\\tnumber\\tof\\t\\nthreads\\tper\\tintra-op\\tpool\\tby\\tsetting\\tthe\\nintra_op_parallelism_threads\\n\\toption.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 404}), Document(page_content='Control\\tDependencies\\nIn\\t\\nsome\\tcases,\\tit\\tmay\\tbe\\twise\\tto\\tpostpone\\tthe\\tevaluation\\tof\\tan\\toperation\\teven\\tthough\\tall\\tthe\\toperations\\tit\\ndepends\\ton\\thave\\tbeen\\texecuted.\\tFor\\texample,\\tif\\tit\\tuses\\ta\\tlot\\tof\\tmemory\\tbut\\tits\\tvalue\\tis\\tneeded\\tonly\\tmuch\\nfurther\\tin\\tthe\\tgraph,\\tit\\twould\\tbe\\tbest\\tto\\tevaluate\\tit\\tat\\tthe\\tlast\\tmoment\\tto\\tavoid\\tneedlessly\\toccupying\\tRAM\\nthat\\tother\\toperations\\tmay\\tneed.\\tAnother\\texample\\tis\\ta\\tset\\tof\\toperations\\tthat\\tdepend\\ton\\tdata\\tlocated\\toutside\\nof\\tthe\\tdevice.\\tIf\\tthey\\tall\\trun\\tat\\tthe\\tsame\\ttime,\\tthey\\tmay\\tsaturate\\tthe\\tdevice’s\\tcommunication\\tbandwidth,\\nand\\tthey\\twill\\tend\\tup\\tall\\twaiting\\ton\\tI/O.\\tOther\\toperations\\tthat\\tneed\\tto\\tcommunicate\\tdata\\twill\\talso\\tbe\\nblocked.\\tIt\\twould\\tbe\\tpreferable\\tto\\texecute\\tthese\\tcommunication-heavy\\toperations\\tsequentially,\\tallowing\\nthe\\tdevice\\tto\\tperform\\tother\\toperations\\tin\\tparallel.\\nTo\\tpostpone\\tevaluation\\tof\\tsome\\tnodes,\\ta\\tsimple\\tsolution\\tis\\tto\\tadd\\t\\ncontrol\\tdependencies\\n.\\tFor\\texample,\\nthe\\tfollowing\\tcode\\ttells\\tTensorFlow\\tto\\tevaluate\\t\\nx\\n\\tand\\t\\ny\\n\\tonly\\tafter\\t\\na\\n\\tand\\t\\nb\\n\\thave\\tbeen\\t\\nevaluated:\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n1.0\\n)\\nb\\n\\t\\n=\\n\\t\\na\\n\\t\\n+\\n\\t\\n2.0\\nwith\\n\\t\\ntf\\n.\\ncontrol_dependencies\\n([\\na\\n,\\n\\t\\nb\\n]):\\n\\t\\t\\t\\t\\nx\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n3.0\\n)\\n\\t\\t\\t\\t\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n4.0\\n)\\nz\\n\\t\\n=\\n\\t\\nx\\n\\t\\n+\\n\\t\\ny\\nObviously,\\tsince\\t\\nz\\n\\tdepends\\ton\\t\\nx\\n\\tand\\t\\ny\\n,\\tevaluating\\t\\nz\\n\\talso\\timplies\\twaiting\\tfor\\t\\na\\n\\tand\\t\\nb\\n\\tto\\tbe\\tevaluated,\\neven\\tthough\\tit\\tis\\tnot\\texplicitly\\tin\\tthe\\t\\ncontrol_dependencies()\\n\\tblock.\\tAlso,\\tsince\\t\\nb\\n\\tdepends\\ton\\t\\na\\n,\\twe\\ncould\\tsimplify\\tthe\\tpreceding\\tcode\\tby\\tjust\\tcreating\\ta\\tcontrol\\tdependency\\ton\\t\\n[b]\\n\\tinstead\\tof\\t\\n[a,\\tb]\\n,\\tbut\\tin\\nsome\\tcases\\t“explicit\\tis\\tbetter\\tthan\\timplicit.”\\nGreat!\\tNow\\tyou\\tknow:\\nHow\\tto\\tplace\\toperations\\ton\\tmultiple\\tdevices\\tin\\tany\\tway\\tyou\\tplease\\nHow\\tthese\\toperations\\tget\\texecuted\\tin\\tparallel\\nHow\\tto\\tcreate\\tcontrol\\tdependencies\\tto\\toptimize\\t\\nparallel\\texecution\\nIt’s\\ttime\\tto\\tdistribute\\tcomputations\\tacross\\tmultiple\\tservers!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 405}), Document(page_content='Multiple\\tDevices\\tAcross\\tMultiple\\tServers\\nTo\\t\\nrun\\ta\\tgraph\\tacross\\tmultiple\\tservers,\\tyou\\tfirst\\tneed\\tto\\tdefine\\ta\\t\\ncluster\\n.\\tA\\t\\ncluster\\tis\\tcomposed\\tof\\tone\\tor\\nmore\\tTensorFlow\\tservers,\\t\\ncalled\\t\\ntasks\\n,\\ttypically\\tspread\\tacross\\tseveral\\tmachines\\t(see\\t\\nFigure\\t12-6\\n).\\tEach\\ntask\\tbelongs\\tto\\t\\na\\t\\njob\\n.\\tA\\tjob\\tis\\tjust\\ta\\tnamed\\tgroup\\tof\\ttasks\\tthat\\ttypically\\thave\\ta\\tcommon\\trole,\\tsuch\\tas\\nkeeping\\ttrack\\tof\\tthe\\tmodel\\tparameters\\t(such\\ta\\tjob\\tis\\tusually\\t\\nnamed\\t\\n\"ps\"\\n\\tfor\\t\\nparameter\\tserver\\n),\\tor\\nperforming\\tcomputations\\t(such\\ta\\tjob\\tis\\tusually\\t\\nnamed\\t\\n\"worker\"\\n).\\nFigure\\t12-6.\\t\\nTensorFlow\\tcluster\\nThe\\tfollowing\\t\\ncluster\\tspecification\\n\\t\\ndefines\\ttwo\\tjobs,\\t\\n\"ps\"\\n\\tand\\t\\n\"worker\"\\n,\\tcontaining\\tone\\ttask\\tand\\ttwo\\ntasks,\\trespectively.\\tIn\\tthis\\texample,\\tmachine\\tA\\thosts\\ttwo\\tTensorFlow\\tservers\\t(i.e.,\\ttasks),\\tlistening\\ton\\ndifferent\\tports:\\tone\\tis\\tpart\\tof\\tthe\\t\\n\"ps\"\\n\\tjob,\\tand\\tthe\\tother\\tis\\tpart\\tof\\tthe\\t\\n\"worker\"\\n\\tjob.\\tMachine\\tB\\tjust\\thosts\\none\\tTensorFlow\\tserver,\\tpart\\tof\\tthe\\t\\n\"worker\"\\n\\tjob.\\ncluster_spec\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nClusterSpec\\n({\\n\\t\\t\\t\\t\\n\"ps\"\\n:\\n\\t\\n[\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\"machine-a.example.com:2221\"\\n,\\n\\t\\t\\n#\\t/job:ps/task:0\\n\\t\\t\\t\\t\\n],\\n\\t\\t\\t\\t\\n\"worker\"\\n:\\n\\t\\n[\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\"machine-a.example.com:2222\"\\n,\\n\\t\\t\\n#\\t/job:worker/task:0\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\"machine-b.example.com:2222\"\\n,\\n\\t\\t\\n#\\t/job:worker/task:1\\n\\t\\t\\t\\t\\n]})\\nTo\\tstart\\ta\\tTensorFlow\\tserver,\\t\\nyou\\tmust\\tcreate\\ta\\t\\nServer\\n\\tobject,\\tpassing\\tit\\tthe\\tcluster\\tspecification\\t(so\\tit\\ncan\\tcommunicate\\twith\\tother\\tservers)\\tand\\tits\\town\\tjob\\tname\\tand\\ttask\\tnumber.\\tFor\\texample,\\tto\\tstart\\tthe\\tfirst\\nworker\\ttask,\\tyou\\twould\\trun\\tthe\\tfollowing\\tcode\\ton\\tmachine\\tA:\\nserver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nServer\\n(\\ncluster_spec\\n,\\n\\t\\njob_name\\n=\\n\"worker\"\\n,\\n\\t\\ntask_index\\n=\\n0\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 406}), Document(page_content='It\\tis\\tusually\\tsimpler\\tto\\tjust\\trun\\tone\\ttask\\tper\\tmachine,\\tbut\\tthe\\tprevious\\texample\\tdemonstrates\\tthat\\nTensorFlow\\tallows\\tyou\\tto\\trun\\tmultiple\\ttasks\\ton\\tthe\\tsame\\tmachine\\tif\\tyou\\twant.\\n2\\n\\tIf\\tyou\\thave\\tseveral\\nservers\\ton\\tone\\tmachine,\\tyou\\twill\\tneed\\tto\\tensure\\tthat\\tthey\\tdon’t\\tall\\ttry\\tto\\tgrab\\tall\\tthe\\tRAM\\tof\\tevery\\tGPU,\\nas\\texplained\\tearlier.\\tFor\\texample,\\tin\\t\\nFigure\\t12-6\\n\\tthe\\t\\n\"ps\"\\n\\ttask\\tdoes\\tnot\\tsee\\tthe\\tGPU\\tdevices,\\tsince\\npresumably\\tits\\tprocess\\twas\\tlaunched\\twith\\t\\nCUDA_VISIBLE_DEVICES=\"\"\\n.\\tNote\\tthat\\tthe\\tCPU\\tis\\tshared\\tby\\nall\\ttasks\\tlocated\\ton\\tthe\\tsame\\tmachine.\\nIf\\tyou\\twant\\tthe\\tprocess\\tto\\tdo\\tnothing\\tother\\tthan\\trun\\tthe\\tTensorFlow\\tserver,\\tyou\\tcan\\tblock\\tthe\\tmain\\tthread\\nby\\ttelling\\tit\\tto\\twait\\tfor\\tthe\\tserver\\tto\\tfinish\\tusing\\tthe\\t\\njoin()\\n\\t\\nmethod\\t(otherwise\\tthe\\tserver\\twill\\tbe\\tkilled\\nas\\tsoon\\tas\\tyour\\tmain\\tthread\\texits).\\tSince\\tthere\\tis\\tcurrently\\tno\\tway\\tto\\tstop\\tthe\\tserver,\\tthis\\twill\\tactually\\nblock\\tforever:\\nserver\\n.\\njoin\\n()\\n\\t\\t\\n#\\tblocks\\tuntil\\tthe\\tserver\\tstops\\t(i.e.,\\tnever)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 407}), Document(page_content='Opening\\ta\\tSession\\nOnce\\t\\nall\\tthe\\ttasks\\tare\\tup\\tand\\trunning\\t(doing\\tnothing\\tyet),\\tyou\\tcan\\topen\\ta\\tsession\\ton\\tany\\tof\\tthe\\tservers,\\nfrom\\ta\\tclient\\tlocated\\tin\\tany\\tprocess\\ton\\tany\\tmachine\\t(even\\tfrom\\ta\\tprocess\\trunning\\tone\\tof\\tthe\\ttasks),\\tand\\nuse\\tthat\\tsession\\tlike\\ta\\tregular\\tlocal\\tsession.\\t\\nFor\\texample:\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n1.0\\n)\\nb\\n\\t\\n=\\n\\t\\na\\n\\t\\n+\\n\\t\\n2\\nc\\n\\t\\n=\\n\\t\\na\\n\\t\\n*\\n\\t\\n3\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\n\"grpc://machine-b.example.com:2222\"\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nprint\\n(\\nc\\n.\\neval\\n())\\n\\t\\t\\n#\\t9.0\\nThis\\tclient\\tcode\\tfirst\\tcreates\\ta\\tsimple\\tgraph,\\tthen\\topens\\ta\\tsession\\ton\\tthe\\tTensorFlow\\tserver\\tlocated\\ton\\nmachine\\tB\\t(which\\twe\\twill\\tcall\\tthe\\t\\nmaster\\n),\\tand\\tinstructs\\tit\\tto\\tevaluate\\t\\nc\\n.\\tThe\\tmaster\\tstarts\\tby\\tplacing\\tthe\\noperations\\ton\\tthe\\tappropriate\\tdevices.\\tIn\\tthis\\texample,\\tsince\\twe\\tdid\\tnot\\tpin\\tany\\toperation\\ton\\tany\\tdevice,\\nthe\\tmaster\\tsimply\\tplaces\\tthem\\tall\\ton\\tits\\town\\tdefault\\tdevice\\t—\\tin\\tthis\\tcase,\\tmachine\\tB’s\\tGPU\\tdevice.\\nThen\\tit\\tjust\\tevaluates\\t\\nc\\n\\tas\\tinstructed\\tby\\tthe\\tclient,\\tand\\tit\\treturns\\tthe\\tresult.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 408}), Document(page_content='The\\tMaster\\tand\\tWorker\\tServices\\nThe\\t\\nclient\\tuses\\tthe\\t\\ngRPC\\n\\tprotocol\\t(\\nGoogle\\tRemote\\tProcedure\\tCall\\n)\\tto\\tcommunicate\\twith\\tthe\\tserver.\\tThis\\nis\\tan\\tefficient\\topen\\tsource\\tframework\\tto\\tcall\\tremote\\tfunctions\\tand\\tget\\ttheir\\toutputs\\tacross\\ta\\tvariety\\tof\\nplatforms\\tand\\tlanguages.\\n3\\n\\tIt\\tis\\tbased\\ton\\tHTTP2,\\twhich\\topens\\ta\\tconnection\\tand\\tleaves\\tit\\topen\\tduring\\tthe\\nwhole\\tsession,\\tallowing\\tefficient\\tbidirectional\\tcommunication\\tonce\\tthe\\tconnection\\tis\\testablished.\\tData\\tis\\ntransmitted\\tin\\tthe\\tform\\tof\\t\\nprotocol\\tbuffers\\n,\\tanother\\topen\\tsource\\tGoogle\\ttechnology.\\tThis\\tis\\ta\\tlightweight\\nbinary\\tdata\\tinterchange\\tformat.\\nWARNING\\nAll\\tservers\\tin\\ta\\tTensorFlow\\tcluster\\tmay\\tcommunicate\\twith\\tany\\tother\\tserver\\tin\\tthe\\tcluster,\\tso\\tmake\\tsure\\tto\\topen\\tthe\\tappropriate\\nports\\ton\\tyour\\tfirewall.\\nEvery\\tTensorFlow\\tserver\\tprovides\\ttwo\\tservices:\\tthe\\t\\nmaster\\tservice\\n\\tand\\tthe\\t\\nworker\\tservice\\n.\\tThe\\tmaster\\nservice\\tallows\\tclients\\tto\\topen\\tsessions\\tand\\tuse\\tthem\\tto\\trun\\tgraphs.\\tIt\\tcoordinates\\tthe\\tcomputations\\tacross\\ntasks,\\trelying\\ton\\tthe\\tworker\\tservice\\tto\\tactually\\texecute\\tcomputations\\ton\\tother\\ttasks\\tand\\tget\\ttheir\\tresults.\\nThis\\tarchitecture\\tgives\\tyou\\ta\\tlot\\tof\\tflexibility.\\tOne\\tclient\\tcan\\tconnect\\tto\\tmultiple\\tservers\\tby\\topening\\nmultiple\\tsessions\\tin\\tdifferent\\tthreads.\\tOne\\tserver\\tcan\\thandle\\tmultiple\\tsessions\\tsimultaneously\\tfrom\\tone\\tor\\nmore\\tclients.\\tYou\\tcan\\trun\\tone\\tclient\\tper\\ttask\\t(typically\\twithin\\tthe\\tsame\\tprocess),\\tor\\tjust\\tone\\tclient\\tto\\ncontrol\\tall\\ttasks.\\tAll\\toptions\\tare\\topen.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 409}), Document(page_content='Pinning\\tOperations\\tAcross\\tTasks\\nYou\\t\\ncan\\tuse\\tdevice\\tblocks\\tto\\tpin\\toperations\\ton\\tany\\tdevice\\tmanaged\\tby\\tany\\ttask,\\tby\\tspecifying\\tthe\\tjob\\nname,\\ttask\\tindex,\\tdevice\\ttype,\\tand\\tdevice\\tindex.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tpins\\t\\na\\n\\tto\\tthe\\tCPU\\tof\\tthe\\nfirst\\ttask\\tin\\tthe\\t\\n\"ps\"\\n\\tjob\\t(that’s\\tthe\\tCPU\\ton\\tmachine\\tA),\\tand\\tit\\tpins\\t\\nb\\n\\tto\\tthe\\tsecond\\tGPU\\tmanaged\\tby\\tthe\\nfirst\\ttask\\tof\\tthe\\t\\n\"worker\"\\n\\tjob\\t(that’s\\tGPU\\t#1\\ton\\tmachine\\tA).\\tFinally,\\t\\nc\\n\\tis\\tnot\\tpinned\\tto\\tany\\tdevice,\\tso\\tthe\\nmaster\\tplaces\\tit\\ton\\tits\\town\\tdefault\\tdevice\\t\\n(machine\\tB’s\\tGPU\\t#0\\tdevice).\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/job:ps/task:0/cpu:0\"\\n)\\n\\t\\t\\t\\t\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n1.0\\n)\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/job:worker/task:0/gpu:1\"\\n)\\n\\t\\t\\t\\t\\nb\\n\\t\\n=\\n\\t\\na\\n\\t\\n+\\n\\t\\n2\\nc\\n\\t\\n=\\n\\t\\na\\n\\t\\n+\\n\\t\\nb\\nAs\\tearlier,\\tif\\tyou\\tomit\\tthe\\tdevice\\ttype\\tand\\tindex,\\tTensorFlow\\twill\\tdefault\\tto\\tthe\\ttask’s\\tdefault\\tdevice;\\tfor\\nexample,\\tpinning\\tan\\toperation\\tto\\t\\n\"/job:ps/task:0\"\\n\\twill\\tplace\\tit\\ton\\tthe\\tdefault\\tdevice\\tof\\tthe\\tfirst\\ttask\\tof\\nthe\\t\\n\"ps\"\\n\\tjob\\t(machine\\tA’s\\tCPU).\\tIf\\tyou\\talso\\tomit\\tthe\\ttask\\tindex\\t(e.g.,\\t\\n\"/job:ps\"\\n),\\tTensorFlow\\tdefaults\\nto\\t\\n\"/task:0\"\\n.\\tIf\\tyou\\tomit\\tthe\\tjob\\tname\\tand\\tthe\\ttask\\tindex,\\tTensorFlow\\tdefaults\\tto\\tthe\\tsession’s\\tmaster\\ntask.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 410}), Document(page_content='Sharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nAs\\twe\\t\\nwill\\tsee\\tshortly,\\ta\\tcommon\\tpattern\\twhen\\ttraining\\ta\\tneural\\tnetwork\\ton\\ta\\tdistributed\\tsetup\\tis\\tto\\tstore\\nthe\\tmodel\\tparameters\\ton\\ta\\tset\\tof\\tparameter\\tservers\\t(i.e.,\\tthe\\ttasks\\tin\\tthe\\t\\n\"ps\"\\n\\tjob)\\twhile\\tother\\ttasks\\tfocus\\non\\tcomputations\\t(i.e.,\\tthe\\ttasks\\tin\\tthe\\t\\n\"worker\"\\n\\tjob).\\tFor\\tlarge\\tmodels\\twith\\tmillions\\tof\\tparameters,\\tit\\tis\\nuseful\\tto\\tshard\\tthese\\tparameters\\tacross\\tmultiple\\tparameter\\tservers,\\tto\\treduce\\tthe\\trisk\\tof\\tsaturating\\ta\\nsingle\\tparameter\\tserver’s\\tnetwork\\tcard.\\tIf\\tyou\\twere\\tto\\tmanually\\tpin\\tevery\\tvariable\\tto\\ta\\tdifferent\\nparameter\\tserver,\\tit\\twould\\tbe\\tquite\\ttedious.\\tFortunately,\\tTensorFlow\\tprovides\\tthe\\nreplica_device_setter()\\n\\tfunction,\\t\\nwhich\\tdistributes\\tvariables\\tacross\\tall\\tthe\\t\\n\"ps\"\\n\\ttasks\\tin\\ta\\tround-\\nrobin\\tfashion.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tpins\\tfive\\tvariables\\tto\\ttwo\\tparameter\\tservers:\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\ntf\\n.\\ntrain\\n.\\nreplica_device_setter\\n(\\nps_tasks\\n=\\n2\\n):\\n\\t\\t\\t\\t\\nv1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n1.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:0\\n\\t\\t\\t\\t\\nv2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n2.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:1\\n\\t\\t\\t\\t\\nv3\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:0\\n\\t\\t\\t\\t\\nv4\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n4.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:1\\n\\t\\t\\t\\t\\nv5\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n5.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:0\\nInstead\\tof\\tpassing\\tthe\\tnumber\\tof\\t\\nps_tasks\\n,\\tyou\\tcan\\tpass\\tthe\\tcluster\\tspec\\t\\ncluster=cluster_spec\\n\\tand\\nTensorFlow\\twill\\tsimply\\tcount\\tthe\\tnumber\\tof\\ttasks\\tin\\tthe\\t\\n\"ps\"\\n\\tjob.\\nIf\\tyou\\tcreate\\tother\\toperations\\tin\\tthe\\tblock,\\tbeyond\\tjust\\tvariables,\\tTensorFlow\\tautomatically\\tpins\\tthem\\tto\\n\"/job:worker\"\\n,\\twhich\\twill\\tdefault\\tto\\tthe\\tfirst\\tdevice\\tmanaged\\tby\\tthe\\tfirst\\ttask\\tin\\tthe\\t\\n\"worker\"\\n\\tjob.\\tYou\\ncan\\tpin\\tthem\\tto\\tanother\\tdevice\\tby\\tsetting\\tthe\\t\\nworker_device\\n\\t\\nparameter,\\tbut\\ta\\tbetter\\tapproach\\tis\\tto\\tuse\\nembedded\\tdevice\\tblocks.\\tAn\\tinner\\tdevice\\tblock\\tcan\\toverride\\tthe\\tjob,\\ttask,\\tor\\tdevice\\tdefined\\tin\\tan\\touter\\nblock.\\tFor\\texample:\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\ntf\\n.\\ntrain\\n.\\nreplica_device_setter\\n(\\nps_tasks\\n=\\n2\\n)):\\n\\t\\t\\t\\t\\nv1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n1.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:0\\t(+\\tdefaults\\tto\\t/cpu:0)\\n\\t\\t\\t\\t\\nv2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n2.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:1\\t(+\\tdefaults\\tto\\t/cpu:0)\\n\\t\\t\\t\\t\\nv3\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:0\\t(+\\tdefaults\\tto\\t/cpu:0)\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\ns\\n\\t\\n=\\n\\t\\nv1\\n\\t\\n+\\n\\t\\nv2\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tpinned\\tto\\t/job:worker\\t(+\\tdefaults\\tto\\ttask:0/gpu:0)\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/gpu:1\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\np1\\n\\t\\n=\\n\\t\\n2\\n\\t\\n*\\n\\t\\ns\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tpinned\\tto\\t/job:worker/gpu:1\\t(+\\tdefaults\\tto\\t/task:0)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/task:1\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\np2\\n\\t\\n=\\n\\t\\n3\\n\\t\\n*\\n\\t\\ns\\n\\t\\t\\t\\t\\t\\n#\\tpinned\\tto\\t/job:worker/task:1/gpu:1\\nNOTE\\nThis\\texample\\tassumes\\tthat\\tthe\\tparameter\\tservers\\tare\\tCPU-only,\\twhich\\tis\\ttypically\\tthe\\tcase\\tsince\\tthey\\tonly\\tneed\\tto\\tstore\\tand\\ncommunicate\\tparameters,\\tnot\\tperform\\tintensive\\tcomputations.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 411}), Document(page_content='Sharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\nWhen\\t\\nyou\\tare\\tusing\\ta\\tplain\\t\\nlocal\\tsession\\n\\t\\n(not\\tthe\\tdistributed\\tkind),\\teach\\tvariable’s\\tstate\\tis\\tmanaged\\tby\\nthe\\tsession\\titself;\\tas\\tsoon\\tas\\tit\\tends,\\tall\\tvariable\\tvalues\\tare\\tlost.\\tMoreover,\\tmultiple\\tlocal\\tsessions\\tcannot\\nshare\\tany\\tstate,\\teven\\tif\\tthey\\tboth\\trun\\tthe\\tsame\\tgraph;\\teach\\tsession\\thas\\tits\\town\\tcopy\\tof\\tevery\\tvariable\\t(as\\nwe\\tdiscussed\\tin\\t\\nChapter\\t9\\n).\\tIn\\tcontrast,\\twhen\\tyou\\tare\\tusing\\t\\ndistributed\\tsessions\\n,\\t\\nvariable\\tstate\\tis\\nmanaged\\tby\\t\\nresource\\tcontainers\\n\\tlocated\\ton\\tthe\\tcluster\\titself,\\tnot\\tby\\tthe\\tsessions.\\tSo\\tif\\tyou\\tcreate\\ta\\nvariable\\tnamed\\t\\nx\\n\\tusing\\tone\\tclient\\tsession,\\tit\\twill\\tautomatically\\tbe\\tavailable\\tto\\tany\\tother\\tsession\\ton\\tthe\\nsame\\tcluster\\t(even\\tif\\tboth\\tsessions\\tare\\tconnected\\tto\\ta\\tdifferent\\tserver).\\tFor\\texample,\\tconsider\\tthe\\nfollowing\\t\\nclient\\tcode:\\n#\\tsimple_client.py\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nimport\\n\\t\\nsys\\nx\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"x\"\\n)\\nincrement_x\\n\\t\\n=\\n\\t\\ntf\\n.\\nassign\\n(\\nx\\n,\\n\\t\\nx\\n\\t\\n+\\n\\t\\n1\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\nsys\\n.\\nargv\\n[\\n1\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nif\\n\\t\\nsys\\n.\\nargv\\n[\\n2\\n:]\\n==\\n[\\n\"init\"\\n]:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nx\\n.\\ninitializer\\n)\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nincrement_x\\n)\\n\\t\\t\\t\\t\\nprint\\n(\\nx\\n.\\neval\\n())\\nLet’s\\tsuppose\\tyou\\thave\\ta\\tTensorFlow\\tcluster\\tup\\tand\\trunning\\ton\\tmachines\\tA\\tand\\tB,\\tport\\t2222.\\tYou\\tcould\\nlaunch\\tthe\\tclient,\\thave\\tit\\topen\\ta\\tsession\\twith\\tthe\\tserver\\ton\\tmachine\\tA,\\tand\\ttell\\tit\\tto\\tinitialize\\tthe\\tvariable,\\nincrement\\tit,\\tand\\tprint\\tits\\tvalue\\tby\\tlaunching\\tthe\\tfollowing\\tcommand:\\n$\\tpython3\\tsimple_client.py\\tgrpc://machine-a.example.com:2222\\tinit\\n1.0\\nNow\\tif\\tyou\\tlaunch\\tthe\\tclient\\twith\\tthe\\tfollowing\\tcommand,\\tit\\twill\\tconnect\\tto\\tthe\\tserver\\ton\\tmachine\\tB\\tand\\nmagically\\treuse\\tthe\\tsame\\tvariable\\t\\nx\\n\\t(this\\ttime\\twe\\tdon’t\\task\\tthe\\tserver\\tto\\tinitialize\\tthe\\tvariable):\\n$\\tpython3\\tsimple_client.py\\tgrpc://machine-b.example.com:2222\\n2.0\\nThis\\tfeature\\tcuts\\tboth\\tways:\\tit’s\\tgreat\\tif\\tyou\\twant\\tto\\tshare\\tvariables\\tacross\\tmultiple\\tsessions,\\tbut\\tif\\tyou\\nwant\\tto\\trun\\tcompletely\\tindependent\\tcomputations\\ton\\tthe\\tsame\\tcluster\\tyou\\twill\\thave\\tto\\tbe\\tcareful\\tnot\\tto\\nuse\\tthe\\tsame\\tvariable\\tnames\\tby\\taccident.\\tOne\\tway\\tto\\tensure\\tthat\\tyou\\twon’t\\thave\\tname\\tclashes\\tis\\tto\\twrap\\nall\\tof\\tyour\\tconstruction\\tphase\\tinside\\ta\\tvariable\\tscope\\twith\\ta\\tunique\\tname\\tfor\\teach\\t\\ncomputation,\\tfor\\nexample:\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"my_problem_1\"\\n):\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tConstruction\\tphase\\tof\\tproblem\\t1\\nA\\tbetter\\toption\\tis\\tto\\tuse\\ta\\tcontainer\\tblock:\\nwith\\n\\t\\ntf\\n.\\ncontainer\\n(\\n\"my_problem_1\"\\n):\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tConstruction\\tphase\\tof\\tproblem\\t1', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 412}), Document(page_content='This\\twill\\tuse\\ta\\tcontainer\\tdedicated\\tto\\tproblem\\t#1,\\tinstead\\tof\\tthe\\tdefault\\tone\\t(whose\\tname\\tis\\tan\\tempty\\nstring\\t\\n\"\"\\n).\\tOne\\tadvantage\\tis\\tthat\\tvariable\\tnames\\tremain\\tnice\\tand\\tshort.\\tAnother\\tadvantage\\tis\\tthat\\tyou\\tcan\\neasily\\treset\\ta\\tnamed\\tcontainer.\\tFor\\texample,\\tthe\\tfollowing\\tcommand\\twill\\tconnect\\tto\\tthe\\tserver\\ton\\nmachine\\tA\\tand\\task\\tit\\tto\\treset\\tthe\\tcontainer\\tnamed\\t\\n\"my_problem_1\"\\n,\\twhich\\twill\\tfree\\tall\\tthe\\tresources\\tthis\\ncontainer\\tused\\t(and\\talso\\tclose\\tall\\tsessions\\topen\\ton\\tthe\\tserver).\\tAny\\tvariable\\tmanaged\\tby\\tthis\\tcontainer\\nmust\\tbe\\tinitialized\\tbefore\\tyou\\tcan\\tuse\\tit\\tagain:\\ntf\\n.\\nSession\\n.\\nreset\\n(\\n\"grpc://machine-a.example.com:2222\"\\n,\\n\\t\\n[\\n\"my_problem_1\"\\n])\\nResource\\tcontainers\\tmake\\tit\\teasy\\tto\\tshare\\tvariables\\tacross\\tsessions\\tin\\tflexible\\tways.\\tFor\\texample,\\nFigure\\t12-7\\n\\tshows\\tfour\\tclients\\trunning\\tdifferent\\tgraphs\\ton\\tthe\\tsame\\tcluster,\\tbut\\tsharing\\tsome\\tvariables.\\nClients\\tA\\tand\\tB\\tshare\\tthe\\tsame\\tvariable\\t\\nx\\n\\tmanaged\\tby\\tthe\\tdefault\\tcontainer,\\twhile\\tclients\\tC\\tand\\tD\\tshare\\nanother\\tvariable\\tnamed\\t\\nx\\n\\tmanaged\\tby\\tthe\\tcontainer\\tnamed\\t\\n\"my_problem_1\"\\n.\\tNote\\tthat\\tclient\\tC\\teven\\tuses\\nvariables\\tfrom\\tboth\\tcontainers.\\nFigure\\t12-7.\\t\\nResource\\tcontainers\\nResource\\tcontainers\\talso\\ttake\\tcare\\tof\\tpreserving\\tthe\\tstate\\tof\\tother\\tstateful\\toperations,\\tnamely\\tqueues\\tand\\nreaders.\\tLet’s\\ttake\\ta\\tlook\\tat\\tqueues\\t\\nfirst.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 413}), Document(page_content='Asynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\nQueues\\t\\nare\\tanother\\tgreat\\tway\\tto\\texchange\\tdata\\tbetween\\tmultiple\\tsessions;\\tfor\\texample,\\tone\\tcommon\\tuse\\ncase\\tis\\tto\\thave\\ta\\tclient\\tcreate\\ta\\tgraph\\tthat\\tloads\\tthe\\ttraining\\tdata\\tand\\tpushes\\tit\\tinto\\ta\\tqueue,\\twhile\\tanother\\nclient\\tcreates\\ta\\tgraph\\tthat\\tpulls\\tthe\\tdata\\tfrom\\tthe\\tqueue\\tand\\ttrains\\ta\\tmodel\\t(see\\t\\nFigure\\t12-8\\n).\\tThis\\tcan\\nspeed\\tup\\ttraining\\tconsiderably\\tbecause\\tthe\\ttraining\\toperations\\tdon’t\\thave\\tto\\twait\\tfor\\tthe\\tnext\\tmini-batch\\nat\\tevery\\tstep.\\nFigure\\t12-8.\\t\\nUsing\\tqueues\\tto\\tload\\tthe\\ttraining\\tdata\\tasynchronously\\nTensorFlow\\tprovides\\tvarious\\tkinds\\tof\\t\\nqueues.\\tThe\\tsimplest\\tkind\\tis\\tthe\\t\\nfirst-in\\tfirst-out\\n\\t(\\nFIFO\\n)\\tqueue.\\nFor\\texample,\\tthe\\tfollowing\\tcode\\tcreates\\ta\\tFIFO\\tqueue\\tthat\\tcan\\tstore\\tup\\tto\\t10\\ttensors\\tcontaining\\ttwo\\tfloat\\nvalues\\teach:\\nq\\n\\t\\n=\\n\\t\\ntf\\n.\\nFIFOQueue\\n(\\ncapacity\\n=\\n10\\n,\\n\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nfloat32\\n],\\n\\t\\nshapes\\n=\\n[[\\n2\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"q\"\\n,\\n\\t\\nshared_name\\n=\\n\"shared_q\"\\n)\\nWARNING\\nTo\\tshare\\tvariables\\tacross\\tsessions,\\tall\\tyou\\thad\\tto\\tdo\\twas\\tto\\tspecify\\tthe\\t\\nsame\\tname\\tand\\tcontainer\\ton\\tboth\\tends.\\tWith\\tqueues\\nTensorFlow\\tdoes\\tnot\\tuse\\tthe\\t\\nname\\n\\tattribute\\tbut\\tinstead\\tuses\\t\\nshared_name\\n,\\tso\\tit\\tis\\timportant\\tto\\tspecify\\tit\\t(even\\tif\\tit\\tis\\tthe\\tsame\\tas\\nthe\\t\\nname\\n).\\tAnd,\\tof\\tcourse,\\tuse\\tthe\\tsame\\tcontainer.\\nEnqueuing\\tdata\\nTo\\t\\npush\\tdata\\tto\\ta\\tqueue,\\tyou\\tmust\\tcreate\\tan\\t\\nenqueue\\n\\toperation.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tpushes\\nthree\\ttraining\\tinstances\\tto\\tthe\\tqueue:\\n#\\ttraining_data_loader.py\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nq\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\ntraining_instance\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\n2\\n))\\nenqueue\\n\\t\\n=\\n\\t\\nq\\n.\\nenqueue\\n([\\ntraining_instance\\n])\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\n\"grpc://machine-a.example.com:2222\"\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\ntraining_instance\\n:\\n\\t\\n[\\n1.\\n,\\n\\t\\n2.\\n]})', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 414}), Document(page_content='\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\ntraining_instance\\n:\\n\\t\\n[\\n3.\\n,\\n\\t\\n4.\\n]})\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\ntraining_instance\\n:\\n\\t\\n[\\n5.\\n,\\n\\t\\n6.\\n]})\\nInstead\\tof\\tenqueuing\\tinstances\\tone\\tby\\tone,\\tyou\\tcan\\tenqueue\\tseveral\\tat\\ta\\ttime\\tusing\\tan\\t\\nenqueue_many\\noperation:\\n[\\n...\\n]\\ntraining_instances\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\n2\\n))\\nenqueue_many\\n\\t\\n=\\n\\t\\nq\\n.\\nenqueue\\n([\\ntraining_instances\\n])\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\n\"grpc://machine-a.example.com:2222\"\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue_many\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n=\\n{\\ntraining_instances\\n:\\n\\t\\n[[\\n1.\\n,\\n\\t\\n2.\\n],\\n\\t\\n[\\n3.\\n,\\n\\t\\n4.\\n],\\n\\t\\n[\\n5.\\n,\\n\\t\\n6.\\n]]})\\nBoth\\texamples\\tenqueue\\tthe\\tsame\\tthree\\ttensors\\tto\\tthe\\tqueue.\\nDequeuing\\tdata\\nTo\\t\\npull\\tthe\\tinstances\\tout\\tof\\tthe\\tqueue,\\ton\\tthe\\tother\\tend,\\tyou\\tneed\\tto\\tuse\\ta\\t\\ndequeue\\n\\toperation:\\n#\\ttrainer.py\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nq\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\ndequeue\\n\\t\\n=\\n\\t\\nq\\n.\\ndequeue\\n()\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\n\"grpc://machine-a.example.com:2222\"\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\t\\n#\\t[1.,\\t2.]\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\t\\n#\\t[3.,\\t4.]\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\t\\n#\\t[5.,\\t6.]\\nIn\\tgeneral\\tyou\\twill\\twant\\tto\\tpull\\ta\\twhole\\tmini-batch\\tat\\tonce,\\tinstead\\tof\\tpulling\\tjust\\tone\\tinstance\\tat\\ta\\ttime.\\nTo\\tdo\\tso,\\tyou\\tmust\\tuse\\ta\\t\\ndequeue_many\\n\\toperation,\\tspecifying\\tthe\\tmini-batch\\tsize:\\n[\\n...\\n]\\nbatch_size\\n\\t\\n=\\n\\t\\n2\\ndequeue_mini_batch\\n=\\n\\t\\nq\\n.\\ndequeue_many\\n(\\nbatch_size\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\n\"grpc://machine-a.example.com:2222\"\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue_mini_batch\\n))\\n\\t\\t\\n#\\t[[1.,\\t2.],\\t[4.,\\t5.]]\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue_mini_batch\\n))\\n\\t\\t\\n#\\tblocked\\twaiting\\tfor\\tanother\\tinstance\\nWhen\\ta\\tqueue\\tis\\tfull,\\tthe\\tenqueue\\toperation\\twill\\tblock\\tuntil\\titems\\tare\\tpulled\\tout\\tby\\ta\\tdequeue\\toperation.\\nSimilarly,\\twhen\\ta\\tqueue\\tis\\tempty\\t(or\\tyou\\tare\\tusing\\t\\ndequeue_many()\\n\\tand\\tthere\\tare\\tfewer\\titems\\tthan\\tthe\\nmini-batch\\tsize),\\tthe\\tdequeue\\toperation\\twill\\tblock\\tuntil\\tenough\\titems\\tare\\tpushed\\tinto\\tthe\\tqueue\\tusing\\tan\\nenqueue\\toperation.\\nQueues\\tof\\ttuples\\nEach\\t\\nitem\\tin\\ta\\tqueue\\tcan\\tbe\\ta\\ttuple\\tof\\ttensors\\t(of\\tvarious\\ttypes\\tand\\tshapes)\\tinstead\\tof\\tjust\\ta\\tsingle\\ttensor.\\nFor\\texample,\\tthe\\tfollowing\\tqueue\\tstores\\tpairs\\tof\\ttensors,\\tone\\tof\\ttype\\t\\nint32\\n\\tand\\tshape\\t\\n()\\n,\\tand\\tthe\\tother\\tof\\ntype\\t\\nfloat32\\n\\tand\\tshape\\t\\n[3,2]\\n:\\nq\\n\\t\\n=\\n\\t\\ntf\\n.\\nFIFOQueue\\n(\\ncapacity\\n=\\n10\\n,\\n\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nint32\\n,\\n\\t\\ntf\\n.\\nfloat32\\n],\\n\\t\\nshapes\\n=\\n[[],[\\n3\\n,\\n2\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"q\"\\n,\\n\\t\\nshared_name\\n=\\n\"shared_q\"\\n)\\nThe\\tenqueue\\toperation\\tmust\\tbe\\tgiven\\tpairs\\tof\\ttensors\\t(note\\tthat\\teach\\tpair\\trepresents\\tonly\\tone\\t\\nitem\\tin\\tthe', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 415}), Document(page_content='queue):\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint32\\n,\\n\\t\\nshape\\n=\\n())\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\n3\\n,\\n\\t\\n2\\n))\\nenqueue\\n\\t\\n=\\n\\t\\nq\\n.\\nenqueue\\n((\\na\\n,\\n\\t\\nb\\n))\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\na\\n:\\n\\t\\n10\\n,\\n\\t\\nb\\n:[[\\n1.\\n,\\n\\t\\n2.\\n],\\n\\t\\n[\\n3.\\n,\\n\\t\\n4.\\n],\\n\\t\\n[\\n5.\\n,\\n\\t\\n6.\\n]]})\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\na\\n:\\n\\t\\n11\\n,\\n\\t\\nb\\n:[[\\n2.\\n,\\n\\t\\n4.\\n],\\n\\t\\n[\\n6.\\n,\\n\\t\\n8.\\n],\\n\\t\\n[\\n0.\\n,\\n\\t\\n2.\\n]]})\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\na\\n:\\n\\t\\n12\\n,\\n\\t\\nb\\n:[[\\n3.\\n,\\n\\t\\n6.\\n],\\n\\t\\n[\\n9.\\n,\\n\\t\\n2.\\n],\\n\\t\\n[\\n5.\\n,\\n\\t\\n8.\\n]]})\\nOn\\tthe\\tother\\tend,\\tthe\\t\\ndequeue()\\n\\t\\nfunction\\tnow\\tcreates\\ta\\tpair\\tof\\tdequeue\\toperations:\\ndequeue_a\\n,\\n\\t\\ndequeue_b\\n\\t\\n=\\n\\t\\nq\\n.\\ndequeue\\n()\\nIn\\tgeneral,\\tyou\\tshould\\trun\\tthese\\toperations\\ttogether:\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\na_val\\n,\\n\\t\\nb_val\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n([\\ndequeue_a\\n,\\n\\t\\ndequeue_b\\n])\\n\\t\\t\\t\\t\\nprint\\n(\\na_val\\n)\\n\\t\\n#\\t10\\n\\t\\t\\t\\t\\nprint\\n(\\nb_val\\n)\\n\\t\\n#\\t[[1.,\\t2.],\\t[3.,\\t4.],\\t[5.,\\t6.]]\\nWARNING\\nIf\\tyou\\trun\\t\\ndequeue_a\\n\\ton\\tits\\town,\\tit\\twill\\tdequeue\\ta\\tpair\\tand\\treturn\\tonly\\tthe\\tfirst\\telement;\\tthe\\tsecond\\telement\\twill\\tbe\\tlost\\t(and\\nsimilarly,\\tif\\tyou\\trun\\t\\ndequeue_b\\n\\ton\\tits\\town,\\tthe\\tfirst\\telement\\twill\\tbe\\tlost).\\nThe\\t\\ndequeue_many()\\n\\t\\nfunction\\talso\\treturns\\ta\\tpair\\tof\\toperations:\\nbatch_size\\n\\t\\n=\\n\\t\\n2\\ndequeue_as\\n,\\n\\t\\ndequeue_bs\\n\\t\\n=\\n\\t\\nq\\n.\\ndequeue_many\\n(\\nbatch_size\\n)\\nYou\\tcan\\tuse\\tit\\tas\\tyou\\twould\\texpect:\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\na\\n,\\n\\t\\nb\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n([\\ndequeue_a\\n,\\n\\t\\ndequeue_b\\n])\\n\\t\\t\\t\\t\\nprint\\n(\\na\\n)\\n\\t\\n#\\t[10,\\t11]\\n\\t\\t\\t\\t\\nprint\\n(\\nb\\n)\\n\\t\\n#\\t[[[1.,\\t2.],\\t[3.,\\t4.],\\t[5.,\\t6.]],\\t[[2.,\\t4.],\\t[6.,\\t8.],\\t[0.,\\t2.]]]\\n\\t\\t\\t\\t\\na\\n,\\n\\t\\nb\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n([\\ndequeue_a\\n,\\n\\t\\ndequeue_b\\n])\\n\\t\\t\\n#\\tblocked\\twaiting\\tfor\\tanother\\tpair\\nClosing\\ta\\tqueue\\nIt\\t\\nis\\tpossible\\tto\\tclose\\ta\\tqueue\\tto\\tsignal\\tto\\tthe\\tother\\tsessions\\tthat\\tno\\tmore\\tdata\\twill\\tbe\\tenqueued:\\nclose_q\\n\\t\\n=\\n\\t\\nq\\n.\\nclose\\n()\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nclose_q\\n)\\nSubsequent\\texecutions\\tof\\t\\nenqueue\\n\\tor\\t\\nenqueue_many\\n\\toperations\\twill\\traise\\tan\\texception.\\tBy\\tdefault,\\tany\\npending\\tenqueue\\trequest\\twill\\tbe\\thonored,\\tunless\\tyou\\tcall\\t\\nq.close(cancel_pending_enqueues=True)\\n.\\nSubsequent\\texecutions\\tof\\t\\ndequeue\\n\\tor\\t\\ndequeue_many\\n\\toperations\\twill\\tcontinue\\tto\\tsucceed\\tas\\tlong\\tas\\tthere', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 416}), Document(page_content='are\\titems\\tin\\tthe\\tqueue,\\tbut\\tthey\\twill\\tfail\\twhen\\tthere\\tare\\tnot\\tenough\\titems\\tleft\\tin\\tthe\\tqueue.\\tIf\\tyou\\tare\\tusing\\na\\t\\ndequeue_many\\n\\toperation\\tand\\tthere\\tare\\ta\\tfew\\tinstances\\tleft\\tin\\tthe\\tqueue,\\tbut\\tfewer\\tthan\\tthe\\tmini-batch\\nsize,\\tthey\\twill\\tbe\\tlost.\\tYou\\tmay\\tprefer\\tto\\tuse\\ta\\t\\ndequeue_up_to\\n\\t\\noperation\\tinstead;\\tit\\tbehaves\\texactly\\tlike\\ndequeue_many\\n\\texcept\\twhen\\ta\\tqueue\\tis\\tclosed\\tand\\tthere\\tare\\tfewer\\tthan\\t\\nbatch_size\\n\\tinstances\\tleft\\tin\\tthe\\nqueue,\\tin\\twhich\\tcase\\tit\\tjust\\treturns\\tthem.\\nRandomShuffleQueue\\nTensorFlow\\talso\\t\\nsupports\\ta\\tcouple\\tmore\\ttypes\\tof\\tqueues,\\tincluding\\t\\nRandomShuffleQueue\\n,\\twhich\\tcan\\tbe\\nused\\tjust\\tlike\\ta\\t\\nFIFOQueue\\n\\t\\nexcept\\tthat\\titems\\tare\\tdequeued\\tin\\ta\\trandom\\torder.\\tThis\\tcan\\tbe\\tuseful\\tto\\tshuffle\\ntraining\\tinstances\\tat\\teach\\tepoch\\tduring\\ttraining.\\tFirst,\\tlet’s\\tcreate\\tthe\\tqueue:\\nq\\n\\t\\n=\\n\\t\\ntf\\n.\\nRandomShuffleQueue\\n(\\ncapacity\\n=\\n50\\n,\\n\\t\\nmin_after_dequeue\\n=\\n10\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nfloat32\\n],\\n\\t\\nshapes\\n=\\n[()],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"q\"\\n,\\n\\t\\nshared_name\\n=\\n\"shared_q\"\\n)\\nThe\\t\\nmin_after_dequeue\\n\\t\\nspecifies\\tthe\\tminimum\\tnumber\\tof\\titems\\tthat\\tmust\\tremain\\tin\\tthe\\tqueue\\tafter\\ta\\ndequeue\\toperation.\\tThis\\tensures\\tthat\\tthere\\twill\\tbe\\tenough\\tinstances\\tin\\tthe\\tqueue\\tto\\thave\\tenough\\nrandomness\\t(once\\tthe\\tqueue\\tis\\tclosed,\\tthe\\t\\nmin_after_dequeue\\n\\tlimit\\tis\\tignored).\\tNow\\tsuppose\\tthat\\tyou\\nenqueued\\t22\\titems\\tin\\tthis\\tqueue\\t(floats\\t\\n1.\\n\\tto\\t\\n22.\\n).\\tHere\\tis\\thow\\tyou\\tcould\\tdequeue\\tthem:\\ndequeue\\n\\t\\n=\\n\\t\\nq\\n.\\ndequeue_many\\n(\\n5\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\n#\\t[\\t20.\\t\\t15.\\t\\t11.\\t\\t12.\\t\\t\\t4.]\\t\\t\\t(17\\titems\\tleft)\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\n#\\t[\\t\\t5.\\t\\t13.\\t\\t\\t6.\\t\\t\\t0.\\t\\t17.]\\t\\t\\t(12\\titems\\tleft)\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\n#\\t12\\t-\\t5\\t<\\t10:\\tblocked\\twaiting\\tfor\\t3\\tmore\\tinstances\\nPaddingFifoQueue\\nA\\t\\nPaddingFIFOQueue\\n\\t\\ncan\\talso\\tbe\\tused\\tjust\\tlike\\ta\\t\\nFIFOQueue\\n\\texcept\\tthat\\tit\\taccepts\\ttensors\\tof\\tvariable\\nsizes\\talong\\tany\\tdimension\\t(but\\twith\\ta\\tfixed\\trank).\\tWhen\\tyou\\tare\\tdequeuing\\tthem\\twith\\ta\\t\\ndequeue_many\\n\\tor\\ndequeue_up_to\\n\\toperation,\\teach\\ttensor\\tis\\tpadded\\twith\\tzeros\\talong\\tevery\\tvariable\\tdimension\\tto\\tmake\\tit\\nthe\\tsame\\tsize\\tas\\tthe\\tlargest\\ttensor\\tin\\tthe\\tmini-batch.\\tFor\\texample,\\tyou\\tcould\\tenqueue\\t2D\\ttensors\\n(matrices)\\tof\\tarbitrary\\tsizes:\\nq\\n\\t\\n=\\n\\t\\ntf\\n.\\nPaddingFIFOQueue\\n(\\ncapacity\\n=\\n50\\n,\\n\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nfloat32\\n],\\n\\t\\nshapes\\n=\\n[(\\nNone\\n,\\n\\t\\nNone\\n)]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"q\"\\n,\\n\\t\\nshared_name\\n=\\n\"shared_q\"\\n)\\nv\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nNone\\n))\\nenqueue\\n\\t\\n=\\n\\t\\nq\\n.\\nenqueue\\n([\\nv\\n])\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nv\\n:\\n\\t\\n[[\\n1.\\n,\\n\\t\\n2.\\n],\\n\\t\\n[\\n3.\\n,\\n\\t\\n4.\\n],\\n\\t\\n[\\n5.\\n,\\n\\t\\n6.\\n]]})\\n\\t\\t\\t\\t\\t\\t\\t\\n#\\t3x2\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nv\\n:\\n\\t\\n[[\\n1.\\n]]})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\t1x1\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nv\\n:\\n\\t\\n[[\\n7.\\n,\\n\\t\\n8.\\n,\\n\\t\\n9.\\n,\\n\\t\\n5.\\n],\\n\\t\\n[\\n6.\\n,\\n\\t\\n7.\\n,\\n\\t\\n8.\\n,\\n\\t\\n9.\\n]]})\\n\\t\\n#\\t2x4\\nIf\\twe\\tjust\\tdequeue\\tone\\titem\\tat\\ta\\ttime,\\twe\\tget\\tthe\\texact\\tsame\\ttensors\\tthat\\twere\\tenqueued.\\tBut\\tif\\twe\\ndequeue\\tseveral\\titems\\tat\\ta\\ttime\\t(using\\t\\ndequeue_many()\\n\\tor\\t\\ndequeue_up_to()\\n),\\t\\nthe\\tqueue\\tautomatically\\npads\\tthe\\ttensors\\t\\nappropriately.\\tFor\\texample,\\tif\\twe\\tdequeue\\tall\\tthree\\titems\\tat\\tonce,\\tall\\ttensors\\twill\\tbe\\npadded\\twith\\tzeros\\tto\\tbecome\\t3\\t×\\t4\\ttensors,\\tsince\\tthe\\tmaximum\\tsize\\tfor\\tthe\\tfirst\\tdimension\\tis\\t3\\t(first\\titem)\\nand\\tthe\\tmaximum\\tsize\\tfor\\tthe\\tsecond\\tdimension\\tis\\t4\\t(third\\titem):', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 417}), Document(page_content='>>>\\t\\nq\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n>>>\\t\\ndequeue\\n\\t\\n=\\n\\t\\nq\\n.\\ndequeue_many\\n(\\n3\\n)\\n>>>\\t\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n[[[\\t1.\\t\\t2.\\t\\t0.\\t\\t0.]\\n\\t\\t[\\t3.\\t\\t4.\\t\\t0.\\t\\t0.]\\n\\t\\t[\\t5.\\t\\t6.\\t\\t0.\\t\\t0.]]\\n\\t[[\\t1.\\t\\t0.\\t\\t0.\\t\\t0.]\\n\\t\\t[\\t0.\\t\\t0.\\t\\t0.\\t\\t0.]\\n\\t\\t[\\t0.\\t\\t0.\\t\\t0.\\t\\t0.]]\\n\\t[[\\t7.\\t\\t8.\\t\\t9.\\t\\t5.]\\n\\t\\t[\\t6.\\t\\t7.\\t\\t8.\\t\\t9.]\\n\\t\\t[\\t0.\\t\\t0.\\t\\t0.\\t\\t0.]]]\\nThis\\ttype\\tof\\tqueue\\tcan\\tbe\\tuseful\\twhen\\tyou\\tare\\tdealing\\twith\\tvariable\\tlength\\tinputs,\\tsuch\\tas\\tsequences\\tof\\nwords\\t(see\\t\\nChapter\\t14\\n).\\nOkay,\\tnow\\tlet’s\\tpause\\tfor\\ta\\tsecond:\\tso\\tfar\\tyou\\thave\\tlearned\\tto\\tdistribute\\tcomputations\\tacross\\tmultiple\\ndevices\\tand\\tservers,\\tshare\\tvariables\\tacross\\tsessions,\\tand\\tcommunicate\\tasynchronously\\tusing\\tqueues.\\nBefore\\tyou\\tstart\\ttraining\\tneural\\tnetworks,\\tthough,\\tthere’s\\tone\\tlast\\ttopic\\twe\\tneed\\tto\\tdiscuss:\\thow\\tto\\nefficiently\\tload\\ttraining\\tdata.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 418}), Document(page_content='Loading\\tData\\tDirectly\\tfrom\\tthe\\tGraph\\nSo\\t\\nfar\\twe\\thave\\tassumed\\tthat\\tthe\\tclients\\twould\\tload\\tthe\\ttraining\\tdata\\tand\\tfeed\\tit\\tto\\tthe\\tcluster\\tusing\\nplaceholders.\\tThis\\tis\\tsimple\\tand\\tworks\\tquite\\twell\\tfor\\tsimple\\tsetups,\\tbut\\tit\\tis\\trather\\tinefficient\\tsince\\tit\\ntransfers\\tthe\\ttraining\\tdata\\tseveral\\ttimes:\\n1\\n.\\t\\nFrom\\tthe\\tfilesystem\\tto\\tthe\\tclient\\n2\\n.\\t\\nFrom\\tthe\\tclient\\tto\\tthe\\tmaster\\ttask\\n3\\n.\\t\\nPossibly\\tfrom\\tthe\\tmaster\\ttask\\tto\\tother\\ttasks\\twhere\\tthe\\tdata\\tis\\tneeded\\nIt\\tgets\\tworse\\tif\\tyou\\thave\\tseveral\\tclients\\ttraining\\tvarious\\tneural\\tnetworks\\tusing\\tthe\\tsame\\ttraining\\tdata\\t(for\\nexample,\\tfor\\thyperparameter\\ttuning):\\tif\\tevery\\tclient\\tloads\\tthe\\tdata\\tsimultaneously,\\tyou\\tmay\\tend\\tup\\teven\\nsaturating\\tyour\\tfile\\tserver\\tor\\tthe\\tnetwork’s\\tbandwidth.\\nPreload\\tthe\\tdata\\tinto\\ta\\tvariable\\nFor\\tdatasets\\tthat\\tcan\\tfit\\tin\\tmemory,\\ta\\tbetter\\toption\\tis\\tto\\tload\\tthe\\ttraining\\tdata\\tonce\\tand\\tassign\\tit\\tto\\ta\\nvariable,\\tthen\\tjust\\tuse\\tthat\\tvariable\\tin\\tyour\\tgraph.\\tThis\\tis\\t\\ncalled\\t\\npreloading\\n\\tthe\\ttraining\\tset.\\tThis\\tway\\tthe\\ndata\\twill\\tbe\\ttransferred\\tonly\\tonce\\tfrom\\tthe\\tclient\\tto\\tthe\\tcluster\\t(but\\tit\\tmay\\tstill\\tneed\\tto\\tbe\\tmoved\\taround\\nfrom\\ttask\\tto\\ttask\\tdepending\\ton\\twhich\\toperations\\tneed\\tit).\\tThe\\tfollowing\\tcode\\tshows\\thow\\tto\\tload\\tthe\\tfull\\ntraining\\tset\\tinto\\ta\\tvariable:\\ntraining_set_init\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n))\\ntraining_set\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntraining_set_init\\n,\\n\\t\\ntrainable\\n=\\nFalse\\n,\\n\\t\\ncollections\\n=\\n[],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"training_set\"\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ndata\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\t\\n#\\tload\\tthe\\ttraining\\tdata\\tfrom\\tthe\\tdatastore\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_set\\n.\\ninitializer\\n,\\n\\t\\nfeed_dict\\n=\\n{\\ntraining_set_init\\n:\\n\\t\\ndata\\n})\\nYou\\tmust\\tset\\t\\ntrainable=False\\n\\tso\\tthe\\toptimizers\\tdon’t\\ttry\\tto\\ttweak\\tthis\\tvariable.\\tYou\\tshould\\talso\\tset\\ncollections=[]\\n\\tto\\tensure\\tthat\\tthis\\tvariable\\twon’t\\tget\\tadded\\tto\\tthe\\t\\nGraphKeys.GLOBAL_VARIABLES\\ncollection,\\twhich\\tis\\tused\\tfor\\tsaving\\tand\\trestoring\\tcheckpoints.\\nNOTE\\nThis\\texample\\tassumes\\tthat\\tall\\tof\\tyour\\ttraining\\tset\\t(including\\tthe\\tlabels)\\tconsists\\tonly\\tof\\t\\nfloat32\\n\\tvalues.\\tIf\\tthat’s\\tnot\\tthe\\tcase,\\tyou\\nwill\\tneed\\tone\\tvariable\\tper\\ttype.\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\nIf\\tthe\\ttraining\\tset\\tdoes\\tnot\\tfit\\tin\\tmemory,\\ta\\tgood\\tsolution\\tis\\tto\\tuse\\t\\nreader\\toperations\\n:\\t\\nthese\\tare\\toperations\\ncapable\\tof\\treading\\tdata\\tdirectly\\tfrom\\tthe\\tfilesystem.\\tThis\\tway\\tthe\\ttraining\\tdata\\tnever\\tneeds\\tto\\tflow\\nthrough\\tthe\\tclients\\tat\\tall.\\tTensorFlow\\tprovides\\treaders\\tfor\\tvarious\\tfile\\tformats:\\nCSV', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 419}), Document(page_content='Fixed-length\\tbinary\\trecords\\nTensorFlow’s\\town\\t\\nTFRecords\\n\\tformat,\\tbased\\ton\\tprotocol\\tbuffers\\nLet’s\\tlook\\tat\\ta\\tsimple\\texample\\treading\\tfrom\\ta\\tCSV\\tfile\\t(for\\tother\\tformats,\\tplease\\tcheck\\tout\\tthe\\tAPI\\ndocumentation).\\tSuppose\\tyou\\thave\\tfile\\tnamed\\t\\nmy_test.csv\\n\\tthat\\tcontains\\ttraining\\tinstances,\\tand\\tyou\\twant\\nto\\tcreate\\toperations\\tto\\tread\\tit.\\tSuppose\\tit\\thas\\tthe\\tfollowing\\tcontent,\\twith\\ttwo\\tfloat\\tfeatures\\t\\nx1\\n\\tand\\t\\nx2\\n\\tand\\none\\tinteger\\t\\ntarget\\n\\trepresenting\\ta\\tbinary\\tclass:\\nx1\\n,\\n\\t\\t\\nx2\\n,\\n\\t\\t\\ntarget\\n1.\\n\\t\\n,\\n\\t\\n2.\\n\\t\\n,\\n\\t\\n0\\n4.\\n\\t\\n,\\n\\t\\n5\\n\\t\\t\\n,\\n\\t\\n1\\n7.\\n\\t\\n,\\n\\t\\t\\t\\t\\n,\\n\\t\\n0\\nFirst,\\tlet’s\\tcreate\\ta\\t\\nTextLineReader\\n\\tto\\t\\nread\\tthis\\tfile.\\tA\\t\\nTextLineReader\\n\\topens\\ta\\tfile\\t(once\\twe\\ttell\\tit\\nwhich\\tone\\tto\\topen)\\tand\\treads\\tlines\\tone\\tby\\tone.\\tIt\\tis\\ta\\tstateful\\toperation,\\tlike\\tvariables\\tand\\tqueues:\\tit\\npreserves\\tits\\tstate\\tacross\\tmultiple\\truns\\tof\\tthe\\tgraph,\\tkeeping\\ttrack\\tof\\twhich\\tfile\\tit\\tis\\tcurrently\\treading\\tand\\nwhat\\tits\\tcurrent\\tposition\\tis\\tin\\tthis\\tfile.\\nreader\\n\\t\\n=\\n\\t\\ntf\\n.\\nTextLineReader\\n(\\nskip_header_lines\\n=\\n1\\n)\\nNext,\\twe\\tcreate\\ta\\tqueue\\tthat\\tthe\\treader\\twill\\tpull\\tfrom\\tto\\tknow\\twhich\\tfile\\tto\\tread\\tnext.\\tWe\\talso\\tcreate\\tan\\nenqueue\\toperation\\tand\\ta\\tplaceholder\\tto\\tpush\\tany\\tfilename\\twe\\twant\\tto\\tthe\\tqueue,\\tand\\twe\\tcreate\\tan\\noperation\\tto\\tclose\\tthe\\tqueue\\tonce\\twe\\thave\\tno\\tmore\\t\\nfiles\\tto\\tread:\\nfilename_queue\\n\\t\\n=\\n\\t\\ntf\\n.\\nFIFOQueue\\n(\\ncapacity\\n=\\n10\\n,\\n\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nstring\\n],\\n\\t\\nshapes\\n=\\n[()])\\nfilename\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nstring\\n)\\nenqueue_filename\\n\\t\\n=\\n\\t\\nfilename_queue\\n.\\nenqueue\\n([\\nfilename\\n])\\nclose_filename_queue\\n\\t\\n=\\n\\t\\nfilename_queue\\n.\\nclose\\n()\\nNow\\twe\\tare\\tready\\tto\\tcreate\\ta\\t\\nread\\n\\toperation\\tthat\\twill\\tread\\tone\\trecord\\t(i.e.,\\ta\\tline)\\tat\\ta\\ttime\\tand\\treturn\\ta\\nkey/value\\tpair.\\tThe\\tkey\\tis\\tthe\\trecord’s\\tunique\\tidentifier\\t—\\ta\\tstring\\tcomposed\\tof\\tthe\\tfilename,\\ta\\tcolon\\t(\\n:\\n),\\nand\\tthe\\tline\\tnumber\\t—\\tand\\tthe\\tvalue\\tis\\tsimply\\ta\\tstring\\tcontaining\\tthe\\tcontent\\tof\\tthe\\tline:\\nkey\\n,\\n\\t\\nvalue\\n\\t\\n=\\n\\t\\nreader\\n.\\nread\\n(\\nfilename_queue\\n)\\nWe\\thave\\tall\\twe\\tneed\\tto\\tread\\tthe\\tfile\\tline\\tby\\tline!\\tBut\\twe\\tare\\tnot\\tquite\\tdone\\tyet\\t—\\twe\\tneed\\tto\\tparse\\tthis\\nstring\\tto\\tget\\tthe\\t\\nfeatures\\tand\\ttarget:\\nx1\\n,\\n\\t\\nx2\\n,\\n\\t\\ntarget\\n\\t\\n=\\n\\t\\ntf\\n.\\ndecode_csv\\n(\\nvalue\\n,\\n\\t\\nrecord_defaults\\n=\\n[[\\n-\\n1.\\n],\\n\\t\\n[\\n-\\n1.\\n],\\n\\t\\n[\\n-\\n1\\n]])\\nfeatures\\n\\t\\n=\\n\\t\\ntf\\n.\\nstack\\n([\\nx1\\n,\\n\\t\\nx2\\n])\\nThe\\tfirst\\tline\\tuses\\tTensorFlow’s\\tCSV\\tparser\\tto\\textract\\tthe\\tvalues\\tfrom\\tthe\\tcurrent\\tline.\\tThe\\tdefault\\tvalues\\nare\\tused\\twhen\\ta\\tfield\\tis\\tmissing\\t(in\\tthis\\texample\\tthe\\tthird\\ttraining\\tinstance’s\\t\\nx2\\n\\tfeature),\\tand\\tthey\\tare\\talso\\nused\\tto\\tdetermine\\tthe\\ttype\\tof\\teach\\tfield\\t(in\\tthis\\tcase\\ttwo\\tfloats\\tand\\tone\\tinteger).\\nFinally,\\twe\\tcan\\tpush\\tthis\\ttraining\\tinstance\\tand\\tits\\ttarget\\tto\\ta\\t\\nRandomShuffleQueue\\n\\tthat\\twe\\twill\\t\\nshare\\nwith\\tthe\\ttraining\\tgraph\\t(so\\tit\\tcan\\tpull\\tmini-batches\\tfrom\\tit),\\tand\\twe\\tcreate\\tan\\toperation\\tto\\tclose\\tthat\\tqueue\\nwhen\\twe\\tare\\tdone\\tpushing\\tinstances\\tto\\tit:', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 420}), Document(page_content='instance_queue\\n\\t\\n=\\n\\t\\ntf\\n.\\nRandomShuffleQueue\\n(\\n\\t\\t\\t\\t\\ncapacity\\n=\\n10\\n,\\n\\t\\nmin_after_dequeue\\n=\\n2\\n,\\n\\t\\t\\t\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nfloat32\\n,\\n\\t\\ntf\\n.\\nint32\\n],\\n\\t\\nshapes\\n=\\n[[\\n2\\n],[]],\\n\\t\\t\\t\\t\\nname\\n=\\n\"instance_q\"\\n,\\n\\t\\nshared_name\\n=\\n\"shared_instance_q\"\\n)\\nenqueue_instance\\n\\t\\n=\\n\\t\\ninstance_queue\\n.\\nenqueue\\n([\\nfeatures\\n,\\n\\t\\ntarget\\n])\\nclose_instance_queue\\n\\t\\n=\\n\\t\\ninstance_queue\\n.\\nclose\\n()\\nWow!\\tThat\\twas\\ta\\tlot\\tof\\twork\\tjust\\tto\\tread\\ta\\tfile.\\tPlus\\twe\\tonly\\tcreated\\tthe\\tgraph,\\tso\\tnow\\twe\\tneed\\tto\\trun\\tit:\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue_filename\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nfilename\\n:\\n\\t\\n\"my_test.csv\"\\n})\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nclose_filename_queue\\n)\\n\\t\\t\\t\\t\\ntry\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nwhile\\n\\t\\nTrue\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue_instance\\n)\\n\\t\\t\\t\\t\\nexcept\\n\\t\\ntf\\n.\\nerrors\\n.\\nOutOfRangeError\\n\\t\\nas\\n\\t\\nex\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npass\\n\\t\\n#\\tno\\tmore\\trecords\\tin\\tthe\\tcurrent\\tfile\\tand\\tno\\tmore\\tfiles\\tto\\tread\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nclose_instance_queue\\n)\\nFirst\\twe\\topen\\tthe\\tsession,\\tand\\tthen\\twe\\tenqueue\\tthe\\tfilename\\t\\n\"my_test.csv\"\\n\\tand\\timmediately\\tclose\\tthat\\nqueue\\tsince\\twe\\twill\\tnot\\tenqueue\\tany\\tmore\\tfilenames.\\tThen\\twe\\trun\\tan\\tinfinite\\tloop\\tto\\tenqueue\\tinstances\\none\\tby\\tone.\\tThe\\t\\nenqueue_instance\\n\\tdepends\\ton\\tthe\\treader\\treading\\tthe\\tnext\\tline,\\tso\\tat\\tevery\\titeration\\ta\\nnew\\trecord\\tis\\tread\\tuntil\\tit\\treaches\\tthe\\tend\\tof\\tthe\\tfile.\\tAt\\tthat\\tpoint\\tit\\ttries\\tto\\tread\\tthe\\tfilename\\tqueue\\tto\\nknow\\twhich\\tfile\\tto\\tread\\tnext,\\tand\\tsince\\tthe\\tqueue\\tis\\tclosed\\tit\\tthrows\\t\\nan\\t\\nOutOfRangeError\\n\\texception\\t(if\\nwe\\tdid\\tnot\\tclose\\tthe\\tqueue,\\tit\\twould\\tjust\\tremain\\tblocked\\tuntil\\twe\\tpushed\\tanother\\tfilename\\tor\\tclosed\\tthe\\nqueue).\\tLastly,\\twe\\tclose\\tthe\\tinstance\\tqueue\\tso\\tthat\\tthe\\ttraining\\toperations\\tpulling\\tfrom\\tit\\twon’t\\tget\\nblocked\\tforever.\\t\\nFigure\\t12-9\\n\\tsummarizes\\twhat\\twe\\thave\\tlearned;\\tit\\trepresents\\ta\\ttypical\\tgraph\\tfor\\treading\\ntraining\\tinstances\\tfrom\\ta\\tset\\tof\\tCSV\\tfiles.\\nFigure\\t12-9.\\t\\nA\\tgraph\\tdedicated\\tto\\treading\\ttraining\\tinstances\\tfrom\\tCSV\\tfiles\\nIn\\tthe\\ttraining\\tgraph,\\tyou\\tneed\\tto\\tcreate\\tthe\\tshared\\tinstance\\tqueue\\tand\\tsimply\\tdequeue\\tmini-batches\\t\\nfrom\\nit:\\ninstance_queue\\n\\t\\n=\\n\\t\\ntf\\n.\\nRandomShuffleQueue\\n([\\n...\\n],\\n\\t\\nshared_name\\n=\\n\"shared_instance_q\"\\n)\\nmini_batch_instances\\n,\\n\\t\\nmini_batch_targets\\n\\t\\n=\\n\\t\\ninstance_queue\\n.\\ndequeue_up_to\\n(\\n2\\n)\\n[\\n...\\n]\\n\\t\\n#\\tuse\\tthe\\tmini_batch\\tinstances\\tand\\ttargets\\tto\\tbuild\\tthe\\ttraining\\tgraph\\ntraining_op\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ntry\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nstep\\n\\t\\nin\\n\\t\\nrange\\n(\\nmax_steps\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n)\\n\\t\\t\\t\\t\\nexcept\\n\\t\\ntf\\n.\\nerrors\\n.\\nOutOfRangeError\\n\\t\\nas\\n\\t\\nex\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npass\\n\\t\\n#\\tno\\tmore\\ttraining\\tinstances', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 421}), Document(page_content='In\\tthis\\texample,\\tthe\\tfirst\\tmini-batch\\twill\\tcontain\\tthe\\tfirst\\ttwo\\tinstances\\tof\\tthe\\tCSV\\tfile,\\tand\\tthe\\tsecond\\nmini-batch\\twill\\tcontain\\tthe\\tlast\\tinstance.\\nWARNING\\nTensorFlow\\tqueues\\tdon’t\\thandle\\tsparse\\ttensors\\twell,\\tso\\tif\\tyour\\ttraining\\tinstances\\tare\\tsparse\\tyou\\tshould\\tparse\\tthe\\trecords\\tafter\\nthe\\tinstance\\tqueue.\\nThis\\tarchitecture\\twill\\tonly\\tuse\\tone\\tthread\\tto\\tread\\trecords\\tand\\tpush\\tthem\\tto\\tthe\\tinstance\\tqueue.\\tYou\\tcan\\nget\\ta\\tmuch\\thigher\\tthroughput\\tby\\thaving\\tmultiple\\tthreads\\tread\\tsimultaneously\\tfrom\\tmultiple\\tfiles\\tusing\\nmultiple\\treaders.\\tLet’s\\tsee\\thow.\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\nTo\\thave\\tmultiple\\tthreads\\tread\\tinstances\\tsimultaneously,\\tyou\\tcould\\tcreate\\t\\nPython\\tthreads\\t(using\\tthe\\nthreading\\n\\tmodule)\\tand\\tmanage\\tthem\\tyourself.\\tHowever,\\tTensorFlow\\tprovides\\tsome\\ttools\\tto\\tmake\\tthis\\nsimpler:\\tthe\\t\\nCoordinator\\n\\t\\nclass\\tand\\t\\nthe\\t\\nQueueRunner\\n\\tclass.\\nA\\t\\nCoordinator\\n\\tis\\ta\\tvery\\tsimple\\tobject\\twhose\\tsole\\tpurpose\\tis\\tto\\tcoordinate\\tstopping\\tmultiple\\tthreads.\\nFirst\\tyou\\tcreate\\ta\\t\\nCoordinator\\n:\\ncoord\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nCoordinator\\n()\\nThen\\tyou\\tgive\\tit\\tto\\tall\\tthreads\\tthat\\tneed\\tto\\tstop\\tjointly,\\tand\\ttheir\\tmain\\tloop\\tlooks\\tlike\\tthis:\\nwhile\\n\\t\\nnot\\n\\t\\ncoord\\n.\\nshould_stop\\n():\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tdo\\tsomething\\nAny\\tthread\\tcan\\trequest\\tthat\\tevery\\tthread\\tstop\\tby\\tcalling\\tthe\\t\\nCoordinator\\n’s\\t\\nrequest_stop()\\n\\t\\nmethod:\\ncoord\\n.\\nrequest_stop\\n()\\nEvery\\tthread\\twill\\tstop\\tas\\tsoon\\tas\\tit\\tfinishes\\tits\\tcurrent\\titeration.\\tYou\\tcan\\twait\\tfor\\tall\\tof\\tthe\\tthreads\\tto\\nfinish\\tby\\tcalling\\tthe\\t\\nCoordinator\\n’s\\t\\njoin()\\n\\tmethod,\\t\\npassing\\tit\\tthe\\tlist\\tof\\tthreads:\\ncoord\\n.\\njoin\\n(\\nlist_of_threads\\n)\\nA\\t\\nQueueRunner\\n\\truns\\tmultiple\\tthreads\\tthat\\teach\\trun\\tan\\tenqueue\\toperation\\trepeatedly,\\tfilling\\tup\\ta\\tqueue\\tas\\nfast\\tas\\tpossible.\\tAs\\tsoon\\tas\\tthe\\tqueue\\tis\\tclosed,\\tthe\\tnext\\tthread\\tthat\\ttries\\tto\\tpush\\tan\\titem\\tto\\tthe\\tqueue\\twill\\nget\\tan\\t\\nOutOfRangeError\\n;\\t\\nthis\\tthread\\tcatches\\tthe\\terror\\tand\\timmediately\\ttells\\tother\\tthreads\\tto\\tstop\\tusing\\ta\\nCoordinator\\n.\\tThe\\tfollowing\\tcode\\tshows\\thow\\tyou\\tcan\\tuse\\ta\\t\\nQueueRunner\\n\\tto\\thave\\tfive\\tthreads\\treading\\ninstances\\tsimultaneously\\tand\\tpushing\\tthem\\tto\\tan\\tinstance\\tqueue:\\n[\\n...\\n]\\n\\t\\n#\\tsame\\tconstruction\\tphase\\tas\\tearlier\\nqueue_runner\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nQueueRunner\\n(\\ninstance_queue\\n,\\n\\t\\n[\\nenqueue_instance\\n]\\n\\t\\n*\\n\\t\\n5\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 422}), Document(page_content='\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue_filename\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nfilename\\n:\\n\\t\\n\"my_test.csv\"\\n})\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nclose_filename_queue\\n)\\n\\t\\t\\t\\t\\ncoord\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nCoordinator\\n()\\n\\t\\t\\t\\t\\nenqueue_threads\\n\\t\\n=\\n\\t\\nqueue_runner\\n.\\ncreate_threads\\n(\\nsess\\n,\\n\\t\\ncoord\\n=\\ncoord\\n,\\n\\t\\nstart\\n=\\nTrue\\n)\\nThe\\tfirst\\tline\\tcreates\\tthe\\t\\nQueueRunner\\n\\tand\\ttells\\tit\\tto\\trun\\tfive\\tthreads,\\tall\\trunning\\tthe\\tsame\\nenqueue_instance\\n\\toperation\\trepeatedly.\\tThen\\twe\\tstart\\ta\\tsession\\tand\\twe\\tenqueue\\tthe\\tname\\tof\\tthe\\tfiles\\tto\\nread\\t(in\\tthis\\tcase\\tjust\\t\\n\"my_test.csv\"\\n).\\tNext\\twe\\tcreate\\ta\\t\\nCoordinator\\n\\tthat\\tthe\\t\\nQueueRunner\\n\\twill\\tuse\\tto\\nstop\\tgracefully,\\tas\\tjust\\texplained.\\tFinally,\\twe\\ttell\\tthe\\t\\nQueueRunner\\n\\tto\\tcreate\\tthe\\tthreads\\tand\\tstart\\tthem.\\nThe\\tthreads\\twill\\tread\\tall\\ttraining\\tinstances\\tand\\tpush\\tthem\\tto\\tthe\\tinstance\\tqueue,\\tand\\tthen\\tthey\\twill\\tall\\tstop\\ngracefully.\\nThis\\twill\\tbe\\ta\\tbit\\tmore\\tefficient\\tthan\\tearlier,\\tbut\\twe\\tcan\\tdo\\tbetter.\\tCurrently\\tall\\tthreads\\tare\\treading\\tfrom\\nthe\\tsame\\tfile.\\tWe\\tcan\\tmake\\tthem\\tread\\tsimultaneously\\tfrom\\tseparate\\tfiles\\tinstead\\t(assuming\\tthe\\ttraining\\ndata\\tis\\tsharded\\tacross\\tmultiple\\tCSV\\tfiles)\\tby\\tcreating\\tmultiple\\treaders\\t(see\\t\\nFigure\\t12-10\\n).\\nFigure\\t12-10.\\t\\nReading\\tsimultaneously\\tfrom\\tmultiple\\tfiles\\nFor\\tthis\\twe\\tneed\\tto\\twrite\\ta\\tsmall\\tfunction\\tto\\tcreate\\ta\\treader\\tand\\tthe\\tnodes\\tthat\\twill\\tread\\tand\\tpush\\tone\\ninstance\\t\\nto\\tthe\\tinstance\\tqueue:\\ndef\\n\\t\\nread_and_push_instance\\n(\\nfilename_queue\\n,\\n\\t\\ninstance_queue\\n):\\n\\t\\t\\t\\t\\nreader\\n\\t\\n=\\n\\t\\ntf\\n.\\nTextLineReader\\n(\\nskip_header_lines\\n=\\n1\\n)\\n\\t\\t\\t\\t\\nkey\\n,\\n\\t\\nvalue\\n\\t\\n=\\n\\t\\nreader\\n.\\nread\\n(\\nfilename_queue\\n)\\n\\t\\t\\t\\t\\nx1\\n,\\n\\t\\nx2\\n,\\n\\t\\ntarget\\n\\t\\n=\\n\\t\\ntf\\n.\\ndecode_csv\\n(\\nvalue\\n,\\n\\t\\nrecord_defaults\\n=\\n[[\\n-\\n1.\\n],\\n\\t\\n[\\n-\\n1.\\n],\\n\\t\\n[\\n-\\n1\\n]])\\n\\t\\t\\t\\t\\nfeatures\\n\\t\\n=\\n\\t\\ntf\\n.\\nstack\\n([\\nx1\\n,\\n\\t\\nx2\\n])\\n\\t\\t\\t\\t\\nenqueue_instance\\n\\t\\n=\\n\\t\\ninstance_queue\\n.\\nenqueue\\n([\\nfeatures\\n,\\n\\t\\ntarget\\n])\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nenqueue_instance\\nNext\\twe\\tdefine\\tthe\\t\\nqueues:\\nfilename_queue\\n\\t\\n=\\n\\t\\ntf\\n.\\nFIFOQueue\\n(\\ncapacity\\n=\\n10\\n,\\n\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nstring\\n],\\n\\t\\nshapes\\n=\\n[()])\\nfilename\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nstring\\n)\\nenqueue_filename\\n\\t\\n=\\n\\t\\nfilename_queue\\n.\\nenqueue\\n([\\nfilename\\n])\\nclose_filename_queue\\n\\t\\n=\\n\\t\\nfilename_queue\\n.\\nclose\\n()\\ninstance_queue\\n\\t\\n=\\n\\t\\ntf\\n.\\nRandomShuffleQueue\\n([\\n...\\n])\\nAnd\\tfinally\\twe\\tcreate\\tthe\\t\\nQueueRunner\\n,\\tbut\\tthis\\ttime\\twe\\tgive\\tit\\ta\\tlist\\tof\\tdifferent\\tenqueue\\toperations.\\nEach\\toperation\\twill\\tuse\\ta\\tdifferent\\treader,\\tso\\tthe\\tthreads\\twill\\tsimultaneously\\tread\\tfrom\\tdifferent\\tfiles:\\nread_and_enqueue_ops\\n\\t\\n=\\n\\t\\n[\\n\\t\\t\\t\\t\\nread_and_push_instance\\n(\\nfilename_queue\\n,\\n\\t\\ninstance_queue\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 423}), Document(page_content='\\t\\t\\t\\t\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n)]\\nqueue_runner\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nQueueRunner\\n(\\ninstance_queue\\n,\\n\\t\\nread_and_enqueue_ops\\n)\\nThe\\texecution\\tphase\\tis\\tthen\\tthe\\tsame\\tas\\tbefore:\\tfirst\\tpush\\tthe\\tnames\\tof\\tthe\\tfiles\\tto\\tread,\\tthen\\tcreate\\t\\na\\nCoordinator\\n\\tand\\tcreate\\tand\\tstart\\tthe\\t\\nQueueRunner\\n\\tthreads.\\tThis\\ttime\\tall\\tthreads\\twill\\tread\\tfrom\\ndifferent\\tfiles\\tsimultaneously\\tuntil\\tall\\tfiles\\tare\\tread\\tentirely,\\tand\\tthen\\tthe\\t\\nQueueRunner\\n\\twill\\tclose\\tthe\\ninstance\\tqueue\\tso\\tthat\\tother\\tops\\tpulling\\tfrom\\tit\\tdon’t\\tget\\t\\nblocked.\\nOther\\tconvenience\\tfunctions\\nTensorFlow\\t\\nalso\\toffers\\ta\\tfew\\tconvenience\\tfunctions\\tto\\tsimplify\\tsome\\tcommon\\ttasks\\twhen\\treading\\ntraining\\tinstances.\\tWe\\twill\\tgo\\tover\\tjust\\ta\\tfew\\t(see\\tthe\\tAPI\\tdocumentation\\tfor\\tthe\\tfull\\tlist).\\nThe\\t\\nstring_input_producer()\\n\\t\\ntakes\\ta\\t1D\\ttensor\\tcontaining\\ta\\tlist\\tof\\tfilenames,\\tcreates\\ta\\tthread\\tthat\\npushes\\tone\\tfilename\\tat\\ta\\ttime\\tto\\tthe\\tfilename\\tqueue,\\tand\\tthen\\tcloses\\tthe\\tqueue.\\tIf\\tyou\\tspecify\\ta\\tnumber\\tof\\nepochs,\\tit\\twill\\tcycle\\tthrough\\tthe\\tfilenames\\tonce\\tper\\tepoch\\tbefore\\tclosing\\tthe\\tqueue.\\tBy\\tdefault,\\tit\\tshuffles\\nthe\\tfilenames\\tat\\teach\\tepoch.\\tIt\\tcreates\\ta\\t\\nQueueRunner\\n\\tto\\tmanage\\tits\\tthread,\\tand\\tadds\\tit\\tto\\tthe\\nGraphKeys.QUEUE_RUNNERS\\n\\tcollection.\\tTo\\tstart\\tevery\\t\\nQueueRunner\\n\\tin\\tthat\\tcollection,\\tyou\\tcan\\tcall\\tthe\\ntf.train.start_queue_runners()\\n\\tfunction.\\tNote\\tthat\\tif\\tyou\\tforget\\tto\\tstart\\tthe\\t\\nQueueRunner\\n,\\tthe\\nfilename\\tqueue\\twill\\tbe\\topen\\tand\\tempty,\\tand\\tyour\\treaders\\twill\\tbe\\tblocked\\tforever.\\nThere\\tare\\ta\\tfew\\tother\\t\\nproducer\\n\\t\\nfunctions\\tthat\\tsimilarly\\tcreate\\ta\\tqueue\\tand\\ta\\tcorresponding\\t\\nQueueRunner\\nfor\\trunning\\tan\\tenqueue\\toperation\\t(e.g.,\\t\\ninput_producer()\\n,\\t\\nrange_input_producer()\\n,\\tand\\nslice_input_producer()\\n).\\nThe\\t\\nshuffle_batch()\\n\\tfunction\\t\\ntakes\\ta\\tlist\\tof\\ttensors\\t(e.g.,\\t\\n[features,\\ttarget]\\n)\\tand\\tcreates:\\nA\\t\\nRandomShuffleQueue\\nA\\t\\nQueueRunner\\n\\tto\\t\\nenqueue\\tthe\\ttensors\\tto\\tthe\\tqueue\\t(added\\tto\\tthe\\t\\nGraphKeys.QUEUE_RUNNERS\\ncollection)\\nA\\t\\ndequeue_many\\n\\toperation\\tto\\textract\\ta\\tmini-batch\\tfrom\\tthe\\tqueue\\nThis\\tmakes\\tit\\teasy\\tto\\tmanage\\tin\\ta\\tsingle\\tprocess\\ta\\tmultithreaded\\tinput\\tpipeline\\tfeeding\\ta\\tqueue\\tand\\ta\\ntraining\\tpipeline\\treading\\tmini-batches\\tfrom\\tthat\\tqueue.\\tAlso\\tcheck\\tout\\tthe\\t\\nbatch()\\n,\\t\\nbatch_join()\\n,\\tand\\nshuffle_batch_join()\\n\\tfunctions\\tthat\\t\\nprovide\\tsimilar\\tfunctionality.\\nOkay!\\tYou\\tnow\\thave\\tall\\tthe\\ttools\\tyou\\tneed\\tto\\tstart\\ttraining\\tand\\trunning\\tneural\\tnetworks\\tefficiently\\tacross\\nmultiple\\tdevices\\tand\\tservers\\ton\\ta\\tTensorFlow\\tcluster.\\tLet’s\\treview\\twhat\\tyou\\thave\\tlearned:\\nUsing\\tmultiple\\tGPU\\tdevices\\nSetting\\tup\\tand\\tstarting\\ta\\tTensorFlow\\tcluster\\nDistributing\\tcomputations\\tacross\\tmultiple\\tdevices\\tand\\tservers\\nSharing\\tvariables\\t(and\\tother\\tstateful\\tops\\tsuch\\tas\\tqueues\\tand\\treaders)\\tacross\\tsessions\\tusing\\ncontainers', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 424}), Document(page_content='Coordinating\\tmultiple\\tgraphs\\tworking\\tasynchronously\\tusing\\tqueues\\nReading\\tinputs\\tefficiently\\tusing\\treaders,\\tqueue\\trunners,\\tand\\tcoordinators\\nNow\\tlet’s\\tuse\\tall\\tof\\tthis\\tto\\tparallelize\\tneural\\t\\nnetworks!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 425}), Document(page_content='Parallelizing\\tNeural\\tNetworks\\ton\\ta\\tTensorFlow\\tCluster\\nIn\\tthis\\tsection,\\tfirst\\twe\\twill\\tlook\\tat\\thow\\tto\\tparallelize\\tseveral\\tneural\\tnetworks\\tby\\tsimply\\tplacing\\teach\\none\\ton\\ta\\tdifferent\\tdevice.\\tThen\\twe\\twill\\tlook\\tat\\tthe\\tmuch\\ttrickier\\tproblem\\tof\\ttraining\\ta\\tsingle\\tneural\\nnetwork\\tacross\\tmultiple\\tdevices\\tand\\tservers.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 426}), Document(page_content='One\\tNeural\\tNetwork\\tper\\tDevice\\nThe\\t\\nmost\\ttrivial\\tway\\tto\\ttrain\\tand\\trun\\tneural\\tnetworks\\ton\\ta\\tTensorFlow\\tcluster\\tis\\tto\\ttake\\tthe\\texact\\tsame\\ncode\\tyou\\twould\\tuse\\tfor\\ta\\tsingle\\tdevice\\ton\\ta\\tsingle\\tmachine,\\tand\\tspecify\\tthe\\tmaster\\tserver’s\\taddress\\nwhen\\tcreating\\tthe\\tsession.\\tThat’s\\tit\\t—\\tyou’re\\tdone!\\tYour\\tcode\\twill\\tbe\\trunning\\ton\\tthe\\tserver’s\\tdefault\\ndevice.\\tYou\\tcan\\tchange\\tthe\\tdevice\\tthat\\twill\\trun\\tyour\\tgraph\\tsimply\\tby\\tputting\\tyour\\tcode’s\\tconstruction\\nphase\\twithin\\ta\\tdevice\\tblock.\\nBy\\trunning\\tseveral\\tclient\\tsessions\\tin\\tparallel\\t(in\\tdifferent\\tthreads\\tor\\tdifferent\\tprocesses),\\tconnecting\\tthem\\nto\\tdifferent\\tservers,\\tand\\tconfiguring\\tthem\\tto\\tuse\\tdifferent\\tdevices,\\tyou\\tcan\\tquite\\teasily\\ttrain\\tor\\trun\\tmany\\nneural\\tnetworks\\tin\\tparallel,\\tacross\\tall\\tdevices\\tand\\tall\\tmachines\\tin\\tyour\\tcluster\\t(see\\t\\nFigure\\t12-11\\n).\\tThe\\nspeedup\\tis\\talmost\\tlinear.\\n4\\n\\tTraining\\t100\\tneural\\tnetworks\\tacross\\t50\\tservers\\twith\\t2\\tGPUs\\teach\\twill\\tnot\\ttake\\nmuch\\tlonger\\tthan\\ttraining\\tjust\\t1\\tneural\\tnetwork\\ton\\t1\\tGPU.\\nFigure\\t12-11.\\t\\nTraining\\tone\\tneural\\tnetwork\\tper\\tdevice\\nThis\\tsolution\\tis\\tperfect\\tfor\\thyperparameter\\ttuning:\\teach\\tdevice\\tin\\tthe\\tcluster\\twill\\ttrain\\ta\\tdifferent\\tmodel\\nwith\\tits\\town\\tset\\tof\\thyperparameters.\\tThe\\tmore\\tcomputing\\tpower\\tyou\\thave,\\tthe\\tlarger\\tthe\\thyperparameter\\nspace\\tyou\\tcan\\texplore.\\nIt\\talso\\tworks\\tperfectly\\tif\\tyou\\thost\\ta\\tweb\\tservice\\tthat\\treceives\\ta\\tlarge\\tnumber\\t\\nof\\t\\nqueries\\tper\\tsecond\\n(QPS)\\tand\\tyou\\tneed\\tyour\\tneural\\tnetwork\\tto\\tmake\\ta\\tprediction\\tfor\\teach\\tquery.\\tSimply\\treplicate\\tthe\\tneural\\nnetwork\\tacross\\tall\\tdevices\\ton\\tthe\\tcluster\\tand\\tdispatch\\tqueries\\tacross\\tall\\tdevices.\\tBy\\tadding\\tmore\\tservers\\nyou\\tcan\\thandle\\tan\\tunlimited\\tnumber\\tof\\tQPS\\t(however,\\tthis\\twill\\tnot\\treduce\\tthe\\ttime\\tit\\ttakes\\tto\\tprocess\\ta\\nsingle\\trequest\\tsince\\tit\\twill\\tstill\\thave\\tto\\twait\\tfor\\ta\\tneural\\tnetwork\\tto\\tmake\\ta\\tprediction).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 427}), Document(page_content='NOTE\\nAnother\\toption\\tis\\tto\\tserve\\tyour\\tneural\\tnetworks\\tusing\\t\\nTensorFlow\\tServing\\n.\\tIt\\t\\nis\\tan\\topen\\tsource\\tsystem,\\treleased\\tby\\tGoogle\\tin\\nFebruary\\t2016,\\tdesigned\\tto\\tserve\\ta\\thigh\\tvolume\\tof\\tqueries\\tto\\tMachine\\tLearning\\tmodels\\t(typically\\tbuilt\\twith\\tTensorFlow).\\tIt\\nhandles\\tmodel\\tversioning,\\tso\\tyou\\tcan\\teasily\\tdeploy\\ta\\tnew\\tversion\\tof\\tyour\\tnetwork\\tto\\tproduction,\\tor\\texperiment\\twith\\tvarious\\nalgorithms\\twithout\\tinterrupting\\tyour\\tservice,\\tand\\tit\\tcan\\tsustain\\ta\\theavy\\tload\\tby\\tadding\\tmore\\tservers.\\tFor\\tmore\\tdetails,\\t\\ncheck\\tout\\nhttps://tensorflow.github.io/serving/\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 428}), Document(page_content='In-Graph\\tVersus\\tBetween-Graph\\tReplication\\nYou\\t\\ncan\\talso\\tparallelize\\tthe\\ttraining\\tof\\ta\\tlarge\\tensemble\\tof\\tneural\\tnetworks\\tby\\tsimply\\tplacing\\tevery\\nneural\\tnetwork\\ton\\ta\\tdifferent\\tdevice\\t(ensembles\\twere\\tintroduced\\tin\\t\\nChapter\\t7\\n).\\tHowever,\\tonce\\tyou\\twant\\nto\\t\\nrun\\n\\tthe\\tensemble,\\tyou\\twill\\tneed\\tto\\taggregate\\tthe\\tindividual\\tpredictions\\tmade\\tby\\teach\\tneural\\tnetwork\\tto\\nproduce\\tthe\\t\\nensemble’s\\tprediction,\\tand\\tthis\\trequires\\ta\\tbit\\tof\\tcoordination.\\nThere\\tare\\ttwo\\tmajor\\tapproaches\\tto\\thandling\\ta\\tneural\\tnetwork\\tensemble\\t(or\\tany\\tother\\tgraph\\tthat\\tcontains\\nlarge\\tchunks\\tof\\tindependent\\tcomputations):\\nYou\\tcan\\tcreate\\tone\\tbig\\tgraph,\\tcontaining\\tevery\\tneural\\tnetwork,\\teach\\tpinned\\tto\\ta\\tdifferent\\tdevice,\\nplus\\tthe\\tcomputations\\tneeded\\tto\\taggregate\\tthe\\tindividual\\tpredictions\\tfrom\\tall\\tthe\\tneural\\tnetworks\\n(see\\t\\nFigure\\t12-12\\n).\\tThen\\tyou\\tjust\\tcreate\\tone\\tsession\\tto\\tany\\tserver\\tin\\tthe\\tcluster\\tand\\tlet\\tit\\ttake\\tcare\\tof\\neverything\\t(including\\twaiting\\tfor\\tall\\tindividual\\tpredictions\\tto\\tbe\\tavailable\\tbefore\\taggregating\\tthem).\\nThis\\tapproach\\tis\\t\\ncalled\\t\\nin-graph\\treplication\\n.\\nFigure\\t12-12.\\t\\nIn-graph\\treplication\\nAlternatively,\\tyou\\tcan\\tcreate\\tone\\tseparate\\tgraph\\tfor\\teach\\tneural\\tnetwork\\tand\\thandle\\tsynchronization\\nbetween\\tthese\\tgraphs\\tyourself.\\tThis\\tapproach\\tis\\t\\ncalled\\t\\nbetween-graph\\treplication\\n.\\tOne\\ttypical\\nimplementation\\tis\\tto\\tcoordinate\\tthe\\texecution\\tof\\tthese\\tgraphs\\tusing\\tqueues\\t(see\\t\\nFigure\\t12-13\\n).\\tA\\tset\\nof\\tclients\\thandles\\tone\\tneural\\tnetwork\\teach,\\treading\\tfrom\\tits\\tdedicated\\tinput\\tqueue,\\tand\\twriting\\tto\\tits\\ndedicated\\tprediction\\tqueue.\\tAnother\\tclient\\tis\\tin\\tcharge\\tof\\treading\\tthe\\tinputs\\tand\\tpushing\\tthem\\tto\\tall\\nthe\\tinput\\tqueues\\t(copying\\tall\\tinputs\\tto\\tevery\\tqueue).\\tFinally,\\tone\\tlast\\tclient\\tis\\tin\\tcharge\\tof\\treading\\none\\tprediction\\tfrom\\teach\\tprediction\\tqueue\\tand\\taggregating\\tthem\\tto\\tproduce\\tthe\\tensemble’s\\nprediction.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 429}), Document(page_content='Figure\\t12-13.\\t\\nBetween-graph\\treplication\\nThese\\tsolutions\\thave\\ttheir\\tpros\\tand\\tcons.\\tIn-graph\\treplication\\tis\\tsomewhat\\tsimpler\\tto\\timplement\\tsince\\nyou\\tdon’t\\thave\\tto\\tmanage\\tmultiple\\tclients\\tand\\tmultiple\\tqueues.\\tHowever,\\tbetween-graph\\treplication\\tis\\ta\\nbit\\teasier\\tto\\torganize\\tinto\\twell-bounded\\tand\\teasy-to-test\\tmodules.\\tMoreover,\\tit\\tgives\\tyou\\tmore\\nflexibility.\\tFor\\texample,\\tyou\\tcould\\tadd\\ta\\tdequeue\\ttimeout\\tin\\tthe\\taggregator\\tclient\\tso\\tthat\\tthe\\tensemble\\nwould\\tnot\\tfail\\teven\\tif\\tone\\tof\\tthe\\tneural\\tnetwork\\tclients\\tcrashes\\tor\\tif\\tone\\tneural\\tnetwork\\ttakes\\ttoo\\tlong\\tto\\nproduce\\tits\\tprediction.\\tTensorFlow\\tlets\\tyou\\tspecify\\ta\\ttimeout\\twhen\\tcalling\\t\\nthe\\t\\nrun()\\n\\tfunction\\t\\nby\\tpassing\\na\\t\\nRunOptions\\n\\twith\\t\\ntimeout_in_ms\\n:\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\nrun_options\\n\\t\\n=\\n\\t\\ntf\\n.\\nRunOptions\\n()\\n\\t\\t\\t\\t\\nrun_options\\n.\\ntimeout_in_ms\\n\\t\\n=\\n\\t\\n1000\\n\\t\\t\\n#\\t1s\\ttimeout\\n\\t\\t\\t\\t\\ntry\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npred\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\ndequeue_prediction\\n,\\n\\t\\noptions\\n=\\nrun_options\\n)\\n\\t\\t\\t\\t\\nexcept\\n\\t\\ntf\\n.\\nerrors\\n.\\nDeadlineExceededError\\n\\t\\nas\\n\\t\\nex\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tthe\\tdequeue\\toperation\\ttimed\\tout\\tafter\\t1s\\nAnother\\tway\\tyou\\tcan\\tspecify\\ta\\ttimeout\\tis\\tto\\tset\\tthe\\tsession’s\\t\\noperation_timeout_in_ms\\n\\t\\nconfiguration\\noption,\\tbut\\tin\\tthis\\tcase\\tthe\\t\\nrun()\\n\\tfunction\\ttimes\\t\\nout\\tif\\t\\nany\\n\\toperation\\ttakes\\tlonger\\tthan\\tthe\\ttimeout\\t\\ndelay:\\nconfig\\n\\t\\n=\\n\\t\\ntf\\n.\\nConfigProto\\n()\\nconfig\\n.\\noperation_timeout_in_ms\\n\\t\\n=\\n\\t\\n1000\\n\\t\\t\\n#\\t1s\\ttimeout\\tfor\\tevery\\toperation\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n],\\n\\t\\nconfig\\n=\\nconfig\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\ntry\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npred\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\ndequeue_prediction\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 430}), Document(page_content='\\t\\t\\t\\t\\nexcept\\n\\t\\ntf\\n.\\nerrors\\n.\\nDeadlineExceededError\\n\\t\\nas\\n\\t\\nex\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\n#\\tthe\\tdequeue\\toperation\\ttimed\\tout\\tafter\\t1s', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 431}), Document(page_content='Model\\tParallelism\\nSo\\t\\nfar\\twe\\thave\\trun\\teach\\tneural\\tnetwork\\ton\\ta\\tsingle\\tdevice.\\tWhat\\tif\\twe\\twant\\tto\\trun\\ta\\tsingle\\tneural\\nnetwork\\tacross\\tmultiple\\tdevices?\\tThis\\trequires\\tchopping\\tyour\\tmodel\\tinto\\tseparate\\tchunks\\tand\\trunning\\neach\\tchunk\\ton\\ta\\tdifferent\\tdevice.\\tThis\\tis\\tcalled\\t\\nmodel\\tparallelism\\n.\\tUnfortunately,\\tmodel\\tparallelism\\tturns\\nout\\tto\\tbe\\tpretty\\ttricky,\\tand\\tit\\treally\\tdepends\\ton\\tthe\\tarchitecture\\tof\\tyour\\tneural\\tnetwork.\\tFor\\tfully\\nconnected\\tnetworks,\\tthere\\tis\\tgenerally\\tnot\\tmuch\\tto\\tbe\\tgained\\tfrom\\tthis\\tapproach\\t(see\\t\\nFigure\\t12-14\\n).\\nIntuitively,\\tit\\tmay\\tseem\\tthat\\tan\\teasy\\tway\\tto\\tsplit\\tthe\\tmodel\\tis\\tto\\tplace\\teach\\tlayer\\ton\\ta\\tdifferent\\tdevice,\\tbut\\nthis\\tdoes\\tnot\\twork\\tsince\\teach\\tlayer\\tneeds\\tto\\twait\\tfor\\tthe\\toutput\\tof\\tthe\\tprevious\\tlayer\\tbefore\\tit\\tcan\\tdo\\nanything.\\tSo\\tperhaps\\tyou\\tcan\\tslice\\tit\\tvertically\\t—\\tfor\\texample,\\twith\\tthe\\tleft\\thalf\\tof\\teach\\tlayer\\ton\\tone\\ndevice,\\tand\\tthe\\tright\\tpart\\ton\\tanother\\tdevice?\\tThis\\tis\\tslightly\\tbetter,\\tsince\\tboth\\thalves\\tof\\teach\\tlayer\\tcan\\nindeed\\twork\\tin\\tparallel,\\tbut\\tthe\\tproblem\\tis\\tthat\\teach\\thalf\\tof\\tthe\\tnext\\tlayer\\trequires\\tthe\\toutput\\tof\\tboth\\nhalves,\\tso\\tthere\\twill\\tbe\\ta\\tlot\\tof\\tcross-device\\tcommunication\\t(represented\\tby\\tthe\\tdashed\\tarrows).\\tThis\\tis\\nlikely\\tto\\tcompletely\\tcancel\\tout\\tthe\\tbenefit\\tof\\tthe\\tparallel\\tcomputation,\\tsince\\tcross-device\\tcommunication\\nis\\tslow\\t(especially\\tif\\tit\\tis\\tacross\\tseparate\\tmachines).\\nFigure\\t12-14.\\t\\nSplitting\\ta\\tfully\\tconnected\\tneural\\tnetwork\\nHowever,\\tas\\twe\\twill\\tsee\\tin\\t\\nChapter\\t13\\n,\\tsome\\tneural\\tnetwork\\tarchitectures,\\tsuch\\tas\\tconvolutional\\tneural\\nnetworks,\\tcontain\\tlayers\\tthat\\tare\\tonly\\tpartially\\tconnected\\tto\\tthe\\tlower\\tlayers,\\tso\\tit\\tis\\tmuch\\teasier\\tto\\ndistribute\\tchunks\\tacross\\tdevices\\tin\\tan\\tefficient\\tway.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 432}), Document(page_content='Figure\\t12-15.\\t\\nSplitting\\ta\\tpartially\\tconnected\\tneural\\tnetwork\\nMoreover,\\tas\\twe\\twill\\tsee\\tin\\t\\nChapter\\t14\\n,\\tsome\\tdeep\\trecurrent\\tneural\\tnetworks\\tare\\tcomposed\\tof\\tseveral\\nlayers\\tof\\t\\nmemory\\tcells\\n\\t\\n(see\\tthe\\tleft\\tside\\tof\\t\\nFigure\\t12-16\\n).\\tA\\tcell’s\\toutput\\tat\\ttime\\t\\nt\\n\\tis\\tfed\\tback\\tto\\tits\\tinput\\nat\\ttime\\t\\nt\\n\\t+\\t1\\t(as\\tyou\\tcan\\tsee\\tmore\\tclearly\\ton\\tthe\\tright\\tside\\tof\\t\\nFigure\\t12-16\\n).\\tIf\\tyou\\tsplit\\tsuch\\ta\\tnetwork\\nhorizontally,\\tplacing\\teach\\tlayer\\ton\\ta\\tdifferent\\tdevice,\\tthen\\tat\\tthe\\tfirst\\tstep\\tonly\\tone\\tdevice\\twill\\tbe\\tactive,\\nat\\tthe\\tsecond\\tstep\\ttwo\\twill\\tbe\\tactive,\\tand\\tby\\tthe\\ttime\\tthe\\tsignal\\tpropagates\\tto\\tthe\\toutput\\tlayer\\tall\\tdevices\\nwill\\tbe\\tactive\\tsimultaneously.\\tThere\\tis\\tstill\\ta\\tlot\\tof\\tcross-device\\tcommunication\\tgoing\\ton,\\tbut\\tsince\\teach\\ncell\\tmay\\tbe\\tfairly\\tcomplex,\\tthe\\tbenefit\\tof\\trunning\\tmultiple\\tcells\\tin\\tparallel\\toften\\toutweighs\\tthe\\ncommunication\\tpenalty.\\nFigure\\t12-16.\\t\\nSplitting\\ta\\tdeep\\trecurrent\\tneural\\tnetwork', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 433}), Document(page_content='In\\tshort,\\tmodel\\tparallelism\\tcan\\tspeed\\tup\\trunning\\tor\\ttraining\\tsome\\ttypes\\tof\\tneural\\tnetworks,\\tbut\\tnot\\tall,\\nand\\tit\\trequires\\tspecial\\tcare\\tand\\ttuning,\\tsuch\\tas\\tmaking\\tsure\\tthat\\tdevices\\tthat\\tneed\\tto\\tcommunicate\\tthe\\nmost\\trun\\ton\\tthe\\tsame\\t\\nmachine.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 434}), Document(page_content='Data\\tParallelism\\nAnother\\t\\nway\\tto\\tparallelize\\tthe\\ttraining\\tof\\ta\\tneural\\tnetwork\\tis\\tto\\treplicate\\tit\\ton\\teach\\tdevice,\\trun\\ta\\ttraining\\nstep\\tsimultaneously\\ton\\tall\\treplicas\\tusing\\ta\\tdifferent\\tmini-batch\\tfor\\teach,\\tand\\tthen\\taggregate\\tthe\\tgradients\\nto\\tupdate\\tthe\\tmodel\\tparameters.\\tThis\\tis\\tcalled\\t\\ndata\\tparallelism\\n\\t(see\\t\\nFigure\\t12-17\\n).\\nFigure\\t12-17.\\t\\nData\\tparallelism\\nThere\\tare\\ttwo\\tvariants\\tof\\tthis\\tapproach:\\t\\nsynchronous\\tupdates\\n\\tand\\t\\nasynchronous\\tupdates\\n.\\nSynchronous\\tupdates\\nWith\\t\\nsynchronous\\tupdates\\n,\\t\\nthe\\taggregator\\twaits\\tfor\\tall\\tgradients\\tto\\tbe\\tavailable\\tbefore\\tcomputing\\tthe\\naverage\\tand\\tapplying\\tthe\\tresult\\t(i.e.,\\tusing\\tthe\\taggregated\\tgradients\\tto\\tupdate\\tthe\\tmodel\\tparameters).\\nOnce\\ta\\treplica\\thas\\tfinished\\tcomputing\\tits\\tgradients,\\tit\\tmust\\twait\\tfor\\tthe\\tparameters\\tto\\tbe\\tupdated\\tbefore\\tit\\ncan\\tproceed\\tto\\tthe\\tnext\\tmini-batch.\\tThe\\tdownside\\tis\\tthat\\tsome\\tdevices\\tmay\\tbe\\tslower\\tthan\\tothers,\\tso\\tall\\nother\\tdevices\\twill\\thave\\tto\\twait\\tfor\\tthem\\tat\\tevery\\tstep.\\tMoreover,\\tthe\\tparameters\\twill\\tbe\\tcopied\\tto\\tevery\\ndevice\\talmost\\tat\\tthe\\tsame\\ttime\\t(immediately\\tafter\\tthe\\tgradients\\tare\\tapplied),\\twhich\\tmay\\tsaturate\\tthe\\nparameter\\tservers’\\tbandwidth.\\nTIP\\nTo\\treduce\\tthe\\twaiting\\ttime\\tat\\teach\\tstep,\\tyou\\tcould\\tignore\\tthe\\tgradients\\tfrom\\tthe\\tslowest\\tfew\\treplicas\\t(typically\\t~10%).\\tFor\\nexample,\\tyou\\tcould\\trun\\t20\\treplicas,\\tbut\\tonly\\taggregate\\tthe\\tgradients\\tfrom\\tthe\\tfastest\\t18\\treplicas\\tat\\teach\\tstep,\\tand\\tjust\\tignore\\tthe\\ngradients\\tfrom\\tthe\\tlast\\t2.\\tAs\\tsoon\\tas\\tthe\\tparameters\\tare\\tupdated,\\tthe\\tfirst\\t18\\treplicas\\tcan\\tstart\\tworking\\tagain\\timmediately,\\nwithout\\thaving\\tto\\twait\\tfor\\tthe\\t2\\tslowest\\treplicas.\\tThis\\tsetup\\tis\\tgenerally\\tdescribed\\tas\\thaving\\t18\\treplicas\\tplus\\t2\\t\\nspare\\treplicas\\n.\\n5', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 435}), Document(page_content='Asynchronous\\tupdates\\nWith\\t\\nasynchronous\\tupdates,\\twhenever\\ta\\treplica\\thas\\tfinished\\tcomputing\\tthe\\tgradients,\\tit\\timmediately\\tuses\\nthem\\tto\\tupdate\\tthe\\tmodel\\tparameters.\\tThere\\tis\\tno\\taggregation\\t(remove\\tthe\\t“mean”\\tstep\\tin\\t\\nFigure\\t12-17\\n),\\nand\\tno\\tsynchronization.\\tReplicas\\tjust\\twork\\tindependently\\tof\\tthe\\tother\\treplicas.\\tSince\\tthere\\tis\\tno\\twaiting\\nfor\\tthe\\tother\\treplicas,\\tthis\\tapproach\\truns\\tmore\\ttraining\\tsteps\\tper\\tminute.\\tMoreover,\\talthough\\tthe\\nparameters\\tstill\\tneed\\tto\\tbe\\tcopied\\tto\\tevery\\tdevice\\tat\\tevery\\tstep,\\tthis\\thappens\\tat\\tdifferent\\ttimes\\tfor\\teach\\nreplica\\tso\\tthe\\trisk\\tof\\tbandwidth\\tsaturation\\tis\\treduced.\\nData\\tparallelism\\twith\\tasynchronous\\tupdates\\tis\\tan\\tattractive\\tchoice,\\tbecause\\tof\\tits\\tsimplicity,\\tthe\\tabsence\\nof\\tsynchronization\\tdelay,\\tand\\ta\\tbetter\\tuse\\tof\\tthe\\tbandwidth.\\tHowever,\\talthough\\tit\\tworks\\treasonably\\twell\\nin\\tpractice,\\tit\\tis\\talmost\\tsurprising\\tthat\\tit\\tworks\\tat\\tall!\\tIndeed,\\tby\\tthe\\ttime\\ta\\treplica\\thas\\tfinished\\tcomputing\\nthe\\tgradients\\tbased\\ton\\tsome\\tparameter\\tvalues,\\tthese\\tparameters\\twill\\thave\\tbeen\\tupdated\\tseveral\\ttimes\\tby\\nother\\treplicas\\t(on\\taverage\\t\\nN\\n\\t–\\t1\\ttimes\\tif\\tthere\\tare\\t\\nN\\n\\treplicas)\\tand\\tthere\\tis\\tno\\tguarantee\\tthat\\tthe\\tcomputed\\ngradients\\twill\\tstill\\tbe\\tpointing\\tin\\tthe\\tright\\tdirection\\t(see\\t\\nFigure\\t12-18\\n).\\tWhen\\tgradients\\tare\\tseverely\\tout-\\nof-date,\\tthey\\tare\\tcalled\\t\\nstale\\tgradients\\n:\\t\\nthey\\tcan\\tslow\\tdown\\tconvergence,\\tintroducing\\tnoise\\tand\\twobble\\neffects\\t(the\\tlearning\\tcurve\\tmay\\tcontain\\ttemporary\\toscillations),\\tor\\tthey\\tcan\\teven\\tmake\\tthe\\ttraining\\nalgorithm\\tdiverge.\\nFigure\\t12-18.\\t\\nStale\\tgradients\\twhen\\tusing\\tasynchronous\\tupdates\\nThere\\tare\\ta\\tfew\\tways\\tto\\treduce\\tthe\\teffect\\tof\\tstale\\tgradients:\\nReduce\\tthe\\tlearning\\trate.\\nDrop\\tstale\\tgradients\\tor\\tscale\\tthem\\tdown.\\nAdjust\\tthe\\tmini-batch\\tsize.\\nStart\\tthe\\tfirst\\tfew\\tepochs\\tusing\\tjust\\tone\\t\\nreplica\\t(this\\tis\\tcalled\\tthe\\t\\nwarmup\\tphase\\n).\\tStale\\tgradients\\ntend\\tto\\tbe\\tmore\\tdamaging\\tat\\tthe\\tbeginning\\tof\\ttraining,\\twhen\\tgradients\\tare\\ttypically\\tlarge\\tand\\tthe\\nparameters\\thave\\tnot\\tsettled\\tinto\\ta\\tvalley\\tof\\tthe\\t\\ncost\\tfunction\\tyet,\\tso\\tdifferent\\treplicas\\tmay\\tpush\\tthe\\nparameters\\tin\\tquite\\tdifferent\\tdirections.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 436}), Document(page_content='A\\t\\npaper\\tpublished\\tby\\tthe\\tGoogle\\tBrain\\tteam\\tin\\tApril\\t2016\\n\\tbenchmarked\\tvarious\\tapproaches\\tand\\tfound\\nthat\\tdata\\tparallelism\\twith\\tsynchronous\\tupdates\\tusing\\ta\\tfew\\tspare\\treplicas\\twas\\tthe\\tmost\\tefficient,\\tnot\\tonly\\nconverging\\tfaster\\tbut\\talso\\tproducing\\ta\\tbetter\\tmodel.\\tHowever,\\tthis\\tis\\tstill\\tan\\tactive\\tarea\\tof\\tresearch,\\tso\\nyou\\tshould\\tnot\\trule\\tout\\tasynchronous\\tupdates\\t\\nquite\\tyet.\\nBandwidth\\tsaturation\\nWhether\\t\\nyou\\tuse\\tsynchronous\\tor\\tasynchronous\\tupdates,\\tdata\\tparallelism\\tstill\\trequires\\tcommunicating\\tthe\\nmodel\\tparameters\\tfrom\\tthe\\tparameter\\tservers\\tto\\tevery\\treplica\\tat\\tthe\\tbeginning\\tof\\tevery\\ttraining\\tstep,\\tand\\nthe\\tgradients\\tin\\tthe\\tother\\tdirection\\tat\\tthe\\tend\\tof\\teach\\ttraining\\tstep.\\tUnfortunately,\\tthis\\tmeans\\tthat\\tthere\\nalways\\tcomes\\ta\\tpoint\\twhere\\tadding\\tan\\textra\\tGPU\\twill\\tnot\\timprove\\tperformance\\tat\\tall\\tbecause\\tthe\\ttime\\nspent\\tmoving\\tthe\\tdata\\tin\\tand\\tout\\tof\\tGPU\\tRAM\\t(and\\tpossibly\\tacross\\tthe\\tnetwork)\\twill\\toutweigh\\tthe\\nspeedup\\tobtained\\tby\\tsplitting\\tthe\\tcomputation\\tload.\\tAt\\tthat\\tpoint,\\tadding\\tmore\\tGPUs\\twill\\tjust\\tincrease\\nsaturation\\tand\\tslow\\tdown\\ttraining.\\nTIP\\nFor\\tsome\\tmodels,\\ttypically\\trelatively\\tsmall\\tand\\ttrained\\ton\\ta\\tvery\\tlarge\\ttraining\\tset,\\tyou\\tare\\toften\\tbetter\\toff\\ttraining\\tthe\\tmodel\\ton\\ta\\nsingle\\tmachine\\twith\\ta\\tsingle\\tGPU.\\nSaturation\\tis\\tmore\\tsevere\\tfor\\tlarge\\tdense\\tmodels,\\tsince\\tthey\\thave\\ta\\tlot\\tof\\tparameters\\tand\\tgradients\\tto\\ntransfer.\\tIt\\tis\\tless\\tsevere\\tfor\\tsmall\\tmodels\\t(but\\tthe\\tparallelization\\tgain\\tis\\tsmall)\\tand\\talso\\tfor\\tlarge\\tsparse\\nmodels\\tsince\\tthe\\tgradients\\tare\\ttypically\\tmostly\\tzeros,\\tso\\tthey\\tcan\\tbe\\tcommunicated\\tefficiently.\\tJeff\\tDean,\\ninitiator\\tand\\tlead\\tof\\tthe\\tGoogle\\tBrain\\tproject,\\t\\nreported\\n\\ttypical\\tspeedups\\tof\\t25–40x\\twhen\\tdistributing\\ncomputations\\tacross\\t50\\tGPUs\\tfor\\tdense\\tmodels,\\tand\\t300x\\tspeedup\\tfor\\tsparser\\tmodels\\ttrained\\tacross\\t500\\nGPUs.\\tAs\\tyou\\tcan\\tsee,\\tsparse\\tmodels\\treally\\tdo\\tscale\\tbetter.\\tHere\\tare\\ta\\tfew\\tconcrete\\texamples:\\nNeural\\tMachine\\tTranslation:\\t6x\\tspeedup\\ton\\t8\\tGPUs\\nInception/ImageNet:\\t32x\\tspeedup\\ton\\t50\\tGPUs\\nRankBrain:\\t300x\\tspeedup\\ton\\t500\\tGPUs\\nThese\\tnumbers\\trepresent\\tthe\\tstate\\tof\\tthe\\tart\\tin\\tQ1\\t2016.\\tBeyond\\ta\\tfew\\tdozen\\tGPUs\\tfor\\ta\\tdense\\tmodel\\tor\\nfew\\thundred\\tGPUs\\tfor\\ta\\tsparse\\tmodel,\\tsaturation\\tkicks\\tin\\tand\\tperformance\\tdegrades.\\tThere\\tis\\tplenty\\tof\\nresearch\\tgoing\\ton\\tto\\tsolve\\tthis\\tproblem\\t(exploring\\tpeer-to-peer\\tarchitectures\\trather\\tthan\\tcentralized\\nparameter\\tservers,\\tusing\\tlossy\\tmodel\\tcompression,\\toptimizing\\twhen\\tand\\twhat\\tthe\\treplicas\\tneed\\tto\\ncommunicate,\\tand\\tso\\ton),\\tso\\tthere\\twill\\tlikely\\tbe\\ta\\tlot\\tof\\tprogress\\tin\\tparallelizing\\tneural\\tnetworks\\tin\\tthe\\nnext\\tfew\\tyears.\\nIn\\tthe\\tmeantime,\\there\\tare\\ta\\tfew\\tsimple\\tsteps\\tyou\\tcan\\ttake\\tto\\treduce\\tthe\\tsaturation\\tproblem:\\nGroup\\tyour\\tGPUs\\ton\\ta\\tfew\\tservers\\trather\\tthan\\tscattering\\tthem\\tacross\\tmany\\tservers.\\tThis\\twill\\tavoid\\nunnecessary\\tnetwork\\thops.\\nShard\\tthe\\tparameters\\tacross\\tmultiple\\tparameter\\tservers\\t(as\\tdiscussed\\tearlier).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 437}), Document(page_content='Drop\\tthe\\tmodel\\tparameters’\\tfloat\\tprecision\\tfrom\\t32\\tbits\\t(\\ntf.float32\\n)\\tto\\t16\\tbits\\t(\\ntf.bfloat16\\n).\\nThis\\twill\\tcut\\tin\\thalf\\tthe\\tamount\\tof\\tdata\\tto\\ttransfer,\\twithout\\tmuch\\timpact\\ton\\tthe\\tconvergence\\trate\\tor\\nthe\\tmodel’s\\tperformance.\\nTIP\\nAlthough\\t16-bit\\tprecision\\tis\\tthe\\tminimum\\tfor\\ttraining\\tneural\\tnetwork,\\tyou\\tcan\\tactually\\tdrop\\tdown\\tto\\t8-bit\\tprecision\\tafter\\ttraining\\nto\\treduce\\tthe\\tsize\\tof\\tthe\\tmodel\\tand\\tspeed\\tup\\tcomputations.\\tThis\\tis\\tcalled\\t\\nquantizing\\n\\tthe\\t\\nneural\\tnetwork.\\tIt\\tis\\tparticularly\\tuseful\\nfor\\tdeploying\\tand\\trunning\\tpretrained\\tmodels\\ton\\tmobile\\tphones.\\tSee\\tPete\\tWarden’s\\t\\ngreat\\tpost\\n\\ton\\tthe\\t\\nsubject.\\nTensorFlow\\timplementation\\nTo\\t\\nimplement\\tdata\\tparallelism\\tusing\\tTensorFlow,\\tyou\\tfirst\\tneed\\tto\\tchoose\\twhether\\tyou\\twant\\tin-graph\\nreplication\\tor\\tbetween-graph\\treplication,\\tand\\twhether\\tyou\\twant\\tsynchronous\\tupdates\\tor\\tasynchronous\\nupdates.\\tLet’s\\tlook\\tat\\thow\\tyou\\twould\\timplement\\teach\\tcombination\\t(see\\tthe\\texercises\\tand\\tthe\\tJupyter\\nnotebooks\\tfor\\tcomplete\\tcode\\texamples).\\nWith\\tin-graph\\treplication\\t+\\tsynchronous\\tupdates,\\tyou\\tbuild\\tone\\tbig\\tgraph\\tcontaining\\tall\\tthe\\tmodel\\nreplicas\\t(placed\\ton\\tdifferent\\tdevices),\\tand\\ta\\tfew\\tnodes\\tto\\taggregate\\tall\\ttheir\\tgradients\\tand\\tfeed\\tthem\\tto\\nan\\toptimizer.\\tYour\\tcode\\topens\\ta\\tsession\\tto\\tthe\\tcluster\\tand\\tsimply\\truns\\tthe\\ttraining\\toperation\\trepeatedly.\\nWith\\tin-graph\\treplication\\t+\\tasynchronous\\tupdates,\\tyou\\talso\\tcreate\\tone\\tbig\\tgraph,\\tbut\\twith\\tone\\toptimizer\\nper\\treplica,\\tand\\tyou\\trun\\tone\\tthread\\tper\\treplica,\\trepeatedly\\trunning\\tthe\\treplica’s\\toptimizer.\\nWith\\tbetween-graph\\treplication\\t+\\tasynchronous\\t\\nupdates,\\tyou\\trun\\tmultiple\\tindependent\\tclients\\t(typically\\nin\\tseparate\\tprocesses),\\teach\\ttraining\\tthe\\tmodel\\treplica\\tas\\tif\\tit\\twere\\talone\\tin\\tthe\\tworld,\\tbut\\tthe\\tparameters\\nare\\tactually\\tshared\\twith\\tother\\treplicas\\t(using\\ta\\tresource\\tcontainer).\\nWith\\tbetween-graph\\treplication\\t+\\tsynchronous\\tupdates,\\tonce\\tagain\\tyou\\trun\\tmultiple\\tclients,\\teach\\ttraining\\na\\tmodel\\treplica\\tbased\\ton\\tshared\\tparameters,\\tbut\\tthis\\ttime\\tyou\\twrap\\tthe\\toptimizer\\t(e.g.,\\ta\\nMomentumOptimizer\\n)\\twithin\\ta\\t\\nSyncReplicasOptimizer\\n.\\tEach\\treplica\\tuses\\tthis\\toptimizer\\tas\\tit\\twould\\nuse\\tany\\tother\\t\\noptimizer,\\tbut\\tunder\\tthe\\thood\\tthis\\toptimizer\\tsends\\tthe\\tgradients\\tto\\ta\\tset\\tof\\tqueues\\t(one\\tper\\nvariable),\\twhich\\tis\\tread\\tby\\tone\\tof\\tthe\\treplica’s\\t\\nSyncReplicasOptimizer\\n,\\tcalled\\tthe\\t\\nchief\\n.\\tThe\\tchief\\naggregates\\tthe\\tgradients\\tand\\tapplies\\tthem,\\tthen\\twrites\\ta\\ttoken\\tto\\ta\\t\\ntoken\\tqueue\\n\\tfor\\teach\\treplica,\\tsignaling\\nit\\tthat\\tit\\tcan\\tgo\\tahead\\tand\\tcompute\\tthe\\tnext\\tgradients.\\tThis\\tapproach\\tsupports\\thaving\\t\\nspare\\treplicas\\n.\\nIf\\tyou\\tgo\\tthrough\\tthe\\texercises,\\tyou\\twill\\timplement\\teach\\tof\\tthese\\tfour\\tsolutions.\\tYou\\twill\\teasily\\tbe\\table\\tto\\napply\\twhat\\tyou\\thave\\tlearned\\tto\\ttrain\\tlarge\\tdeep\\tneural\\tnetworks\\tacross\\tdozens\\tof\\tservers\\tand\\tGPUs!\\tIn\\nthe\\tfollowing\\tchapters\\twe\\twill\\tgo\\tthrough\\ta\\tfew\\tmore\\timportant\\tneural\\tnetwork\\tarchitectures\\tbefore\\twe\\ntackle\\t\\nReinforcement\\tLearning.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 438}), Document(page_content='Exercises\\n1\\n.\\t\\nIf\\tyou\\tget\\ta\\t\\nCUDA_ERROR_OUT_OF_MEMORY\\n\\twhen\\tstarting\\tyour\\tTensorFlow\\tprogram,\\twhat\\tis\\nprobably\\tgoing\\ton?\\tWhat\\tcan\\tyou\\tdo\\tabout\\tit?\\n2\\n.\\t\\nWhat\\tis\\tthe\\tdifference\\tbetween\\tpinning\\tan\\toperation\\ton\\ta\\tdevice\\tand\\tplacing\\tan\\toperation\\ton\\ta\\ndevice?\\n3\\n.\\t\\nIf\\tyou\\tare\\trunning\\ton\\ta\\tGPU-enabled\\tTensorFlow\\tinstallation,\\tand\\tyou\\tjust\\tuse\\tthe\\tdefault\\tplacement,\\nwill\\tall\\toperations\\tbe\\tplaced\\ton\\tthe\\tfirst\\tGPU?\\n4\\n.\\t\\nIf\\tyou\\tpin\\ta\\tvariable\\tto\\t\\n\"/gpu:0\"\\n,\\tcan\\tit\\tbe\\tused\\tby\\toperations\\tplaced\\ton\\t\\n/gpu:1\\n?\\tOr\\tby\\toperations\\nplaced\\ton\\t\\n\"/cpu:0\"\\n?\\tOr\\tby\\toperations\\tpinned\\tto\\tdevices\\tlocated\\ton\\tother\\tservers?\\n5\\n.\\t\\nCan\\ttwo\\toperations\\tplaced\\ton\\tthe\\tsame\\tdevice\\trun\\tin\\tparallel?\\n6\\n.\\t\\nWhat\\tis\\ta\\tcontrol\\tdependency\\tand\\twhen\\twould\\tyou\\twant\\tto\\tuse\\tone?\\n7\\n.\\t\\nSuppose\\tyou\\ttrain\\ta\\tDNN\\tfor\\tdays\\ton\\ta\\tTensorFlow\\tcluster,\\tand\\timmediately\\tafter\\tyour\\ttraining\\nprogram\\tends\\tyou\\trealize\\tthat\\tyou\\tforgot\\tto\\tsave\\tthe\\tmodel\\tusing\\ta\\t\\nSaver\\n.\\tIs\\tyour\\ttrained\\tmodel\\tlost?\\n8\\n.\\t\\nTrain\\tseveral\\tDNNs\\tin\\tparallel\\ton\\ta\\tTensorFlow\\tcluster,\\tusing\\tdifferent\\thyperparameter\\tvalues.\\tThis\\ncould\\tbe\\tDNNs\\tfor\\tMNIST\\tclassification\\tor\\tany\\tother\\ttask\\tyou\\tare\\tinterested\\tin.\\tThe\\tsimplest\\toption\\nis\\tto\\twrite\\ta\\tsingle\\tclient\\tprogram\\tthat\\ttrains\\tonly\\tone\\tDNN,\\tthen\\trun\\tthis\\tprogram\\tin\\tmultiple\\nprocesses\\tin\\tparallel,\\twith\\tdifferent\\thyperparameter\\tvalues\\tfor\\teach\\tclient.\\tThe\\tprogram\\tshould\\thave\\ncommand-line\\toptions\\tto\\tcontrol\\twhat\\tserver\\tand\\tdevice\\tthe\\tDNN\\tshould\\tbe\\tplaced\\ton,\\tand\\twhat\\nresource\\tcontainer\\t\\nand\\thyperparameter\\tvalues\\tto\\tuse\\t(make\\tsure\\tto\\tuse\\ta\\tdifferent\\tresource\\tcontainer\\nfor\\teach\\tDNN).\\tUse\\ta\\tvalidation\\tset\\tor\\tcross-validation\\tto\\tselect\\tthe\\ttop\\tthree\\tmodels.\\n9\\n.\\t\\nCreate\\tan\\tensemble\\tusing\\tthe\\ttop\\tthree\\tmodels\\tfrom\\tthe\\tprevious\\texercise.\\tDefine\\tit\\tin\\ta\\tsingle\\ngraph,\\tensuring\\tthat\\teach\\tDNN\\truns\\ton\\ta\\tdifferent\\tdevice.\\tEvaluate\\tit\\ton\\tthe\\tvalidation\\tset:\\tdoes\\tthe\\nensemble\\tperform\\tbetter\\tthan\\tthe\\tindividual\\tDNNs?\\n10\\n.\\t\\nTrain\\ta\\tDNN\\tusing\\tbetween-graph\\treplication\\tand\\tdata\\tparallelism\\twith\\tasynchronous\\tupdates,\\ntiming\\thow\\tlong\\tit\\ttakes\\tto\\treach\\ta\\tsatisfying\\tperformance.\\tNext,\\ttry\\tagain\\tusing\\tsynchronous\\nupdates.\\tDo\\tsynchronous\\tupdates\\tproduce\\ta\\tbetter\\tmodel?\\tIs\\ttraining\\tfaster?\\tSplit\\tthe\\tDNN\\nvertically\\tand\\tplace\\teach\\tvertical\\tslice\\ton\\ta\\tdifferent\\tdevice,\\tand\\ttrain\\tthe\\tmodel\\tagain.\\tIs\\ttraining\\nany\\tfaster?\\tIs\\t\\nthe\\tperformance\\tany\\tdifferent?\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\n“TensorFlow:\\tLarge-Scale\\tMachine\\tLearning\\ton\\tHeterogeneous\\tDistributed\\tSystems,”\\tGoogle\\tResearch\\t(2015).\\nYou\\tcan\\teven\\tstart\\tmultiple\\ttasks\\tin\\tthe\\tsame\\tprocess.\\tIt\\tmay\\tbe\\tuseful\\tfor\\ttests,\\tbut\\tit\\tis\\tnot\\trecommended\\tin\\tproduction.\\nIt\\tis\\tthe\\tnext\\tversion\\tof\\tGoogle’s\\tinternal\\t\\nStubby\\n\\tservice,\\twhich\\tGoogle\\thas\\tused\\tsuccessfully\\tfor\\tover\\ta\\tdecade.\\tSee\\t\\nhttp://grpc.io/\\n\\tfor\\nmore\\tdetails.\\nNot\\t100%\\tlinear\\tif\\tyou\\twait\\tfor\\tall\\tdevices\\tto\\tfinish,\\tsince\\tthe\\ttotal\\ttime\\twill\\tbe\\tthe\\ttime\\ttaken\\tby\\tthe\\tslowest\\tdevice.\\n1\\n2\\n3\\n4', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 439}), Document(page_content='This\\tname\\tis\\tslightly\\tconfusing\\tsince\\tit\\tsounds\\tlike\\tsome\\treplicas\\tare\\tspecial,\\tdoing\\tnothing.\\tIn\\treality,\\tall\\treplicas\\tare\\tequivalent:\\tthey\\tall\\nwork\\thard\\tto\\tbe\\tamong\\tthe\\tfastest\\tat\\teach\\ttraining\\tstep,\\tand\\tthe\\tlosers\\tvary\\tat\\tevery\\tstep\\t(unless\\tsome\\tdevices\\tare\\treally\\tslower\\tthan\\nothers).\\n5', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 440}), Document(page_content='Chapter\\t13.\\t\\nConvolutional\\tNeural\\tNetworks\\nAlthough\\tIBM’s\\tDeep\\tBlue\\tsupercomputer\\tbeat\\tthe\\tchess\\tworld\\tchampion\\tGarry\\tKasparov\\tback\\tin\\t1996,\\nuntil\\tquite\\trecently\\tcomputers\\twere\\tunable\\tto\\treliably\\tperform\\tseemingly\\ttrivial\\ttasks\\tsuch\\tas\\tdetecting\\ta\\npuppy\\tin\\ta\\tpicture\\tor\\trecognizing\\tspoken\\twords.\\tWhy\\tare\\tthese\\ttasks\\tso\\teffortless\\tto\\tus\\thumans?\\tThe\\nanswer\\tlies\\tin\\tthe\\tfact\\tthat\\tperception\\tlargely\\ttakes\\tplace\\toutside\\tthe\\trealm\\tof\\tour\\tconsciousness,\\twithin\\nspecialized\\tvisual,\\tauditory,\\tand\\tother\\tsensory\\tmodules\\tin\\tour\\tbrains.\\tBy\\tthe\\ttime\\tsensory\\tinformation\\nreaches\\tour\\tconsciousness,\\tit\\tis\\talready\\tadorned\\twith\\thigh-level\\tfeatures;\\tfor\\texample,\\twhen\\tyou\\tlook\\tat\\ta\\npicture\\tof\\ta\\tcute\\tpuppy,\\tyou\\tcannot\\tchoose\\t\\nnot\\n\\tto\\tsee\\tthe\\tpuppy,\\tor\\t\\nnot\\n\\tto\\tnotice\\tits\\tcuteness.\\tNor\\tcan\\tyou\\nexplain\\t\\nhow\\n\\tyou\\trecognize\\ta\\tcute\\tpuppy;\\tit’s\\tjust\\tobvious\\tto\\tyou.\\tThus,\\twe\\tcannot\\ttrust\\tour\\tsubjective\\nexperience:\\tperception\\tis\\tnot\\ttrivial\\tat\\tall,\\tand\\tto\\tunderstand\\tit\\twe\\tmust\\tlook\\tat\\thow\\tthe\\tsensory\\tmodules\\nwork.\\nConvolutional\\tneural\\tnetworks\\t(CNNs)\\t\\nemerged\\tfrom\\tthe\\tstudy\\tof\\tthe\\tbrain’s\\tvisual\\tcortex,\\tand\\tthey\\thave\\nbeen\\tused\\tin\\timage\\trecognition\\tsince\\tthe\\t1980s.\\tIn\\tthe\\tlast\\tfew\\tyears,\\tthanks\\tto\\tthe\\tincrease\\tin\\ncomputational\\tpower,\\tthe\\tamount\\tof\\tavailable\\ttraining\\tdata,\\tand\\tthe\\ttricks\\tpresented\\tin\\t\\nChapter\\t11\\n\\tfor\\ntraining\\tdeep\\tnets,\\tCNNs\\thave\\tmanaged\\tto\\tachieve\\tsuperhuman\\tperformance\\ton\\tsome\\tcomplex\\tvisual\\ntasks.\\tThey\\tpower\\timage\\tsearch\\tservices,\\tself-driving\\tcars,\\tautomatic\\tvideo\\tclassification\\tsystems,\\tand\\nmore.\\tMoreover,\\tCNNs\\tare\\tnot\\trestricted\\tto\\tvisual\\tperception:\\tthey\\tare\\talso\\tsuccessful\\tat\\tother\\ttasks,\\nsuch\\t\\nas\\t\\nvoice\\trecognition\\n\\tor\\t\\nnatural\\tlanguage\\tprocessing\\n\\t(NLP);\\thowever,\\twe\\twill\\tfocus\\ton\\tvisual\\napplications\\tfor\\tnow.\\nIn\\tthis\\tchapter\\twe\\twill\\tpresent\\twhere\\tCNNs\\tcame\\tfrom,\\twhat\\ttheir\\tbuilding\\tblocks\\tlook\\tlike,\\tand\\thow\\tto\\nimplement\\tthem\\tusing\\tTensorFlow.\\tThen\\twe\\twill\\tpresent\\tsome\\tof\\tthe\\tbest\\tCNN\\tarchitectures.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 441}), Document(page_content='The\\tArchitecture\\tof\\tthe\\tVisual\\tCortex\\nDavid\\tH.\\tHubel\\tand\\tTorsten\\tWiesel\\t\\nperformed\\ta\\tseries\\tof\\texperiments\\ton\\tcats\\tin\\t\\n1958\\n1\\n\\tand\\t\\n1959\\n2\\n\\t(and\\ta\\nfew\\tyears\\tlater\\ton\\tmonkeys\\n3\\n),\\tgiving\\tcrucial\\tinsights\\ton\\tthe\\tstructure\\tof\\tthe\\tvisual\\tcortex\\t(the\\tauthors\\nreceived\\tthe\\tNobel\\tPrize\\tin\\tPhysiology\\tor\\tMedicine\\tin\\t1981\\tfor\\ttheir\\twork).\\tIn\\tparticular,\\tthey\\tshowed\\nthat\\tmany\\tneurons\\tin\\tthe\\tvisual\\tcortex\\thave\\ta\\t\\nsmall\\t\\nlocal\\treceptive\\tfield\\n,\\tmeaning\\tthey\\treact\\tonly\\tto\\tvisual\\nstimuli\\tlocated\\tin\\ta\\tlimited\\tregion\\tof\\tthe\\tvisual\\tfield\\t(see\\t\\nFigure\\t13-1\\n,\\tin\\twhich\\tthe\\tlocal\\treceptive\\tfields\\nof\\tfive\\tneurons\\tare\\trepresented\\tby\\tdashed\\tcircles).\\tThe\\treceptive\\tfields\\tof\\tdifferent\\tneurons\\tmay\\toverlap,\\nand\\ttogether\\tthey\\ttile\\tthe\\twhole\\tvisual\\tfield.\\tMoreover,\\tthe\\tauthors\\tshowed\\tthat\\tsome\\tneurons\\treact\\tonly\\nto\\timages\\tof\\thorizontal\\tlines,\\twhile\\tothers\\treact\\tonly\\tto\\tlines\\twith\\tdifferent\\torientations\\t(two\\tneurons\\tmay\\nhave\\tthe\\tsame\\treceptive\\tfield\\tbut\\treact\\tto\\tdifferent\\tline\\torientations).\\tThey\\talso\\tnoticed\\tthat\\tsome\\tneurons\\nhave\\tlarger\\treceptive\\tfields,\\tand\\tthey\\treact\\tto\\tmore\\tcomplex\\tpatterns\\tthat\\tare\\tcombinations\\tof\\tthe\\tlower-\\nlevel\\tpatterns.\\tThese\\tobservations\\tled\\tto\\tthe\\tidea\\tthat\\tthe\\thigher-level\\tneurons\\tare\\tbased\\ton\\tthe\\toutputs\\tof\\nneighboring\\tlower-level\\tneurons\\t(in\\t\\nFigure\\t13-1\\n,\\tnotice\\tthat\\teach\\tneuron\\tis\\tconnected\\tonly\\tto\\ta\\tfew\\nneurons\\tfrom\\tthe\\tprevious\\tlayer).\\tThis\\tpowerful\\tarchitecture\\tis\\table\\tto\\tdetect\\tall\\tsorts\\tof\\tcomplex\\tpatterns\\nin\\tany\\tarea\\tof\\tthe\\tvisual\\tfield.\\nFigure\\t13-1.\\t\\nLocal\\treceptive\\tfields\\tin\\tthe\\tvisual\\tcortex\\nThese\\tstudies\\tof\\tthe\\tvisual\\tcortex\\tinspired\\tthe\\t\\nneocognitron,\\tintroduced\\tin\\t1980\\n,\\n4\\n\\twhich\\tgradually\\nevolved\\tinto\\twhat\\twe\\tnow\\tcall\\t\\nconvolutional\\tneural\\tnetworks\\n.\\tAn\\timportant\\tmilestone\\twas\\ta\\t\\n1998\\npaper\\n5\\n\\tby\\tYann\\tLeCun,\\tLéon\\tBottou,\\tYoshua\\tBengio,\\tand\\tPatrick\\tHaffner,\\twhich\\tintroduced\\tthe\\tfamous\\nLeNet-5\\n\\tarchitecture,\\t\\nwidely\\tused\\tto\\trecognize\\thandwritten\\tcheck\\tnumbers.\\tThis\\tarchitecture\\thas\\tsome\\nbuilding\\tblocks\\tthat\\tyou\\talready\\tknow,\\tsuch\\tas\\tfully\\tconnected\\tlayers\\tand\\tsigmoid\\tactivation\\tfunctions,\\nbut\\tit\\talso\\tintroduces\\ttwo\\tnew\\tbuilding\\tblocks:\\t\\nconvolutional\\tlayers\\n\\tand\\t\\npooling\\tlayers\\n.\\tLet’s\\tlook\\tat\\nthem\\tnow.\\nNOTE\\nWhy\\tnot\\tsimply\\tuse\\ta\\tregular\\tdeep\\tneural\\tnetwork\\twith\\tfully\\tconnected\\tlayers\\tfor\\timage\\trecognition\\ttasks?\\tUnfortunately,\\nalthough\\tthis\\tworks\\tfine\\tfor\\tsmall\\timages\\t(e.g.,\\tMNIST),\\tit\\tbreaks\\tdown\\tfor\\tlarger\\timages\\tbecause\\tof\\tthe\\thuge\\tnumber\\tof\\nparameters\\tit\\trequires.\\tFor\\texample,\\ta\\t100\\t×\\t100\\timage\\thas\\t10,000\\tpixels,\\tand\\tif\\tthe\\tfirst\\tlayer\\thas\\tjust\\t1,000\\tneurons\\t(which\\nalready\\tseverely\\trestricts\\tthe\\tamount\\tof\\tinformation\\ttransmitted\\tto\\tthe\\tnext\\tlayer),\\tthis\\tmeans\\ta\\ttotal\\tof\\t10\\tmillion\\tconnections.\\nAnd\\tthat’s\\tjust\\tthe\\tfirst\\tlayer.\\tCNNs\\tsolve\\tthis\\tproblem\\tusing\\tpartially\\tconnected\\tlayers.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 442}), Document(page_content='Convolutional\\tLayer\\nThe\\tmost\\t\\nimportant\\tbuilding\\tblock\\tof\\ta\\tCNN\\tis\\tthe\\t\\nconvolutional\\tlayer\\n:\\n6\\n\\tneurons\\tin\\tthe\\tfirst\\nconvolutional\\tlayer\\tare\\tnot\\tconnected\\tto\\tevery\\tsingle\\tpixel\\tin\\tthe\\tinput\\timage\\t(like\\tthey\\twere\\tin\\tprevious\\nchapters),\\tbut\\tonly\\tto\\tpixels\\tin\\ttheir\\treceptive\\tfields\\t(see\\t\\nFigure\\t13-2\\n).\\tIn\\tturn,\\teach\\tneuron\\tin\\tthe\\tsecond\\nconvolutional\\tlayer\\tis\\tconnected\\tonly\\tto\\tneurons\\tlocated\\twithin\\ta\\tsmall\\trectangle\\tin\\tthe\\tfirst\\tlayer.\\tThis\\narchitecture\\tallows\\tthe\\tnetwork\\tto\\tconcentrate\\ton\\tlow-level\\tfeatures\\tin\\tthe\\tfirst\\thidden\\tlayer,\\tthen\\nassemble\\tthem\\tinto\\thigher-level\\tfeatures\\tin\\tthe\\tnext\\thidden\\tlayer,\\tand\\tso\\ton.\\tThis\\thierarchical\\tstructure\\tis\\ncommon\\tin\\treal-world\\timages,\\twhich\\tis\\tone\\tof\\tthe\\treasons\\twhy\\tCNNs\\twork\\tso\\twell\\tfor\\timage\\nrecognition.\\nFigure\\t13-2.\\t\\nCNN\\tlayers\\twith\\trectangular\\tlocal\\treceptive\\tfields\\nNOTE\\nUntil\\tnow,\\tall\\tmultilayer\\tneural\\tnetworks\\twe\\tlooked\\tat\\thad\\tlayers\\tcomposed\\tof\\ta\\tlong\\tline\\tof\\tneurons,\\tand\\twe\\thad\\tto\\tflatten\\tinput\\nimages\\tto\\t1D\\tbefore\\tfeeding\\tthem\\tto\\tthe\\tneural\\tnetwork.\\tNow\\teach\\tlayer\\tis\\trepresented\\tin\\t2D,\\twhich\\tmakes\\tit\\teasier\\tto\\tmatch\\nneurons\\twith\\ttheir\\tcorresponding\\tinputs.\\nA\\tneuron\\tlocated\\tin\\trow\\t\\ni\\n,\\tcolumn\\t\\nj\\n\\tof\\ta\\tgiven\\tlayer\\tis\\tconnected\\tto\\tthe\\toutputs\\tof\\tthe\\tneurons\\tin\\tthe\\nprevious\\tlayer\\tlocated\\tin\\trows\\t\\ni\\n\\tto\\t\\ni\\n\\t+\\t\\nf\\nh\\n\\t–\\t1,\\tcolumns\\t\\nj\\n\\tto\\t\\nj\\n\\t+\\t\\nf\\nw\\n\\t–\\t1,\\twhere\\t\\nf\\nh\\n\\tand\\t\\nf\\nw\\n\\tare\\tthe\\theight\\tand\\nwidth\\tof\\tthe\\treceptive\\tfield\\t(see\\t\\nFigure\\t13-3\\n).\\tIn\\torder\\tfor\\ta\\tlayer\\tto\\thave\\tthe\\tsame\\theight\\tand\\twidth\\tas\\nthe\\tprevious\\tlayer,\\tit\\tis\\tcommon\\tto\\tadd\\tzeros\\taround\\tthe\\tinputs,\\tas\\tshown\\tin\\tthe\\tdiagram.\\tThis\\tis\\t\\ncalled\\nzero\\tpadding\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 443}), Document(page_content='Figure\\t13-3.\\t\\nConnections\\tbetween\\tlayers\\tand\\tzero\\tpadding\\nIt\\tis\\talso\\tpossible\\tto\\tconnect\\ta\\tlarge\\tinput\\tlayer\\tto\\ta\\tmuch\\tsmaller\\tlayer\\tby\\tspacing\\tout\\tthe\\treceptive\\nfields,\\tas\\tshown\\tin\\t\\nFigure\\t13-4\\n.\\tThe\\tdistance\\tbetween\\ttwo\\tconsecutive\\treceptive\\tfields\\tis\\tcalled\\t\\nthe\\nstride\\n.\\tIn\\tthe\\tdiagram,\\ta\\t5\\t×\\t7\\tinput\\tlayer\\t(plus\\tzero\\tpadding)\\tis\\tconnected\\tto\\ta\\t3\\t×\\t4\\tlayer,\\tusing\\t3\\t×\\t3\\nreceptive\\tfields\\tand\\ta\\tstride\\tof\\t2\\t(in\\tthis\\texample\\tthe\\tstride\\tis\\tthe\\tsame\\tin\\tboth\\tdirections,\\tbut\\tit\\tdoes\\tnot\\nhave\\tto\\tbe\\tso).\\tA\\tneuron\\tlocated\\tin\\trow\\t\\ni\\n,\\tcolumn\\t\\nj\\n\\tin\\tthe\\tupper\\tlayer\\tis\\tconnected\\tto\\tthe\\toutputs\\tof\\tthe\\nneurons\\tin\\tthe\\tprevious\\tlayer\\tlocated\\tin\\trows\\t\\ni\\n\\t×\\t\\ns\\nh\\n\\tto\\t\\ni\\n\\t×\\t\\ns\\nh\\n\\t+\\t\\nf\\nh\\n\\t–\\t1,\\tcolumns\\t\\nj\\n\\t×\\t\\ns\\nw\\n\\t+\\t\\nf\\nw\\n\\t–\\t1,\\twhere\\t\\ns\\nh\\nand\\t\\ns\\nw\\n\\tare\\tthe\\tvertical\\tand\\thorizontal\\tstrides.\\nFigure\\t13-4.\\t\\nReducing\\tdimensionality\\tusing\\ta\\tstride', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 444}), Document(page_content='Filters\\nA\\tneuron’s\\t\\nweights\\tcan\\tbe\\trepresented\\tas\\ta\\tsmall\\timage\\tthe\\tsize\\tof\\tthe\\treceptive\\tfield.\\tFor\\texample,\\nFigure\\t13-5\\n\\tshows\\ttwo\\tpossible\\tsets\\tof\\tweights,\\tcalled\\t\\nfilters\\n\\t\\n(or\\t\\nconvolution\\tkernels\\n).\\tThe\\tfirst\\tone\\tis\\nrepresented\\tas\\ta\\tblack\\tsquare\\twith\\ta\\tvertical\\twhite\\tline\\tin\\tthe\\tmiddle\\t(it\\tis\\ta\\t7\\t×\\t7\\tmatrix\\tfull\\tof\\t0s\\texcept\\nfor\\tthe\\tcentral\\tcolumn,\\twhich\\tis\\tfull\\tof\\t1s);\\tneurons\\tusing\\tthese\\tweights\\twill\\tignore\\teverything\\tin\\ttheir\\nreceptive\\tfield\\texcept\\tfor\\tthe\\tcentral\\tvertical\\tline\\t(since\\tall\\tinputs\\twill\\tget\\tmultiplied\\tby\\t0,\\texcept\\tfor\\tthe\\nones\\tlocated\\tin\\tthe\\tcentral\\tvertical\\tline).\\tThe\\tsecond\\tfilter\\tis\\ta\\tblack\\tsquare\\twith\\ta\\thorizontal\\twhite\\tline\\nin\\tthe\\tmiddle.\\tOnce\\tagain,\\tneurons\\tusing\\tthese\\tweights\\twill\\tignore\\teverything\\tin\\ttheir\\treceptive\\tfield\\nexcept\\tfor\\tthe\\tcentral\\thorizontal\\tline.\\nNow\\tif\\tall\\tneurons\\tin\\ta\\tlayer\\tuse\\tthe\\tsame\\tvertical\\tline\\tfilter\\t(and\\tthe\\tsame\\tbias\\tterm),\\tand\\tyou\\tfeed\\tthe\\nnetwork\\tthe\\tinput\\timage\\tshown\\tin\\t\\nFigure\\t13-5\\n\\t(bottom\\timage),\\tthe\\tlayer\\twill\\toutput\\tthe\\ttop-left\\timage.\\nNotice\\tthat\\tthe\\tvertical\\twhite\\tlines\\tget\\tenhanced\\twhile\\tthe\\trest\\tgets\\tblurred.\\tSimilarly,\\tthe\\tupper-right\\nimage\\tis\\twhat\\tyou\\tget\\tif\\tall\\tneurons\\tuse\\tthe\\thorizontal\\tline\\tfilter;\\tnotice\\tthat\\tthe\\thorizontal\\twhite\\tlines\\tget\\nenhanced\\twhile\\tthe\\trest\\tis\\tblurred\\tout.\\tThus,\\ta\\tlayer\\tfull\\tof\\tneurons\\tusing\\tthe\\tsame\\tfilter\\tgives\\t\\nyou\\ta\\nfeature\\tmap\\n,\\twhich\\thighlights\\tthe\\tareas\\tin\\tan\\timage\\tthat\\tare\\tmost\\tsimilar\\tto\\tthe\\tfilter.\\tDuring\\ttraining,\\ta\\nCNN\\tfinds\\tthe\\tmost\\tuseful\\tfilters\\tfor\\tits\\ttask,\\tand\\tit\\tlearns\\tto\\tcombine\\tthem\\tinto\\tmore\\tcomplex\\tpatterns\\n(e.g.,\\ta\\tcross\\tis\\tan\\tarea\\tin\\tan\\timage\\twhere\\tboth\\tthe\\tvertical\\tfilter\\tand\\tthe\\thorizontal\\tfilter\\tare\\tactive).\\nFigure\\t13-5.\\t\\nApplying\\ttwo\\tdifferent\\tfilters\\tto\\tget\\ttwo\\tfeature\\tmaps', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 445}), Document(page_content='Stacking\\tMultiple\\tFeature\\tMaps\\nUp\\t\\nto\\tnow,\\tfor\\tsimplicity,\\twe\\thave\\trepresented\\teach\\tconvolutional\\tlayer\\tas\\ta\\tthin\\t2D\\tlayer,\\tbut\\tin\\treality\\nit\\tis\\tcomposed\\tof\\tseveral\\tfeature\\tmaps\\tof\\tequal\\tsizes,\\tso\\tit\\tis\\tmore\\taccurately\\trepresented\\tin\\t3D\\t(see\\nFigure\\t13-6\\n).\\tWithin\\tone\\tfeature\\tmap,\\tall\\tneurons\\tshare\\tthe\\tsame\\tparameters\\t(weights\\tand\\tbias\\tterm),\\tbut\\ndifferent\\tfeature\\tmaps\\tmay\\thave\\tdifferent\\tparameters.\\tA\\tneuron’s\\treceptive\\tfield\\tis\\tthe\\tsame\\tas\\tdescribed\\nearlier,\\tbut\\tit\\textends\\tacross\\tall\\tthe\\tprevious\\tlayers’\\tfeature\\tmaps.\\tIn\\tshort,\\ta\\tconvolutional\\tlayer\\nsimultaneously\\tapplies\\tmultiple\\tfilters\\tto\\tits\\tinputs,\\tmaking\\tit\\tcapable\\tof\\tdetecting\\tmultiple\\tfeatures\\nanywhere\\tin\\tits\\tinputs.\\nNOTE\\nThe\\tfact\\tthat\\tall\\tneurons\\tin\\ta\\tfeature\\tmap\\tshare\\tthe\\tsame\\tparameters\\tdramatically\\treduces\\tthe\\tnumber\\tof\\tparameters\\tin\\tthe\\nmodel,\\tbut\\tmost\\timportantly\\tit\\tmeans\\tthat\\tonce\\tthe\\tCNN\\thas\\tlearned\\tto\\trecognize\\ta\\tpattern\\tin\\tone\\tlocation,\\tit\\tcan\\trecognize\\tit\\tin\\nany\\tother\\tlocation.\\tIn\\tcontrast,\\tonce\\ta\\tregular\\tDNN\\thas\\tlearned\\tto\\trecognize\\ta\\tpattern\\tin\\tone\\tlocation,\\tit\\tcan\\trecognize\\tit\\tonly\\tin\\nthat\\tparticular\\tlocation.\\nMoreover,\\tinput\\timages\\tare\\talso\\tcomposed\\tof\\tmultiple\\tsublayers:\\tone\\tper\\t\\ncolor\\tchannel\\n.\\tThere\\tare\\ntypically\\tthree:\\tred,\\tgreen,\\tand\\tblue\\t(RGB).\\tGrayscale\\timages\\thave\\tjust\\tone\\tchannel,\\tbut\\tsome\\timages\\nmay\\thave\\tmuch\\tmore\\t—\\tfor\\texample,\\tsatellite\\timages\\tthat\\tcapture\\textra\\tlight\\tfrequencies\\t(such\\tas\\ninfrared).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 446}), Document(page_content='Figure\\t13-6.\\t\\nConvolution\\tlayers\\twith\\tmultiple\\tfeature\\tmaps,\\tand\\timages\\twith\\tthree\\tchannels\\nSpecifically,\\ta\\tneuron\\tlocated\\tin\\trow\\t\\ni\\n,\\tcolumn\\t\\nj\\n\\tof\\tthe\\tfeature\\tmap\\t\\nk\\n\\tin\\ta\\tgiven\\tconvolutional\\tlayer\\t\\nl\\n\\tis\\nconnected\\tto\\tthe\\toutputs\\tof\\tthe\\tneurons\\tin\\tthe\\tprevious\\tlayer\\t\\nl\\n\\t–\\t1,\\tlocated\\tin\\trows\\t\\ni\\n\\t×\\t\\ns\\nh\\n\\tto\\t\\ni\\n\\t×\\t\\ns\\nh\\n\\t+\\t\\nf\\nh\\n\\t–\\t1\\nand\\tcolumns\\t\\nj\\n\\t×\\t\\ns\\nw\\n\\tto\\t\\nj\\n\\t×\\t\\ns\\nw\\n\\t+\\t\\nf\\nw\\n\\t–\\t1,\\tacross\\tall\\tfeature\\tmaps\\t(in\\tlayer\\t\\nl\\n\\t–\\t\\n1\\n).\\tNote\\tthat\\tall\\tneurons\\tlocated\\nin\\tthe\\tsame\\trow\\t\\ni\\n\\tand\\tcolumn\\t\\nj\\n\\tbut\\tin\\tdifferent\\tfeature\\tmaps\\tare\\tconnected\\tto\\tthe\\toutputs\\tof\\tthe\\texact\\tsame\\nneurons\\tin\\tthe\\tprevious\\tlayer.\\nEquation\\t13-1\\n\\tsummarizes\\tthe\\tpreceding\\texplanations\\tin\\tone\\tbig\\tmathematical\\tequation:\\tit\\tshows\\thow\\tto\\ncompute\\tthe\\toutput\\tof\\ta\\tgiven\\tneuron\\tin\\ta\\tconvolutional\\tlayer.\\tIt\\tis\\ta\\tbit\\tugly\\tdue\\tto\\tall\\tthe\\tdifferent\\nindices,\\tbut\\tall\\tit\\tdoes\\tis\\tcalculate\\tthe\\tweighted\\tsum\\tof\\tall\\tthe\\tinputs,\\tplus\\tthe\\tbias\\tterm.\\nEquation\\t13-1.\\t\\nComputing\\tthe\\toutput\\tof\\ta\\tneuron\\tin\\ta\\tconvolutional\\tlayer\\nz\\ni,\\tj,\\tk\\n\\tis\\tthe\\toutput\\tof\\tthe\\tneuron\\tlocated\\tin\\trow\\t\\ni\\n,\\tcolumn\\t\\nj\\n\\tin\\tfeature\\tmap\\t\\nk\\n\\tof\\tthe\\tconvolutional\\tlayer\\n(layer\\t\\nl\\n).\\nAs\\texplained\\tearlier,\\t\\ns\\nh\\n\\tand\\t\\ns\\nw\\n\\tare\\tthe\\tvertical\\tand\\thorizontal\\tstrides,\\t\\nf\\nh\\n\\tand\\t\\nf\\nw\\n\\tare\\tthe\\theight\\tand\\nwidth\\tof\\tthe\\treceptive\\tfield,\\tand\\t\\nf\\nn\\n′\\n\\tis\\tthe\\tnumber\\tof\\tfeature\\tmaps\\tin\\tthe\\tprevious\\tlayer\\t(layer\\t\\nl\\n\\t–\\t1).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 447}), Document(page_content='x\\ni\\n′,\\t\\nj\\n′,\\t\\nk\\n′\\n\\tis\\tthe\\toutput\\tof\\tthe\\tneuron\\tlocated\\tin\\tlayer\\t\\nl\\n\\t–\\t1,\\trow\\t\\ni\\n′,\\tcolumn\\t\\nj\\n′,\\tfeature\\tmap\\t\\nk\\n′\\t(or\\tchannel\\t\\nk\\n′\\nif\\tthe\\tprevious\\tlayer\\tis\\tthe\\tinput\\tlayer).\\nb\\nk\\n\\tis\\tthe\\tbias\\tterm\\tfor\\tfeature\\tmap\\t\\nk\\n\\t(in\\tlayer\\t\\nl\\n).\\tYou\\tcan\\tthink\\tof\\tit\\tas\\ta\\tknob\\tthat\\ttweaks\\tthe\\toverall\\nbrightness\\tof\\tthe\\tfeature\\tmap\\t\\nk\\n.\\nw\\nu\\n,\\t\\nv\\n,\\t\\nk\\n′\\t,\\nk\\n\\tis\\tthe\\tconnection\\tweight\\tbetween\\tany\\tneuron\\tin\\tfeature\\tmap\\t\\nk\\n\\tof\\tthe\\tlayer\\t\\nl\\n\\tand\\tits\\tinput\\nlocated\\tat\\trow\\t\\nu\\n,\\tcolumn\\t\\nv\\n\\t(relative\\tto\\tthe\\tneuron’s\\treceptive\\tfield),\\tand\\tfeature\\tmap\\t\\nk\\n′.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 448}), Document(page_content='TensorFlow\\tImplementation\\nIn\\t\\nTensorFlow,\\teach\\tinput\\timage\\tis\\ttypically\\trepresented\\tas\\ta\\t3D\\ttensor\\tof\\t\\nshape\\t[height,\\twidth,\\nchannels]\\n.\\tA\\tmini-batch\\tis\\trepresented\\tas\\ta\\t4D\\ttensor\\tof\\t\\nshape\\t[mini-batch\\tsize,\\theight,\\nwidth,\\tchannels]\\n.\\tThe\\tweights\\tof\\ta\\tconvolutional\\tlayer\\tare\\trepresented\\tas\\ta\\t4D\\ttensor\\tof\\tshape\\t[\\nf\\nh\\n,\\t\\nf\\nw\\n,\\nf\\nn\\n′\\n,\\t\\nf\\nn\\n].\\tThe\\tbias\\tterms\\tof\\ta\\tconvolutional\\tlayer\\tare\\tsimply\\trepresented\\tas\\ta\\t1D\\ttensor\\tof\\t\\nshape\\t[f\\nn\\n]\\n.\\nLet’s\\tlook\\tat\\ta\\tsimple\\texample.\\tThe\\tfollowing\\tcode\\tloads\\ttwo\\tsample\\timages,\\tusing\\tScikit-Learn’s\\nload_sample_images()\\n\\t\\n(which\\tloads\\ttwo\\tcolor\\timages,\\tone\\tof\\ta\\tChinese\\ttemple,\\tand\\tthe\\tother\\tof\\ta\\nflower).\\tThen\\tit\\tcreates\\ttwo\\t7\\t×\\t7\\tfilters\\t(one\\twith\\ta\\tvertical\\twhite\\tline\\tin\\tthe\\tmiddle,\\tand\\tthe\\tother\\twith\\ta\\nhorizontal\\twhite\\tline\\tin\\tthe\\tmiddle),\\tand\\tapplies\\tthem\\tto\\tboth\\timages\\tusing\\ta\\tconvolutional\\tlayer\\tbuilt\\nusing\\tTensorFlow’s\\t\\ntf.nn.conv2d()\\n\\tfunction\\t(with\\tzero\\tpadding\\tand\\ta\\tstride\\tof\\t\\n2\\n).\\t\\nFinally,\\tit\\tplots\\tone\\nof\\tthe\\tresulting\\tfeature\\tmaps\\t(similar\\tto\\tthe\\ttop-right\\timage\\tin\\t\\nFigure\\t13-5\\n).\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nload_sample_images\\n#\\tLoad\\tsample\\timages\\nchina\\n\\t\\n=\\n\\t\\nload_sample_image\\n(\\n\"china.jpg\"\\n)\\nflower\\n\\t\\n=\\n\\t\\nload_sample_image\\n(\\n\"flower.jpg\"\\n)\\ndataset\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\nchina\\n,\\n\\t\\nflower\\n],\\n\\t\\ndtype\\n=\\nnp\\n.\\nfloat32\\n)\\nbatch_size\\n,\\n\\t\\nheight\\n,\\n\\t\\nwidth\\n,\\n\\t\\nchannels\\n\\t\\n=\\n\\t\\ndataset\\n.\\nshape\\n#\\tCreate\\t2\\tfilters\\nfilters\\n\\t\\n=\\n\\t\\nnp\\n.\\nzeros\\n(\\nshape\\n=\\n(\\n7\\n,\\n\\t\\n7\\n,\\n\\t\\nchannels\\n,\\n\\t\\n2\\n),\\n\\t\\ndtype\\n=\\nnp\\n.\\nfloat32\\n)\\nfilters\\n[:,\\n\\t\\n3\\n,\\n\\t\\n:,\\n\\t\\n0\\n]\\n\\t\\n=\\n\\t\\n1\\n\\t\\t\\n#\\tvertical\\tline\\nfilters\\n[\\n3\\n,\\n\\t\\n:,\\n\\t\\n:,\\n\\t\\n1\\n]\\n\\t\\n=\\n\\t\\n1\\n\\t\\t\\n#\\thorizontal\\tline\\n#\\tCreate\\ta\\tgraph\\twith\\tinput\\tX\\tplus\\ta\\tconvolutional\\tlayer\\tapplying\\tthe\\t2\\tfilters\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nheight\\n,\\n\\t\\nwidth\\n,\\n\\t\\nchannels\\n))\\nconvolution\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nconv2d\\n(\\nX\\n,\\n\\t\\nfilters\\n,\\n\\t\\nstrides\\n=\\n[\\n1\\n,\\n2\\n,\\n2\\n,\\n1\\n],\\n\\t\\npadding\\n=\\n\"SAME\"\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\noutput\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\nconvolution\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\ndataset\\n})\\nplt\\n.\\nimshow\\n(\\noutput\\n[\\n0\\n,\\n\\t\\n:,\\n\\t\\n:,\\n\\t\\n1\\n],\\n\\t\\ncmap\\n=\\n\"gray\"\\n)\\n\\t\\n#\\tplot\\t1st\\timage\\'s\\t2nd\\tfeature\\tmap\\nplt\\n.\\nshow\\n()\\nMost\\tof\\tthis\\tcode\\tis\\tself-explanatory,\\t\\nbut\\tthe\\t\\ntf.nn.conv2d()\\n\\tline\\tdeserves\\ta\\tbit\\tof\\texplanation:\\nX\\n\\tis\\tthe\\tinput\\tmini-batch\\t(a\\t4D\\ttensor,\\tas\\texplained\\tearlier).\\nfilters\\n\\tis\\tthe\\tset\\tof\\tfilters\\tto\\tapply\\t(also\\ta\\t4D\\ttensor,\\tas\\texplained\\tearlier).\\nstrides\\n\\tis\\ta\\tfour-element\\t1D\\tarray,\\twhere\\tthe\\ttwo\\tcentral\\telements\\tare\\tthe\\tvertical\\tand\\thorizontal\\nstrides\\t(\\ns\\nh\\n\\tand\\t\\ns\\nw\\n).\\tThe\\tfirst\\tand\\tlast\\telements\\tmust\\tcurrently\\tbe\\tequal\\tto\\t1.\\tThey\\tmay\\tone\\tday\\tbe\\nused\\tto\\tspecify\\ta\\tbatch\\tstride\\t(to\\tskip\\tsome\\tinstances)\\tand\\ta\\tchannel\\tstride\\t(to\\tskip\\tsome\\tof\\tthe\\nprevious\\tlayer’s\\tfeature\\tmaps\\tor\\tchannels).\\npadding\\n\\tmust\\tbe\\teither\\t\\n\"VALID\"\\n\\tor\\t\\n\"SAME\"\\n:\\nIf\\tset\\tto\\t\\n\"VALID\"\\n,\\tthe\\tconvolutional\\tlayer\\tdoes\\t\\nnot\\n\\tuse\\tzero\\tpadding,\\tand\\tmay\\tignore\\tsome\\nrows\\tand\\tcolumns\\tat\\tthe\\tbottom\\tand\\tright\\tof\\tthe\\tinput\\timage,\\tdepending\\ton\\tthe\\tstride,\\tas\\tshown\\nin\\t\\nFigure\\t13-7\\n\\t(for\\tsimplicity,\\tonly\\tthe\\thorizontal\\tdimension\\tis\\tshown\\there,\\tbut\\tof\\tcourse\\tthe\\nsame\\tlogic\\tapplies\\tto\\tthe\\tvertical\\tdimension).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 449}), Document(page_content='If\\tset\\tto\\t\\n\"SAME\"\\n,\\tthe\\tconvolutional\\tlayer\\tuses\\t\\nzero\\tpadding\\tif\\tnecessary.\\tIn\\tthis\\tcase,\\tthe\\tnumber\\nof\\toutput\\tneurons\\tis\\tequal\\tto\\tthe\\tnumber\\tof\\tinput\\tneurons\\tdivided\\tby\\tthe\\tstride,\\trounded\\tup\\t(in\\nthis\\texample,\\tceil\\t(13\\t/\\t5)\\t=\\t3).\\tThen\\tzeros\\tare\\tadded\\tas\\tevenly\\tas\\tpossible\\taround\\tthe\\tinputs.\\nFigure\\t13-7.\\t\\nPadding\\toptions\\t—\\tinput\\twidth:\\t13,\\tfilter\\twidth:\\t6,\\tstride:\\t5\\nIn\\tthis\\tsimple\\texample,\\twe\\tmanually\\tcreated\\tthe\\tfilters,\\tbut\\tin\\ta\\treal\\tCNN\\tyou\\twould\\tlet\\tthe\\ttraining\\nalgorithm\\tdiscover\\tthe\\tbest\\tfilters\\tautomatically.\\tTensorFlow\\thas\\ta\\t\\ntf.layers.conv2d()\\n\\tfunction\\twhich\\ncreates\\tthe\\tfilters\\tvariable\\tfor\\tyou\\t(called\\t\\nkernel\\n),\\tand\\tinitializes\\tit\\trandomly.\\tFor\\texample,\\tthe\\nfollowing\\tcode\\tcreates\\tan\\tinput\\tplaceholder\\tfollowed\\tby\\ta\\tconvolutional\\tlayer\\twith\\ttwo\\t7\\t×\\t7\\tfeature\\nmaps,\\tusing\\t2\\t×\\t2\\tstrides\\t(note\\tthat\\tthis\\tfunction\\tonly\\texpects\\tthe\\tvertical\\tand\\thorizontal\\tstrides),\\tand\\n\"SAME\"\\n\\tpadding:\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nheight\\n,\\n\\t\\nwidth\\n,\\n\\t\\nchannels\\n),\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nconv\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\nconv2d\\n(\\nX\\n,\\n\\t\\nfilters\\n=\\n2\\n,\\n\\t\\nkernel_size\\n=\\n7\\n,\\n\\t\\nstrides\\n=\\n[\\n2\\n,\\n2\\n],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\npadding\\n=\\n\"SAME\"\\n)\\nUnfortunately,\\tconvolutional\\tlayers\\thave\\tquite\\ta\\tfew\\thyperparameters:\\tyou\\tmust\\tchoose\\tthe\\tnumber\\tof\\nfilters,\\ttheir\\theight\\tand\\twidth,\\tthe\\tstrides,\\tand\\tthe\\tpadding\\ttype.\\tAs\\talways,\\tyou\\tcan\\tuse\\tcross-validation\\nto\\tfind\\tthe\\tright\\thyperparameter\\tvalues,\\tbut\\tthis\\tis\\tvery\\ttime-consuming.\\tWe\\twill\\tdiscuss\\tcommon\\tCNN\\narchitectures\\tlater,\\tto\\tgive\\tyou\\tsome\\tidea\\tof\\twhat\\thyperparameter\\tvalues\\twork\\tbest\\tin\\t\\npractice.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 450}), Document(page_content='Memory\\tRequirements\\nAnother\\t\\nproblem\\twith\\tCNNs\\tis\\tthat\\tthe\\tconvolutional\\tlayers\\trequire\\ta\\thuge\\tamount\\tof\\tRAM,\\tespecially\\nduring\\ttraining,\\tbecause\\tthe\\treverse\\tpass\\tof\\tbackpropagation\\trequires\\tall\\tthe\\tintermediate\\tvalues\\ncomputed\\tduring\\tthe\\tforward\\tpass.\\nFor\\texample,\\tconsider\\ta\\tconvolutional\\tlayer\\twith\\t5\\t×\\t5\\tfilters,\\toutputting\\t200\\tfeature\\tmaps\\tof\\tsize\\t150\\t×\\n100,\\twith\\tstride\\t1\\tand\\tSAME\\tpadding.\\tIf\\tthe\\tinput\\tis\\ta\\t150\\t×\\t100\\tRGB\\timage\\t(three\\tchannels),\\tthen\\tthe\\nnumber\\tof\\tparameters\\tis\\t(5\\t×\\t5\\t×\\t3\\t+\\t1)\\t×\\t200\\t=\\t15,200\\t(the\\t+1\\tcorresponds\\tto\\tthe\\tbias\\tterms),\\twhich\\tis\\nfairly\\tsmall\\tcompared\\tto\\ta\\tfully\\tconnected\\tlayer.\\n7\\n\\tHowever,\\teach\\tof\\tthe\\t200\\tfeature\\tmaps\\tcontains\\t150\\t×\\n100\\tneurons,\\tand\\teach\\tof\\tthese\\tneurons\\tneeds\\tto\\tcompute\\ta\\tweighted\\tsum\\tof\\tits\\t5\\t×\\t5\\t×\\t3\\t=\\t75\\t\\ninputs:\\nthat’s\\ta\\ttotal\\tof\\t225\\tmillion\\tfloat\\tmultiplications.\\tNot\\tas\\tbad\\tas\\ta\\tfully\\tconnected\\tlayer,\\tbut\\tstill\\tquite\\ncomputationally\\tintensive.\\tMoreover,\\tif\\tthe\\tfeature\\tmaps\\tare\\trepresented\\tusing\\t32-bit\\tfloats,\\tthen\\tthe\\nconvolutional\\tlayer’s\\toutput\\twill\\toccupy\\t200\\t×\\t150\\t×\\t100\\t×\\t32\\t=\\t96\\tmillion\\tbits\\t(about\\t11.4\\tMB)\\tof\\nRAM.\\n8\\n\\tAnd\\tthat’s\\tjust\\tfor\\tone\\tinstance!\\tIf\\ta\\ttraining\\tbatch\\tcontains\\t100\\tinstances,\\tthen\\tthis\\tlayer\\twill\\tuse\\nup\\tover\\t1\\tGB\\tof\\tRAM!\\nDuring\\t\\ninference\\t(i.e.,\\twhen\\tmaking\\ta\\tprediction\\tfor\\ta\\tnew\\tinstance)\\tthe\\tRAM\\toccupied\\tby\\tone\\tlayer\\tcan\\nbe\\treleased\\tas\\tsoon\\tas\\tthe\\tnext\\tlayer\\thas\\tbeen\\tcomputed,\\tso\\tyou\\tonly\\tneed\\tas\\tmuch\\tRAM\\tas\\trequired\\tby\\ntwo\\tconsecutive\\tlayers.\\tBut\\tduring\\ttraining\\teverything\\tcomputed\\tduring\\tthe\\tforward\\tpass\\tneeds\\tto\\tbe\\npreserved\\tfor\\tthe\\treverse\\tpass,\\tso\\tthe\\tamount\\tof\\tRAM\\tneeded\\tis\\t(at\\tleast)\\tthe\\ttotal\\tamount\\tof\\tRAM\\nrequired\\tby\\tall\\tlayers.\\nTIP\\nIf\\ttraining\\tcrashes\\tbecause\\tof\\tan\\tout-of-memory\\terror,\\tyou\\tcan\\ttry\\treducing\\tthe\\tmini-batch\\tsize.\\tAlternatively,\\tyou\\tcan\\ttry\\nreducing\\tdimensionality\\tusing\\ta\\tstride,\\tor\\tremoving\\ta\\tfew\\tlayers.\\tOr\\tyou\\tcan\\ttry\\tusing\\t16-bit\\tfloats\\tinstead\\tof\\t32-bit\\tfloats.\\tOr\\tyou\\ncould\\tdistribute\\tthe\\tCNN\\tacross\\tmultiple\\tdevices.\\nNow\\t\\nlet’s\\tlook\\tat\\tthe\\tsecond\\tcommon\\tbuilding\\tblock\\t\\nof\\tCNNs:\\tthe\\t\\npooling\\tlayer\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 451}), Document(page_content='Pooling\\tLayer\\nOnce\\t\\nyou\\tunderstand\\thow\\tconvolutional\\tlayers\\twork,\\tthe\\tpooling\\tlayers\\tare\\tquite\\teasy\\tto\\tgrasp.\\tTheir\\ngoal\\tis\\t\\nto\\t\\nsubsample\\n\\t(i.e.,\\tshrink)\\tthe\\tinput\\timage\\tin\\torder\\tto\\treduce\\tthe\\tcomputational\\tload,\\tthe\\tmemory\\nusage,\\tand\\tthe\\tnumber\\tof\\tparameters\\t(thereby\\tlimiting\\tthe\\trisk\\tof\\toverfitting).\\tReducing\\tthe\\tinput\\timage\\nsize\\talso\\tmakes\\tthe\\tneural\\tnetwork\\ttolerate\\ta\\tlittle\\tbit\\tof\\t\\nimage\\tshift\\t(\\nlocation\\tinvariance\\n).\\nJust\\tlike\\tin\\tconvolutional\\tlayers,\\teach\\tneuron\\tin\\ta\\tpooling\\tlayer\\tis\\tconnected\\tto\\tthe\\toutputs\\tof\\ta\\tlimited\\nnumber\\tof\\tneurons\\tin\\tthe\\tprevious\\tlayer,\\tlocated\\twithin\\ta\\tsmall\\trectangular\\treceptive\\tfield.\\tYou\\tmust\\ndefine\\tits\\tsize,\\tthe\\tstride,\\tand\\tthe\\tpadding\\ttype,\\tjust\\tlike\\tbefore.\\tHowever,\\ta\\tpooling\\tneuron\\thas\\tno\\nweights;\\tall\\tit\\tdoes\\tis\\taggregate\\tthe\\tinputs\\tusing\\tan\\taggregation\\tfunction\\tsuch\\tas\\tthe\\tmax\\tor\\tmean.\\nFigure\\t13-8\\n\\tshows\\ta\\t\\nmax\\tpooling\\tlayer\\n,\\t\\nwhich\\tis\\tthe\\tmost\\tcommon\\ttype\\tof\\tpooling\\tlayer.\\tIn\\tthis\\texample,\\nwe\\tuse\\ta\\t2\\t×\\t2\\t\\npooling\\tkernel\\n,\\t\\na\\tstride\\tof\\t2,\\tand\\tno\\tpadding.\\tNote\\tthat\\tonly\\tthe\\tmax\\tinput\\tvalue\\tin\\teach\\nkernel\\tmakes\\tit\\tto\\tthe\\tnext\\tlayer.\\tThe\\tother\\tinputs\\tare\\tdropped.\\nFigure\\t13-8.\\t\\nMax\\tpooling\\tlayer\\t(2\\t×\\t2\\tpooling\\tkernel,\\tstride\\t2,\\tno\\tpadding)\\nThis\\tis\\tobviously\\ta\\tvery\\tdestructive\\tkind\\tof\\tlayer:\\teven\\twith\\ta\\ttiny\\t2\\t×\\t2\\tkernel\\tand\\ta\\tstride\\tof\\t2,\\tthe\\noutput\\twill\\tbe\\ttwo\\ttimes\\tsmaller\\tin\\tboth\\tdirections\\t(so\\tits\\tarea\\twill\\tbe\\tfour\\ttimes\\tsmaller),\\tsimply\\ndropping\\t75%\\tof\\tthe\\tinput\\tvalues.\\nA\\tpooling\\tlayer\\ttypically\\tworks\\ton\\tevery\\tinput\\tchannel\\tindependently,\\tso\\tthe\\toutput\\tdepth\\tis\\tthe\\tsame\\tas\\nthe\\tinput\\tdepth.\\tYou\\tmay\\talternatively\\tpool\\tover\\tthe\\tdepth\\tdimension,\\tas\\twe\\twill\\tsee\\tnext,\\tin\\twhich\\tcase\\nthe\\timage’s\\tspatial\\tdimensions\\t(height\\tand\\twidth)\\tremain\\tunchanged,\\tbut\\tthe\\tnumber\\tof\\tchannels\\tis\\nreduced.\\nImplementing\\ta\\tmax\\tpooling\\tlayer\\tin\\t\\nTensorFlow\\tis\\tquite\\teasy.\\tThe\\tfollowing\\tcode\\tcreates\\ta\\tmax\\tpooling\\nlayer\\tusing\\ta\\t2\\t×\\t2\\tkernel,\\tstride\\t2,\\tand\\tno\\tpadding,\\tthen\\tapplies\\tit\\tto\\tall\\tthe\\timages\\tin\\t\\nthe\\tdataset:\\n[\\n...\\n]\\n\\t\\n#\\tload\\tthe\\timage\\tdataset,\\tjust\\tlike\\tabove\\n#\\tCreate\\ta\\tgraph\\twith\\tinput\\tX\\tplus\\ta\\tmax\\tpooling\\tlayer\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nheight\\n,\\n\\t\\nwidth\\n,\\n\\t\\nchannels\\n))\\nmax_pool\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nmax_pool\\n(\\nX\\n,\\n\\t\\nksize\\n=\\n[\\n1\\n,\\n2\\n,\\n2\\n,\\n1\\n],\\n\\t\\nstrides\\n=\\n[\\n1\\n,\\n2\\n,\\n2\\n,\\n1\\n],\\npadding\\n=\\n\"VALID\"\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\noutput\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\nmax_pool\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\ndataset\\n})', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 452}), Document(page_content='plt\\n.\\nimshow\\n(\\noutput\\n[\\n0\\n]\\n.\\nastype\\n(\\nnp\\n.\\nuint8\\n))\\n\\t\\t\\n#\\tplot\\tthe\\toutput\\tfor\\tthe\\t1st\\timage\\nplt\\n.\\nshow\\n()\\nThe\\t\\nksize\\n\\targument\\tcontains\\tthe\\tkernel\\tshape\\talong\\tall\\tfour\\tdimensions\\tof\\tthe\\tinput\\ttensor:\\t\\n[batch\\nsize,\\theight,\\twidth,\\tchannels]\\n.\\tTensorFlow\\tcurrently\\tdoes\\tnot\\tsupport\\tpooling\\tover\\tmultiple\\ninstances,\\tso\\tthe\\tfirst\\telement\\tof\\t\\nksize\\n\\tmust\\tbe\\tequal\\tto\\t1.\\tMoreover,\\tit\\tdoes\\tnot\\tsupport\\tpooling\\tover\\nboth\\tthe\\tspatial\\tdimensions\\t(height\\tand\\twidth)\\tand\\tthe\\tdepth\\tdimension,\\tso\\teither\\t\\nksize[1]\\n\\tand\\nksize[2]\\n\\tmust\\tboth\\tbe\\tequal\\tto\\t1,\\tor\\t\\nksize[3]\\n\\tmust\\tbe\\tequal\\tto\\t1.\\nTo\\tcreate\\tan\\t\\naverage\\tpooling\\tlayer\\n,\\t\\njust\\tuse\\tthe\\t\\navg_pool()\\n\\t\\nfunction\\tinstead\\t\\nof\\t\\nmax_pool()\\n.\\nNow\\tyou\\tknow\\tall\\tthe\\tbuilding\\tblocks\\tto\\tcreate\\ta\\tconvolutional\\tneural\\tnetwork.\\tLet’s\\tsee\\t\\nhow\\tto\\nassemble\\tthem.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 453}), Document(page_content='CNN\\tArchitectures\\nTypical\\t\\nCNN\\tarchitectures\\tstack\\ta\\tfew\\tconvolutional\\tlayers\\t(each\\tone\\tgenerally\\tfollowed\\tby\\ta\\tReLU\\nlayer),\\tthen\\ta\\tpooling\\tlayer,\\tthen\\tanother\\tfew\\tconvolutional\\tlayers\\t(+ReLU),\\tthen\\tanother\\tpooling\\tlayer,\\nand\\tso\\ton.\\tThe\\timage\\tgets\\tsmaller\\tand\\tsmaller\\tas\\tit\\tprogresses\\tthrough\\tthe\\tnetwork,\\tbut\\tit\\talso\\ttypically\\ngets\\tdeeper\\tand\\tdeeper\\t(i.e.,\\twith\\tmore\\tfeature\\tmaps)\\tthanks\\tto\\tthe\\tconvolutional\\tlayers\\t(see\\t\\nFigure\\t13-\\n9\\n).\\tAt\\tthe\\ttop\\tof\\tthe\\tstack,\\ta\\tregular\\tfeedforward\\tneural\\tnetwork\\tis\\tadded,\\tcomposed\\tof\\ta\\tfew\\tfully\\nconnected\\tlayers\\t(+ReLUs),\\tand\\tthe\\tfinal\\tlayer\\toutputs\\tthe\\tprediction\\t(e.g.,\\ta\\tsoftmax\\tlayer\\tthat\\toutputs\\nestimated\\tclass\\tprobabilities).\\nFigure\\t13-9.\\t\\nTypical\\tCNN\\tarchitecture\\nTIP\\nA\\tcommon\\tmistake\\tis\\tto\\tuse\\t\\nconvolution\\tkernels\\tthat\\tare\\ttoo\\tlarge.\\tYou\\tcan\\toften\\tget\\tthe\\tsame\\teffect\\tas\\ta\\t9\\t×\\t9\\tkernel\\tby\\nstacking\\ttwo\\t3\\t×\\t3\\tkernels\\ton\\ttop\\tof\\teach\\tother,\\tfor\\ta\\tlot\\tless\\tcompute.\\nOver\\tthe\\tyears,\\tvariants\\tof\\tthis\\tfundamental\\tarchitecture\\thave\\tbeen\\tdeveloped,\\tleading\\tto\\tamazing\\nadvances\\tin\\tthe\\tfield.\\tA\\tgood\\tmeasure\\tof\\tthis\\tprogress\\tis\\tthe\\terror\\trate\\tin\\tcompetitions\\tsuch\\tas\\t\\nthe\\nILSVRC\\t\\nImageNet\\tchallenge\\n.\\tIn\\tthis\\tcompetition\\tthe\\ttop-5\\terror\\trate\\tfor\\t\\nimage\\tclassification\\tfell\\tfrom\\nover\\t26%\\tto\\tbarely\\tover\\t3%\\tin\\tjust\\tfive\\tyears.\\tThe\\ttop-five\\terror\\trate\\tis\\tthe\\tnumber\\tof\\ttest\\timages\\tfor\\nwhich\\tthe\\tsystem’s\\ttop\\t5\\tpredictions\\tdid\\tnot\\tinclude\\tthe\\tcorrect\\tanswer.\\tThe\\timages\\tare\\tlarge\\t(256\\tpixels\\nhigh)\\tand\\tthere\\tare\\t1,000\\tclasses,\\tsome\\tof\\twhich\\tare\\treally\\tsubtle\\t(try\\tdistinguishing\\t120\\tdog\\tbreeds).\\nLooking\\tat\\tthe\\tevolution\\tof\\tthe\\twinning\\tentries\\tis\\ta\\tgood\\tway\\tto\\tunderstand\\thow\\tCNNs\\twork.\\nWe\\twill\\tfirst\\tlook\\tat\\tthe\\tclassical\\tLeNet-5\\tarchitecture\\t(1998),\\tthen\\tthree\\tof\\tthe\\twinners\\tof\\tthe\\tILSVRC\\nchallenge:\\tAlexNet\\t(2012),\\tGoogLeNet\\t(2014),\\tand\\tResNet\\t(2015).\\nOTHER\\tVISUAL\\tTASKS\\nThere\\twas\\tstunning\\tprogress\\tas\\twell\\tin\\tother\\tvisual\\ttasks\\tsuch\\tas\\tobject\\tdetection\\tand\\tlocalization,\\tand\\timage\\tsegmentation.\\tIn\\tobject\\ndetection\\tand\\tlocalization,\\tthe\\tneural\\tnetwork\\ttypically\\toutputs\\ta\\tsequence\\tof\\tbounding\\tboxes\\taround\\tvarious\\tobjects\\tin\\tthe\\timage.\\tFor\\nexample,\\tsee\\tMaxine\\tOquab\\tet\\tal.’s\\t2015\\t\\npaper\\n\\tthat\\toutputs\\ta\\theat\\tmap\\tfor\\teach\\tobject\\tclass,\\tor\\tRussell\\tStewart\\tet\\tal.’s\\t2015\\t\\npaper\\n\\tthat\\nuses\\ta\\tcombination\\tof\\ta\\tCNN\\tto\\tdetect\\tfaces\\tand\\ta\\trecurrent\\tneural\\tnetwork\\tto\\toutput\\ta\\tsequence\\tof\\tbounding\\tboxes\\taround\\tthem.\\tIn\\nimage\\tsegmentation,\\tthe\\tnet\\toutputs\\tan\\timage\\t(usually\\tof\\tthe\\tsame\\tsize\\tas\\tthe\\tinput)\\twhere\\teach\\tpixel\\tindicates\\tthe\\tclass\\tof\\tthe\\tobject\\tto\\nwhich\\tthe\\tcorresponding\\tinput\\tpixel\\tbelongs.\\tFor\\texample,\\tcheck\\tout\\tEvan\\tShelhamer\\tet\\tal.’s\\t2016\\t\\npaper\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 454}), Document(page_content='LeNet-5\\nThe\\t\\nLeNet-5\\tarchitecture\\tis\\tperhaps\\tthe\\tmost\\twidely\\tknown\\tCNN\\tarchitecture.\\tAs\\tmentioned\\tearlier,\\tit\\nwas\\tcreated\\tby\\tYann\\tLeCun\\tin\\t1998\\tand\\twidely\\tused\\tfor\\thandwritten\\tdigit\\trecognition\\t(MNIST).\\tIt\\tis\\ncomposed\\tof\\tthe\\tlayers\\tshown\\tin\\t\\nTable\\t13-1\\n.\\nTable\\t13-1.\\t\\nLeNet-5\\tarchitecture\\nLayer\\nType\\nMaps\\nSize\\nKernel\\tsize\\nStride\\nActivation\\nOut\\nFully\\tConnected\\n–\\n10\\n–\\n–\\nRBF\\nF6\\nFully\\tConnected\\n–\\n84\\n–\\n–\\ntanh\\nC5\\nConvolution\\n120\\n1\\t×\\t1\\n5\\t×\\t5\\n1\\ntanh\\nS4\\nAvg\\tPooling\\n16\\n5\\t×\\t5\\n2\\t×\\t2\\n2\\ntanh\\nC3\\nConvolution\\n16\\n10\\t×\\t10\\n5\\t×\\t5\\n1\\ntanh\\nS2\\nAvg\\tPooling\\n6\\n14\\t×\\t14\\n2\\t×\\t2\\n2\\ntanh\\nC1\\nConvolution\\n6\\n28\\t×\\t28\\n5\\t×\\t5\\n1\\ntanh\\nIn\\nInput\\n1\\n32\\t×\\t32\\n–\\n–\\n–\\nThere\\tare\\ta\\tfew\\textra\\tdetails\\tto\\tbe\\tnoted:\\nMNIST\\timages\\tare\\t28\\t×\\t28\\tpixels,\\tbut\\tthey\\tare\\tzero-padded\\tto\\t32\\t×\\t32\\tpixels\\tand\\tnormalized\\tbefore\\nbeing\\tfed\\tto\\tthe\\tnetwork.\\tThe\\trest\\tof\\tthe\\tnetwork\\tdoes\\tnot\\tuse\\tany\\tpadding,\\twhich\\tis\\twhy\\tthe\\tsize\\nkeeps\\tshrinking\\tas\\tthe\\timage\\tprogresses\\tthrough\\tthe\\tnetwork.\\nThe\\taverage\\tpooling\\tlayers\\tare\\tslightly\\tmore\\tcomplex\\tthan\\tusual:\\teach\\tneuron\\tcomputes\\tthe\\tmean\\tof\\nits\\tinputs,\\tthen\\tmultiplies\\tthe\\tresult\\tby\\ta\\tlearnable\\tcoefficient\\t(one\\tper\\tmap)\\tand\\tadds\\ta\\tlearnable\\nbias\\tterm\\t(again,\\tone\\tper\\tmap),\\tthen\\tfinally\\tapplies\\tthe\\tactivation\\tfunction.\\nMost\\tneurons\\tin\\tC3\\tmaps\\tare\\tconnected\\tto\\tneurons\\tin\\tonly\\tthree\\tor\\tfour\\tS2\\tmaps\\t(instead\\tof\\tall\\tsix\\nS2\\tmaps).\\tSee\\ttable\\t1\\tin\\tthe\\toriginal\\tpaper\\tfor\\tdetails.\\nThe\\toutput\\tlayer\\tis\\ta\\tbit\\tspecial:\\tinstead\\tof\\tcomputing\\tthe\\tdot\\tproduct\\tof\\tthe\\tinputs\\tand\\tthe\\tweight\\nvector,\\teach\\tneuron\\toutputs\\tthe\\tsquare\\tof\\tthe\\tEuclidian\\tdistance\\tbetween\\tits\\tinput\\tvector\\tand\\tits\\nweight\\tvector.\\tEach\\toutput\\tmeasures\\thow\\tmuch\\tthe\\timage\\tbelongs\\tto\\ta\\tparticular\\tdigit\\tclass.\\tThe\\ncross\\tentropy\\t\\ncost\\tfunction\\tis\\tnow\\tpreferred,\\tas\\tit\\tpenalizes\\tbad\\tpredictions\\tmuch\\tmore,\\tproducing\\nlarger\\tgradients\\tand\\tthus\\tconverging\\tfaster.\\nYann\\tLeCun’s\\t\\nwebsite\\n\\t(“LENET”\\tsection)\\tfeatures\\tgreat\\tdemos\\tof\\tLeNet-5\\tclassifying\\t\\ndigits.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 455}), Document(page_content='AlexNet\\nThe\\n\\t\\nAlexNet\\n\\tCNN\\tarchitecture\\n9\\n\\twon\\tthe\\t2012\\tImageNet\\tILSVRC\\tchallenge\\tby\\ta\\tlarge\\tmargin:\\tit\\tachieved\\n17%\\ttop-5\\terror\\trate\\twhile\\tthe\\tsecond\\tbest\\tachieved\\tonly\\t26%!\\tIt\\twas\\tdeveloped\\tby\\tAlex\\tKrizhevsky\\n(hence\\tthe\\tname),\\tIlya\\tSutskever,\\tand\\tGeoffrey\\tHinton.\\tIt\\tis\\tquite\\tsimilar\\tto\\tLeNet-5,\\tonly\\tmuch\\tlarger\\tand\\ndeeper,\\tand\\tit\\twas\\tthe\\tfirst\\tto\\tstack\\tconvolutional\\tlayers\\tdirectly\\ton\\ttop\\tof\\teach\\tother,\\tinstead\\tof\\tstacking\\ta\\npooling\\tlayer\\ton\\ttop\\tof\\teach\\tconvolutional\\tlayer.\\t\\nTable\\t13-2\\n\\tpresents\\tthis\\tarchitecture.\\nTable\\t13-2.\\t\\nAlexNet\\tarchitecture\\nLayer\\nType\\nMaps\\nSize\\nKernel\\tsize\\nStride\\nPadding\\nActivation\\nOut\\nFully\\tConnected\\n–\\n1,000\\n–\\n–\\n–\\nSoftmax\\nF9\\nFully\\tConnected\\n–\\n4,096\\n–\\n–\\n–\\nReLU\\nF8\\nFully\\tConnected\\n–\\n4,096\\n–\\n–\\n–\\nReLU\\nC7\\nConvolution\\n256\\n13\\t×\\t13\\n3\\t×\\t3\\n1\\nSAME\\nReLU\\nC6\\nConvolution\\n384\\n13\\t×\\t13\\n3\\t×\\t3\\n1\\nSAME\\nReLU\\nC5\\nConvolution\\n384\\n13\\t×\\t13\\n3\\t×\\t3\\n1\\nSAME\\nReLU\\nS4\\nMax\\tPooling\\n256\\n13\\t×\\t13\\n3\\t×\\t3\\n2\\nVALID\\n–\\nC3\\nConvolution\\n256\\n27\\t×\\t27\\n5\\t×\\t5\\n1\\nSAME\\nReLU\\nS2\\nMax\\tPooling\\n96\\n27\\t×\\t27\\n3\\t×\\t3\\n2\\nVALID\\n–\\nC1\\nConvolution\\n96\\n55\\t×\\t55\\n11\\t×\\t11\\n4\\nSAME\\nReLU\\nIn\\nInput\\n3\\t(RGB)\\n224\\t×\\t224\\n–\\n–\\n–\\n–\\nTo\\treduce\\toverfitting,\\tthe\\tauthors\\tused\\ttwo\\tregularization\\ttechniques\\twe\\tdiscussed\\tin\\tprevious\\tchapters:\\nfirst\\tthey\\tapplied\\tdropout\\t(with\\ta\\t50%\\tdropout\\trate)\\tduring\\ttraining\\tto\\tthe\\toutputs\\tof\\tlayers\\tF8\\tand\\tF9.\\nSecond,\\tthey\\tperformed\\tdata\\taugmentation\\tby\\trandomly\\tshifting\\tthe\\ttraining\\timages\\tby\\tvarious\\toffsets,\\nflipping\\tthem\\thorizontally,\\tand\\tchanging\\tthe\\tlighting\\tconditions.\\nAlexNet\\talso\\tuses\\ta\\tcompetitive\\tnormalization\\tstep\\timmediately\\tafter\\tthe\\tReLU\\tstep\\tof\\tlayers\\tC1\\tand\\tC3,\\ncalled\\t\\nlocal\\tresponse\\tnormalization\\n.\\tThis\\tform\\tof\\tnormalization\\tmakes\\tthe\\tneurons\\tthat\\tmost\\tstrongly\\nactivate\\tinhibit\\tneurons\\tat\\tthe\\tsame\\tlocation\\tbut\\tin\\tneighboring\\tfeature\\tmaps\\t(such\\tcompetitive\\tactivation\\nhas\\tbeen\\tobserved\\tin\\tbiological\\tneurons).\\tThis\\tencourages\\tdifferent\\tfeature\\tmaps\\tto\\tspecialize,\\tpushing\\nthem\\tapart\\tand\\tforcing\\tthem\\tto\\texplore\\ta\\twider\\trange\\tof\\tfeatures,\\tultimately\\timproving\\tgeneralization.\\nEquation\\t13-2\\n\\tshows\\thow\\tto\\tapply\\tLRN.\\nEquation\\t13-2.\\t\\nLocal\\tresponse\\tnormalization\\n', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 456}), Document(page_content='b\\ni\\n\\tis\\tthe\\tnormalized\\toutput\\tof\\tthe\\tneuron\\tlocated\\tin\\tfeature\\tmap\\t\\ni\\n,\\tat\\tsome\\trow\\t\\nu\\n\\tand\\tcolumn\\t\\nv\\n\\t(note\\nthat\\tin\\tthis\\tequation\\twe\\tconsider\\tonly\\tneurons\\tlocated\\tat\\tthis\\trow\\tand\\tcolumn,\\tso\\t\\nu\\n\\tand\\t\\nv\\n\\tare\\tnot\\nshown).\\na\\ni\\n\\tis\\tthe\\tactivation\\tof\\tthat\\tneuron\\tafter\\tthe\\tReLU\\tstep,\\tbut\\tbefore\\tnormalization.\\nk\\n,\\t\\nα\\n,\\t\\nβ\\n,\\tand\\t\\nr\\n\\tare\\thyperparameters.\\t\\nk\\n\\tis\\tcalled\\tthe\\t\\nbias\\n,\\tand\\t\\nr\\n\\tis\\tcalled\\t\\nthe\\t\\ndepth\\tradius\\n.\\nf\\nn\\n\\tis\\tthe\\tnumber\\tof\\tfeature\\tmaps.\\nFor\\texample,\\tif\\t\\nr\\n\\t=\\t2\\tand\\ta\\tneuron\\thas\\ta\\tstrong\\tactivation,\\tit\\twill\\tinhibit\\tthe\\tactivation\\tof\\tthe\\tneurons\\nlocated\\tin\\tthe\\tfeature\\tmaps\\timmediately\\tabove\\tand\\tbelow\\tits\\town.\\nIn\\tAlexNet,\\tthe\\thyperparameters\\tare\\tset\\tas\\tfollows:\\t\\nr\\n\\t=\\t2,\\t\\nα\\n\\t=\\t0.00002,\\t\\nβ\\n\\t=\\t0.75,\\tand\\t\\nk\\n\\t=\\t1.\\tThis\\tstep\\tcan\\nbe\\timplemented\\tusing\\tTensorFlow’s\\t\\ntf.nn.local_response_normalization()\\n\\toperation.\\nA\\tvariant\\tof\\tAlexNet\\tcalled\\t\\nZF\\tNet\\n\\twas\\tdeveloped\\tby\\tMatthew\\tZeiler\\tand\\tRob\\tFergus\\tand\\twon\\tthe\\t2013\\nILSVRC\\tchallenge.\\tIt\\tis\\tessentially\\tAlexNet\\twith\\ta\\tfew\\ttweaked\\t\\nhyperparameters\\t(number\\tof\\tfeature\\nmaps,\\tkernel\\tsize,\\tstride,\\tetc.).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 457}), Document(page_content='GoogLeNet\\nThe\\t\\nGoogLeNet\\tarchitecture\\n\\twas\\tdeveloped\\tby\\tChristian\\tSzegedy\\tet\\tal.\\tfrom\\tGoogle\\tResearch,\\n10\\n\\tand\\tit\\nwon\\tthe\\tILSVRC\\t2014\\tchallenge\\tby\\tpushing\\tthe\\ttop-5\\terror\\trate\\tbelow\\t7%.\\tThis\\tgreat\\tperformance\\tcame\\nin\\tlarge\\tpart\\tfrom\\tthe\\tfact\\tthat\\tthe\\tnetwork\\twas\\tmuch\\tdeeper\\tthan\\tprevious\\tCNNs\\t(see\\t\\nFigure\\t13-11\\n).\\tThis\\nwas\\tmade\\tpossible\\tby\\tsub-networks\\tcalled\\t\\ninception\\tmodules\\n,\\n11\\n\\twhich\\tallow\\tGoogLeNet\\tto\\tuse\\nparameters\\tmuch\\tmore\\tefficiently\\tthan\\tprevious\\tarchitectures:\\tGoogLeNet\\tactually\\thas\\t10\\ttimes\\tfewer\\nparameters\\tthan\\tAlexNet\\t(roughly\\t6\\tmillion\\tinstead\\tof\\t60\\tmillion).\\nFigure\\t13-10\\n\\tshows\\tthe\\tarchitecture\\tof\\tan\\t\\ninception\\tmodule.\\tThe\\tnotation\\t“3\\t×\\t3\\t+\\t2(S)”\\tmeans\\tthat\\tthe\\nlayer\\tuses\\ta\\t3\\t×\\t3\\tkernel,\\tstride\\t2,\\tand\\tSAME\\tpadding.\\tThe\\tinput\\tsignal\\tis\\tfirst\\tcopied\\tand\\tfed\\tto\\tfour\\ndifferent\\tlayers.\\tAll\\tconvolutional\\tlayers\\tuse\\tthe\\tReLU\\tactivation\\tfunction.\\tNote\\tthat\\tthe\\tsecond\\tset\\tof\\nconvolutional\\tlayers\\tuses\\tdifferent\\tkernel\\tsizes\\t(1\\t×\\t1,\\t3\\t×\\t3,\\tand\\t5\\t×\\t5),\\tallowing\\tthem\\tto\\tcapture\\npatterns\\tat\\tdifferent\\tscales.\\tAlso\\tnote\\tthat\\tevery\\tsingle\\tlayer\\tuses\\ta\\tstride\\tof\\t1\\tand\\tSAME\\tpadding\\t(even\\nthe\\tmax\\tpooling\\tlayer),\\tso\\ttheir\\toutputs\\tall\\thave\\tthe\\tsame\\theight\\tand\\twidth\\tas\\ttheir\\tinputs.\\tThis\\tmakes\\tit\\npossible\\tto\\tconcatenate\\tall\\tthe\\toutputs\\talong\\tthe\\tdepth\\tdimension\\tin\\tthe\\t\\nfinal\\t\\ndepth\\tconcat\\tlayer\\n\\t\\n(i.e.,\\nstack\\tthe\\tfeature\\tmaps\\tfrom\\tall\\tfour\\ttop\\tconvolutional\\tlayers).\\tThis\\tconcatenation\\tlayer\\tcan\\tbe\\nimplemented\\tin\\tTensorFlow\\tusing\\tthe\\t\\ntf.concat()\\n\\t\\noperation,\\twith\\t\\naxis=3\\n\\t(axis\\t3\\tis\\tthe\\tdepth).\\nFigure\\t13-10.\\t\\nInception\\tmodule\\nYou\\tmay\\twonder\\twhy\\tinception\\tmodules\\thave\\tconvolutional\\tlayers\\twith\\t1\\t×\\t1\\tkernels.\\tSurely\\tthese\\tlayers\\ncannot\\tcapture\\tany\\tfeatures\\tsince\\tthey\\tlook\\tat\\tonly\\tone\\tpixel\\tat\\ta\\ttime?\\tIn\\tfact,\\tthese\\tlayers\\tserve\\ttwo\\npurposes:\\nFirst,\\tthey\\tare\\tconfigured\\tto\\toutput\\tmany\\tfewer\\tfeature\\tmaps\\tthan\\ttheir\\tinputs,\\tso\\tthey\\tserve\\t\\nas\\nbottleneck\\tlayers\\n,\\tmeaning\\tthey\\treduce\\tdimensionality.\\tThis\\tis\\tparticularly\\tuseful\\tbefore\\tthe\\t3\\t×\\t3\\nand\\t5\\t×\\t5\\tconvolutions,\\tsince\\tthese\\tare\\tvery\\tcomputationally\\texpensive\\tlayers.\\nSecond,\\teach\\tpair\\tof\\t\\nconvolutional\\tlayers\\t([1\\t×\\t1,\\t3\\t×\\t3]\\tand\\t[1\\t×\\t1,\\t5\\t×\\t5])\\tacts\\tlike\\ta\\tsingle,', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 458}), Document(page_content='powerful\\tconvolutional\\tlayer,\\tcapable\\tof\\tcapturing\\tmore\\tcomplex\\tpatterns.\\tIndeed,\\tinstead\\tof\\nsweeping\\ta\\tsimple\\tlinear\\tclassifier\\tacross\\tthe\\timage\\t(as\\ta\\tsingle\\tconvolutional\\tlayer\\tdoes),\\tthis\\tpair\\nof\\tconvolutional\\tlayers\\tsweeps\\ta\\ttwo-layer\\tneural\\tnetwork\\tacross\\tthe\\timage.\\nIn\\tshort,\\tyou\\tcan\\tthink\\tof\\tthe\\twhole\\tinception\\tmodule\\tas\\ta\\tconvolutional\\tlayer\\ton\\tsteroids,\\table\\tto\\toutput\\nfeature\\tmaps\\tthat\\tcapture\\tcomplex\\tpatterns\\tat\\tvarious\\tscales.\\nWARNING\\nThe\\tnumber\\tof\\t\\nconvolutional\\tkernels\\tfor\\teach\\tconvolutional\\tlayer\\tis\\ta\\thyperparameter.\\tUnfortunately,\\tthis\\tmeans\\tthat\\tyou\\thave\\nsix\\tmore\\thyperparameters\\tto\\ttweak\\tfor\\tevery\\tinception\\tlayer\\tyou\\tadd.\\nNow\\tlet’s\\tlook\\tat\\tthe\\tarchitecture\\tof\\tthe\\tGoogLeNet\\tCNN\\t(see\\t\\nFigure\\t13-11\\n).\\tIt\\tis\\tso\\tdeep\\tthat\\twe\\thad\\tto\\nrepresent\\tit\\tin\\tthree\\tcolumns,\\tbut\\tGoogLeNet\\tis\\tactually\\tone\\ttall\\tstack,\\tincluding\\tnine\\tinception\\tmodules\\n(the\\tboxes\\twith\\tthe\\tspinning\\ttops)\\tthat\\tactually\\tcontain\\tthree\\tlayers\\teach.\\tThe\\tnumber\\tof\\tfeature\\tmaps\\noutput\\tby\\teach\\tconvolutional\\tlayer\\tand\\teach\\tpooling\\tlayer\\tis\\tshown\\tbefore\\tthe\\tkernel\\tsize.\\tThe\\tsix\\nnumbers\\tin\\tthe\\tinception\\tmodules\\trepresent\\tthe\\tnumber\\tof\\tfeature\\tmaps\\toutput\\tby\\teach\\tconvolutional\\tlayer\\nin\\tthe\\tmodule\\t(in\\tthe\\tsame\\torder\\tas\\tin\\t\\nFigure\\t13-10\\n).\\tNote\\tthat\\tall\\tthe\\tconvolutional\\tlayers\\tuse\\tthe\\tReLU\\nactivation\\tfunction.\\nFigure\\t13-11.\\t\\nGoogLeNet\\tarchitecture', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 459}), Document(page_content='Let’s\\tgo\\tthrough\\tthis\\tnetwork:\\nThe\\tfirst\\ttwo\\tlayers\\tdivide\\tthe\\timage’s\\theight\\tand\\twidth\\tby\\t4\\t(so\\tits\\tarea\\tis\\tdivided\\tby\\t16),\\tto\\treduce\\nthe\\tcomputational\\tload.\\nThen\\tthe\\tlocal\\tresponse\\tnormalization\\tlayer\\tensures\\tthat\\tthe\\tprevious\\tlayers\\tlearn\\ta\\twide\\tvariety\\tof\\nfeatures\\t(as\\tdiscussed\\tearlier).\\nTwo\\tconvolutional\\tlayers\\tfollow,\\twhere\\tthe\\tfirst\\tacts\\tlike\\ta\\t\\nbottleneck\\tlayer\\n.\\tAs\\texplained\\tearlier,\\nyou\\tcan\\tthink\\tof\\tthis\\tpair\\tas\\ta\\tsingle\\tsmarter\\tconvolutional\\tlayer.\\nAgain,\\ta\\tlocal\\tresponse\\tnormalization\\tlayer\\tensures\\tthat\\tthe\\tprevious\\tlayers\\tcapture\\ta\\twide\\tvariety\\nof\\tpatterns.\\nNext\\ta\\tmax\\tpooling\\tlayer\\treduces\\tthe\\timage\\theight\\tand\\twidth\\tby\\t2,\\tagain\\tto\\tspeed\\tup\\tcomputations.\\nThen\\tcomes\\tthe\\ttall\\tstack\\tof\\tnine\\tinception\\tmodules,\\tinterleaved\\twith\\ta\\tcouple\\tmax\\tpooling\\tlayers\\tto\\nreduce\\tdimensionality\\tand\\tspeed\\tup\\tthe\\tnet.\\nNext,\\tthe\\taverage\\tpooling\\tlayer\\tuses\\ta\\tkernel\\tthe\\tsize\\tof\\tthe\\tfeature\\tmaps\\twith\\tVALID\\tpadding,\\noutputting\\t1\\t×\\t1\\tfeature\\tmaps:\\tthis\\tsurprising\\tstrategy\\tis\\t\\ncalled\\t\\nglobal\\taverage\\tpooling\\n.\\tIt\\teffectively\\nforces\\tthe\\tprevious\\tlayers\\tto\\tproduce\\tfeature\\tmaps\\tthat\\tare\\tactually\\tconfidence\\tmaps\\tfor\\teach\\ttarget\\nclass\\t(since\\tother\\tkinds\\tof\\tfeatures\\twould\\tbe\\tdestroyed\\tby\\tthe\\taveraging\\tstep).\\tThis\\tmakes\\tit\\nunnecessary\\tto\\thave\\tseveral\\tfully\\tconnected\\tlayers\\tat\\tthe\\ttop\\tof\\tthe\\tCNN\\t(like\\tin\\tAlexNet),\\nconsiderably\\treducing\\tthe\\tnumber\\tof\\tparameters\\tin\\tthe\\tnetwork\\tand\\tlimiting\\tthe\\trisk\\tof\\toverfitting.\\nThe\\tlast\\tlayers\\tare\\tself-explanatory:\\tdropout\\tfor\\tregularization,\\tthen\\ta\\tfully\\tconnected\\tlayer\\twith\\ta\\nsoftmax\\tactivation\\tfunction\\tto\\toutput\\testimated\\tclass\\tprobabilities.\\nThis\\tdiagram\\tis\\tslightly\\tsimplified:\\tthe\\toriginal\\tGoogLeNet\\tarchitecture\\talso\\tincluded\\ttwo\\tauxiliary\\nclassifiers\\tplugged\\ton\\ttop\\tof\\tthe\\tthird\\tand\\tsixth\\tinception\\tmodules.\\tThey\\twere\\tboth\\tcomposed\\tof\\tone\\naverage\\tpooling\\tlayer,\\tone\\tconvolutional\\tlayer,\\ttwo\\tfully\\tconnected\\tlayers,\\tand\\ta\\tsoftmax\\tactivation\\tlayer.\\nDuring\\ttraining,\\ttheir\\tloss\\t(scaled\\tdown\\tby\\t70%)\\twas\\tadded\\tto\\tthe\\toverall\\tloss.\\tThe\\tgoal\\twas\\tto\\tfight\\tthe\\nvanishing\\tgradients\\tproblem\\tand\\tregularize\\tthe\\tnetwork.\\tHowever,\\tit\\twas\\tshown\\tthat\\ttheir\\teffect\\twas\\nrelatively\\t\\nminor.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 460}), Document(page_content='ResNet\\nLast\\t\\nbut\\tnot\\tleast,\\tthe\\twinner\\tof\\tthe\\tILSVRC\\t2015\\tchallenge\\twas\\tthe\\t\\nResidual\\tNetwork\\n\\t(or\\t\\nResNet\\n),\\ndeveloped\\tby\\tKaiming\\tHe\\tet\\tal.,\\n12\\n\\twhich\\tdelivered\\tan\\tastounding\\ttop-5\\terror\\trate\\tunder\\t3.6%,\\tusing\\tan\\nextremely\\tdeep\\tCNN\\tcomposed\\tof\\t152\\tlayers.\\tThe\\tkey\\tto\\tbeing\\table\\tto\\ttrain\\tsuch\\ta\\tdeep\\tnetwork\\tis\\tto\\t\\nuse\\nskip\\tconnections\\n\\t(also\\tcalled\\t\\nshortcut\\tconnections\\n):\\tthe\\tsignal\\tfeeding\\tinto\\ta\\tlayer\\tis\\talso\\tadded\\tto\\tthe\\noutput\\tof\\ta\\tlayer\\tlocated\\ta\\tbit\\thigher\\tup\\tthe\\tstack.\\tLet’s\\tsee\\twhy\\tthis\\tis\\tuseful.\\nWhen\\ttraining\\ta\\tneural\\tnetwork,\\tthe\\tgoal\\tis\\tto\\tmake\\tit\\tmodel\\ta\\ttarget\\tfunction\\t\\nh\\n(\\nx\\n).\\tIf\\tyou\\tadd\\tthe\\tinput\\t\\nx\\nto\\tthe\\toutput\\tof\\tthe\\tnetwork\\t(i.e.,\\tyou\\tadd\\ta\\tskip\\tconnection),\\tthen\\tthe\\tnetwork\\twill\\tbe\\tforced\\tto\\tmodel\\nf\\n(\\nx\\n)\\t=\\t\\nh\\n(\\nx\\n)\\t–\\t\\nx\\n\\trather\\tthan\\t\\nh\\n(\\nx\\n).\\tThis\\tis\\tcalled\\t\\nresidual\\tlearning\\n\\t\\n(see\\t\\nFigure\\t13-12\\n).\\nFigure\\t13-12.\\t\\nResidual\\tlearning\\nWhen\\tyou\\tinitialize\\ta\\tregular\\tneural\\tnetwork,\\tits\\tweights\\tare\\tclose\\tto\\tzero,\\tso\\tthe\\tnetwork\\tjust\\toutputs\\nvalues\\tclose\\tto\\tzero.\\tIf\\tyou\\tadd\\ta\\tskip\\tconnection,\\tthe\\tresulting\\tnetwork\\tjust\\toutputs\\ta\\tcopy\\tof\\tits\\tinputs;\\tin\\nother\\twords,\\tit\\tinitially\\tmodels\\tthe\\tidentity\\tfunction.\\tIf\\tthe\\ttarget\\tfunction\\tis\\tfairly\\tclose\\tto\\tthe\\tidentity\\nfunction\\t(which\\tis\\toften\\tthe\\tcase),\\tthis\\twill\\tspeed\\tup\\ttraining\\tconsiderably.\\nMoreover,\\tif\\tyou\\tadd\\tmany\\tskip\\tconnections,\\tthe\\tnetwork\\tcan\\tstart\\tmaking\\tprogress\\teven\\tif\\tseveral\\tlayers\\nhave\\tnot\\tstarted\\tlearning\\tyet\\t(see\\t\\nFigure\\t13-13\\n).\\tThanks\\tto\\tskip\\tconnections,\\tthe\\tsignal\\tcan\\teasily\\tmake\\tits\\nway\\tacross\\tthe\\twhole\\tnetwork.\\tThe\\tdeep\\tresidual\\tnetwork\\tcan\\tbe\\tseen\\tas\\ta\\tstack\\tof\\t\\nresidual\\tunits\\n,\\t\\nwhere\\neach\\tresidual\\tunit\\tis\\ta\\tsmall\\tneural\\tnetwork\\twith\\ta\\tskip\\tconnection.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 461}), Document(page_content='Figure\\t13-13.\\t\\nRegular\\tdeep\\tneural\\tnetwork\\t(left)\\tand\\tdeep\\tresidual\\tnetwork\\t(right)\\nNow\\tlet’s\\tlook\\tat\\tResNet’s\\tarchitecture\\t(see\\t\\nFigure\\t13-14\\n).\\tIt\\tis\\tactually\\tsurprisingly\\tsimple.\\tIt\\tstarts\\tand\\nends\\texactly\\tlike\\tGoogLeNet\\t(except\\twithout\\ta\\tdropout\\tlayer),\\tand\\tin\\tbetween\\tis\\tjust\\ta\\tvery\\tdeep\\tstack\\tof\\nsimple\\tresidual\\tunits.\\tEach\\tresidual\\tunit\\tis\\tcomposed\\tof\\ttwo\\tconvolutional\\tlayers,\\twith\\t\\nBatch\\nNormalization\\t(BN)\\tand\\tReLU\\tactivation,\\tusing\\t3\\t×\\t3\\tkernels\\tand\\tpreserving\\tspatial\\tdimensions\\t(stride\\t1,\\nSAME\\tpadding).\\nFigure\\t13-14.\\t\\nResNet\\tarchitecture\\nNote\\tthat\\tthe\\tnumber\\tof\\t\\nfeature\\tmaps\\tis\\tdoubled\\tevery\\tfew\\tresidual\\tunits,\\tat\\tthe\\tsame\\ttime\\tas\\ttheir\\theight\\nand\\twidth\\tare\\thalved\\t(using\\ta\\tconvolutional\\tlayer\\twith\\tstride\\t2).\\tWhen\\tthis\\thappens\\tthe\\tinputs\\tcannot\\tbe\\nadded\\tdirectly\\tto\\tthe\\toutputs\\tof\\tthe\\tresidual\\tunit\\tsince\\tthey\\tdon’t\\thave\\tthe\\tsame\\tshape\\t(for\\texample,\\tthis', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 462}), Document(page_content='problem\\taffects\\tthe\\tskip\\tconnection\\trepresented\\tby\\tthe\\tdashed\\tarrow\\tin\\t\\nFigure\\t13-14\\n).\\tTo\\tsolve\\tthis\\nproblem,\\tthe\\tinputs\\tare\\tpassed\\tthrough\\ta\\t1\\t×\\t1\\tconvolutional\\tlayer\\twith\\tstride\\t2\\tand\\tthe\\tright\\tnumber\\tof\\noutput\\tfeature\\tmaps\\t(see\\t\\nFigure\\t13-15\\n).\\nFigure\\t13-15.\\t\\nSkip\\tconnection\\twhen\\tchanging\\tfeature\\tmap\\tsize\\tand\\tdepth\\nResNet-34\\tis\\tthe\\tResNet\\twith\\t34\\tlayers\\t(only\\tcounting\\tthe\\tconvolutional\\tlayers\\tand\\tthe\\tfully\\tconnected\\nlayer)\\tcontaining\\tthree\\tresidual\\tunits\\tthat\\toutput\\t64\\tfeature\\tmaps,\\t4\\tRUs\\twith\\t128\\tmaps,\\t6\\tRUs\\twith\\t256\\nmaps,\\tand\\t3\\tRUs\\twith\\t512\\tmaps.\\nResNets\\tdeeper\\tthan\\tthat,\\tsuch\\tas\\tResNet-152,\\tuse\\tslightly\\tdifferent\\tresidual\\tunits.\\tInstead\\tof\\ttwo\\t3\\t×\\t3\\nconvolutional\\tlayers\\twith\\t(say)\\t256\\tfeature\\tmaps,\\tthey\\tuse\\tthree\\tconvolutional\\tlayers:\\tfirst\\ta\\t1\\t×\\t1\\nconvolutional\\tlayer\\twith\\tjust\\t64\\tfeature\\tmaps\\t(4\\ttimes\\tless),\\twhich\\tacts\\ta\\ta\\tbottleneck\\tlayer\\t(as\\tdiscussed\\nalready),\\tthen\\ta\\t3\\t×\\t3\\tlayer\\twith\\t64\\tfeature\\tmaps,\\tand\\tfinally\\tanother\\t1\\t×\\t1\\tconvolutional\\tlayer\\twith\\t256\\nfeature\\tmaps\\t(4\\ttimes\\t64)\\tthat\\trestores\\tthe\\toriginal\\tdepth.\\tResNet-152\\tcontains\\tthree\\tsuch\\tRUs\\tthat\\toutput\\n256\\tmaps,\\tthen\\t8\\tRUs\\twith\\t512\\tmaps,\\ta\\twhopping\\t36\\tRUs\\twith\\t1,024\\tmaps,\\tand\\tfinally\\t3\\tRUs\\twith\\t2,048\\nmaps.\\nAs\\tyou\\tcan\\tsee,\\tthe\\tfield\\tis\\tmoving\\trapidly,\\twith\\tall\\tsorts\\tof\\tarchitectures\\tpopping\\tout\\tevery\\tyear.\\tOne\\nclear\\ttrend\\tis\\tthat\\tCNNs\\tkeep\\tgetting\\tdeeper\\tand\\tdeeper.\\tThey\\tare\\talso\\tgetting\\tlighter,\\trequiring\\tfewer\\tand\\nfewer\\tparameters.\\tAt\\tpresent,\\tthe\\tResNet\\tarchitecture\\tis\\tboth\\tthe\\tmost\\tpowerful\\tand\\targuably\\tthe\\nsimplest,\\tso\\tit\\tis\\treally\\tthe\\tone\\tyou\\tshould\\tprobably\\tuse\\tfor\\tnow,\\tbut\\tkeep\\tlooking\\tat\\tthe\\tILSVRC\\nchallenge\\tevery\\tyear.\\tThe\\t2016\\twinners\\twere\\tthe\\tTrimps-Soushen\\tteam\\tfrom\\tChina\\twith\\tan\\tastounding\\n2.99%\\terror\\trate.\\tTo\\tachieve\\tthis\\tthey\\ttrained\\tcombinations\\tof\\tthe\\tprevious\\tmodels\\tand\\tjoined\\tthem\\tinto\\nan\\tensemble.\\tDepending\\ton\\tthe\\ttask,\\tthe\\treduced\\terror\\trate\\tmay\\tor\\tmay\\tnot\\tbe\\tworth\\tthe\\textra\\tc\\nomplexity.\\nThere\\tare\\ta\\tfew\\tother\\tarchitectures\\tthat\\tyou\\tmay\\twant\\tto\\tlook\\tat,\\tin\\tparticular\\n\\t\\nVGGNet\\n13\\n\\t(runner-up\\tof\\tthe\\nILSVRC\\t2014\\tchallenge)\\tand\\t\\nInception-v4\\n14\\n\\t(which\\tmerges\\t\\nthe\\tideas\\tof\\tGoogLeNet\\tand\\tResNet\\tand\\nachieves\\tclose\\tto\\t3%\\ttop-5\\terror\\trate\\ton\\tImageNet\\tclassification).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 463}), Document(page_content='NOTE\\nThere\\tis\\treally\\tnothing\\tspecial\\tabout\\timplementing\\tthe\\tvarious\\tCNN\\tarchitectures\\twe\\tjust\\tdiscussed.\\tWe\\tsaw\\tearlier\\thow\\tto\\tbuild\\nall\\tthe\\tindividual\\tbuilding\\tblocks,\\tso\\tnow\\tall\\tyou\\tneed\\tis\\tto\\tassemble\\tthem\\tto\\tcreate\\tthe\\tdesired\\tarchitecture.\\tWe\\twill\\tbuild\\ta\\ncomplete\\tCNN\\tin\\tthe\\tupcoming\\texercises\\tand\\tyou\\twill\\tfind\\tfull\\tworking\\tcode\\tin\\tthe\\tJupyter\\tnotebooks.\\nTENSORFLOW\\tCONVOLUTION\\tOPERATIONS\\nTensorFlow\\talso\\t\\noffers\\ta\\tfew\\tother\\tkinds\\tof\\tconvolutional\\tlayers:\\ntf.layers.conv1d()\\n\\t\\ncreates\\ta\\tconvolutional\\tlayer\\tfor\\t1D\\tinputs.\\tThis\\tis\\tuseful,\\tfor\\texample,\\tin\\tnatural\\tlanguage\\tprocessing,\\nwhere\\ta\\tsentence\\tmay\\tbe\\trepresented\\tas\\ta\\t1D\\tarray\\tof\\twords,\\tand\\tthe\\treceptive\\tfield\\tcovers\\ta\\tfew\\tneighboring\\twords.\\ntf.layers.conv3d()\\n\\t\\ncreates\\ta\\tconvolutional\\tlayer\\tfor\\t3D\\tinputs,\\tsuch\\tas\\t3D\\tPET\\tscan.\\ntf.nn.atrous_conv2d()\\n\\t\\ncreates\\tan\\t\\natrous\\tconvolutional\\tlayer\\n\\t(“à\\ttrous”\\tis\\tFrench\\tfor\\t“with\\tholes”).\\tThis\\tis\\tequivalent\\tto\\tusing\\na\\tregular\\tconvolutional\\tlayer\\twith\\ta\\tfilter\\tdilated\\tby\\tinserting\\trows\\tand\\tcolumns\\tof\\tzeros\\t(i.e.,\\tholes).\\tFor\\texample,\\ta\\t1\\t×\\t3\\tfilter\\nequal\\tto\\t\\n[[1,2,3]]\\n\\tmay\\tbe\\tdilated\\twith\\ta\\t\\ndilation\\trate\\n\\tof\\t4,\\tresulting\\tin\\ta\\t\\ndilated\\tfilter\\n\\t\\n[[1,\\t0,\\t0,\\t0,\\t2,\\t0,\\t0,\\t0,\\t3]]\\n.\\tThis\\nallows\\tthe\\tconvolutional\\tlayer\\tto\\thave\\ta\\tlarger\\treceptive\\tfield\\tat\\tno\\tcomputational\\tprice\\tand\\tusing\\tno\\textra\\tparameters.\\ntf.layers.conv2d_transpose()\\n\\t\\ncreates\\ta\\t\\ntranspose\\tconvolutional\\tlayer\\n,\\tsometimes\\t\\ncalled\\ta\\t\\ndeconvolutional\\tlayer\\n,\\n15\\n\\twhich\\nupsamples\\n\\tan\\timage.\\tIt\\tdoes\\tso\\tby\\tinserting\\tzeros\\tbetween\\tthe\\tinputs,\\tso\\tyou\\tcan\\tthink\\tof\\tthis\\tas\\ta\\tregular\\tconvolutional\\tlayer\\nusing\\ta\\tfractional\\tstride.\\tUpsampling\\t\\nis\\tuseful,\\tfor\\texample,\\tin\\timage\\tsegmentation:\\tin\\ta\\ttypical\\tCNN,\\tfeature\\tmaps\\tget\\tsmaller\\tand\\nsmaller\\tas\\tyou\\tprogress\\tthrough\\tthe\\tnetwork,\\tso\\tif\\tyou\\twant\\tto\\toutput\\tan\\timage\\tof\\tthe\\tsame\\tsize\\tas\\tthe\\tinput,\\tyou\\tneed\\tan\\nupsampling\\tlayer.\\ntf.nn.depthwise_conv2d()\\n\\t\\ncreates\\ta\\t\\ndepthwise\\tconvolutional\\tlayer\\n\\tthat\\tapplies\\tevery\\tfilter\\tto\\tevery\\tindividual\\tinput\\tchannel\\nindependently.\\tThus,\\tif\\tthere\\tare\\t\\nf\\nn\\n\\tfilters\\tand\\t\\nf\\nn\\n′\\n\\t\\ninput\\tchannels,\\tthen\\tthis\\twill\\toutput\\t\\nf\\nn\\n\\t×\\t\\nf\\nn\\n′\\n\\tfeature\\tmaps.\\ntf.layers.separable_conv2d()\\n\\t\\ncreates\\ta\\t\\nseparable\\tconvolutional\\tlayer\\n\\tthat\\tfirst\\tacts\\tlike\\ta\\tdepthwise\\tconvolutional\\tlayer,\\nthen\\tapplies\\ta\\t1\\t×\\t1\\tconvolutional\\tlayer\\tto\\tthe\\tresulting\\tfeature\\tmaps.\\tThis\\tmakes\\tit\\tpossible\\tto\\tapply\\tfilters\\tto\\tarbitrary\\tsets\\tof\\ninputs\\t\\nchannels.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 464}), Document(page_content='Exercises\\n1\\n.\\t\\nWhat\\tare\\tthe\\tadvantages\\tof\\ta\\tCNN\\tover\\ta\\tfully\\tconnected\\tDNN\\tfor\\timage\\tclassification?\\n2\\n.\\t\\nConsider\\ta\\tCNN\\tcomposed\\tof\\tthree\\tconvolutional\\tlayers,\\teach\\twith\\t3\\t×\\t3\\tkernels,\\ta\\tstride\\tof\\t2,\\tand\\nSAME\\tpadding.\\tThe\\tlowest\\tlayer\\toutputs\\t100\\tfeature\\tmaps,\\tthe\\tmiddle\\tone\\toutputs\\t200,\\tand\\tthe\\ttop\\none\\toutputs\\t400.\\tThe\\tinput\\timages\\tare\\tRGB\\timages\\tof\\t200\\t×\\t300\\tpixels.\\tWhat\\tis\\tthe\\ttotal\\tnumber\\tof\\nparameters\\tin\\tthe\\tCNN?\\tIf\\twe\\tare\\tusing\\t32-bit\\tfloats,\\tat\\tleast\\thow\\tmuch\\tRAM\\twill\\tthis\\tnetwork\\nrequire\\twhen\\tmaking\\ta\\tprediction\\tfor\\ta\\tsingle\\tinstance?\\tWhat\\tabout\\twhen\\ttraining\\ton\\ta\\tmini-batch\\tof\\n50\\timages?\\n3\\n.\\t\\nIf\\tyour\\tGPU\\truns\\tout\\tof\\tmemory\\twhile\\ttraining\\ta\\tCNN,\\twhat\\tare\\tfive\\tthings\\tyou\\tcould\\ttry\\tto\\tsolve\\tthe\\nproblem?\\n4\\n.\\t\\nWhy\\twould\\tyou\\twant\\tto\\tadd\\ta\\tmax\\tpooling\\tlayer\\trather\\tthan\\ta\\tconvolutional\\tlayer\\twith\\tthe\\tsame\\nstride?\\n5\\n.\\t\\nWhen\\twould\\tyou\\twant\\tto\\tadd\\ta\\t\\nlocal\\tresponse\\tnormalization\\n\\tlayer?\\n6\\n.\\t\\nCan\\tyou\\tname\\tthe\\tmain\\tinnovations\\tin\\tAlexNet,\\tcompared\\tto\\tLeNet-5?\\tWhat\\tabout\\tthe\\tmain\\ninnovations\\tin\\tGoogLeNet\\tand\\tResNet?\\n7\\n.\\t\\nBuild\\tyour\\town\\tCNN\\tand\\ttry\\tto\\tachieve\\tthe\\thighest\\tpossible\\taccuracy\\ton\\tMNIST.\\n8\\n.\\t\\nClassifying\\tlarge\\timages\\tusing\\tInception\\tv3.\\na\\n.\\t\\nDownload\\tsome\\timages\\tof\\tvarious\\tanimals.\\tLoad\\tthem\\tin\\tPython,\\tfor\\texample\\tusing\\tthe\\nmatplotlib.image.mpimg.imread()\\n\\tfunction\\tor\\tthe\\t\\nscipy.misc.imread()\\n\\tfunction.\\tResize\\nand/or\\tcrop\\tthem\\tto\\t299\\t×\\t299\\tpixels,\\tand\\tensure\\tthat\\tthey\\thave\\tjust\\tthree\\tchannels\\t(RGB),\\twith\\nno\\ttransparency\\tchannel.\\nb\\n.\\t\\nDownload\\tthe\\tlatest\\tpretrained\\tInception\\tv3\\tmodel:\\tthe\\tcheckpoint\\tis\\tavailable\\tat\\nhttps://goo.gl/nxSQvl\\n.\\nc\\n.\\t\\nCreate\\tthe\\tInception\\tv3\\tmodel\\tby\\tcalling\\tthe\\t\\ninception_v3()\\n\\tfunction,\\tas\\tshown\\tbelow.\\tThis\\nmust\\tbe\\tdone\\twithin\\tan\\targument\\tscope\\tcreated\\tby\\tthe\\t\\ninception_v3_arg_scope()\\n\\tfunction.\\nAlso,\\tyou\\tmust\\tset\\t\\nis_training=False\\n\\tand\\t\\nnum_classes=1001\\n\\t\\nlike\\tso:\\nfrom\\n\\t\\ntensorflow.contrib.slim.nets\\n\\t\\nimport\\n\\t\\ninception\\nimport\\n\\t\\ntensorflow.contrib.slim\\n\\t\\nas\\n\\t\\nslim\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\n299\\n,\\n\\t\\n299\\n,\\n\\t\\n3\\n],\\n\\t\\nname\\n=\\n\"X\"\\n)\\nwith\\n\\t\\nslim\\n.\\narg_scope\\n(\\ninception\\n.\\ninception_v3_arg_scope\\n()):\\n\\t\\t\\t\\t\\nlogits\\n,\\n\\t\\nend_points\\n\\t\\n=\\n\\t\\ninception\\n.\\ninception_v3\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nX\\n,\\n\\t\\nnum_classes\\n=\\n1001\\n,\\n\\t\\nis_training\\n=\\nFalse\\n)\\npredictions\\n\\t\\n=\\n\\t\\nend_points\\n[\\n\"Predictions\"\\n]\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\nd\\n.\\t\\nOpen\\ta\\tsession\\tand\\tuse\\tthe\\t\\nSaver\\n\\tto\\trestore\\tthe\\tpretrained\\tmodel\\tcheckpoint\\tyou\\tdownloaded\\nearlier.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 465}), Document(page_content='e\\n.\\t\\nRun\\tthe\\tmodel\\tto\\tclassify\\tthe\\timages\\tyou\\tprepared.\\tDisplay\\tthe\\ttop\\tfive\\tpredictions\\tfor\\teach\\nimage,\\talong\\twith\\tthe\\testimated\\tprobability\\t(the\\tlist\\tof\\tclass\\tnames\\tis\\tavailable\\tat\\nhttps://goo.gl/brXRtZ\\n).\\tHow\\taccurate\\tis\\tthe\\tmodel?\\n9\\n.\\t\\nTransfer\\tlearning\\tfor\\tlarge\\timage\\tclassification.\\na\\n.\\t\\nCreate\\ta\\ttraining\\tset\\tcontaining\\tat\\tleast\\t100\\timages\\tper\\tclass.\\tFor\\texample,\\tyou\\tcould\\tclassify\\nyour\\town\\tpictures\\tbased\\ton\\tthe\\tlocation\\t(beach,\\tmountain,\\tcity,\\tetc.),\\tor\\talternatively\\tyou\\tcan\\njust\\tuse\\tan\\texisting\\tdataset,\\tsuch\\tas\\tthe\\t\\nflowers\\tdataset\\n\\tor\\tMIT’s\\t\\nplaces\\tdataset\\n\\t(requires\\nregistration,\\tand\\tit\\tis\\thuge).\\nb\\n.\\t\\nWrite\\ta\\tpreprocessing\\tstep\\tthat\\twill\\tresize\\tand\\tcrop\\tthe\\timage\\tto\\t299\\t×\\t299,\\twith\\tsome\\nrandomness\\tfor\\tdata\\taugmentation.\\nc\\n.\\t\\nUsing\\tthe\\tpretrained\\tInception\\tv3\\tmodel\\tfrom\\tthe\\tprevious\\texercise,\\tfreeze\\tall\\tlayers\\tup\\tto\\tthe\\nbottleneck\\tlayer\\t(i.e.,\\tthe\\tlast\\tlayer\\tbefore\\tthe\\toutput\\tlayer),\\tand\\treplace\\tthe\\toutput\\tlayer\\twith\\nthe\\tappropriate\\tnumber\\tof\\toutputs\\tfor\\tyour\\tnew\\tclassification\\ttask\\t(e.g.,\\tthe\\tflowers\\tdataset\\thas\\nfive\\tmutually\\texclusive\\tclasses\\tso\\tthe\\toutput\\tlayer\\tmust\\thave\\tfive\\tneurons\\tand\\tuse\\tthe\\tsoftmax\\nactivation\\tfunction).\\nd\\n.\\t\\nSplit\\tyour\\tdataset\\tinto\\ta\\ttraining\\tset\\tand\\ta\\ttest\\tset.\\tTrain\\tthe\\tmodel\\ton\\tthe\\ttraining\\tset\\tand\\nevaluate\\tit\\ton\\tthe\\ttest\\tset.\\n10\\n.\\t\\nGo\\tthrough\\tTensorFlow’s\\t\\nDeepDream\\ttutorial\\n.\\tIt\\tis\\ta\\tfun\\tway\\tto\\tfamiliarize\\tyourself\\twith\\tvarious\\nways\\tof\\tvisualizing\\tthe\\tpatterns\\tlearned\\tby\\ta\\tCNN,\\tand\\tto\\tgenerate\\tart\\tusing\\tDeep\\tLearning.\\nSolutions\\tto\\t\\nthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\n“Single\\tUnit\\tActivity\\tin\\tStriate\\tCortex\\tof\\tUnrestrained\\tCats,”\\tD.\\tHubel\\tand\\tT.\\tWiesel\\t(1958).\\n“Receptive\\tFields\\tof\\tSingle\\tNeurones\\tin\\tthe\\tCat’s\\tStriate\\tCortex,”\\tD.\\tHubel\\tand\\tT.\\tWiesel\\t(1959).\\n“Receptive\\tFields\\tand\\tFunctional\\tArchitecture\\tof\\tMonkey\\tStriate\\tCortex,”\\tD.\\tHubel\\tand\\tT.\\tWiesel\\t(1968).\\n“Neocognitron:\\tA\\tSelf-organizing\\tNeural\\tNetwork\\tModel\\tfor\\ta\\tMechanism\\tof\\tPattern\\tRecognition\\tUnaffected\\tby\\tShift\\tin\\tPosition,”\\tK.\\nFukushima\\t(1980).\\n“Gradient-Based\\tLearning\\tApplied\\tto\\tDocument\\tRecognition,”\\tY.\\tLeCun\\tet\\tal.\\t(1998).\\nA\\tconvolution\\tis\\ta\\tmathematical\\toperation\\tthat\\tslides\\tone\\tfunction\\tover\\tanother\\tand\\tmeasures\\tthe\\tintegral\\tof\\ttheir\\tpointwise\\tmultiplication.\\nIt\\thas\\tdeep\\tconnections\\twith\\tthe\\tFourier\\ttransform\\tand\\tthe\\tLaplace\\ttransform,\\tand\\tis\\theavily\\tused\\tin\\tsignal\\tprocessing.\\tConvolutional\\nlayers\\tactually\\tuse\\tcross-correlations,\\twhich\\tare\\tvery\\tsimilar\\tto\\tconvolutions\\t(see\\t\\nhttp://goo.gl/HAfxXd\\n\\tfor\\tmore\\tdetails).\\nA\\tfully\\tconnected\\tlayer\\twith\\t150\\t×\\t100\\tneurons,\\teach\\tconnected\\tto\\tall\\t150\\t×\\t100\\t×\\t3\\tinputs,\\twould\\thave\\t150\\n×\\t100\\n×\\t3\\t=\\t675\\tmillion\\tparameters!\\n1\\tMB\\t=\\t1,024\\tkB\\t=\\t1,024\\t×\\t1,024\\tbytes\\t=\\t1,024\\t×\\t1,024\\t×\\t8\\tbits.\\n“ImageNet\\tClassification\\twith\\tDeep\\tConvolutional\\tNeural\\tNetworks,”\\tA.\\tKrizhevsky\\tet\\tal.\\t(2012).\\n“Going\\tDeeper\\twith\\tConvolutions,”\\tC.\\tSzegedy\\tet\\tal.\\t(2015).\\nIn\\tthe\\t2010\\tmovie\\t\\nInception\\n,\\tthe\\tcharacters\\tkeep\\tgoing\\tdeeper\\tand\\tdeeper\\tinto\\tmultiple\\tlayers\\tof\\tdreams,\\thence\\tthe\\tname\\tof\\tthese\\nmodules.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n2\\n2\\n8\\n9\\n10\\n11\\n12', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 466}), Document(page_content='“Deep\\tResidual\\tLearning\\tfor\\tImage\\tRecognition,”\\tK.\\tHe\\t(2015).\\n“Very\\tDeep\\tConvolutional\\tNetworks\\tfor\\tLarge-Scale\\tImage\\tRecognition,”\\tK.\\tSimonyan\\tand\\tA.\\tZisserman\\t(2015).\\n“Inception-v4,\\tInception-ResNet\\tand\\tthe\\tImpact\\tof\\tResidual\\tConnections\\ton\\tLearning,”\\tC.\\tSzegedy\\tet\\tal.\\t(2016).\\nThis\\tname\\tis\\tquite\\tmisleading\\tsince\\tthis\\tlayer\\tdoes\\t\\nnot\\n\\tperform\\ta\\tdeconvolution,\\twhich\\tis\\ta\\twell-defined\\tmathematical\\toperation\\t(the\\ninverse\\tof\\ta\\tconvolution).\\n12\\n13\\n14\\n15', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 467}), Document(page_content='Chapter\\t14.\\t\\nRecurrent\\tNeural\\tNetworks\\nThe\\t\\nbatter\\thits\\tthe\\tball.\\tYou\\timmediately\\tstart\\trunning,\\tanticipating\\tthe\\tball’s\\ttrajectory.\\tYou\\ttrack\\tit\\tand\\nadapt\\tyour\\tmovements,\\tand\\tfinally\\tcatch\\tit\\t(under\\ta\\tthunder\\tof\\tapplause).\\tPredicting\\tthe\\tfuture\\tis\\twhat\\tyou\\ndo\\tall\\tthe\\ttime,\\twhether\\tyou\\tare\\tfinishing\\ta\\tfriend’s\\tsentence\\tor\\tanticipating\\tthe\\tsmell\\tof\\tcoffee\\tat\\nbreakfast.\\tIn\\tthis\\tchapter,\\twe\\tare\\tgoing\\tto\\tdiscuss\\t\\nrecurrent\\tneural\\tnetworks\\n\\t(RNN),\\ta\\tclass\\tof\\tnets\\tthat\\ncan\\tpredict\\tthe\\tfuture\\t(well,\\tup\\tto\\ta\\tpoint,\\tof\\tcourse).\\tThey\\tcan\\tanalyze\\t\\ntime\\tseries\\n\\t\\ndata\\tsuch\\tas\\tstock\\nprices,\\tand\\ttell\\tyou\\twhen\\tto\\tbuy\\tor\\tsell.\\tIn\\t\\nautonomous\\tdriving\\tsystems,\\tthey\\tcan\\tanticipate\\tcar\\ntrajectories\\tand\\thelp\\tavoid\\taccidents.\\tMore\\tgenerally,\\tthey\\tcan\\twork\\ton\\t\\nsequences\\n\\t\\nof\\tarbitrary\\tlengths,\\nrather\\tthan\\ton\\tfixed-sized\\tinputs\\tlike\\tall\\tthe\\tnets\\twe\\thave\\tdiscussed\\tso\\tfar.\\tFor\\texample,\\tthey\\tcan\\ttake\\nsentences,\\tdocuments,\\tor\\taudio\\tsamples\\tas\\tinput,\\tmaking\\tthem\\textremely\\tuseful\\tfor\\t\\nnatural\\tlanguage\\nprocessing\\t(NLP)\\tsystems\\tsuch\\tas\\tautomatic\\ttranslation,\\tspeech-to-text,\\tor\\t\\nsentiment\\tanalysis\\n\\t\\n(e.g.,\\nreading\\tmovie\\treviews\\tand\\textracting\\tthe\\trater’s\\tfeeling\\tabout\\tthe\\tmovie).\\nMoreover,\\tRNNs’\\tability\\tto\\tanticipate\\talso\\tmakes\\tthem\\tcapable\\tof\\tsurprising\\tcreativity.\\tYou\\tcan\\task\\tthem\\nto\\tpredict\\twhich\\tare\\tthe\\tmost\\tlikely\\tnext\\tnotes\\tin\\ta\\tmelody,\\tthen\\trandomly\\tpick\\tone\\tof\\tthese\\tnotes\\tand\\tplay\\nit.\\tThen\\task\\tthe\\tnet\\tfor\\tthe\\tnext\\tmost\\tlikely\\tnotes,\\tplay\\tit,\\tand\\trepeat\\tthe\\tprocess\\tagain\\tand\\tagain.\\tBefore\\nyou\\tknow\\tit,\\tyour\\tnet\\twill\\tcompose\\ta\\tmelody\\tsuch\\tas\\t\\nthe\\tone\\n\\tproduced\\tby\\tGoogle’s\\t\\nMagenta\\tproject\\n.\\nSimilarly,\\tRNNs\\tcan\\t\\ngenerate\\tsentences\\n,\\t\\nimage\\tcaptions\\n,\\tand\\tmuch\\tmore.\\tThe\\tresult\\tis\\tnot\\texactly\\nShakespeare\\tor\\tMozart\\tyet,\\tbut\\twho\\tknows\\twhat\\tthey\\twill\\tproduce\\ta\\tfew\\tyears\\tfrom\\tnow?\\nIn\\tthis\\tchapter,\\twe\\twill\\tlook\\tat\\tthe\\tfundamental\\tconcepts\\tunderlying\\tRNNs,\\tthe\\tmain\\tproblem\\tthey\\tface\\n(namely,\\tvanishing/exploding\\tgradients,\\tdiscussed\\tin\\t\\nChapter\\t11\\n),\\tand\\tthe\\tsolutions\\twidely\\tused\\tto\\tfight\\nit:\\tLSTM\\tand\\tGRU\\tcells.\\tAlong\\tthe\\tway,\\tas\\talways,\\twe\\twill\\tshow\\thow\\tto\\timplement\\tRNNs\\tusing\\nTensorFlow.\\tFinally,\\twe\\twill\\ttake\\ta\\tlook\\tat\\tthe\\tarchitecture\\tof\\ta\\tmachine\\ttranslation\\tsystem.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 468}), Document(page_content='Recurrent\\tNeurons\\nUp\\t\\nto\\tnow\\twe\\thave\\tmostly\\tlooked\\tat\\tfeedforward\\tneural\\tnetworks,\\twhere\\tthe\\tactivations\\tflow\\tonly\\tin\\tone\\ndirection,\\tfrom\\tthe\\tinput\\tlayer\\tto\\tthe\\toutput\\tlayer\\t(except\\tfor\\ta\\tfew\\tnetworks\\tin\\t\\nAppendix\\tE\\n).\\tA\\trecurrent\\nneural\\tnetwork\\tlooks\\tvery\\tmuch\\tlike\\ta\\tfeedforward\\tneural\\tnetwork,\\texcept\\tit\\talso\\thas\\tconnections\\npointing\\tbackward.\\tLet’s\\tlook\\tat\\tthe\\tsimplest\\tpossible\\tRNN,\\tcomposed\\tof\\tjust\\tone\\tneuron\\treceiving\\ninputs,\\tproducing\\tan\\toutput,\\tand\\tsending\\tthat\\toutput\\tback\\tto\\titself,\\tas\\tshown\\tin\\t\\nFigure\\t14-1\\n\\t(left).\\tAt\\teach\\ntime\\tstep\\n\\t\\nt\\n\\t(also\\tcalled\\ta\\t\\nframe\\n),\\tthis\\t\\nrecurrent\\tneuron\\n\\treceives\\tthe\\tinputs\\t\\nx\\n(\\nt\\n)\\n\\tas\\twell\\tas\\tits\\town\\toutput\\nfrom\\tthe\\tprevious\\ttime\\tstep,\\t\\ny\\n(\\nt\\n–1)\\n.\\tWe\\tcan\\trepresent\\tthis\\ttiny\\tnetwork\\tagainst\\tthe\\ttime\\taxis,\\tas\\tshown\\tin\\nFigure\\t14-1\\n\\t(right).\\tThis\\tis\\tcalled\\t\\nunrolling\\tthe\\tnetwork\\tthrough\\ttime\\n.\\nFigure\\t14-1.\\t\\nA\\trecurrent\\tneuron\\t(left),\\tunrolled\\tthrough\\ttime\\t(right)\\nYou\\tcan\\teasily\\tcreate\\ta\\tlayer\\tof\\trecurrent\\tneurons.\\tAt\\teach\\ttime\\tstep\\t\\nt\\n,\\tevery\\tneuron\\treceives\\tboth\\tthe\\ninput\\tvector\\t\\nx\\n(\\nt\\n)\\n\\tand\\tthe\\toutput\\tvector\\tfrom\\tthe\\tprevious\\ttime\\tstep\\t\\ny\\n(\\nt\\n–1)\\n,\\tas\\tshown\\tin\\t\\nFigure\\t14-2\\n.\\tNote\\nthat\\tboth\\tthe\\tinputs\\tand\\toutputs\\tare\\tvectors\\tnow\\t(when\\tthere\\twas\\tjust\\ta\\tsingle\\tneuron,\\tthe\\toutput\\twas\\ta\\nscalar).\\nFigure\\t14-2.\\t\\nA\\tlayer\\tof\\trecurrent\\tneurons\\t(left),\\tunrolled\\tthrough\\ttime\\t(right)\\nEach\\trecurrent\\tneuron\\thas\\ttwo\\tsets\\tof\\tweights:\\tone\\tfor\\tthe\\tinputs\\t\\nx\\n(\\nt\\n)\\n\\tand\\tthe\\tother\\tfor\\tthe\\toutputs\\tof\\tthe\\nprevious\\ttime\\tstep,\\t\\ny\\n(\\nt\\n–1)\\n.\\tLet’s\\tcall\\tthese\\tweight\\tvectors\\t\\nw\\nx\\n\\tand\\t\\nw\\ny\\n.\\tThe\\toutput\\tof\\ta\\trecurrent\\tlayer\\tcan\\tbe', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 469}), Document(page_content='computed\\tpretty\\tmuch\\tas\\tyou\\tmight\\texpect,\\tas\\tshown\\tin\\t\\nEquation\\t14-1\\n\\t(\\nb\\n\\tis\\tthe\\tbias\\tterm\\tand\\tϕ(·)\\tis\\tthe\\nactivation\\tfunction,\\te.g.,\\tReLU\\n1\\n).\\nEquation\\t14-1.\\t\\nOutput\\tof\\ta\\trecurrent\\tlayer\\tfor\\ta\\tsingle\\tinstance\\nJust\\tlike\\tfor\\tfeedforward\\tneural\\tnetworks,\\twe\\tcan\\tcompute\\ta\\trecurrent\\tlayer’s\\toutput\\tin\\tone\\tshot\\tfor\\ta\\nwhole\\tmini-batch\\tusing\\ta\\tvectorized\\tform\\tof\\tthe\\tprevious\\tequation\\t(see\\t\\nEquation\\t14-2\\n).\\nEquation\\t14-2.\\t\\nOutputs\\tof\\ta\\tlayer\\tof\\trecurrent\\tneurons\\tfor\\tall\\tinstances\\tin\\ta\\tmini-batch\\nY\\n(\\nt\\n)\\n\\tis\\tan\\t\\nm\\n\\t×\\t\\nn\\nneurons\\n\\tmatrix\\tcontaining\\tthe\\tlayer’s\\toutputs\\tat\\ttime\\tstep\\t\\nt\\n\\tfor\\teach\\tinstance\\tin\\tthe\\tmini-\\nbatch\\t(\\nm\\n\\tis\\tthe\\tnumber\\tof\\tinstances\\tin\\tthe\\tmini-batch\\tand\\t\\nn\\nneurons\\n\\tis\\tthe\\tnumber\\tof\\tneurons).\\nX\\n(\\nt\\n)\\n\\tis\\tan\\t\\nm\\n\\t×\\t\\nn\\ninputs\\n\\tmatrix\\tcontaining\\tthe\\tinputs\\tfor\\tall\\tinstances\\t(\\nn\\ninputs\\n\\tis\\tthe\\tnumber\\tof\\tinput\\nfeatures).\\nW\\nx\\n\\tis\\tan\\t\\nn\\ninputs\\n\\t×\\t\\nn\\nneurons\\n\\tmatrix\\tcontaining\\tthe\\tconnection\\tweights\\tfor\\tthe\\tinputs\\tof\\tthe\\tcurrent\\ttime\\nstep.\\nW\\ny\\n\\tis\\tan\\t\\nn\\nneurons\\n\\t×\\t\\nn\\nneurons\\n\\tmatrix\\tcontaining\\tthe\\tconnection\\tweights\\tfor\\tthe\\toutputs\\tof\\tthe\\tprevious\\ntime\\tstep.\\nThe\\tweight\\tmatrices\\t\\nW\\nx\\n\\tand\\t\\nW\\ny\\n\\tare\\toften\\tconcatenated\\tinto\\ta\\tsingle\\tweight\\tmatrix\\t\\nW\\n\\tof\\tshape\\n(\\nn\\ninputs\\n\\t+\\t\\nn\\nneurons\\n)\\t×\\t\\nn\\nneurons\\n\\t(see\\tthe\\tsecond\\tline\\tof\\t\\nEquation\\t14-2\\n).\\nb\\n\\tis\\ta\\tvector\\tof\\tsize\\t\\nn\\nneurons\\n\\tcontaining\\teach\\tneuron’s\\tbias\\tterm.\\nNotice\\tthat\\t\\nY\\n(\\nt\\n)\\n\\tis\\ta\\tfunction\\tof\\t\\nX\\n(\\nt\\n)\\n\\tand\\t\\nY\\n(\\nt\\n–1)\\n,\\twhich\\tis\\ta\\tfunction\\tof\\t\\nX\\n(\\nt\\n–1)\\n\\tand\\t\\nY\\n(\\nt\\n–2)\\n,\\twhich\\tis\\ta\\tfunction\\nof\\t\\nX\\n(\\nt\\n–2)\\n\\tand\\t\\nY\\n(\\nt\\n–3)\\n,\\tand\\tso\\ton.\\tThis\\tmakes\\t\\nY\\n(\\nt\\n)\\n\\ta\\tfunction\\tof\\tall\\tthe\\tinputs\\tsince\\ttime\\t\\nt\\n\\t=\\t0\\t(that\\tis,\\t\\nX\\n(0)\\n,\\nX\\n(1)\\n,\\t…,\\t\\nX\\n(\\nt\\n)\\n).\\tAt\\tthe\\tfirst\\ttime\\tstep,\\t\\nt\\n\\t=\\t0,\\tthere\\tare\\tno\\tprevious\\toutputs,\\tso\\tthey\\tare\\ttypically\\tassumed\\tto\\nbe\\tall\\tzeros.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 470}), Document(page_content='Memory\\tCells\\nSince\\t\\nthe\\toutput\\tof\\ta\\trecurrent\\tneuron\\tat\\ttime\\tstep\\t\\nt\\n\\tis\\ta\\tfunction\\tof\\tall\\tthe\\tinputs\\tfrom\\tprevious\\ttime\\tsteps,\\nyou\\tcould\\tsay\\tit\\thas\\ta\\tform\\tof\\t\\nmemory\\n.\\tA\\tpart\\tof\\ta\\tneural\\tnetwork\\tthat\\tpreserves\\tsome\\tstate\\tacross\\ttime\\nsteps\\tis\\tcalled\\ta\\t\\nmemory\\tcell\\n\\t(or\\tsimply\\ta\\t\\ncell\\n).\\tA\\tsingle\\trecurrent\\tneuron,\\tor\\ta\\tlayer\\tof\\trecurrent\\nneurons,\\tis\\ta\\tvery\\t\\nbasic\\tcell\\n,\\tbut\\tlater\\tin\\tthis\\tchapter\\twe\\twill\\tlook\\tat\\tsome\\tmore\\tcomplex\\tand\\tpowerful\\ntypes\\tof\\tcells.\\nIn\\tgeneral\\ta\\tcell’s\\tstate\\tat\\ttime\\tstep\\t\\nt\\n,\\tdenoted\\t\\nh\\n(\\nt\\n)\\n\\t(the\\t“h”\\tstands\\tfor\\t“hidden”),\\tis\\ta\\tfunction\\tof\\tsome\\ninputs\\tat\\tthat\\ttime\\tstep\\tand\\tits\\tstate\\tat\\tthe\\tprevious\\ttime\\tstep:\\t\\nh\\n(\\nt\\n)\\n\\t=\\t\\nf\\n(\\nh\\n(\\nt\\n–1)\\n,\\t\\nx\\n(\\nt\\n)\\n).\\tIts\\toutput\\tat\\ttime\\tstep\\t\\nt\\n,\\ndenoted\\t\\ny\\n(\\nt\\n)\\n,\\tis\\talso\\ta\\tfunction\\tof\\tthe\\tprevious\\tstate\\tand\\tthe\\tcurrent\\tinputs.\\tIn\\tthe\\tcase\\tof\\tthe\\tbasic\\tcells\\twe\\nhave\\tdiscussed\\tso\\tfar,\\tthe\\toutput\\tis\\tsimply\\tequal\\tto\\tthe\\tstate,\\tbut\\tin\\tmore\\tcomplex\\tcells\\tthis\\tis\\tnot\\talways\\nthe\\tcase,\\tas\\tshown\\tin\\t\\nFigure\\t14-3\\n.\\nFigure\\t14-3.\\t\\nA\\tcell’s\\thidden\\tstate\\tand\\tits\\toutput\\tmay\\tbe\\tdifferent', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 471}), Document(page_content='Input\\tand\\tOutput\\tSequences\\nAn\\t\\nRNN\\tcan\\tsimultaneously\\ttake\\ta\\tsequence\\tof\\tinputs\\tand\\tproduce\\ta\\tsequence\\tof\\toutputs\\t(see\\t\\nFigure\\t14-\\n4\\n,\\ttop-left\\tnetwork).\\tFor\\texample,\\tthis\\ttype\\tof\\tnetwork\\tis\\tuseful\\tfor\\tpredicting\\ttime\\tseries\\tsuch\\tas\\tstock\\nprices:\\tyou\\tfeed\\tit\\tthe\\tprices\\tover\\tthe\\tlast\\t\\nN\\n\\tdays,\\tand\\tit\\tmust\\toutput\\tthe\\tprices\\tshifted\\tby\\tone\\tday\\tinto\\tthe\\nfuture\\t(i.e.,\\tfrom\\t\\nN\\n\\t–\\t1\\tdays\\tago\\tto\\ttomorrow).\\nAlternatively,\\tyou\\tcould\\tfeed\\tthe\\tnetwork\\ta\\tsequence\\tof\\tinputs,\\tand\\tignore\\tall\\toutputs\\texcept\\tfor\\tthe\\tlast\\none\\t(see\\tthe\\ttop-right\\tnetwork).\\tIn\\tother\\twords,\\tthis\\tis\\ta\\tsequence-to-vector\\tnetwork.\\tFor\\texample,\\tyou\\ncould\\tfeed\\tthe\\tnetwork\\ta\\tsequence\\tof\\twords\\tcorresponding\\tto\\ta\\tmovie\\treview,\\tand\\tthe\\tnetwork\\twould\\noutput\\ta\\tsentiment\\tscore\\t(e.g.,\\tfrom\\t–1\\t[hate]\\tto\\t+1\\t[love]).\\nConversely,\\tyou\\tcould\\tfeed\\tthe\\tnetwork\\ta\\tsingle\\tinput\\tat\\tthe\\tfirst\\ttime\\tstep\\t(and\\tzeros\\tfor\\tall\\tother\\ttime\\nsteps),\\tand\\tlet\\tit\\toutput\\ta\\tsequence\\t(see\\tthe\\tbottom-left\\tnetwork).\\tThis\\tis\\ta\\tvector-to-sequence\\tnetwork.\\nFor\\texample,\\tthe\\tinput\\tcould\\tbe\\tan\\timage,\\tand\\tthe\\toutput\\tcould\\tbe\\ta\\tcaption\\tfor\\tthat\\timage.\\nLastly,\\tyou\\tcould\\thave\\ta\\tsequence-to-vector\\tnetwork,\\tcalled\\t\\nan\\t\\nencoder\\n,\\tfollowed\\tby\\ta\\tvector-to-\\nsequence\\tnetwork,\\tcalled\\ta\\t\\ndecoder\\n\\t(see\\tthe\\tbottom-right\\tnetwork).\\tFor\\texample,\\tthis\\tcan\\tbe\\tused\\tfor\\ntranslating\\ta\\tsentence\\tfrom\\tone\\tlanguage\\tto\\tanother.\\tYou\\twould\\tfeed\\tthe\\tnetwork\\ta\\tsentence\\tin\\tone\\nlanguage,\\tthe\\tencoder\\twould\\tconvert\\tthis\\tsentence\\tinto\\ta\\tsingle\\tvector\\trepresentation,\\tand\\tthen\\tthe\\ndecoder\\twould\\tdecode\\tthis\\tvector\\tinto\\ta\\tsentence\\tin\\tanother\\tlanguage.\\tThis\\ttwo-step\\tmodel,\\tcalled\\tan\\nEncoder–Decoder,\\tworks\\tmuch\\tbetter\\tthan\\ttrying\\tto\\ttranslate\\ton\\tthe\\tfly\\twith\\ta\\tsingle\\tsequence-to-\\nsequence\\tRNN\\t(like\\tthe\\tone\\trepresented\\ton\\tthe\\ttop\\tleft),\\tsince\\tthe\\tlast\\twords\\tof\\ta\\tsentence\\tcan\\taffect\\tthe\\nfirst\\twords\\tof\\tthe\\ttranslation,\\tso\\tyou\\tneed\\tto\\twait\\tuntil\\tyou\\thave\\theard\\tthe\\twhole\\tsentence\\tbefore\\ntranslating\\tit.\\nFigure\\t14-4.\\t\\nSeq\\tto\\tseq\\t(top\\tleft),\\tseq\\tto\\tvector\\t(top\\tright),\\tvector\\tto\\tseq\\t(bottom\\tleft),\\tdelayed\\tseq\\tto\\tseq\\t(bottom\\tright)\\nSounds\\tpromising,\\tso\\tlet’s\\t\\nstart\\tcoding!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 472}), Document(page_content='Basic\\tRNNs\\tin\\tTensorFlow\\nFirst,\\t\\nlet’s\\timplement\\ta\\tvery\\tsimple\\tRNN\\tmodel,\\twithout\\tusing\\tany\\tof\\tTensorFlow’s\\tRNN\\toperations,\\tto\\nbetter\\tunderstand\\twhat\\tgoes\\ton\\tunder\\tthe\\thood.\\tWe\\twill\\tcreate\\tan\\tRNN\\tcomposed\\tof\\ta\\tlayer\\tof\\tfive\\nrecurrent\\tneurons\\t(like\\tthe\\tRNN\\trepresented\\tin\\t\\nFigure\\t14-2\\n),\\tusing\\tthe\\ttanh\\tactivation\\tfunction.\\tWe\\twill\\nassume\\tthat\\tthe\\tRNN\\truns\\tover\\tonly\\ttwo\\ttime\\tsteps,\\ttaking\\tinput\\tvectors\\tof\\tsize\\t3\\tat\\teach\\ttime\\tstep.\\tThe\\nfollowing\\tcode\\tbuilds\\tthis\\tRNN,\\tunrolled\\tthrough\\ttwo\\ttime\\tsteps:\\nn_inputs\\n\\t\\n=\\n\\t\\n3\\nn_neurons\\n\\t\\n=\\n\\t\\n5\\nX0\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nX1\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nWx\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_normal\\n(\\nshape\\n=\\n[\\nn_inputs\\n,\\n\\t\\nn_neurons\\n],\\ndtype\\n=\\ntf\\n.\\nfloat32\\n))\\nWy\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_normal\\n(\\nshape\\n=\\n[\\nn_neurons\\n,\\nn_neurons\\n],\\ndtype\\n=\\ntf\\n.\\nfloat32\\n))\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n([\\n1\\n,\\n\\t\\nn_neurons\\n],\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n))\\nY0\\n\\t\\n=\\n\\t\\ntf\\n.\\ntanh\\n(\\ntf\\n.\\nmatmul\\n(\\nX0\\n,\\n\\t\\nWx\\n)\\n\\t\\n+\\n\\t\\nb\\n)\\nY1\\n\\t\\n=\\n\\t\\ntf\\n.\\ntanh\\n(\\ntf\\n.\\nmatmul\\n(\\nY0\\n,\\n\\t\\nWy\\n)\\n\\t\\n+\\n\\t\\ntf\\n.\\nmatmul\\n(\\nX1\\n,\\n\\t\\nWx\\n)\\n\\t\\n+\\n\\t\\nb\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nThis\\tnetwork\\tlooks\\tmuch\\tlike\\ta\\ttwo-layer\\tfeedforward\\tneural\\tnetwork,\\twith\\ta\\tfew\\ttwists:\\tfirst,\\tthe\\tsame\\nweights\\tand\\tbias\\tterms\\tare\\tshared\\tby\\tboth\\tlayers,\\tand\\tsecond,\\twe\\tfeed\\tinputs\\tat\\teach\\tlayer,\\tand\\twe\\tget\\noutputs\\tfrom\\teach\\tlayer.\\tTo\\trun\\tthe\\tmodel,\\twe\\tneed\\tto\\tfeed\\tit\\tthe\\tinputs\\tat\\tboth\\ttime\\tsteps,\\tlike\\tso:\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\n#\\tMini-batch:\\t\\t\\t\\t\\t\\t\\t\\tinstance\\t0,instance\\t1,instance\\t2,instance\\t3\\nX0_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([[\\n0\\n,\\n\\t\\n1\\n,\\n\\t\\n2\\n],\\n\\t\\n[\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n5\\n],\\n\\t\\n[\\n6\\n,\\n\\t\\n7\\n,\\n\\t\\n8\\n],\\n\\t\\n[\\n9\\n,\\n\\t\\n0\\n,\\n\\t\\n1\\n]])\\n\\t\\n#\\tt\\t=\\t0\\nX1_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([[\\n9\\n,\\n\\t\\n8\\n,\\n\\t\\n7\\n],\\n\\t\\n[\\n0\\n,\\n\\t\\n0\\n,\\n\\t\\n0\\n],\\n\\t\\n[\\n6\\n,\\n\\t\\n5\\n,\\n\\t\\n4\\n],\\n\\t\\n[\\n3\\n,\\n\\t\\n2\\n,\\n\\t\\n1\\n]])\\n\\t\\n#\\tt\\t=\\t1\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nY0_val\\n,\\n\\t\\nY1_val\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n([\\nY0\\n,\\n\\t\\nY1\\n],\\n\\t\\nfeed_dict\\n=\\n{\\nX0\\n:\\n\\t\\nX0_batch\\n,\\n\\t\\nX1\\n:\\n\\t\\nX1_batch\\n})\\nThis\\tmini-batch\\tcontains\\tfour\\tinstances,\\teach\\twith\\tan\\tinput\\tsequence\\tcomposed\\tof\\texactly\\ttwo\\tinputs.\\tAt\\nthe\\tend,\\t\\nY0_val\\n\\tand\\t\\nY1_val\\n\\tcontain\\tthe\\toutputs\\tof\\tthe\\tnetwork\\tat\\tboth\\ttime\\tsteps\\tfor\\tall\\tneurons\\tand\\tall\\ninstances\\tin\\tthe\\tmini-batch:\\n>>>\\t\\nprint\\n(\\nY0_val\\n)\\n\\t\\t\\n#\\toutput\\tat\\tt\\t=\\t0\\n[[-0.0664006\\t\\t\\t0.96257669\\t\\t0.68105787\\t\\t0.70918542\\t-0.89821595]\\t\\t#\\tinstance\\t0\\n\\t[\\t0.9977755\\t\\t-0.71978885\\t-0.99657625\\t\\t0.9673925\\t\\t-0.99989718]\\t\\t#\\tinstance\\t1\\n\\t[\\t0.99999774\\t-0.99898815\\t-0.99999893\\t\\t0.99677622\\t-0.99999988]\\t\\t#\\tinstance\\t2\\n\\t[\\t1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.99818915\\t\\t0.99950868]]\\t#\\tinstance\\t3\\n>>>\\t\\nprint\\n(\\nY1_val\\n)\\n\\t\\t\\n#\\toutput\\tat\\tt\\t=\\t1\\n[[\\t1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.40200216\\t-1.\\t\\t\\t\\t\\t\\t\\t\\t]\\t\\t#\\tinstance\\t0\\n\\t[-0.12210433\\t\\t0.62805319\\t\\t0.96718419\\t-0.99371207\\t-0.25839335]\\t\\t#\\tinstance\\t1\\n\\t[\\t0.99999827\\t-0.9999994\\t\\t-0.9999975\\t\\t-0.85943311\\t-0.9999879\\t]\\t\\t#\\tinstance\\t2\\n\\t[\\t0.99928284\\t-0.99999815\\t-0.99990582\\t\\t0.98579615\\t-0.92205751]]\\t#\\tinstance\\t3\\nThat\\twasn’t\\ttoo\\thard,\\tbut\\tof\\tcourse\\tif\\tyou\\twant\\tto\\tbe\\table\\tto\\trun\\tan\\tRNN\\tover\\t100\\ttime\\tsteps,\\tthe\\tgraph\\tis\\ngoing\\tto\\tbe\\tpretty\\tbig.\\tNow\\tlet’s\\tlook\\tat\\thow\\tto\\tcreate\\tthe\\tsame\\tmodel\\tusing\\tTensorFlow’s\\tRNN\\noperations.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 473}), Document(page_content='Static\\tUnrolling\\tThrough\\tTime\\nThe\\t\\nstatic_rnn()\\n\\t\\nfunction\\tcreates\\tan\\tunrolled\\tRNN\\tnetwork\\tby\\tchaining\\tcells.\\tThe\\tfollowing\\tcode\\ncreates\\tthe\\texact\\tsame\\tmodel\\tas\\tthe\\tprevious\\tone:\\nX0\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nX1\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nbasic_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\noutput_seqs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nstatic_rnn\\n(\\nbasic_cell\\n,\\n\\t\\n[\\nX0\\n,\\n\\t\\nX1\\n],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nY0\\n,\\n\\t\\nY1\\n\\t\\n=\\n\\t\\noutput_seqs\\nFirst\\twe\\tcreate\\tthe\\tinput\\tplaceholders,\\tas\\tbefore.\\tThen\\twe\\tcreate\\ta\\t\\nBasicRNNCell\\n,\\twhich\\tyou\\tcan\\tthink\\nof\\tas\\ta\\tfactory\\tthat\\tcreates\\tcopies\\tof\\tthe\\tcell\\tto\\tbuild\\tthe\\tunrolled\\tRNN\\t\\n(one\\tfor\\teach\\ttime\\tstep).\\tThen\\twe\\ncall\\t\\nstatic_rnn()\\n,\\tgiving\\tit\\tthe\\tcell\\tfactory\\tand\\tthe\\tinput\\ttensors,\\tand\\ttelling\\tit\\tthe\\tdata\\ttype\\tof\\tthe\\tinputs\\n(this\\tis\\tused\\tto\\tcreate\\tthe\\tinitial\\tstate\\tmatrix,\\twhich\\tby\\tdefault\\tis\\tfull\\tof\\tzeros).\\tThe\\t\\nstatic_rnn()\\nfunction\\tcalls\\tthe\\tcell\\tfactory’s\\t\\n__call__()\\n\\t\\nfunction\\tonce\\tper\\tinput,\\tcreating\\ttwo\\tcopies\\tof\\tthe\\tcell\\t(each\\ncontaining\\ta\\tlayer\\tof\\tfive\\trecurrent\\tneurons),\\twith\\tshared\\tweights\\tand\\tbias\\tterms,\\tand\\tit\\tchains\\tthem\\tjust\\nlike\\twe\\tdid\\tearlier.\\tThe\\t\\nstatic_rnn()\\n\\tfunction\\treturns\\ttwo\\tobjects.\\tThe\\tfirst\\tis\\ta\\tPython\\tlist\\tcontaining\\nthe\\toutput\\ttensors\\tfor\\teach\\ttime\\tstep.\\tThe\\tsecond\\tis\\ta\\ttensor\\tcontaining\\tthe\\tfinal\\tstates\\tof\\tthe\\tnetwork.\\nWhen\\tyou\\tare\\tusing\\tbasic\\tcells,\\tthe\\tfinal\\tstate\\tis\\tsimply\\tequal\\tto\\tthe\\tlast\\toutput.\\nIf\\tthere\\twere\\t50\\ttime\\tsteps,\\tit\\twould\\tnot\\tbe\\tvery\\tconvenient\\tto\\thave\\tto\\tdefine\\t50\\tinput\\tplaceholders\\tand\\n50\\toutput\\ttensors.\\tMoreover,\\tat\\texecution\\ttime\\tyou\\twould\\thave\\tto\\tfeed\\teach\\tof\\tthe\\t50\\tplaceholders\\tand\\nmanipulate\\tthe\\t50\\toutputs.\\tLet’s\\tsimplify\\tthis.\\tThe\\tfollowing\\tcode\\tbuilds\\tthe\\tsame\\tRNN\\tagain,\\tbut\\tthis\\ntime\\tit\\ttakes\\ta\\tsingle\\tinput\\tplaceholder\\tof\\tshape\\t\\n[None,\\tn_steps,\\tn_inputs]\\n\\twhere\\tthe\\tfirst\\tdimension\\nis\\tthe\\tmini-batch\\tsize.\\tThen\\tit\\textracts\\tthe\\tlist\\tof\\tinput\\tsequences\\tfor\\teach\\ttime\\tstep.\\t\\nX_seqs\\n\\tis\\ta\\tPython\\nlist\\tof\\t\\nn_steps\\n\\ttensors\\tof\\tshape\\t\\n[None,\\tn_inputs]\\n,\\twhere\\tonce\\tagain\\tthe\\tfirst\\tdimension\\tis\\tthe\\tmini-\\nbatch\\tsize.\\tTo\\tdo\\tthis,\\twe\\tfirst\\tswap\\tthe\\tfirst\\ttwo\\tdimensions\\tusing\\tthe\\t\\ntranspose()\\n\\t\\nfunction,\\tso\\tthat\\tthe\\ntime\\tsteps\\tare\\tnow\\tthe\\tfirst\\tdimension.\\tThen\\twe\\textract\\ta\\tPython\\tlist\\tof\\ttensors\\talong\\tthe\\tfirst\\tdimension\\n(i.e.,\\tone\\ttensor\\tper\\ttime\\tstep)\\tusing\\tthe\\t\\nunstack()\\n\\t\\nfunction.\\tThe\\tnext\\ttwo\\tlines\\tare\\tthe\\tsame\\tas\\tbefore.\\nFinally,\\twe\\tmerge\\tall\\tthe\\toutput\\ttensors\\tinto\\ta\\tsingle\\ttensor\\tusing\\tthe\\t\\nstack()\\n\\t\\nfunction,\\tand\\twe\\tswap\\tthe\\nfirst\\ttwo\\tdimensions\\tto\\tget\\ta\\tfinal\\t\\noutputs\\n\\ttensor\\tof\\tshape\\t\\n[None,\\tn_steps,\\tn_neurons]\\n\\t(again\\tthe\\nfirst\\tdimension\\tis\\tthe\\tmini-batch\\tsize).\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n])\\nX_seqs\\n\\t\\n=\\n\\t\\ntf\\n.\\nunstack\\n(\\ntf\\n.\\ntranspose\\n(\\nX\\n,\\n\\t\\nperm\\n=\\n[\\n1\\n,\\n\\t\\n0\\n,\\n\\t\\n2\\n]))\\nbasic_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\noutput_seqs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nstatic_rnn\\n(\\nbasic_cell\\n,\\n\\t\\nX_seqs\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\ntranspose\\n(\\ntf\\n.\\nstack\\n(\\noutput_seqs\\n),\\n\\t\\nperm\\n=\\n[\\n1\\n,\\n\\t\\n0\\n,\\n\\t\\n2\\n])\\nNow\\twe\\t\\ncan\\trun\\tthe\\tnetwork\\tby\\tfeeding\\tit\\ta\\tsingle\\ttensor\\tthat\\tcontains\\tall\\tthe\\tmini-batch\\tsequences:\\nX_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tt\\t=\\t0\\t\\t\\t\\t\\tt\\t=\\t1\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n0\\n,\\n\\t\\n1\\n,\\n\\t\\n2\\n],\\n\\t\\n[\\n9\\n,\\n\\t\\n8\\n,\\n\\t\\n7\\n]],\\n\\t\\n#\\tinstance\\t0\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n5\\n],\\n\\t\\n[\\n0\\n,\\n\\t\\n0\\n,\\n\\t\\n0\\n]],\\n\\t\\n#\\tinstance\\t1', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 474}), Document(page_content='\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n6\\n,\\n\\t\\n7\\n,\\n\\t\\n8\\n],\\n\\t\\n[\\n6\\n,\\n\\t\\n5\\n,\\n\\t\\n4\\n]],\\n\\t\\n#\\tinstance\\t2\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n9\\n,\\n\\t\\n0\\n,\\n\\t\\n1\\n],\\n\\t\\n[\\n3\\n,\\n\\t\\n2\\n,\\n\\t\\n1\\n]],\\n\\t\\n#\\tinstance\\t3\\n\\t\\t\\t\\t\\n])\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\noutputs_val\\n\\t\\n=\\n\\t\\noutputs\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n})\\nAnd\\twe\\tget\\ta\\tsingle\\t\\noutputs_val\\n\\ttensor\\tfor\\tall\\tinstances,\\tall\\ttime\\tsteps,\\tand\\tall\\tneurons:\\n>>>\\t\\nprint\\n(\\noutputs_val\\n)\\n[[[-0.91279727\\t\\t0.83698678\\t-0.89277941\\t\\t0.80308062\\t-0.5283336\\t]\\n\\t\\t[-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.99794829\\t\\t0.99985468\\t-0.99273592]]\\n\\t[[-0.99994391\\t\\t0.99951613\\t-0.9946925\\t\\t\\t0.99030769\\t-0.94413054]\\n\\t\\t[\\t0.48733309\\t\\t0.93389565\\t-0.31362072\\t\\t0.88573611\\t\\t0.2424476\\t]]\\n\\t[[-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.99999875\\t-0.99975014\\t\\t0.99956584\\t-0.99466234]\\n\\t\\t[-0.99994856\\t\\t0.99999434\\t-0.96058172\\t\\t0.99784708\\t-0.9099462\\t]]\\n\\t[[-0.95972425\\t\\t0.99951482\\t\\t0.96938795\\t-0.969908\\t\\t\\t-0.67668229]\\n\\t\\t[-0.84596014\\t\\t0.96288228\\t\\t0.96856463\\t-0.14777924\\t-0.9119423\\t]]]\\nHowever,\\tthis\\tapproach\\tstill\\tbuilds\\ta\\tgraph\\tcontaining\\tone\\tcell\\tper\\ttime\\tstep.\\tIf\\tthere\\twere\\t50\\ttime\\tsteps,\\nthe\\tgraph\\twould\\tlook\\tpretty\\tugly.\\tIt\\tis\\ta\\tbit\\tlike\\twriting\\ta\\tprogram\\twithout\\tever\\tusing\\tloops\\t(e.g.,\\t\\nY0=f(0,\\nX0);\\tY1=f(Y0,\\tX1);\\tY2=f(Y1,\\tX2);\\t...;\\tY50=f(Y49,\\tX50)\\n).\\tWith\\tsuch\\tas\\tlarge\\tgraph,\\tyou\\tmay\\neven\\tget\\tout-of-memory\\t(OOM)\\terrors\\t\\nduring\\tbackpropagation\\t(especially\\twith\\tthe\\tlimited\\tmemory\\tof\\nGPU\\tcards),\\tsince\\tit\\tmust\\tstore\\tall\\ttensor\\tvalues\\tduring\\tthe\\tforward\\tpass\\tso\\tit\\tcan\\tuse\\tthem\\tto\\tcompute\\ngradients\\tduring\\tthe\\t\\nreverse\\tpass.\\nFortunately,\\tthere\\tis\\ta\\t\\nbetter\\tsolution:\\tthe\\t\\ndynamic_rnn()\\n\\tfunction.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 475}), Document(page_content='Dynamic\\tUnrolling\\tThrough\\tTime\\nThe\\t\\ndynamic_rnn()\\n\\t\\nfunction\\tuses\\ta\\t\\nwhile_loop()\\n\\t\\noperation\\tto\\trun\\tover\\tthe\\tcell\\tthe\\tappropriate\\tnumber\\nof\\ttimes,\\tand\\tyou\\tcan\\tset\\t\\nswap_memory=True\\n\\tif\\tyou\\twant\\tit\\tto\\tswap\\tthe\\tGPU’s\\tmemory\\tto\\tthe\\tCPU’s\\nmemory\\tduring\\tbackpropagation\\tto\\tavoid\\tOOM\\terrors.\\tConveniently,\\tit\\talso\\taccepts\\ta\\tsingle\\ttensor\\tfor\\tall\\ninputs\\tat\\tevery\\ttime\\tstep\\t(shape\\t\\n[None,\\tn_steps,\\tn_inputs]\\n)\\tand\\tit\\toutputs\\ta\\tsingle\\ttensor\\tfor\\tall\\noutputs\\tat\\tevery\\ttime\\tstep\\t(shape\\t\\n[None,\\tn_steps,\\tn_neurons]\\n);\\tthere\\tis\\tno\\tneed\\tto\\tstack,\\tunstack,\\tor\\ntranspose.\\tThe\\tfollowing\\tcode\\tcreates\\tthe\\tsame\\tRNN\\tas\\tearlier\\tusing\\tthe\\t\\ndynamic_rnn()\\n\\tfunction.\\t\\nIt’s\\tso\\nmuch\\tnicer!\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n])\\nbasic_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nbasic_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nNOTE\\nDuring\\tbackpropagation,\\tthe\\t\\nwhile_loop()\\n\\toperation\\tdoes\\tthe\\tappropriate\\tmagic:\\tit\\tstores\\tthe\\ttensor\\tvalues\\tfor\\teach\\titeration\\nduring\\tthe\\tforward\\tpass\\tso\\tit\\tcan\\tuse\\tthem\\tto\\tcompute\\tgradients\\tduring\\tthe\\treverse\\tpass.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 476}), Document(page_content='Handling\\tVariable\\tLength\\tInput\\tSequences\\nSo\\tfar\\t\\nwe\\thave\\tused\\tonly\\tfixed-size\\tinput\\tsequences\\t(all\\texactly\\ttwo\\tsteps\\tlong).\\tWhat\\tif\\tthe\\tinput\\nsequences\\thave\\tvariable\\tlengths\\t(e.g.,\\tlike\\tsentences)?\\tIn\\tthis\\tcase\\tyou\\tshould\\tset\\tthe\\t\\nsequence_length\\nargument\\twhen\\tcalling\\tthe\\t\\ndynamic_rnn()\\n\\t(or\\t\\nstatic_rnn()\\n)\\tfunction;\\tit\\tmust\\tbe\\ta\\t1D\\ttensor\\tindicating\\nthe\\tlength\\tof\\tthe\\tinput\\tsequence\\tfor\\teach\\tinstance.\\tFor\\texample:\\nseq_length\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint32\\n,\\n\\t\\n[\\nNone\\n])\\n[\\n...\\n]\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nbasic_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsequence_length\\n=\\nseq_length\\n)\\nFor\\texample,\\tsuppose\\tthe\\tsecond\\tinput\\tsequence\\tcontains\\tonly\\tone\\tinput\\tinstead\\tof\\ttwo.\\tIt\\tmust\\tbe\\tpadded\\nwith\\ta\\tzero\\tvector\\tin\\torder\\tto\\tfit\\tin\\tthe\\tinput\\ttensor\\t\\nX\\n\\t(because\\tthe\\tinput\\ttensor’s\\tsecond\\tdimension\\tis\\tthe\\nsize\\tof\\tthe\\tlongest\\tsequence\\t—\\ti.e.,\\t2).\\nX_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tstep\\t0\\t\\t\\t\\t\\tstep\\t1\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n0\\n,\\n\\t\\n1\\n,\\n\\t\\n2\\n],\\n\\t\\n[\\n9\\n,\\n\\t\\n8\\n,\\n\\t\\n7\\n]],\\n\\t\\n#\\tinstance\\t0\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n5\\n],\\n\\t\\n[\\n0\\n,\\n\\t\\n0\\n,\\n\\t\\n0\\n]],\\n\\t\\n#\\tinstance\\t1\\t(padded\\twith\\ta\\tzero\\tvector)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n6\\n,\\n\\t\\n7\\n,\\n\\t\\n8\\n],\\n\\t\\n[\\n6\\n,\\n\\t\\n5\\n,\\n\\t\\n4\\n]],\\n\\t\\n#\\tinstance\\t2\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n9\\n,\\n\\t\\n0\\n,\\n\\t\\n1\\n],\\n\\t\\n[\\n3\\n,\\n\\t\\n2\\n,\\n\\t\\n1\\n]],\\n\\t\\n#\\tinstance\\t3\\n\\t\\t\\t\\t\\n])\\nseq_length_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n2\\n,\\n\\t\\n1\\n,\\n\\t\\n2\\n,\\n\\t\\n2\\n])\\nOf\\tcourse,\\tyou\\tnow\\tneed\\tto\\tfeed\\tvalues\\tfor\\tboth\\tplaceholders\\t\\nX\\n\\tand\\t\\nseq_length\\n:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\noutputs_val\\n,\\n\\t\\nstates_val\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\noutputs\\n,\\n\\t\\nstates\\n],\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\nseq_length\\n:\\n\\t\\nseq_length_batch\\n})\\nNow\\tthe\\tRNN\\toutputs\\tzero\\tvectors\\tfor\\tevery\\ttime\\tstep\\tpast\\tthe\\tinput\\tsequence\\tlength\\t(look\\tat\\tthe\\tsecond\\ninstance’s\\toutput\\tfor\\tthe\\tsecond\\ttime\\tstep):\\n>>>\\t\\nprint\\n(\\noutputs_val\\n)\\n[[[-0.68579948\\t-0.25901747\\t-0.80249101\\t-0.18141513\\t-0.37491536]\\n\\t\\t[-0.99996698\\t-0.94501185\\t\\t0.98072106\\t-0.9689762\\t\\t\\t0.99966913]]\\t\\t#\\tfinal\\tstate\\n\\t[[-0.99099374\\t-0.64768541\\t-0.67801034\\t-0.7415446\\t\\t\\t0.7719509\\t]\\t\\t\\t#\\tfinal\\tstate\\n\\t\\t[\\t0.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t]]\\t\\t#\\tzero\\tvector\\n\\t[[-0.99978048\\t-0.85583007\\t-0.49696958\\t-0.93838578\\t\\t0.98505187]\\n\\t\\t[-0.99951065\\t-0.89148796\\t\\t0.94170523\\t-0.38407657\\t\\t0.97499216]]\\t\\t#\\tfinal\\tstate\\n\\t[[-0.02052618\\t-0.94588047\\t\\t0.99935204\\t\\t0.37283331\\t\\t0.9998163\\t]\\n\\t\\t[-0.91052347\\t\\t0.05769409\\t\\t0.47446665\\t-0.44611037\\t\\t0.89394671]]]\\t#\\tfinal\\tstate\\nMoreover,\\tthe\\t\\nstates\\n\\t\\ntensor\\tcontains\\tthe\\tfinal\\tstate\\tof\\teach\\tcell\\t(excluding\\tthe\\tzero\\tvectors):\\n>>>\\t\\nprint\\n(\\nstates_val\\n)\\n[[-0.99996698\\t-0.94501185\\t\\t0.98072106\\t-0.9689762\\t\\t\\t0.99966913]\\t\\t#\\tt\\t=\\t1\\n\\t[-0.99099374\\t-0.64768541\\t-0.67801034\\t-0.7415446\\t\\t\\t0.7719509\\t]\\t\\t#\\tt\\t=\\t0\\t!!!\\n\\t[-0.99951065\\t-0.89148796\\t\\t0.94170523\\t-0.38407657\\t\\t0.97499216]\\t\\t#\\tt\\t=\\t1\\n\\t[-0.91052347\\t\\t0.05769409\\t\\t0.47446665\\t-0.44611037\\t\\t0.89394671]]\\t#\\tt\\t=\\t1', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 477}), Document(page_content='Handling\\tVariable-Length\\tOutput\\tSequences\\nWhat\\tif\\t\\nthe\\toutput\\tsequences\\thave\\tvariable\\tlengths\\tas\\twell?\\tIf\\tyou\\tknow\\tin\\tadvance\\twhat\\tlength\\teach\\nsequence\\twill\\thave\\t(for\\texample\\tif\\tyou\\tknow\\tthat\\tit\\twill\\tbe\\tthe\\tsame\\tlength\\tas\\tthe\\tinput\\tsequence),\\tthen\\nyou\\tcan\\tset\\tthe\\t\\nsequence_length\\n\\t\\nparameter\\tas\\tdescribed\\tabove.\\tUnfortunately,\\tin\\tgeneral\\tthis\\twill\\tnot\\nbe\\tpossible:\\tfor\\texample,\\tthe\\tlength\\tof\\ta\\ttranslated\\tsentence\\tis\\tgenerally\\tdifferent\\tfrom\\tthe\\tlength\\tof\\tthe\\ninput\\tsentence.\\tIn\\tthis\\tcase,\\tthe\\tmost\\tcommon\\tsolution\\tis\\tto\\tdefine\\ta\\tspecial\\toutput\\tcalled\\t\\nan\\t\\nend-of-\\nsequence\\ttoken\\n\\t(EOS\\ttoken).\\tAny\\toutput\\tpast\\tthe\\tEOS\\tshould\\tbe\\tignored\\t(we\\twill\\tdiscuss\\tthis\\tlater\\tin\\nthis\\tchapter).\\nOkay,\\tnow\\tyou\\tknow\\thow\\tto\\tbuild\\tan\\tRNN\\tnetwork\\t(or\\tmore\\tprecisely\\tan\\tRNN\\tnetwork\\tunrolled\\tthrough\\ntime).\\t\\nBut\\thow\\tdo\\tyou\\ttrain\\tit?', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 478}), Document(page_content='Training\\tRNNs\\nTo\\t\\ntrain\\tan\\tRNN,\\tthe\\ttrick\\tis\\tto\\tunroll\\tit\\tthrough\\ttime\\t(like\\twe\\tjust\\tdid)\\tand\\tthen\\tsimply\\tuse\\tregular\\nbackpropagation\\t(see\\t\\nFigure\\t14-5\\n).\\tThis\\t\\nstrategy\\tis\\tcalled\\t\\nbackpropagation\\tthrough\\ttime\\n\\t(BPTT).\\nFigure\\t14-5.\\t\\nBackpropagation\\tthrough\\ttime\\nJust\\tlike\\tin\\tregular\\tbackpropagation,\\tthere\\tis\\ta\\tfirst\\tforward\\tpass\\tthrough\\tthe\\tunrolled\\tnetwork\\n(represented\\tby\\tthe\\tdashed\\tarrows);\\tthen\\tthe\\toutput\\tsequence\\tis\\tevaluated\\tusing\\ta\\t\\ncost\\tfunction\\t\\n\\t(where\\t\\nt\\nmin\\n\\tand\\t\\nt\\nmax\\n\\tare\\tthe\\tfirst\\tand\\tlast\\toutput\\ttime\\tsteps,\\tnot\\tcounting\\nthe\\tignored\\toutputs),\\tand\\tthe\\tgradients\\tof\\tthat\\tcost\\tfunction\\tare\\tpropagated\\tbackward\\tthrough\\tthe\\tunrolled\\nnetwork\\t(represented\\tby\\tthe\\tsolid\\tarrows);\\tand\\tfinally\\tthe\\t\\nmodel\\tparameters\\tare\\tupdated\\tusing\\tthe\\ngradients\\tcomputed\\tduring\\tBPTT.\\tNote\\tthat\\tthe\\tgradients\\tflow\\tbackward\\tthrough\\tall\\tthe\\toutputs\\tused\\tby\\nthe\\tcost\\tfunction,\\tnot\\tjust\\tthrough\\tthe\\tfinal\\toutput\\t(for\\texample,\\tin\\t\\nFigure\\t14-5\\n\\tthe\\tcost\\tfunction\\tis\\ncomputed\\tusing\\tthe\\tlast\\tthree\\toutputs\\tof\\tthe\\tnetwork,\\t\\nY\\n(2)\\n,\\t\\nY\\n(3)\\n,\\tand\\t\\nY\\n(4)\\n,\\tso\\tgradients\\tflow\\tthrough\\tthese\\nthree\\toutputs,\\tbut\\tnot\\tthrough\\t\\nY\\n(0)\\n\\tand\\t\\nY\\n(1)\\n).\\tMoreover,\\tsince\\tthe\\tsame\\tparameters\\t\\nW\\n\\tand\\t\\nb\\n\\tare\\tused\\tat\\neach\\ttime\\tstep,\\tbackpropagation\\twill\\tdo\\tthe\\tright\\tthing\\tand\\tsum\\tover\\tall\\ttime\\tsteps.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 479}), Document(page_content='Training\\ta\\tSequence\\tClassifier\\nLet’s\\t\\ntrain\\tan\\tRNN\\tto\\tclassify\\tMNIST\\timages.\\tA\\tconvolutional\\tneural\\tnetwork\\twould\\tbe\\tbetter\\tsuited\\tfor\\nimage\\tclassification\\t(see\\t\\nChapter\\t13\\n),\\tbut\\tthis\\tmakes\\tfor\\ta\\tsimple\\texample\\tthat\\tyou\\tare\\talready\\tfamiliar\\nwith.\\tWe\\twill\\ttreat\\teach\\timage\\tas\\ta\\tsequence\\tof\\t28\\trows\\tof\\t28\\tpixels\\teach\\t(since\\teach\\tMNIST\\timage\\tis\\n28\\t×\\t28\\tpixels).\\tWe\\twill\\tuse\\tcells\\tof\\t150\\trecurrent\\tneurons,\\tplus\\ta\\tfully\\tconnected\\tlayer\\tcontaining\\t10\\nneurons\\t(one\\tper\\tclass)\\tconnected\\tto\\tthe\\toutput\\tof\\tthe\\tlast\\ttime\\tstep,\\tfollowed\\tby\\ta\\tsoftmax\\tlayer\\t(see\\nFigure\\t14-6\\n).\\nFigure\\t14-6.\\t\\nSequence\\tclassifier\\nThe\\tconstruction\\tphase\\tis\\tquite\\tstraightforward;\\tit’s\\tpretty\\tmuch\\tthe\\tsame\\tas\\tthe\\tMNIST\\tclassifier\\twe\\nbuilt\\tin\\t\\nChapter\\t10\\n\\texcept\\tthat\\tan\\tunrolled\\tRNN\\treplaces\\tthe\\thidden\\tlayers.\\tNote\\tthat\\tthe\\tfully\\tconnected\\nlayer\\tis\\tconnected\\tto\\tthe\\t\\nstates\\n\\ttensor,\\twhich\\tcontains\\tonly\\tthe\\tfinal\\tstate\\tof\\tthe\\tRNN\\t(i.e.,\\tthe\\t28\\nth\\noutput).\\tAlso\\tnote\\tthat\\t\\ny\\n\\tis\\ta\\tplaceholder\\tfor\\t\\nthe\\ttarget\\tclasses.\\nn_steps\\n\\t\\n=\\n\\t\\n28\\nn_inputs\\n\\t\\n=\\n\\t\\n28\\nn_neurons\\n\\t\\n=\\n\\t\\n150\\nn_outputs\\n\\t\\n=\\n\\t\\n10\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.001\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n])\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint32\\n,\\n\\t\\n[\\nNone\\n])\\nbasic_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nbasic_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nstates\\n,\\n\\t\\nn_outputs\\n)\\nxentropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsparse_softmax_cross_entropy_with_logits\\n(\\nlabels\\n=\\ny\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlogits\\n=\\nlogits\\n)\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\nxentropy\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\ncorrect\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nin_top_k\\n(\\nlogits\\n,\\n\\t\\ny\\n,\\n\\t\\n1\\n)\\naccuracy\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\ncast\\n(\\ncorrect\\n,\\n\\t\\ntf\\n.\\nfloat32\\n))', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 480}), Document(page_content='init\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nNow\\tlet’s\\t\\nload\\tthe\\tMNIST\\tdata\\tand\\treshape\\tthe\\ttest\\tdata\\tto\\t\\n[batch_size,\\tn_steps,\\tn_inputs]\\n\\tas\\tis\\nexpected\\tby\\tthe\\tnetwork.\\tWe\\twill\\ttake\\tcare\\tof\\treshaping\\tthe\\ttraining\\tdata\\tin\\ta\\tmoment.\\nfrom\\n\\t\\ntensorflow.examples.tutorials.mnist\\n\\t\\nimport\\n\\t\\ninput_data\\nmnist\\n\\t\\n=\\n\\t\\ninput_data\\n.\\nread_data_sets\\n(\\n\"/tmp/data/\"\\n)\\nX_test\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntest\\n.\\nimages\\n.\\nreshape\\n((\\n-\\n1\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n))\\ny_test\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntest\\n.\\nlabels\\nNow\\twe\\tare\\tready\\tto\\ttrain\\tthe\\tRNN.\\tThe\\texecution\\tphase\\tis\\texactly\\tthe\\tsame\\tas\\tfor\\tthe\\tMNIST\\tclassifier\\nin\\t\\nChapter\\t10\\n,\\texcept\\tthat\\twe\\treshape\\teach\\ttraining\\tbatch\\tbefore\\tfeeding\\tit\\tto\\tthe\\tnetwork.\\nn_epochs\\n\\t\\n=\\n\\t\\n100\\nbatch_size\\n\\t\\n=\\n\\t\\n150\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n\\t\\n=\\n\\t\\nX_batch\\n.\\nreshape\\n((\\n-\\n1\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nacc_train\\n\\t\\n=\\n\\t\\naccuracy\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nacc_test\\n\\t\\n=\\n\\t\\naccuracy\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_test\\n,\\n\\t\\ny\\n:\\n\\t\\ny_test\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nprint\\n(\\nepoch\\n,\\n\\t\\n\"Train\\taccuracy:\"\\n,\\n\\t\\nacc_train\\n,\\n\\t\\n\"Test\\taccuracy:\"\\n,\\n\\t\\nacc_test\\n)\\nThe\\toutput\\tshould\\tlook\\tlike\\tthis:\\n0\\tTrain\\taccuracy:\\t0.94\\tTest\\taccuracy:\\t0.9308\\n1\\tTrain\\taccuracy:\\t0.933333\\tTest\\taccuracy:\\t0.9431\\n[...]\\n98\\tTrain\\taccuracy:\\t0.98\\tTest\\taccuracy:\\t0.9794\\n99\\tTrain\\taccuracy:\\t1.0\\tTest\\taccuracy:\\t0.9804\\nWe\\tget\\tover\\t98%\\taccuracy\\t—\\tnot\\tbad!\\tPlus\\tyou\\twould\\tcertainly\\tget\\ta\\tbetter\\tresult\\tby\\ttuning\\tthe\\nhyperparameters,\\tinitializing\\tthe\\tRNN\\tweights\\tusing\\tHe\\tinitialization,\\ttraining\\tlonger,\\tor\\tadding\\ta\\tbit\\tof\\nregularization\\t(e.g.,\\tdropout).\\nTIP\\nYou\\t\\ncan\\tspecify\\tan\\tinitializer\\tfor\\tthe\\tRNN\\tby\\twrapping\\tits\\t\\nconstruction\\n\\tcode\\tin\\ta\\tvariable\\tscope\\t(e.g.,\\tuse\\nvariable_scope\\n(\"rnn\",\\tinitializer=variance_scaling_initializer())\\n\\tto\\tuse\\tHe\\tinitialization).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 481}), Document(page_content='Training\\tto\\tPredict\\tTime\\tSeries\\nNow\\t\\nlet’s\\ttake\\ta\\tlook\\tat\\thow\\tto\\thandle\\ttime\\tseries,\\tsuch\\tas\\tstock\\tprices,\\tair\\ttemperature,\\tbrain\\twave\\npatterns,\\tand\\tso\\ton.\\tIn\\tthis\\tsection\\twe\\twill\\ttrain\\tan\\tRNN\\tto\\tpredict\\tthe\\tnext\\tvalue\\tin\\ta\\tgenerated\\ttime\\nseries.\\tEach\\ttraining\\tinstance\\tis\\ta\\trandomly\\tselected\\tsequence\\tof\\t20\\tconsecutive\\tvalues\\tfrom\\tthe\\ttime\\nseries,\\tand\\tthe\\ttarget\\tsequence\\tis\\tthe\\tsame\\tas\\tthe\\tinput\\tsequence,\\texcept\\tit\\tis\\tshifted\\tby\\tone\\ttime\\tstep\\tinto\\nthe\\tfuture\\t(see\\t\\nFigure\\t14-7\\n).\\nFigure\\t14-7.\\t\\nTime\\tseries\\t(left),\\tand\\ta\\ttraining\\tinstance\\tfrom\\tthat\\tseries\\t(right)\\nFirst,\\tlet’s\\tcreate\\tthe\\tRNN.\\tIt\\twill\\tcontain\\t100\\trecurrent\\tneurons\\tand\\twe\\twill\\tunroll\\tit\\tover\\t20\\ttime\\tsteps\\nsince\\teach\\ttraining\\tinstance\\twill\\tbe\\t20\\tinputs\\tlong.\\tEach\\tinput\\twill\\tcontain\\tonly\\tone\\tfeature\\t(the\\tvalue\\tat\\nthat\\ttime).\\tThe\\ttargets\\t\\nare\\talso\\tsequences\\tof\\t20\\tinputs,\\teach\\tcontaining\\ta\\tsingle\\tvalue.\\tThe\\tcode\\tis\\talmost\\nthe\\tsame\\tas\\tearlier:\\nn_steps\\n\\t\\n=\\n\\t\\n20\\nn_inputs\\n\\t\\n=\\n\\t\\n1\\nn_neurons\\n\\t\\n=\\n\\t\\n100\\nn_outputs\\n\\t\\n=\\n\\t\\n1\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n])\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_outputs\\n])\\ncell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\ncell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nNOTE\\nIn\\tgeneral\\tyou\\twould\\thave\\tmore\\tthan\\tjust\\tone\\tinput\\tfeature.\\tFor\\texample,\\tif\\tyou\\twere\\ttrying\\tto\\tpredict\\tstock\\tprices,\\tyou\\twould\\nlikely\\thave\\tmany\\tother\\tinput\\tfeatures\\tat\\teach\\ttime\\tstep,\\tsuch\\tas\\tprices\\tof\\tcompeting\\tstocks,\\tratings\\tfrom\\tanalysts,\\tor\\tany\\tother\\nfeature\\tthat\\tmight\\thelp\\tthe\\tsystem\\tmake\\tits\\tpredictions.\\nAt\\teach\\ttime\\tstep\\twe\\tnow\\thave\\tan\\toutput\\tvector\\tof\\tsize\\t100.\\tBut\\twhat\\twe\\tactually\\twant\\tis\\ta\\tsingle\\toutput\\nvalue\\tat\\teach\\ttime\\tstep.\\tThe\\tsimplest\\tsolution\\tis\\tto\\twrap\\tthe\\tcell\\tin\\t\\nan\\t\\nOutputProjectionWrapper\\n.\\tA\\ncell\\twrapper\\tacts\\tlike\\ta\\tnormal\\tcell,\\tproxying\\tevery\\tmethod\\tcall\\tto\\tan\\tunderlying\\tcell,\\tbut\\tit\\talso\\tadds\\nsome\\tfunctionality.\\tThe\\t\\nOutputProjectionWrapper\\n\\tadds\\ta\\tfully\\tconnected\\tlayer\\tof\\tlinear\\tneurons\\t(i.e.,\\nwithout\\tany\\tactivation\\tfunction)\\ton\\ttop\\tof\\teach\\toutput\\t(but\\tit\\tdoes\\tnot\\taffect\\tthe\\tcell\\tstate).\\tAll\\tthese\\tfully\\nconnected\\tlayers\\tshare\\tthe\\tsame\\t(trainable)\\tweights\\tand\\tbias\\tterms.\\tThe\\tresulting\\tRNN\\tis\\trepresented\\tin', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 482}), Document(page_content='Figure\\t14-8\\n.\\nFigure\\t14-8.\\t\\nRNN\\tcells\\tusing\\toutput\\tprojections\\nWrapping\\ta\\tcell\\tis\\tquite\\teasy.\\t\\nLet’s\\ttweak\\tthe\\tpreceding\\tcode\\tby\\twrapping\\tthe\\t\\nBasicRNNCell\\n\\tinto\\tan\\nOutputProjectionWrapper\\n:\\ncell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nOutputProjectionWrapper\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n),\\n\\t\\t\\t\\t\\noutput_size\\n=\\nn_outputs\\n)\\nSo\\tfar,\\tso\\tgood.\\tNow\\twe\\tneed\\tto\\tdefine\\tthe\\t\\ncost\\tfunction.\\tWe\\twill\\tuse\\tthe\\tMean\\tSquared\\tError\\t(MSE),\\tas\\nwe\\tdid\\tin\\tprevious\\tregression\\ttasks.\\tNext\\twe\\twill\\tcreate\\tan\\tAdam\\toptimizer,\\tthe\\ttraining\\top,\\tand\\tthe\\nvariable\\tinitialization\\top,\\tas\\tusual:\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.001\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\ny\\n))\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nNow\\ton\\tto\\tthe\\texecution\\tphase:\\nn_iterations\\n\\t\\n=\\n\\t\\n1500\\nbatch_size\\n\\t\\n=\\n\\t\\n50\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\t\\n#\\tfetch\\tthe\\tnext\\ttraining\\tbatch\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\niteration\\n\\t\\n%\\n\\t\\n100\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmse\\n\\t\\n=\\n\\t\\nloss\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nprint\\n(\\niteration\\n,\\n\\t\\n\"\\n\\\\t\\nMSE:\"\\n,\\n\\t\\nmse\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 483}), Document(page_content='The\\tprogram’s\\toutput\\tshould\\tlook\\tlike\\tthis:\\n0\\t\\t\\t\\t\\t\\t\\tMSE:\\t13.6543\\n100\\t\\t\\t\\t\\tMSE:\\t0.538476\\n200\\t\\t\\t\\t\\tMSE:\\t0.168532\\n300\\t\\t\\t\\t\\tMSE:\\t0.0879579\\n400\\t\\t\\t\\t\\tMSE:\\t0.0633425\\n[...]\\nOnce\\tthe\\tmodel\\tis\\ttrained,\\tyou\\tcan\\tmake\\tpredictions:\\nX_new\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\t\\n#\\tNew\\tsequences\\ny_pred\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\noutputs\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_new\\n})\\nFigure\\t14-9\\n\\tshows\\tthe\\tpredicted\\tsequence\\tfor\\tthe\\tinstance\\twe\\tlooked\\tat\\tearlier\\t(in\\t\\nFigure\\t14-7\\n),\\tafter\\tjust\\n1,000\\ttraining\\titerations.\\nFigure\\t14-9.\\t\\nTime\\tseries\\tprediction\\nAlthough\\tusing\\tan\\t\\nOutputProjectionWrapper\\n\\t\\nis\\tthe\\tsimplest\\tsolution\\tto\\treduce\\tthe\\tdimensionality\\tof\\nthe\\tRNN’s\\toutput\\tsequences\\tdown\\tto\\tjust\\tone\\tvalue\\tper\\ttime\\tstep\\t(per\\tinstance),\\tit\\tis\\tnot\\tthe\\tmost\\nefficient.\\tThere\\tis\\ta\\ttrickier\\tbut\\tmore\\tefficient\\tsolution:\\tyou\\tcan\\treshape\\tthe\\tRNN\\toutputs\\tfrom\\n[batch_size,\\tn_steps,\\tn_neurons]\\n\\tto\\t\\n[batch_size\\t*\\tn_steps,\\tn_neurons]\\n,\\tthen\\tapply\\ta\\tsingle\\nfully\\tconnected\\tlayer\\twith\\tthe\\tappropriate\\toutput\\tsize\\t(in\\tour\\tcase\\tjust\\t1),\\twhich\\twill\\tresult\\tin\\tan\\toutput\\ntensor\\tof\\tshape\\t\\n[batch_size\\t*\\tn_steps,\\tn_outputs]\\n,\\tand\\tthen\\treshape\\tthis\\ttensor\\tto\\t\\n[batch_size,\\nn_steps,\\tn_outputs]\\n.\\tThese\\toperations\\tare\\t\\nrepresented\\tin\\t\\nFigure\\t14-10\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 484}), Document(page_content='Figure\\t14-10.\\t\\nStack\\tall\\tthe\\toutputs,\\tapply\\tthe\\tprojection,\\t\\nthen\\tunstack\\tthe\\tresult\\nTo\\timplement\\tthis\\tsolution,\\twe\\tfirst\\trevert\\tto\\ta\\tbasic\\tcell,\\twithout\\t\\nthe\\t\\nOutputProjectionWrapper\\n:\\ncell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\nrnn_outputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\ncell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nThen\\twe\\tstack\\tall\\tthe\\toutputs\\tusing\\tthe\\t\\nreshape()\\n\\t\\noperation,\\tapply\\tthe\\tfully\\tconnected\\tlinear\\tlayer\\n(without\\tusing\\tany\\tactivation\\tfunction;\\tthis\\tis\\tjust\\ta\\tprojection),\\tand\\tfinally\\tunstack\\tall\\tthe\\toutputs,\\tagain\\nusing\\t\\nreshape()\\n:\\nstacked_rnn_outputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nreshape\\n(\\nrnn_outputs\\n,\\n\\t\\n[\\n-\\n1\\n,\\n\\t\\nn_neurons\\n])\\nstacked_outputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nstacked_rnn_outputs\\n,\\n\\t\\nn_outputs\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nreshape\\n(\\nstacked_outputs\\n,\\n\\t\\n[\\n-\\n1\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_outputs\\n])\\nThe\\trest\\tof\\tthe\\tcode\\tis\\tthe\\tsame\\tas\\tearlier.\\tThis\\tcan\\tprovide\\ta\\tsignificant\\tspeed\\tboost\\tsince\\tthere\\tis\\tjust\\none\\tfully\\tconnected\\tlayer\\tinstead\\tof\\tone\\tper\\t\\ntime\\tstep.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 485}), Document(page_content='Creative\\tRNN\\nNow\\tthat\\twe\\thave\\ta\\t\\nmodel\\tthat\\tcan\\tpredict\\tthe\\tfuture,\\twe\\tcan\\tuse\\tit\\tto\\tgenerate\\tsome\\tcreative\\tsequences,\\nas\\texplained\\tat\\tthe\\tbeginning\\tof\\tthe\\tchapter.\\tAll\\twe\\tneed\\tis\\tto\\tprovide\\tit\\ta\\tseed\\tsequence\\tcontaining\\nn_steps\\n\\tvalues\\t(e.g.,\\tfull\\tof\\tzeros),\\tuse\\tthe\\tmodel\\tto\\tpredict\\tthe\\tnext\\tvalue,\\tappend\\tthis\\tpredicted\\tvalue\\nto\\tthe\\tsequence,\\tfeed\\tthe\\tlast\\t\\nn_steps\\n\\tvalues\\tto\\tthe\\tmodel\\tto\\tpredict\\tthe\\tnext\\tvalue,\\tand\\tso\\ton.\\tThis\\nprocess\\tgenerates\\ta\\tnew\\tsequence\\tthat\\thas\\tsome\\tresemblance\\tto\\tthe\\toriginal\\ttime\\tseries\\t(see\\t\\nFigure\\t14-\\n11\\n).\\nsequence\\n\\t\\n=\\n\\t\\n[\\n0.\\n]\\n\\t\\n*\\n\\t\\nn_steps\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\n300\\n):\\n\\t\\t\\t\\t\\nX_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n(\\nsequence\\n[\\n-\\nn_steps\\n:])\\n.\\nreshape\\n(\\n1\\n,\\n\\t\\nn_steps\\n,\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\ny_pred\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\noutputs\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n})\\n\\t\\t\\t\\t\\nsequence\\n.\\nappend\\n(\\ny_pred\\n[\\n0\\n,\\n\\t\\n-\\n1\\n,\\n\\t\\n0\\n])\\nFigure\\t14-11.\\t\\nCreative\\tsequences,\\tseeded\\twith\\tzeros\\t(left)\\tor\\twith\\tan\\tinstance\\t(right)\\nNow\\tyou\\tcan\\ttry\\tto\\tfeed\\tall\\tyour\\tJohn\\tLennon\\talbums\\tto\\tan\\tRNN\\tand\\tsee\\tif\\tit\\tcan\\tgenerate\\tthe\\tnext\\n“Imagine.”\\tHowever,\\tyou\\twill\\tprobably\\tneed\\ta\\tmuch\\tmore\\tpowerful\\tRNN,\\twith\\tmore\\tneurons,\\tand\\talso\\nmuch\\tdeeper.\\tLet’s\\tlook\\tat\\tdeep\\tRNNs\\t\\nnow.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 486}), Document(page_content='Deep\\tRNNs\\nIt\\t\\nis\\tquite\\tcommon\\tto\\tstack\\tmultiple\\tlayers\\tof\\tcells,\\tas\\tshown\\tin\\t\\nFigure\\t14-12\\n.\\tThis\\tgives\\tyou\\ta\\t\\ndeep\\nRNN\\n.\\nTo\\timplement\\ta\\tdeep\\tRNN\\tin\\tTensorFlow,\\t\\nyou\\tcan\\tcreate\\tseveral\\tcells\\tand\\tstack\\tthem\\tinto\\ta\\nMultiRNNCell\\n.\\tIn\\tthe\\tfollowing\\tcode\\twe\\tstack\\tthree\\tidentical\\tcells\\t(but\\tyou\\tcould\\tvery\\twell\\tuse\\tvarious\\nkinds\\tof\\tcells\\twith\\ta\\tdifferent\\tnumber\\tof\\tneurons):\\nn_neurons\\n\\t\\n=\\n\\t\\n100\\nn_layers\\n\\t\\n=\\n\\t\\n3\\nlayers\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nlayer\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_layers\\n)]\\nmulti_layer_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nMultiRNNCell\\n(\\nlayers\\n)\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nmulti_layer_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nFigure\\t14-12.\\t\\nDeep\\tRNN\\t(left),\\tunrolled\\tthrough\\ttime\\t(right)\\nThat’s\\tall\\tthere\\tis\\tto\\tit!\\tThe\\t\\nstates\\n\\tvariable\\tis\\ta\\ttuple\\tcontaining\\tone\\ttensor\\tper\\tlayer,\\teach\\trepresenting\\nthe\\tfinal\\tstate\\tof\\tthat\\tlayer’s\\tcell\\t(with\\tshape\\t\\n[batch_size,\\tn_neurons]\\n).\\tIf\\tyou\\tset\\nstate_is_tuple=False\\n\\twhen\\tcreating\\tthe\\t\\nMultiRNNCell\\n,\\tthen\\t\\nstates\\n\\tbecomes\\ta\\tsingle\\ttensor\\ncontaining\\tthe\\tstates\\tfrom\\tevery\\tlayer,\\tconcatenated\\talong\\tthe\\tcolumn\\taxis\\t(i.e.,\\tits\\tshape\\tis\\n[batch_size,\\tn_layers\\t*\\tn_neurons]\\n).\\tNote\\tthat\\tbefore\\tTensorFlow\\t0.11.0,\\tthis\\tbehavior\\twas\\tthe\\ndefault.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 487}), Document(page_content='Distributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\nChapter\\t12\\n\\tpointed\\t\\nout\\tthat\\twe\\tcan\\tefficiently\\tdistribute\\tdeep\\tRNNs\\tacross\\tmultiple\\tGPUs\\tby\\tpinning\\neach\\tlayer\\tto\\ta\\tdifferent\\tGPU\\t(see\\t\\nFigure\\t12-16\\n).\\tHowever,\\tif\\tyou\\ttry\\tto\\tcreate\\teach\\tcell\\tin\\ta\\tdifferent\\ndevice()\\n\\tblock,\\tit\\twill\\t\\nnot\\twork:\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/gpu:0\"\\n):\\n\\t\\t\\n#\\tBAD!\\tThis\\tis\\tignored.\\n\\t\\t\\t\\t\\nlayer1\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/gpu:1\"\\n):\\n\\t\\t\\n#\\tBAD!\\tIgnored\\tagain.\\n\\t\\t\\t\\t\\nlayer2\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\nThis\\tfails\\tbecause\\ta\\t\\nBasicRNNCell\\n\\tis\\t\\na\\tcell\\tfactory,\\tnot\\ta\\tcell\\t\\nper\\tse\\n\\t(as\\tmentioned\\tearlier);\\tno\\tcells\\tget\\ncreated\\twhen\\tyou\\tcreate\\tthe\\tfactory,\\tand\\tthus\\tno\\tvariables\\tdo\\teither.\\tThe\\tdevice\\tblock\\tis\\tsimply\\tignored.\\nThe\\tcells\\tactually\\tget\\tcreated\\tlater.\\tWhen\\tyou\\tcall\\t\\ndynamic_rnn()\\n,\\t\\nit\\tcalls\\tthe\\t\\nMultiRNNCell\\n,\\t\\nwhich\\ncalls\\teach\\tindividual\\t\\nBasicRNNCell\\n,\\t\\nwhich\\tcreate\\tthe\\tactual\\tcells\\t(including\\ttheir\\tvariables).\\nUnfortunately,\\t\\nnone\\tof\\tthese\\tclasses\\tprovide\\tany\\tway\\tto\\tcontrol\\tthe\\tdevices\\ton\\twhich\\tthe\\tvariables\\tget\\ncreated.\\tIf\\tyou\\ttry\\tto\\tput\\tthe\\t\\ndynamic_rnn()\\n\\tcall\\twithin\\ta\\tdevice\\tblock,\\tthe\\twhole\\tRNN\\tgets\\tpinned\\tto\\ta\\nsingle\\tdevice.\\tSo\\tare\\tyou\\tstuck?\\tFortunately\\tnot!\\tThe\\ttrick\\tis\\tto\\tcreate\\tyour\\town\\tcell\\twrapper:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nclass\\n\\t\\nDeviceCellWrapper\\n(\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nRNNCell\\n):\\n\\t\\t\\ndef\\n\\t\\n__init__\\n(\\nself\\n,\\n\\t\\ndevice\\n,\\n\\t\\ncell\\n):\\n\\t\\t\\t\\t\\nself\\n.\\n_cell\\n\\t\\n=\\n\\t\\ncell\\n\\t\\t\\t\\t\\nself\\n.\\n_device\\n\\t\\n=\\n\\t\\ndevice\\n\\t\\t\\n@property\\n\\t\\t\\ndef\\n\\t\\nstate_size\\n(\\nself\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nself\\n.\\n_cell\\n.\\nstate_size\\n\\t\\t\\n@property\\n\\t\\t\\ndef\\n\\t\\noutput_size\\n(\\nself\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nself\\n.\\n_cell\\n.\\noutput_size\\n\\t\\t\\ndef\\n\\t\\n__call__\\n(\\nself\\n,\\n\\t\\ninputs\\n,\\n\\t\\nstate\\n,\\n\\t\\nscope\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\nself\\n.\\n_device\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nself\\n.\\n_cell\\n(\\ninputs\\n,\\n\\t\\nstate\\n,\\n\\t\\nscope\\n)\\nThis\\twrapper\\tsimply\\tproxies\\tevery\\tmethod\\tcall\\tto\\tanother\\tcell,\\texcept\\tit\\twraps\\tthe\\t\\n__call__()\\n\\tfunction\\nwithin\\ta\\tdevice\\tblock.\\n2\\n\\tNow\\tyou\\tcan\\tdistribute\\teach\\tlayer\\ton\\ta\\tdifferent\\tGPU:\\ndevices\\n\\t\\n=\\n\\t\\n[\\n\"/gpu:0\"\\n,\\n\\t\\n\"/gpu:1\"\\n,\\n\\t\\n\"/gpu:2\"\\n]\\ncells\\n\\t\\n=\\n\\t\\n[\\nDeviceCellWrapper\\n(\\ndev\\n,\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ndev\\n\\t\\nin\\n\\t\\ndevices\\n]\\nmulti_layer_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nMultiRNNCell\\n(\\ncells\\n)\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nmulti_layer_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nWARNING\\nDo\\tnot\\tset\\t\\nstate_is_tuple=False\\n,\\t\\nor\\tthe\\t\\nMultiRNNCell\\n\\twill\\tconcatenate\\tall\\tthe\\tcell\\tstates\\tinto\\ta\\tsingle\\ttensor,\\ton\\ta\\tsingle\\tGPU.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 488}), Document(page_content='Applying\\tDropout\\nIf\\t\\nyou\\tbuild\\ta\\tvery\\tdeep\\tRNN,\\tit\\tmay\\tend\\tup\\toverfitting\\tthe\\ttraining\\tset.\\t\\nTo\\tprevent\\tthat,\\ta\\tcommon\\ntechnique\\tis\\tto\\tapply\\tdropout\\t(introduced\\tin\\t\\nChapter\\t11\\n).\\tYou\\tcan\\tsimply\\tadd\\ta\\tdropout\\tlayer\\tbefore\\tor\\nafter\\tthe\\tRNN\\tas\\tusual,\\tbut\\tif\\tyou\\talso\\twant\\tto\\tapply\\tdropout\\tbetween\\tthe\\tRNN\\tlayers,\\tyou\\tneed\\tto\\tuse\\ta\\nDropoutWrapper\\n.\\tThe\\tfollowing\\tcode\\tapplies\\tdropout\\tto\\tthe\\tinputs\\tof\\teach\\tlayer\\tin\\tthe\\tRNN,\\tdropping\\neach\\tinput\\twith\\ta\\t50%\\tprobability:\\nkeep_prob\\n\\t\\n=\\n\\t\\n0.5\\ncells\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nlayer\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_layers\\n)]\\ncells_drop\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nDropoutWrapper\\n(\\ncell\\n,\\n\\t\\ninput_keep_prob\\n=\\nkeep_prob\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ncell\\n\\t\\nin\\n\\t\\ncells\\n]\\nmulti_layer_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nMultiRNNCell\\n(\\ncells_drop\\n)\\nrnn_outputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nmulti_layer_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nNote\\tthat\\tit\\tis\\talso\\tpossible\\tto\\tapply\\tdropout\\tto\\tthe\\toutputs\\tby\\t\\nsetting\\t\\noutput_keep_prob\\n.\\nThe\\tmain\\tproblem\\twith\\tthis\\tcode\\tis\\tthat\\tit\\twill\\tapply\\tdropout\\tnot\\tonly\\tduring\\ttraining\\tbut\\talso\\tduring\\ntesting,\\twhich\\tis\\tnot\\twhat\\tyou\\twant\\t(recall\\tthat\\tdropout\\tshould\\tbe\\tapplied\\tonly\\tduring\\ttraining).\\nUnfortunately,\\tthe\\t\\nDropoutWrapper\\n\\t\\ndoes\\tnot\\tsupport\\ta\\t\\ntraining\\n\\t\\nplaceholder\\t(yet?),\\tso\\tyou\\tmust\\teither\\nwrite\\tyour\\town\\tdropout\\twrapper\\tclass,\\tor\\thave\\ttwo\\tdifferent\\tgraphs:\\tone\\tfor\\ttraining,\\tand\\tthe\\tother\\tfor\\ntesting.\\tThe\\tsecond\\toption\\t\\nlooks\\tlike\\tthis:\\nimport\\n\\t\\nsys\\ntraining\\n\\t\\n=\\n\\t\\n(\\nsys\\n.\\nargv\\n[\\n-\\n1\\n]\\n\\t\\n==\\n\\t\\n\"train\"\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n])\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_outputs\\n])\\ncells\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nlayer\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_layers\\n)]\\nif\\n\\t\\ntraining\\n:\\n\\t\\t\\t\\t\\ncells\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nDropoutWrapper\\n(\\ncell\\n,\\n\\t\\ninput_keep_prob\\n=\\nkeep_prob\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ncell\\n\\t\\nin\\n\\t\\ncells\\n]\\nmulti_layer_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nMultiRNNCell\\n(\\ncells\\n)\\nrnn_outputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nmulti_layer_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\n[\\n...\\n]\\n\\t\\n#\\tbuild\\tthe\\trest\\tof\\tthe\\tgraph\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nif\\n\\t\\ntraining\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\ttrain\\tthe\\tmodel\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model.ckpt\"\\n)\\n\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model.ckpt\"\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tuse\\tthe\\tmodel\\nWith\\tthat\\tyou\\tshould\\tbe\\table\\tto\\ttrain\\tall\\tsorts\\tof\\tRNNs!\\tUnfortunately,\\tif\\tyou\\twant\\tto\\ttrain\\tan\\tRNN\\ton\\tlong\\nsequences,\\tthings\\twill\\tget\\ta\\tbit\\tharder.\\tLet’s\\tsee\\twhy\\tand\\twhat\\tyou\\tcan\\tdo\\tabout\\tit.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 489}), Document(page_content='The\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps\\nTo\\ttrain\\tan\\tRNN\\ton\\t\\nlong\\tsequences,\\tyou\\twill\\tneed\\tto\\trun\\tit\\tover\\tmany\\ttime\\tsteps,\\tmaking\\tthe\\tunrolled\\nRNN\\ta\\tvery\\tdeep\\tnetwork.\\tJust\\tlike\\tany\\tdeep\\tneural\\tnetwork\\tit\\tmay\\tsuffer\\tfrom\\tthe\\t\\nvanishing/exploding\\ngradients\\tproblem\\t(discussed\\tin\\t\\nChapter\\t11\\n)\\tand\\ttake\\tforever\\tto\\ttrain.\\tMany\\tof\\tthe\\ttricks\\twe\\tdiscussed\\tto\\nalleviate\\tthis\\tproblem\\tcan\\tbe\\tused\\tfor\\tdeep\\tunrolled\\tRNNs\\tas\\twell:\\tgood\\tparameter\\tinitialization,\\nnonsaturating\\tactivation\\tfunctions\\t(e.g.,\\tReLU),\\tBatch\\tNormalization,\\tGradient\\tClipping,\\tand\\tfaster\\noptimizers.\\tHowever,\\tif\\tthe\\tRNN\\tneeds\\tto\\thandle\\teven\\tmoderately\\tlong\\tsequences\\t(e.g.,\\t100\\tinputs),\\tthen\\ntraining\\twill\\tstill\\tbe\\tvery\\tslow.\\nThe\\tsimplest\\tand\\tmost\\tcommon\\tsolution\\tto\\tthis\\tproblem\\tis\\tto\\tunroll\\tthe\\tRNN\\tonly\\tover\\ta\\tlimited\\tnumber\\nof\\ttime\\tsteps\\tduring\\ttraining.\\tThis\\tis\\tcalled\\t\\ntruncated\\tbackpropagation\\tthrough\\ttime\\n.\\t\\nIn\\t\\nTensorFlow\\tyou\\ncan\\timplement\\tit\\tsimply\\tby\\ttruncating\\tthe\\tinput\\tsequences.\\tFor\\texample,\\tin\\tthe\\ttime\\tseries\\tprediction\\nproblem,\\tyou\\twould\\tsimply\\treduce\\t\\nn_steps\\n\\tduring\\ttraining.\\tThe\\tproblem,\\tof\\tcourse,\\tis\\tthat\\tthe\\tmodel\\nwill\\tnot\\tbe\\table\\tto\\tlearn\\tlong-term\\tpatterns.\\tOne\\tworkaround\\tcould\\tbe\\tto\\tmake\\tsure\\tthat\\tthese\\tshortened\\nsequences\\tcontain\\tboth\\told\\tand\\trecent\\tdata,\\tso\\tthat\\tthe\\tmodel\\tcan\\tlearn\\tto\\tuse\\tboth\\t(e.g.,\\tthe\\tsequence\\ncould\\tcontain\\tmonthly\\tdata\\tfor\\tthe\\tlast\\tfive\\tmonths,\\tthen\\tweekly\\tdata\\tfor\\tthe\\tlast\\tfive\\tweeks,\\tthen\\tdaily\\ndata\\tover\\tthe\\tlast\\tfive\\tdays).\\tBut\\tthis\\tworkaround\\thas\\tits\\tlimits:\\twhat\\tif\\tfine-grained\\tdata\\tfrom\\tlast\\tyear\\tis\\nactually\\tuseful?\\tWhat\\tif\\tthere\\twas\\ta\\tbrief\\tbut\\tsignificant\\tevent\\tthat\\tabsolutely\\tmust\\tbe\\ttaken\\tinto\\taccount,\\neven\\tyears\\tlater\\t(e.g.,\\tthe\\tresult\\tof\\tan\\telection)?\\nBesides\\tthe\\tlong\\ttraining\\ttime,\\ta\\tsecond\\tproblem\\tfaced\\tby\\tlong-running\\tRNNs\\tis\\tthe\\tfact\\tthat\\tthe\\tmemory\\nof\\tthe\\tfirst\\tinputs\\tgradually\\tfades\\taway.\\tIndeed,\\tdue\\tto\\tthe\\ttransformations\\tthat\\tthe\\tdata\\tgoes\\tthrough\\twhen\\ntraversing\\tan\\tRNN,\\tsome\\tinformation\\tis\\tlost\\tafter\\teach\\ttime\\tstep.\\tAfter\\ta\\twhile,\\tthe\\tRNN’s\\tstate\\tcontains\\nvirtually\\tno\\ttrace\\tof\\tthe\\tfirst\\tinputs.\\tThis\\tcan\\tbe\\ta\\tshowstopper.\\tFor\\texample,\\tsay\\tyou\\twant\\tto\\tperform\\nsentiment\\tanalysis\\ton\\ta\\tlong\\treview\\tthat\\tstarts\\twith\\tthe\\tfour\\twords\\t“I\\tloved\\tthis\\tmovie,”\\tbut\\tthe\\trest\\tof\\tthe\\nreview\\tlists\\tthe\\tmany\\tthings\\tthat\\tcould\\thave\\tmade\\tthe\\tmovie\\teven\\tbetter.\\tIf\\tthe\\tRNN\\tgradually\\tforgets\\tthe\\nfirst\\tfour\\twords,\\tit\\twill\\tcompletely\\tmisinterpret\\tthe\\treview.\\tTo\\tsolve\\tthis\\tproblem,\\tvarious\\ttypes\\tof\\tcells\\nwith\\tlong-term\\tmemory\\thave\\tbeen\\tintroduced.\\tThey\\thave\\tproved\\tso\\tsuccessful\\tthat\\tthe\\tbasic\\tcells\\tare\\tnot\\nmuch\\tused\\tanymore.\\tLet’s\\t\\nfirst\\tlook\\tat\\tthe\\tmost\\tpopular\\tof\\tthese\\tlong\\tmemory\\tcells:\\tthe\\tLSTM\\tcell.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 490}), Document(page_content='LSTM\\tCell\\nThe\\t\\nLong\\tShort-Term\\tMemory\\n\\t(LSTM)\\t\\ncell\\twas\\t\\nproposed\\tin\\t1997\\n3\\n\\tby\\tSepp\\tHochreiter\\tand\\tJürgen\\nSchmidhuber,\\tand\\tit\\twas\\tgradually\\timproved\\tover\\tthe\\tyears\\tby\\tseveral\\tresearchers,\\tsuch\\tas\\tAlex\\tGraves,\\nHaşim\\tSak\\n,\\n4\\n\\t\\nWojciech\\tZaremba\\n,\\n5\\n\\tand\\tmany\\tmore.\\tIf\\tyou\\tconsider\\tthe\\tLSTM\\tcell\\tas\\ta\\tblack\\tbox,\\tit\\tcan\\tbe\\nused\\tvery\\tmuch\\tlike\\ta\\tbasic\\tcell,\\texcept\\tit\\twill\\tperform\\tmuch\\tbetter;\\ttraining\\twill\\tconverge\\tfaster\\tand\\tit\\nwill\\tdetect\\tlong-term\\tdependencies\\tin\\tthe\\tdata.\\tIn\\tTensorFlow,\\tyou\\tcan\\tsimply\\tuse\\ta\\t\\nBasicLSTMCell\\ninstead\\tof\\ta\\t\\nBasicRNNCell\\n:\\nlstm_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicLSTMCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\nLSTM\\tcells\\tmanage\\ttwo\\tstate\\tvectors,\\tand\\tfor\\tperformance\\treasons\\tthey\\tare\\tkept\\t\\nseparate\\n\\tby\\tdefault.\\tYou\\ncan\\tchange\\tthis\\tdefault\\tbehavior\\tby\\tsetting\\t\\nstate_is_tuple=False\\n\\t\\nwhen\\tcreating\\tthe\\t\\nBasicLSTMCell\\n.\\nSo\\thow\\tdoes\\tan\\tLSTM\\tcell\\twork?\\tThe\\tarchitecture\\tof\\ta\\tbasic\\tLSTM\\tcell\\tis\\tshown\\tin\\t\\nFigure\\t14-13\\n.\\nFigure\\t14-13.\\t\\nLSTM\\tcell\\nIf\\tyou\\tdon’t\\tlook\\tat\\twhat’s\\tinside\\tthe\\tbox,\\tthe\\tLSTM\\tcell\\tlooks\\texactly\\tlike\\ta\\tregular\\tcell,\\texcept\\tthat\\tits\\nstate\\tis\\tsplit\\tin\\ttwo\\tvectors:\\t\\nh\\n(\\nt\\n)\\n\\tand\\t\\nc\\n(\\nt\\n)\\n\\t(“c”\\tstands\\tfor\\t“cell”).\\tYou\\tcan\\tthink\\tof\\t\\nh\\n(\\nt\\n)\\n\\tas\\tthe\\tshort-term\\tstate\\nand\\t\\nc\\n(\\nt\\n)\\n\\tas\\tthe\\tlong-term\\tstate.\\nNow\\tlet’s\\topen\\tthe\\tbox!\\tThe\\tkey\\tidea\\tis\\tthat\\tthe\\tnetwork\\tcan\\tlearn\\twhat\\tto\\tstore\\tin\\tthe\\tlong-term\\tstate,\\nwhat\\tto\\tthrow\\taway,\\tand\\twhat\\tto\\tread\\tfrom\\tit.\\tAs\\tthe\\tlong-term\\tstate\\t\\nc\\n(\\nt\\n–1)\\n\\ttraverses\\tthe\\tnetwork\\tfrom\\tleft\\nto\\tright,\\tyou\\tcan\\tsee\\tthat\\tit\\tfirst\\tgoes\\tthrough\\t\\na\\t\\nforget\\tgate\\n,\\tdropping\\tsome\\tmemories,\\tand\\tthen\\tit\\tadds\\nsome\\tnew\\tmemories\\tvia\\tthe\\taddition\\toperation\\t(which\\tadds\\tthe\\tmemories\\tthat\\twere\\tselected\\tby\\tan\\t\\ninput\\ngate\\n).\\tThe\\tresult\\t\\nc\\n(\\nt\\n)\\n\\tis\\tsent\\tstraight\\tout,\\twithout\\tany\\tfurther\\ttransformation.\\tSo,\\tat\\teach\\ttime\\tstep,\\tsome\\nmemories\\tare\\tdropped\\tand\\tsome\\tmemories\\tare\\tadded.\\tMoreover,\\tafter\\tthe\\taddition\\toperation,\\tthe\\tlong-\\nterm\\tstate\\tis\\tcopied\\tand\\tpassed\\tthrough\\tthe\\ttanh\\tfunction,\\tand\\tthen\\tthe\\tresult\\tis\\tfiltered\\tby\\t\\nthe\\t\\noutput\\tgate\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 491}), Document(page_content='This\\tproduces\\tthe\\tshort-term\\tstate\\t\\nh\\n(\\nt\\n)\\n\\t(which\\tis\\tequal\\tto\\tthe\\tcell’s\\toutput\\tfor\\tthis\\ttime\\tstep\\t\\ny\\n(\\nt\\n)\\n).\\tNow\\tlet’s\\nlook\\tat\\twhere\\tnew\\tmemories\\tcome\\tfrom\\tand\\thow\\tthe\\tgates\\twork.\\nFirst,\\tthe\\tcurrent\\tinput\\tvector\\t\\nx\\n(\\nt\\n)\\n\\tand\\tthe\\tprevious\\tshort-term\\tstate\\t\\nh\\n(\\nt\\n–1)\\n\\tare\\tfed\\tto\\tfour\\tdifferent\\tfully\\nconnected\\tlayers.\\tThey\\tall\\tserve\\ta\\tdifferent\\tpurpose:\\nThe\\tmain\\tlayer\\tis\\tthe\\tone\\tthat\\toutputs\\t\\ng\\n(\\nt\\n)\\n.\\tIt\\thas\\tthe\\tusual\\trole\\tof\\tanalyzing\\tthe\\tcurrent\\tinputs\\t\\nx\\n(\\nt\\n)\\n\\tand\\nthe\\tprevious\\t(short-term)\\tstate\\t\\nh\\n(\\nt\\n–1)\\n.\\tIn\\ta\\tbasic\\tcell,\\tthere\\tis\\tnothing\\telse\\tthan\\tthis\\tlayer,\\tand\\tits\\noutput\\tgoes\\tstraight\\tout\\tto\\t\\ny\\n(\\nt\\n)\\n\\tand\\t\\nh\\n(\\nt\\n)\\n.\\tIn\\tcontrast,\\tin\\tan\\tLSTM\\tcell\\tthis\\tlayer’s\\toutput\\tdoes\\tnot\\tgo\\nstraight\\tout,\\tbut\\tinstead\\tit\\tis\\tpartially\\tstored\\tin\\tthe\\tlong-term\\tstate.\\nThe\\tthree\\tother\\tlayers\\tare\\t\\ngate\\tcontrollers\\n.\\tSince\\tthey\\tuse\\tthe\\tlogistic\\tactivation\\tfunction,\\ttheir\\noutputs\\trange\\tfrom\\t0\\tto\\t1.\\tAs\\tyou\\tcan\\tsee,\\ttheir\\toutputs\\tare\\tfed\\tto\\telement-wise\\tmultiplication\\noperations,\\tso\\tif\\tthey\\toutput\\t0s,\\tthey\\tclose\\tthe\\tgate,\\tand\\tif\\tthey\\toutput\\t1s,\\tthey\\topen\\tit.\\tSpecifically:\\nThe\\t\\nforget\\tgate\\n\\t(controlled\\tby\\t\\nf\\n(\\nt\\n)\\n)\\tcontrols\\twhich\\tparts\\tof\\tthe\\tlong-term\\tstate\\tshould\\tbe\\terased.\\nThe\\t\\ninput\\tgate\\n\\t(controlled\\tby\\t\\ni\\n(\\nt\\n)\\n)\\tcontrols\\twhich\\tparts\\tof\\t\\ng\\n(\\nt\\n)\\n\\tshould\\tbe\\tadded\\tto\\tthe\\tlong-term\\nstate\\t(this\\tis\\twhy\\twe\\tsaid\\tit\\twas\\tonly\\t“partially\\tstored”).\\nFinally,\\tthe\\t\\noutput\\tgate\\n\\t(controlled\\tby\\t\\no\\n(\\nt\\n)\\n)\\tcontrols\\twhich\\tparts\\tof\\tthe\\tlong-term\\tstate\\tshould\\tbe\\nread\\tand\\toutput\\tat\\tthis\\ttime\\tstep\\t(both\\tto\\t\\nh\\n(\\nt\\n)\\n)\\tand\\t\\ny\\n(\\nt\\n)\\n.\\nIn\\tshort,\\tan\\tLSTM\\tcell\\tcan\\tlearn\\tto\\trecognize\\tan\\timportant\\tinput\\t(that’s\\tthe\\trole\\tof\\tthe\\tinput\\tgate),\\tstore\\tit\\nin\\tthe\\tlong-term\\tstate,\\tlearn\\tto\\tpreserve\\tit\\tfor\\tas\\tlong\\tas\\tit\\tis\\tneeded\\t(that’s\\tthe\\trole\\tof\\tthe\\tforget\\tgate),\\tand\\nlearn\\tto\\textract\\tit\\twhenever\\tit\\tis\\tneeded.\\tThis\\texplains\\twhy\\tthey\\thave\\tbeen\\tamazingly\\tsuccessful\\tat\\ncapturing\\tlong-term\\tpatterns\\tin\\ttime\\tseries,\\tlong\\ttexts,\\taudio\\trecordings,\\tand\\tmore.\\nEquation\\t14-3\\n\\tsummarizes\\thow\\tto\\tcompute\\tthe\\tcell’s\\tlong-term\\tstate,\\tits\\tshort-term\\tstate,\\tand\\tits\\toutput\\tat\\neach\\ttime\\tstep\\tfor\\ta\\tsingle\\tinstance\\t(the\\tequations\\tfor\\ta\\twhole\\tmini-batch\\tare\\tvery\\tsimilar).\\nEquation\\t14-3.\\t\\nLSTM\\tcomputations\\nW\\nxi\\n,\\t\\nW\\nxf\\n,\\t\\nW\\nxo\\n,\\t\\nW\\nxg\\n\\tare\\tthe\\tweight\\tmatrices\\tof\\teach\\tof\\tthe\\tfour\\tlayers\\tfor\\ttheir\\tconnection\\tto\\tthe', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 492}), Document(page_content='input\\tvector\\t\\nx\\n(\\nt\\n)\\n.\\nW\\nhi\\n,\\t\\nW\\nhf\\n,\\t\\nW\\nho\\n,\\tand\\t\\nW\\nhg\\n\\tare\\tthe\\tweight\\tmatrices\\tof\\teach\\tof\\tthe\\tfour\\tlayers\\tfor\\ttheir\\tconnection\\tto\\tthe\\nprevious\\tshort-term\\tstate\\t\\nh\\n(\\nt\\n–1)\\n.\\nb\\ni\\n,\\t\\nb\\nf\\n,\\t\\nb\\no\\n,\\tand\\t\\nb\\ng\\n\\tare\\tthe\\tbias\\tterms\\tfor\\teach\\tof\\tthe\\tfour\\tlayers.\\tNote\\tthat\\tTensorFlow\\tinitializes\\t\\nb\\nf\\n\\tto\\na\\tvector\\tfull\\tof\\t1s\\tinstead\\tof\\t0s.\\tThis\\tprevents\\tforgetting\\teverything\\tat\\tthe\\tbeginning\\tof\\ttraining.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 493}), Document(page_content='Peephole\\tConnections\\nIn\\ta\\tbasic\\tLSTM\\tcell,\\t\\nthe\\tgate\\tcontrollers\\tcan\\tlook\\tonly\\tat\\tthe\\tinput\\t\\nx\\n(\\nt\\n)\\n\\tand\\tthe\\tprevious\\tshort-term\\tstate\\nh\\n(\\nt\\n–1)\\n.\\tIt\\tmay\\tbe\\ta\\tgood\\tidea\\tto\\tgive\\tthem\\ta\\tbit\\tmore\\tcontext\\tby\\tletting\\tthem\\tpeek\\tat\\tthe\\tlong-term\\tstate\\tas\\nwell.\\tThis\\tidea\\twas\\t\\nproposed\\tby\\tFelix\\tGers\\tand\\tJürgen\\tSchmidhuber\\tin\\t2000\\n.\\n6\\n\\tThey\\tproposed\\tan\\tLSTM\\nvariant\\twith\\textra\\tconnections\\tcalled\\t\\npeephole\\tconnections\\n:\\tthe\\tprevious\\tlong-term\\tstate\\t\\nc\\n(\\nt\\n–1)\\n\\tis\\tadded\\nas\\tan\\tinput\\tto\\tthe\\tcontrollers\\tof\\tthe\\tforget\\tgate\\tand\\tthe\\tinput\\tgate,\\tand\\tthe\\tcurrent\\tlong-term\\tstate\\t\\nc\\n(\\nt\\n)\\n\\tis\\nadded\\tas\\tinput\\tto\\tthe\\tcontroller\\tof\\tthe\\toutput\\tgate.\\nTo\\timplement\\tpeephole\\tconnections\\tin\\tTensorFlow,\\tyou\\tmust\\tuse\\tthe\\t\\nLSTMCell\\n\\tinstead\\tof\\tthe\\nBasicLSTMCell\\n\\tand\\t\\nset\\t\\nuse_peepholes=True\\n:\\nlstm_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nLSTMCell\\n(\\nnum_units\\n=\\nn_neurons\\n,\\n\\t\\nuse_peepholes\\n=\\nTrue\\n)\\nThere\\tare\\tmany\\tother\\tvariants\\tof\\tthe\\tLSTM\\tcell.\\tOne\\tparticularly\\tpopular\\tvariant\\tis\\tthe\\tGRU\\tcell,\\twhich\\nwe\\twill\\tlook\\tat\\tnow.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 494}), Document(page_content='GRU\\tCell\\nThe\\t\\nGated\\tRecurrent\\tUnit\\n\\t(GRU)\\t\\ncell\\t(see\\t\\nFigure\\t14-14\\n)\\twas\\tproposed\\tby\\tKyunghyun\\tCho\\tet\\tal.\\tin\\ta\\n2014\\tpaper\\n7\\n\\tthat\\talso\\tintroduced\\tthe\\tEncoder–Decoder\\tnetwork\\twe\\tmentioned\\tearlier.\\nFigure\\t14-14.\\t\\nGRU\\tcell\\nThe\\tGRU\\tcell\\tis\\ta\\tsimplified\\tversion\\tof\\tthe\\tLSTM\\tcell,\\tand\\tit\\tseems\\tto\\tperform\\tjust\\tas\\twell\\n8\\n\\t(which\\nexplains\\tits\\tgrowing\\tpopularity).\\tThe\\tmain\\tsimplifications\\tare:\\nBoth\\tstate\\tvectors\\tare\\tmerged\\tinto\\ta\\tsingle\\tvector\\t\\nh\\n(\\nt\\n)\\n.\\nA\\tsingle\\tgate\\tcontroller\\tcontrols\\tboth\\tthe\\tforget\\tgate\\tand\\tthe\\tinput\\tgate.\\tIf\\tthe\\tgate\\tcontroller\\toutputs\\ta\\n1,\\tthe\\tinput\\tgate\\tis\\topen\\tand\\tthe\\tforget\\tgate\\tis\\tclosed.\\tIf\\tit\\toutputs\\ta\\t0,\\tthe\\topposite\\thappens.\\tIn\\tother\\nwords,\\twhenever\\ta\\tmemory\\tmust\\tbe\\tstored,\\tthe\\tlocation\\twhere\\tit\\twill\\tbe\\tstored\\tis\\terased\\tfirst.\\tThis\\nis\\tactually\\ta\\tfrequent\\tvariant\\tto\\tthe\\tLSTM\\tcell\\tin\\tand\\tof\\titself.\\nThere\\tis\\tno\\toutput\\tgate;\\tthe\\tfull\\tstate\\tvector\\tis\\toutput\\tat\\tevery\\ttime\\tstep.\\tHowever,\\tthere\\tis\\ta\\tnew\\tgate\\ncontroller\\tthat\\tcontrols\\twhich\\tpart\\tof\\tthe\\tprevious\\tstate\\twill\\tbe\\tshown\\tto\\tthe\\tmain\\tlayer.\\nEquation\\t14-4\\n\\tsummarizes\\thow\\tto\\tcompute\\tthe\\tcell’s\\tstate\\tat\\teach\\ttime\\tstep\\tfor\\ta\\tsingle\\tinstance.\\nEquation\\t14-4.\\t\\nGRU\\tcomputations', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 495}), Document(page_content='Creating\\ta\\tGRU\\tcell\\tin\\tTensorFlow\\tis\\ttrivial:\\ngru_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nGRUCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\nLSTM\\tor\\tGRU\\tcells\\t\\nare\\tone\\tof\\tthe\\tmain\\treasons\\tbehind\\tthe\\tsuccess\\tof\\tRNNs\\tin\\trecent\\tyears,\\tin\\tparticular\\nfor\\tapplications\\tin\\t\\nnatural\\tlanguage\\tprocessing\\n\\t(NLP).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 496}), Document(page_content='Natural\\tLanguage\\tProcessing\\nMost\\t\\nof\\tthe\\tstate-of-the-art\\tNLP\\tapplications,\\tsuch\\tas\\tmachine\\ttranslation,\\tautomatic\\tsummarization,\\nparsing,\\tsentiment\\tanalysis,\\tand\\tmore,\\tare\\tnow\\tbased\\t(at\\tleast\\tin\\tpart)\\ton\\tRNNs.\\tIn\\tthis\\tlast\\tsection,\\twe\\nwill\\ttake\\ta\\tquick\\tlook\\tat\\twhat\\ta\\tmachine\\ttranslation\\tmodel\\tlooks\\tlike.\\tThis\\ttopic\\tis\\tvery\\twell\\tcovered\\tby\\nTensorFlow’s\\tawesome\\t\\nWord2Vec\\n\\tand\\t\\nSeq2Seq\\n\\ttutorials,\\tso\\tyou\\tshould\\tdefinitely\\tcheck\\tthem\\tout.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 497}), Document(page_content='Word\\tEmbeddings\\nBefore\\t\\nwe\\tstart,\\twe\\tneed\\tto\\tchoose\\ta\\tword\\trepresentation.\\tOne\\toption\\tcould\\tbe\\tto\\trepresent\\teach\\tword\\nusing\\ta\\tone-hot\\tvector.\\tSuppose\\tyour\\tvocabulary\\tcontains\\t50,000\\twords,\\tthen\\tthe\\tn\\nth\\n\\tword\\twould\\tbe\\nrepresented\\tas\\ta\\t50,000-dimensional\\tvector,\\tfull\\tof\\t0s\\texcept\\tfor\\ta\\t1\\tat\\tthe\\tn\\nth\\n\\tposition.\\tHowever,\\twith\\nsuch\\ta\\tlarge\\tvocabulary,\\tthis\\tsparse\\trepresentation\\twould\\tnot\\tbe\\tefficient\\tat\\tall.\\tIdeally,\\tyou\\twant\\tsimilar\\nwords\\tto\\thave\\tsimilar\\trepresentations,\\tmaking\\tit\\teasy\\tfor\\tthe\\tmodel\\tto\\tgeneralize\\twhat\\tit\\tlearns\\tabout\\ta\\nword\\tto\\tall\\tsimilar\\twords.\\tFor\\texample,\\tif\\tthe\\tmodel\\tis\\ttold\\tthat\\t“I\\tdrink\\tmilk”\\tis\\ta\\tvalid\\tsentence,\\tand\\tif\\nit\\tknows\\tthat\\t“milk”\\tis\\tclose\\tto\\t“water”\\tbut\\tfar\\tfrom\\t“shoes,”\\tthen\\tit\\twill\\tknow\\tthat\\t“I\\tdrink\\twater”\\tis\\nprobably\\ta\\tvalid\\tsentence\\tas\\twell,\\twhile\\t“I\\tdrink\\tshoes”\\tis\\tprobably\\tnot.\\tBut\\thow\\tcan\\tyou\\tcome\\tup\\twith\\nsuch\\ta\\tmeaningful\\trepresentation?\\nThe\\tmost\\tcommon\\tsolution\\tis\\tto\\trepresent\\teach\\tword\\tin\\tthe\\tvocabulary\\tusing\\ta\\tfairly\\tsmall\\tand\\tdense\\nvector\\t(e.g.,\\t150\\tdimensions),\\tcalled\\tan\\t\\nembedding\\n,\\tand\\tjust\\tlet\\tthe\\tneural\\tnetwork\\tlearn\\ta\\tgood\\nembedding\\tfor\\teach\\tword\\tduring\\ttraining.\\tAt\\tthe\\tbeginning\\tof\\ttraining,\\tembeddings\\tare\\tsimply\\tchosen\\nrandomly,\\tbut\\tduring\\ttraining,\\tbackpropagation\\tautomatically\\tmoves\\tthe\\tembeddings\\taround\\tin\\ta\\tway\\tthat\\nhelps\\tthe\\tneural\\tnetwork\\tperform\\tits\\ttask.\\tTypically\\tthis\\tmeans\\tthat\\tsimilar\\twords\\twill\\tgradually\\tcluster\\nclose\\tto\\tone\\tanother,\\tand\\teven\\tend\\tup\\torganized\\tin\\ta\\trather\\tmeaningful\\tway.\\tFor\\texample,\\tembeddings\\nmay\\tend\\tup\\tplaced\\talong\\tvarious\\taxes\\tthat\\trepresent\\tgender,\\tsingular/plural,\\tadjective/noun,\\tand\\tso\\ton.\\nThe\\tresult\\tcan\\tbe\\ttruly\\tamazing.\\n9\\nIn\\tTensorFlow,\\tyou\\tfirst\\tneed\\tto\\tcreate\\tthe\\tvariable\\trepresenting\\tthe\\tembeddings\\tfor\\tevery\\tword\\tin\\tyour\\nvocabulary\\t(initialized\\trandomly):\\nvocabulary_size\\n\\t\\n=\\n\\t\\n50000\\nembedding_size\\n\\t\\n=\\n\\t\\n150\\ninit_embeds\\n\\t\\n=\\n\\t\\ntf\\n.\\nrandom_uniform\\n([\\nvocabulary_size\\n,\\n\\t\\nembedding_size\\n],\\n\\t\\n-\\n1.0\\n,\\n\\t\\n1.0\\n)\\nembeddings\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ninit_embeds\\n)\\nNow\\tsuppose\\tyou\\twant\\tto\\tfeed\\tthe\\tsentence\\t“I\\tdrink\\tmilk”\\tto\\tyour\\tneural\\tnetwork.\\tYou\\tshould\\tfirst\\npreprocess\\tthe\\tsentence\\tand\\tbreak\\tit\\tinto\\ta\\tlist\\tof\\tknown\\twords.\\tFor\\texample\\tyou\\tmay\\tremove\\nunnecessary\\tcharacters,\\treplace\\tunknown\\twords\\tby\\ta\\tpredefined\\ttoken\\tword\\tsuch\\tas\\t“[UNK]”,\\treplace\\nnumerical\\tvalues\\tby\\t“[NUM]”,\\treplace\\tURLs\\tby\\t“[URL]”,\\tand\\tso\\ton.\\tOnce\\tyou\\thave\\ta\\tlist\\tof\\tknown\\nwords,\\tyou\\tcan\\tlook\\tup\\teach\\tword’s\\tinteger\\tidentifier\\t(from\\t0\\tto\\t49999)\\tin\\ta\\tdictionary,\\tfor\\texample\\t[72,\\n3335,\\t288].\\tAt\\tthat\\tpoint,\\tyou\\tare\\tready\\tto\\tfeed\\tthese\\tword\\tidentifiers\\tto\\tTensorFlow\\tusing\\ta\\tplaceholder,\\nand\\tapply\\tthe\\t\\nembedding_lookup()\\n\\t\\nfunction\\tto\\tget\\tthe\\tcorresponding\\tembeddings:\\ntrain_inputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n])\\n\\t\\t\\n#\\tfrom\\tids...\\nembed\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nembedding_lookup\\n(\\nembeddings\\n,\\n\\t\\ntrain_inputs\\n)\\n\\t\\t\\n#\\t...to\\tembeddings\\nOnce\\tyour\\tmodel\\thas\\tlearned\\tgood\\tword\\tembeddings,\\tthey\\tcan\\tactually\\tbe\\treused\\tfairly\\tefficiently\\tin\\tany\\nNLP\\tapplication:\\tafter\\tall,\\t“milk”\\tis\\tstill\\tclose\\tto\\t“water”\\tand\\tfar\\tfrom\\t“shoes”\\tno\\tmatter\\twhat\\tyour\\napplication\\tis.\\tIn\\tfact,\\tinstead\\tof\\ttraining\\tyour\\town\\tword\\tembeddings,\\tyou\\tmay\\twant\\tto\\tdownload\\npretrained\\tword\\tembeddings.\\tJust\\tlike\\twhen\\treusing\\tpretrained\\tlayers\\t(see\\t\\nChapter\\t11\\n),\\tyou\\tcan\\tchoose\\tto\\nfreeze\\tthe\\tpretrained\\tembeddings\\t(e.g.,\\tcreating\\tthe\\t\\nembeddings\\n\\tvariable\\tusing\\t\\ntrainable=False\\n)\\tor\\tlet', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 498}), Document(page_content='backpropagation\\ttweak\\tthem\\tfor\\tyour\\tapplication.\\tThe\\tfirst\\toption\\twill\\tspeed\\tup\\ttraining,\\tbut\\tthe\\tsecond\\nmay\\tlead\\tto\\tslightly\\thigher\\tperformance.\\nTIP\\nEmbeddings\\tare\\talso\\tuseful\\tfor\\trepresenting\\tcategorical\\tattributes\\tthat\\tcan\\ttake\\ton\\ta\\tlarge\\tnumber\\tof\\tdifferent\\tvalues,\\tespecially\\nwhen\\tthere\\tare\\tcomplex\\tsimilarities\\tbetween\\tvalues.\\tFor\\texample,\\tconsider\\tprofessions,\\thobbies,\\tdishes,\\tspecies,\\tbrands,\\tand\\tso\\non.\\nYou\\tnow\\thave\\talmost\\tall\\tthe\\ttools\\tyou\\tneed\\tto\\timplement\\ta\\tmachine\\ttranslation\\tsystem.\\t\\nLet’s\\tlook\\tat\\tthis\\nnow.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 499}), Document(page_content='An\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nLet’s\\t\\ntake\\ta\\tlook\\tat\\ta\\t\\nsimple\\tmachine\\ttranslation\\tmodel\\n10\\n\\tthat\\twill\\ttranslate\\tEnglish\\tsentences\\tto\\tFrench\\n(see\\t\\nFigure\\t14-15\\n).\\nFigure\\t14-15.\\t\\nA\\tsimple\\tmachine\\ttranslation\\tmodel\\nThe\\tEnglish\\tsentences\\tare\\tfed\\tto\\tthe\\tencoder,\\tand\\tthe\\tdecoder\\toutputs\\tthe\\tFrench\\ttranslations.\\tNote\\tthat\\nthe\\tFrench\\ttranslations\\tare\\talso\\tused\\tas\\tinputs\\tto\\tthe\\tdecoder,\\tbut\\tpushed\\tback\\tby\\tone\\tstep.\\tIn\\tother\\nwords,\\tthe\\tdecoder\\tis\\tgiven\\tas\\tinput\\tthe\\tword\\tthat\\tit\\t\\nshould\\n\\thave\\toutput\\tat\\tthe\\tprevious\\tstep\\t(regardless\\tof\\nwhat\\tit\\tactually\\toutput).\\tFor\\tthe\\tvery\\tfirst\\tword,\\tit\\tis\\tgiven\\ta\\ttoken\\tthat\\trepresents\\tthe\\tbeginning\\tof\\tthe\\nsentence\\t(e.g.,\\t“<go>”).\\tThe\\tdecoder\\tis\\texpected\\tto\\tend\\tthe\\tsentence\\twith\\tan\\tend-of-sequence\\t(EOS)\\ntoken\\t(e.g.,\\t“<eos>”).\\nNote\\tthat\\tthe\\tEnglish\\tsentences\\tare\\treversed\\tbefore\\tthey\\tare\\tfed\\tto\\tthe\\tencoder.\\tFor\\texample\\t“I\\tdrink\\nmilk”\\tis\\treversed\\tto\\t“milk\\tdrink\\tI.”\\tThis\\tensures\\tthat\\tthe\\tbeginning\\tof\\tthe\\tEnglish\\tsentence\\twill\\tbe\\tfed\\tlast\\nto\\tthe\\tencoder,\\twhich\\tis\\tuseful\\tbecause\\tthat’s\\tgenerally\\tthe\\tfirst\\tthing\\tthat\\tthe\\tdecoder\\tneeds\\tto\\ttranslate.\\nEach\\tword\\tis\\tinitially\\trepresented\\tby\\ta\\tsimple\\tinteger\\tidentifier\\t(e.g.,\\t288\\tfor\\tthe\\tword\\t“milk”).\\tNext,\\tan\\nembedding\\tlookup\\treturns\\tthe\\tword\\tembedding\\t(as\\texplained\\tearlier,\\tthis\\tis\\ta\\tdense,\\tfairly\\tlow-\\ndimensional\\tvector).\\tThese\\tword\\tembeddings\\tare\\twhat\\tis\\tactually\\tfed\\tto\\tthe\\tencoder\\tand\\tthe\\tdecoder.\\nAt\\teach\\tstep,\\tthe\\tdecoder\\toutputs\\ta\\tscore\\tfor\\teach\\tword\\tin\\tthe\\toutput\\tvocabulary\\t(i.e.,\\tFrench),\\tand\\tthen\\nthe\\tSoftmax\\tlayer\\tturns\\tthese\\tscores\\tinto\\tprobabilities.\\tFor\\texample,\\tat\\tthe\\tfirst\\tstep\\tthe\\tword\\t“Je”\\tmay\\nhave\\ta\\tprobability\\tof\\t20%,\\t“Tu”\\tmay\\thave\\ta\\tprobability\\tof\\t1%,\\tand\\tso\\ton.\\tThe\\tword\\twith\\tthe\\thighest\\nprobability\\tis\\toutput.\\tThis\\tis\\tvery\\tmuch\\tlike\\ta\\tregular\\tclassification\\ttask,\\tso\\tyou\\tcan\\ttrain\\tthe\\tmodel\\tusing\\nthe\\t\\nsoftmax_cross_entropy_with_logits()\\n\\tfunction.\\nNote\\tthat\\tat\\t\\ninference\\ttime\\t(after\\ttraining),\\tyou\\twill\\tnot\\thave\\tthe\\ttarget\\tsentence\\tto\\tfeed\\tto\\tthe\\tdecoder.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 500}), Document(page_content='Instead,\\tsimply\\tfeed\\tthe\\tdecoder\\tthe\\tword\\tthat\\tit\\toutput\\tat\\tthe\\tprevious\\tstep,\\tas\\tshown\\tin\\t\\nFigure\\t14-16\\n(this\\twill\\trequire\\tan\\tembedding\\tlookup\\tthat\\tis\\tnot\\tshown\\ton\\tthe\\tdiagram).\\nFigure\\t14-16.\\t\\nFeeding\\tthe\\tprevious\\toutput\\tword\\tas\\tinput\\tat\\tinference\\ttime\\nOkay,\\tnow\\tyou\\thave\\tthe\\tbig\\tpicture.\\tHowever,\\tif\\tyou\\tgo\\tthrough\\tTensorFlow’s\\tsequence-to-sequence\\ntutorial\\tand\\tyou\\tlook\\tat\\tthe\\tcode\\tin\\t\\nrnn/translate/seq2seq_model.py\\n\\t(in\\tthe\\t\\nTensorFlow\\tmodels\\n),\\tyou\\nwill\\tnotice\\ta\\tfew\\timportant\\tdifferences:\\nFirst,\\tso\\tfar\\twe\\thave\\tassumed\\tthat\\tall\\tinput\\tsequences\\t(to\\tthe\\tencoder\\tand\\tto\\tthe\\tdecoder)\\t\\nhave\\ta\\nconstant\\tlength.\\tBut\\tobviously\\tsentence\\tlengths\\tmay\\tvary.\\tThere\\tare\\tseveral\\tways\\tthat\\tthis\\tcan\\tbe\\nhandled\\t—\\tfor\\texample,\\tusing\\tthe\\t\\nsequence_length\\n\\t\\nargument\\tto\\tthe\\t\\nstatic_rnn()\\n\\tor\\ndynamic_rnn()\\n\\t\\nfunctions\\tto\\tspecify\\teach\\tsentence’s\\tlength\\t(as\\tdiscussed\\tearlier).\\tHowever,\\tanother\\napproach\\tis\\tused\\tin\\tthe\\ttutorial\\t(presumably\\tfor\\tperformance\\treasons):\\tsentences\\tare\\tgrouped\\tinto\\nbuckets\\tof\\tsimilar\\tlengths\\t(e.g.,\\ta\\tbucket\\tfor\\tthe\\t1-\\tto\\t6-word\\tsentences,\\tanother\\tfor\\tthe\\t7-\\tto\\t12-\\nword\\tsentences,\\tand\\tso\\ton\\n11\\n),\\tand\\tthe\\tshorter\\tsentences\\tare\\tpadded\\tusing\\ta\\tspecial\\tpadding\\ttoken\\n(e.g.,\\t“<pad>”).\\tFor\\texample\\t“I\\tdrink\\tmilk”\\tbecomes\\t“<pad>\\t<pad>\\t<pad>\\tmilk\\tdrink\\tI”,\\tand\\tits\\ntranslation\\tbecomes\\t“Je\\tbois\\tdu\\tlait\\t<eos>\\t<pad>”.\\tOf\\tcourse,\\twe\\twant\\tto\\tignore\\tany\\toutput\\tpast\\tthe\\nEOS\\ttoken.\\tFor\\tthis,\\tthe\\ttutorial’s\\timplementation\\tuses\\ta\\t\\ntarget_weights\\n\\t\\nvector.\\tFor\\texample,\\tfor\\nthe\\ttarget\\tsentence\\t“Je\\tbois\\tdu\\tlait\\t<eos>\\t<pad>”,\\tthe\\tweights\\twould\\tbe\\tset\\tto\\t\\n[1.0,\\t1.0,\\t1.0,\\n1.0,\\t1.0,\\t0.0]\\n\\t(notice\\tthe\\tweight\\t0.0\\tthat\\tcorresponds\\tto\\tthe\\tpadding\\ttoken\\tin\\tthe\\ttarget\\tsentence).\\nSimply\\tmultiplying\\tthe\\tlosses\\tby\\tthe\\ttarget\\tweights\\twill\\tzero\\tout\\tthe\\tlosses\\tthat\\tcorrespond\\tto\\twords\\npast\\tEOS\\ttokens.\\nSecond,\\twhen\\tthe\\toutput\\tvocabulary\\tis\\tlarge\\t(which\\tis\\tthe\\tcase\\there),\\toutputting\\ta\\tprobability\\tfor\\neach\\tand\\tevery\\tpossible\\tword\\twould\\tbe\\tterribly\\tslow.\\tIf\\tthe\\ttarget\\tvocabulary\\tcontains,\\tsay,\\t50,000\\nFrench\\twords,\\tthen\\tthe\\tdecoder\\twould\\toutput\\t50,000-dimensional\\tvectors,\\tand\\tthen\\tcomputing\\tthe\\nsoftmax\\tfunction\\tover\\tsuch\\ta\\tlarge\\tvector\\twould\\tbe\\tvery\\tcomputationally\\tintensive.\\tTo\\tavoid\\tthis,\\none\\tsolution\\tis\\tto\\tlet\\tthe\\tdecoder\\toutput\\tmuch\\tsmaller\\tvectors,\\tsuch\\tas\\t1,000-dimensional\\tvectors,\\nthen\\tuse\\ta\\tsampling\\ttechnique\\tto\\testimate\\tthe\\tloss\\twithout\\thaving\\tto\\tcompute\\tit\\tover\\tevery\\tsingle\\nword\\tin\\tthe\\ttarget\\tvocabulary.\\tThis\\t\\nSampled\\tSoftmax\\n\\t\\ntechnique\\twas\\t\\nintroduced\\tin\\t2015\\tby\\tSébastien\\nJean\\tet\\tal\\n.\\n12\\n\\tIn\\tTensorFlow\\tyou\\tcan\\tuse\\tthe\\t\\nsampled_softmax_loss()\\n\\tfunction.\\nThird,\\tthe\\ttutorial’s\\timplementation\\tuses\\tan\\t\\nattention\\tmechanism\\n\\t\\nthat\\tlets\\tthe\\tdecoder\\tpeek\\tinto\\tthe\\ninput\\tsequence.\\tAttention\\taugmented\\tRNNs\\tare\\tbeyond\\tthe\\tscope\\tof\\tthis\\tbook,\\tbut\\tif\\tyou\\tare', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 501}), Document(page_content='interested\\tthere\\tare\\thelpful\\tpapers\\tabout\\t\\nmachine\\ttranslation\\n,\\n13\\n\\t\\nmachine\\treading\\n,\\n14\\n\\tand\\t\\nimage\\ncaptions\\n15\\n\\tusing\\tattention.\\nFinally,\\tthe\\ttutorial’s\\timplementation\\tmakes\\tuse\\tof\\tthe\\t\\ntf.nn.legacy_seq2seq\\n\\tmodule,\\twhich\\nprovides\\ttools\\tto\\tbuild\\tvarious\\tEncoder–Decoder\\tmodels\\teasily.\\tFor\\texample,\\tthe\\nembedding_rnn_seq2seq()\\n\\tfunction\\tcreates\\ta\\tsimple\\tEncoder–Decoder\\tmodel\\tthat\\tautomatically\\ntakes\\tcare\\tof\\tword\\tembeddings\\tfor\\tyou,\\tjust\\tlike\\tthe\\tone\\trepresented\\tin\\t\\nFigure\\t14-15\\n.\\tThis\\tcode\\twill\\nlikely\\tbe\\tupdated\\tquickly\\tto\\tuse\\tthe\\tnew\\t\\ntf.nn.seq2seq\\n\\tmodule.\\nYou\\tnow\\thave\\tall\\tthe\\ttools\\tyou\\tneed\\tto\\tunderstand\\tthe\\tsequence-to-sequence\\ttutorial’s\\timplementation.\\nCheck\\tit\\tout\\tand\\ttrain\\tyour\\town\\tEnglish-to-French\\ttranslator!', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 502}), Document(page_content='Exercises\\n1\\n.\\t\\nCan\\tyou\\tthink\\tof\\ta\\tfew\\tapplications\\tfor\\ta\\tsequence-to-sequence\\tRNN?\\tWhat\\tabout\\ta\\tsequence-to-\\nvector\\tRNN?\\tAnd\\ta\\tvector-to-sequence\\tRNN?\\n2\\n.\\t\\nWhy\\tdo\\tpeople\\tuse\\tencoder–decoder\\tRNNs\\trather\\tthan\\tplain\\tsequence-to-sequence\\tRNNs\\tfor\\nautomatic\\ttranslation?\\n3\\n.\\t\\nHow\\tcould\\tyou\\tcombine\\ta\\tconvolutional\\tneural\\tnetwork\\twith\\tan\\tRNN\\tto\\tclassify\\tvideos?\\n4\\n.\\t\\nWhat\\tare\\t\\nthe\\tadvantages\\tof\\tbuilding\\tan\\tRNN\\tusing\\t\\ndynamic_rnn()\\n\\trather\\tthan\\t\\nstatic_rnn()\\n?\\n5\\n.\\t\\nHow\\tcan\\tyou\\tdeal\\twith\\tvariable-length\\tinput\\tsequences?\\tWhat\\tabout\\tvariable-length\\toutput\\nsequences?\\n6\\n.\\t\\nWhat\\tis\\ta\\tcommon\\tway\\tto\\tdistribute\\ttraining\\tand\\texecution\\tof\\ta\\tdeep\\tRNN\\tacross\\tmultiple\\tGPUs?\\n7\\n.\\t\\nEmbedded\\tReber\\tgrammars\\n\\t\\nwere\\tused\\tby\\tHochreiter\\tand\\tSchmidhuber\\tin\\ttheir\\tpaper\\tabout\\tLSTMs.\\nThey\\tare\\tartificial\\tgrammars\\tthat\\tproduce\\tstrings\\tsuch\\tas\\t“BPBTSXXVPSEPE.”\\tCheck\\tout\\tJenny\\nOrr’s\\t\\nnice\\tintroduction\\n\\tto\\tthis\\ttopic.\\tChoose\\ta\\tparticular\\tembedded\\tReber\\tgrammar\\t(such\\tas\\tthe\\tone\\nrepresented\\ton\\tJenny\\tOrr’s\\tpage),\\tthen\\ttrain\\tan\\tRNN\\tto\\tidentify\\twhether\\ta\\tstring\\trespects\\tthat\\ngrammar\\tor\\tnot.\\tYou\\twill\\tfirst\\tneed\\tto\\twrite\\ta\\tfunction\\tcapable\\tof\\tgenerating\\ta\\ttraining\\tbatch\\ncontaining\\tabout\\t50%\\tstrings\\tthat\\trespect\\tthe\\tgrammar,\\tand\\t50%\\tthat\\tdon’t.\\n8\\n.\\t\\nTackle\\tthe\\t“How\\tmuch\\tdid\\tit\\train?\\tII”\\t\\nKaggle\\tcompetition\\n.\\tThis\\tis\\ta\\ttime\\tseries\\tprediction\\ttask:\\tyou\\nare\\tgiven\\tsnapshots\\tof\\tpolarimetric\\tradar\\tvalues\\tand\\tasked\\tto\\tpredict\\tthe\\thourly\\train\\tgauge\\ttotal.\\nLuis\\tAndre\\tDutra\\te\\tSilva’s\\t\\ninterview\\n\\tgives\\tsome\\tinteresting\\tinsights\\tinto\\tthe\\ttechniques\\the\\tused\\tto\\nreach\\tsecond\\tplace\\tin\\tthe\\tcompetition.\\tIn\\tparticular,\\the\\tused\\tan\\tRNN\\tcomposed\\tof\\ttwo\\tLSTM\\tlayers.\\n9\\n.\\t\\nGo\\tthrough\\tTensorFlow’s\\t\\nWord2Vec\\n\\ttutorial\\tto\\tcreate\\tword\\tembeddings,\\tand\\tthen\\tgo\\tthrough\\tthe\\nSeq2Seq\\n\\ttutorial\\tto\\ttrain\\tan\\tEnglish-to-French\\ttranslation\\tsystem.\\nSolutions\\tto\\tthese\\texercises\\tare\\t\\navailable\\tin\\t\\nAppendix\\tA\\n.\\nNote\\tthat\\tmany\\tresearchers\\tprefer\\tto\\tuse\\tthe\\thyperbolic\\ttangent\\t(tanh)\\tactivation\\tfunction\\tin\\tRNNs\\trather\\tthan\\tthe\\tReLU\\tactivation\\nfunction.\\tFor\\texample,\\ttake\\ta\\tlook\\tat\\tby\\tVu\\tPham\\tet\\tal.’s\\tpaper\\t\\n“Dropout\\tImproves\\tRecurrent\\tNeural\\tNetworks\\tfor\\tHandwriting\\nRecognition”\\n.\\t\\nHowever,\\tReLU-based\\tRNNs\\tare\\talso\\tpossible,\\tas\\tshown\\tin\\tQuoc\\tV.\\tLe\\tet\\tal.’s\\tpaper\\t\\n“A\\tSimple\\tWay\\tto\\tInitialize\\nRecurrent\\tNetworks\\tof\\tRectified\\tLinear\\tUnits”\\n.\\nThis\\tuses\\tthe\\t\\ndecorator\\n\\tdesign\\tpattern.\\n“Long\\tShort-Term\\tMemory,”\\tS.\\tHochreiter\\tand\\tJ.\\tSchmidhuber\\t(1997).\\n“Long\\tShort-Term\\tMemory\\tRecurrent\\tNeural\\tNetwork\\tArchitectures\\tfor\\tLarge\\tScale\\tAcoustic\\tModeling,”\\tH.\\tSak\\tet\\tal.\\t(2014).\\n“Recurrent\\tNeural\\tNetwork\\tRegularization,”\\tW.\\tZaremba\\tet\\tal.\\t(2015).\\n“Recurrent\\tNets\\tthat\\tTime\\tand\\tCount,”\\tF.\\tGers\\tand\\tJ.\\tSchmidhuber\\t(2000).\\n“Learning\\tPhrase\\tRepresentations\\tusing\\tRNN\\tEncoder–Decoder\\tfor\\tStatistical\\tMachine\\tTranslation,”\\tK.\\tCho\\tet\\tal.\\t(2014).\\nA\\t2015\\tpaper\\tby\\tKlaus\\tGreff\\tet\\tal.,\\t\\n“LSTM:\\tA\\tSearch\\tSpace\\tOdyssey,”\\n\\tseems\\tto\\tshow\\tthat\\tall\\tLSTM\\tvariants\\tperform\\troughly\\tthe\\tsame.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 503}), Document(page_content='For\\tmore\\tdetails,\\tcheck\\tout\\tChristopher\\tOlah’s\\t\\ngreat\\tpost\\n,\\tor\\tSebastian\\tRuder’s\\t\\nseries\\tof\\tposts\\n.\\n“Sequence\\tto\\tSequence\\tlearning\\twith\\tNeural\\tNetworks,”\\tI.\\tSutskever\\tet\\tal.\\t(2014).\\nThe\\tbucket\\tsizes\\tused\\tin\\tthe\\ttutorial\\tare\\tdifferent.\\n“On\\tUsing\\tVery\\tLarge\\tTarget\\tVocabulary\\tfor\\tNeural\\tMachine\\tTranslation,”\\tS.\\tJean\\tet\\tal.\\t(2015).\\n“Neural\\tMachine\\tTranslation\\tby\\tJointly\\tLearning\\tto\\tAlign\\tand\\tTranslate,”\\tD.\\tBahdanau\\tet\\tal.\\t(2014).\\n“Long\\tShort-Term\\tMemory-Networks\\tfor\\tMachine\\tReading,”\\tJ.\\tCheng\\t(2016).\\n“Show,\\tAttend\\tand\\tTell:\\tNeural\\tImage\\tCaption\\tGeneration\\twith\\tVisual\\tAttention,”\\tK.\\tXu\\tet\\tal.\\t(2015).\\n9\\n10\\n11\\n12\\n13\\n14\\n15', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 504}), Document(page_content='Chapter\\t15.\\t\\nAutoencoders\\nAutoencoders\\t\\nare\\tartificial\\tneural\\tnetworks\\tcapable\\tof\\tlearning\\tefficient\\trepresentations\\tof\\tthe\\tinput\\tdata,\\ncalled\\t\\ncodings\\n,\\t\\nwithout\\tany\\tsupervision\\t(i.e.,\\tthe\\ttraining\\tset\\tis\\tunlabeled).\\tThese\\tcodings\\ttypically\\thave\\na\\tmuch\\tlower\\tdimensionality\\tthan\\tthe\\tinput\\tdata,\\tmaking\\tautoencoders\\tuseful\\tfor\\t\\ndimensionality\\treduction\\n(see\\t\\nChapter\\t8\\n).\\tMore\\timportantly,\\tautoencoders\\tact\\tas\\tpowerful\\t\\nfeature\\tdetectors,\\tand\\tthey\\tcan\\tbe\\tused\\nfor\\tunsupervised\\tpretraining\\tof\\tdeep\\tneural\\tnetworks\\t(as\\twe\\tdiscussed\\tin\\t\\nChapter\\t11\\n).\\tLastly,\\tthey\\tare\\ncapable\\tof\\trandomly\\tgenerating\\tnew\\tdata\\tthat\\tlooks\\tvery\\tsimilar\\tto\\tthe\\ttraining\\tdata;\\tthis\\tis\\tcalled\\t\\na\\ngenerative\\tmodel\\n.\\tFor\\texample,\\tyou\\tcould\\ttrain\\tan\\tautoencoder\\ton\\tpictures\\tof\\tfaces,\\tand\\tit\\twould\\tthen\\tbe\\nable\\tto\\tgenerate\\tnew\\tfaces.\\nSurprisingly,\\tautoencoders\\twork\\tby\\tsimply\\tlearning\\tto\\tcopy\\ttheir\\tinputs\\tto\\ttheir\\toutputs.\\tThis\\tmay\\tsound\\nlike\\ta\\ttrivial\\ttask,\\tbut\\twe\\twill\\tsee\\tthat\\tconstraining\\tthe\\tnetwork\\tin\\tvarious\\tways\\tcan\\tmake\\tit\\trather\\ndifficult.\\tFor\\texample,\\tyou\\tcan\\tlimit\\tthe\\tsize\\tof\\tthe\\tinternal\\trepresentation,\\tor\\tyou\\tcan\\tadd\\tnoise\\tto\\tthe\\ninputs\\tand\\ttrain\\tthe\\tnetwork\\tto\\trecover\\tthe\\toriginal\\tinputs.\\tThese\\tconstraints\\tprevent\\tthe\\tautoencoder\\tfrom\\ntrivially\\tcopying\\tthe\\tinputs\\tdirectly\\tto\\tthe\\toutputs,\\twhich\\tforces\\tit\\tto\\tlearn\\tefficient\\tways\\tof\\trepresenting\\nthe\\tdata.\\tIn\\tshort,\\tthe\\tcodings\\tare\\tbyproducts\\tof\\tthe\\tautoencoder’s\\tattempt\\tto\\tlearn\\tthe\\tidentity\\tfunction\\nunder\\tsome\\tconstraints.\\nIn\\tthis\\tchapter\\twe\\twill\\texplain\\tin\\tmore\\tdepth\\thow\\tautoencoders\\twork,\\twhat\\ttypes\\tof\\tconstraints\\tcan\\tbe\\nimposed,\\tand\\thow\\tto\\timplement\\tthem\\tusing\\tTensorFlow,\\twhether\\tit\\tis\\tfor\\tdimensionality\\treduction,\\nfeature\\textraction,\\tunsupervised\\tpretraining,\\tor\\tas\\tgenerative\\tmodels.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 505}), Document(page_content='Efficient\\tData\\tRepresentations\\nWhich\\t\\nof\\tthe\\tfollowing\\tnumber\\tsequences\\tdo\\tyou\\tfind\\tthe\\teasiest\\tto\\tmemorize?\\n40,\\t27,\\t25,\\t36,\\t81,\\t57,\\t10,\\t73,\\t19,\\t68\\n50,\\t25,\\t76,\\t38,\\t19,\\t58,\\t29,\\t88,\\t44,\\t22,\\t11,\\t34,\\t17,\\t52,\\t26,\\t13,\\t40,\\t20\\nAt\\tfirst\\tglance,\\tit\\twould\\tseem\\tthat\\tthe\\tfirst\\tsequence\\tshould\\tbe\\teasier,\\tsince\\tit\\tis\\tmuch\\tshorter.\\tHowever,\\nif\\tyou\\tlook\\tcarefully\\tat\\tthe\\tsecond\\tsequence,\\tyou\\tmay\\tnotice\\tthat\\tit\\tfollows\\ttwo\\tsimple\\trules:\\teven\\nnumbers\\tare\\tfollowed\\tby\\ttheir\\thalf,\\tand\\todd\\tnumbers\\tare\\tfollowed\\tby\\ttheir\\ttriple\\tplus\\tone\\t(this\\tis\\ta\\nfamous\\tsequence\\tknown\\tas\\t\\nthe\\t\\nhailstone\\tsequence\\n).\\tOnce\\tyou\\tnotice\\tthis\\tpattern,\\tthe\\tsecond\\tsequence\\nbecomes\\tmuch\\teasier\\tto\\tmemorize\\tthan\\tthe\\tfirst\\tbecause\\tyou\\tonly\\tneed\\tto\\tmemorize\\tthe\\ttwo\\trules,\\tthe\\tfirst\\nnumber,\\tand\\tthe\\tlength\\tof\\tthe\\tsequence.\\tNote\\tthat\\tif\\tyou\\tcould\\tquickly\\tand\\teasily\\tmemorize\\tvery\\tlong\\nsequences,\\tyou\\twould\\tnot\\tcare\\tmuch\\tabout\\tthe\\texistence\\tof\\ta\\tpattern\\tin\\tthe\\tsecond\\tsequence.\\tYou\\twould\\njust\\tlearn\\tevery\\tnumber\\tby\\theart,\\tand\\tthat\\twould\\tbe\\tthat.\\tIt\\tis\\tthe\\tfact\\tthat\\tit\\tis\\thard\\tto\\tmemorize\\tlong\\nsequences\\tthat\\tmakes\\tit\\tuseful\\tto\\trecognize\\tpatterns,\\tand\\thopefully\\tthis\\tclarifies\\twhy\\tconstraining\\tan\\nautoencoder\\tduring\\ttraining\\tpushes\\tit\\tto\\tdiscover\\tand\\texploit\\tpatterns\\tin\\tthe\\tdata.\\nThe\\trelationship\\tbetween\\tmemory,\\tperception,\\tand\\tpattern\\tmatching\\twas\\t\\nfamously\\tstudied\\tby\\tWilliam\\nChase\\tand\\tHerbert\\tSimon\\tin\\tthe\\tearly\\t1970s\\n.\\n1\\n\\tThey\\tobserved\\tthat\\texpert\\tchess\\tplayers\\twere\\table\\tto\\nmemorize\\tthe\\tpositions\\tof\\tall\\tthe\\tpieces\\tin\\ta\\tgame\\tby\\tlooking\\tat\\tthe\\tboard\\tfor\\tjust\\t5\\tseconds,\\ta\\ttask\\tthat\\nmost\\tpeople\\twould\\tfind\\timpossible.\\tHowever,\\tthis\\twas\\tonly\\tthe\\tcase\\twhen\\tthe\\tpieces\\twere\\tplaced\\tin\\nrealistic\\tpositions\\t(from\\tactual\\tgames),\\tnot\\twhen\\tthe\\tpieces\\twere\\tplaced\\trandomly.\\tChess\\texperts\\tdon’t\\nhave\\ta\\tmuch\\tbetter\\tmemory\\tthan\\tyou\\tand\\tI,\\tthey\\tjust\\tsee\\tchess\\tpatterns\\tmore\\teasily\\tthanks\\tto\\ttheir\\nexperience\\twith\\tthe\\tgame.\\tNoticing\\tpatterns\\thelps\\tthem\\tstore\\tinformation\\tefficiently.\\nJust\\tlike\\tthe\\tchess\\tplayers\\tin\\tthis\\tmemory\\texperiment,\\tan\\tautoencoder\\tlooks\\tat\\tthe\\tinputs,\\tconverts\\tthem\\tto\\nan\\tefficient\\tinternal\\trepresentation,\\tand\\tthen\\tspits\\tout\\tsomething\\tthat\\t(hopefully)\\tlooks\\tvery\\tclose\\tto\\tthe\\ninputs.\\tAn\\tautoencoder\\tis\\talways\\tcomposed\\tof\\ttwo\\tparts:\\tan\\t\\nencoder\\n\\t\\n(or\\t\\nrecognition\\tnetwork\\n)\\tthat\\nconverts\\tthe\\tinputs\\tto\\tan\\tinternal\\trepresentation,\\tfollowed\\tby\\ta\\t\\ndecoder\\n\\t\\n(or\\t\\ngenerative\\tnetwork\\n)\\tthat\\nconverts\\tthe\\tinternal\\trepresentation\\tto\\tthe\\toutputs\\t(see\\t\\nFigure\\t15-1\\n).\\nAs\\tyou\\tcan\\tsee,\\tan\\tautoencoder\\ttypically\\thas\\tthe\\tsame\\tarchitecture\\tas\\ta\\tMulti-Layer\\tPerceptron\\t(MLP;\\nsee\\t\\nChapter\\t10\\n),\\texcept\\tthat\\tthe\\tnumber\\tof\\tneurons\\tin\\tthe\\toutput\\tlayer\\tmust\\tbe\\tequal\\tto\\tthe\\tnumber\\tof\\ninputs.\\tIn\\tthis\\texample,\\tthere\\tis\\tjust\\tone\\thidden\\tlayer\\tcomposed\\tof\\ttwo\\tneurons\\t(the\\tencoder),\\tand\\tone\\noutput\\tlayer\\tcomposed\\tof\\tthree\\tneurons\\t(the\\tdecoder).\\tThe\\toutputs\\tare\\toften\\tcalled\\tthe\\t\\nreconstructions\\nsince\\tthe\\tautoencoder\\ttries\\tto\\treconstruct\\tthe\\tinputs,\\tand\\tthe\\tcost\\tfunction\\tcontains\\ta\\t\\nreconstruction\\tloss\\nthat\\tpenalizes\\tthe\\tmodel\\twhen\\tthe\\treconstructions\\tare\\tdifferent\\tfrom\\tthe\\tinputs.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 506}), Document(page_content='Figure\\t15-1.\\t\\nThe\\tchess\\tmemory\\texperiment\\t(left)\\tand\\ta\\tsimple\\tautoencoder\\t(right)\\nBecause\\tthe\\tinternal\\trepresentation\\thas\\ta\\tlower\\tdimensionality\\tthan\\tthe\\tinput\\tdata\\t(it\\tis\\t2D\\tinstead\\tof\\t3D),\\nthe\\tautoencoder\\tis\\tsaid\\tto\\tbe\\t\\nundercomplete\\n.\\t\\nAn\\tundercomplete\\tautoencoder\\tcannot\\ttrivially\\tcopy\\tits\\ninputs\\tto\\tthe\\tcodings,\\tyet\\tit\\tmust\\tfind\\ta\\tway\\tto\\toutput\\ta\\tcopy\\tof\\tits\\tinputs.\\tIt\\tis\\tforced\\tto\\tlearn\\tthe\\tmost\\nimportant\\tfeatures\\tin\\tthe\\tinput\\tdata\\t(and\\tdrop\\tthe\\tunimportant\\tones).\\nLet’s\\tsee\\thow\\tto\\timplement\\ta\\tvery\\tsimple\\tundercomplete\\tautoencoder\\tfor\\tdimensionality\\treduction.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 507}), Document(page_content='Performing\\tPCA\\twith\\tan\\tUndercomplete\\tLinear\\tAutoencoder\\nIf\\t\\nthe\\tautoencoder\\tuses\\tonly\\tlinear\\tactivations\\tand\\tthe\\tcost\\tfunction\\tis\\tthe\\tMean\\tSquared\\tError\\t(MSE),\\nthen\\tit\\tcan\\tbe\\tshown\\tthat\\tit\\tends\\tup\\tperforming\\tPrincipal\\tComponent\\tAnalysis\\t(see\\t\\nChapter\\t8\\n).\\nThe\\tfollowing\\tcode\\tbuilds\\ta\\tsimple\\tlinear\\tautoencoder\\tto\\tperform\\tPCA\\ton\\ta\\t3D\\tdataset,\\tprojecting\\tit\\tto\\n2D:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nn_inputs\\n\\t\\n=\\n\\t\\n3\\n\\t\\t\\n#\\t3D\\tinputs\\nn_hidden\\n\\t\\n=\\n\\t\\n2\\n\\t\\t\\n#\\t2D\\tcodings\\nn_outputs\\n\\t\\n=\\n\\t\\nn_inputs\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nhidden\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden\\n,\\n\\t\\nn_outputs\\n)\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\t\\n#\\tMSE\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nreconstruction_loss\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nThis\\tcode\\tis\\t\\nreally\\tnot\\tvery\\tdifferent\\tfrom\\tall\\tthe\\tMLPs\\twe\\tbuilt\\tin\\tpast\\tchapters.\\tThe\\ttwo\\tthings\\tto\\tnote\\nare:\\nThe\\tnumber\\tof\\toutputs\\tis\\tequal\\tto\\tthe\\tnumber\\tof\\tinputs.\\nTo\\tperform\\tsimple\\tPCA,\\twe\\tdo\\tnot\\tuse\\tany\\tactivation\\tfunction\\t(i.e.,\\tall\\tneurons\\tare\\tlinear)\\tand\\tthe\\ncost\\tfunction\\tis\\tthe\\tMSE.\\tWe\\twill\\tsee\\tmore\\tcomplex\\tautoencoders\\tshortly.\\nNow\\tlet’s\\tload\\tthe\\tdataset,\\ttrain\\tthe\\tmodel\\ton\\tthe\\ttraining\\tset,\\tand\\tuse\\tit\\tto\\tencode\\tthe\\ttest\\tset\\t(i.e.,\\tproject\\nit\\tto\\t2D):\\nX_train\\n,\\n\\t\\nX_test\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\n#\\tload\\tthe\\tdataset\\nn_iterations\\n\\t\\n=\\n\\t\\n1000\\ncodings\\n\\t\\n=\\n\\t\\nhidden\\n\\t\\t\\n#\\tthe\\toutput\\tof\\tthe\\thidden\\tlayer\\tprovides\\tthe\\tcodings\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntraining_op\\n.\\nrun\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_train\\n})\\n\\t\\t\\n#\\tno\\tlabels\\t(unsupervised)\\n\\t\\t\\t\\t\\ncodings_val\\n\\t\\n=\\n\\t\\ncodings\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_test\\n})\\nFigure\\t15-2\\n\\tshows\\tthe\\toriginal\\t3D\\tdataset\\t(at\\tthe\\tleft)\\tand\\tthe\\toutput\\tof\\tthe\\tautoencoder’s\\thidden\\tlayer\\n(i.e.,\\tthe\\tcoding\\tlayer,\\tat\\tthe\\tright).\\tAs\\tyou\\tcan\\tsee,\\tthe\\tautoencoder\\tfound\\tthe\\tbest\\t2D\\tplane\\tto\\tproject\\tthe\\ndata\\tonto,\\tpreserving\\tas\\tmuch\\tvariance\\tin\\tthe\\tdata\\tas\\tit\\tcould\\t(just\\tlike\\tPCA).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 508}), Document(page_content='Figure\\t15-2.\\t\\nPCA\\tperformed\\tby\\tan\\tundercomplete\\tlinear\\tautoencoder', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 509}), Document(page_content='Stacked\\tAutoencoders\\nJust\\t\\nlike\\tother\\tneural\\tnetworks\\twe\\thave\\tdiscussed,\\tautoencoders\\tcan\\thave\\tmultiple\\thidden\\tlayers.\\tIn\\tthis\\ncase\\tthey\\tare\\tcalled\\t\\nstacked\\tautoencoders\\n\\t(or\\t\\ndeep\\tautoencoders\\n).\\t\\nAdding\\tmore\\tlayers\\thelps\\tthe\\nautoencoder\\tlearn\\tmore\\tcomplex\\tcodings.\\tHowever,\\tone\\tmust\\tbe\\tcareful\\tnot\\tto\\tmake\\tthe\\tautoencoder\\ttoo\\npowerful.\\tImagine\\tan\\tencoder\\tso\\tpowerful\\tthat\\tit\\tjust\\tlearns\\tto\\tmap\\teach\\tinput\\tto\\ta\\tsingle\\tarbitrary\\tnumber\\n(and\\tthe\\tdecoder\\tlearns\\tthe\\treverse\\tmapping).\\tObviously\\tsuch\\tan\\tautoencoder\\twill\\treconstruct\\tthe\\ttraining\\ndata\\tperfectly,\\tbut\\tit\\twill\\tnot\\thave\\tlearned\\tany\\tuseful\\tdata\\trepresentation\\tin\\tthe\\tprocess\\t(and\\tit\\tis\\tunlikely\\nto\\tgeneralize\\twell\\tto\\tnew\\tinstances).\\nThe\\tarchitecture\\tof\\ta\\tstacked\\tautoencoder\\tis\\ttypically\\tsymmetrical\\twith\\tregards\\tto\\tthe\\tcentral\\thidden\\tlayer\\n(the\\tcoding\\tlayer).\\tTo\\tput\\tit\\tsimply,\\tit\\tlooks\\tlike\\ta\\tsandwich.\\tFor\\texample,\\tan\\tautoencoder\\tfor\\tMNIST\\n(introduced\\tin\\t\\nChapter\\t3\\n)\\tmay\\thave\\t784\\tinputs,\\tfollowed\\tby\\ta\\thidden\\tlayer\\twith\\t300\\tneurons,\\tthen\\ta\\ncentral\\thidden\\tlayer\\tof\\t150\\tneurons,\\tthen\\tanother\\thidden\\tlayer\\twith\\t300\\tneurons,\\tand\\tan\\toutput\\tlayer\\twith\\n784\\tneurons.\\tThis\\tstacked\\tautoencoder\\tis\\trepresented\\tin\\t\\nFigure\\t15-3\\n.\\nFigure\\t15-3.\\t\\nStacked\\tautoencoder', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 510}), Document(page_content='TensorFlow\\tImplementation\\nYou\\t\\ncan\\timplement\\ta\\tstacked\\tautoencoder\\tvery\\tmuch\\tlike\\ta\\tregular\\tdeep\\tMLP.\\tIn\\tparticular,\\tthe\\tsame\\ntechniques\\twe\\tused\\tin\\t\\nChapter\\t11\\n\\tfor\\ttraining\\tdeep\\tnets\\tcan\\tbe\\tapplied.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\nbuilds\\ta\\tstacked\\tautoencoder\\tfor\\tMNIST,\\tusing\\tHe\\tinitialization,\\tthe\\tELU\\tactivation\\tfunction,\\tand\\tℓ\\n2\\nregularization.\\tThe\\tcode\\tshould\\tlook\\tvery\\tfamiliar,\\texcept\\tthat\\tthere\\tare\\tno\\tlabels\\t(no\\t\\ny\\n):\\nfrom\\n\\t\\nfunctools\\n\\t\\nimport\\n\\t\\npartial\\nn_inputs\\n\\t\\n=\\n\\t\\n28\\n\\t\\n*\\n\\t\\n28\\n\\t\\t\\n#\\tfor\\tMNIST\\nn_hidden1\\n\\t\\n=\\n\\t\\n300\\nn_hidden2\\n\\t\\n=\\n\\t\\n150\\n\\t\\t\\n#\\tcodings\\nn_hidden3\\n\\t\\n=\\n\\t\\nn_hidden1\\nn_outputs\\n\\t\\n=\\n\\t\\nn_inputs\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nl2_reg\\n\\t\\n=\\n\\t\\n0.0001\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nhe_init\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\nl2_regularizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nl2_regularizer\\n(\\nl2_reg\\n)\\nmy_dense_layer\\n\\t\\n=\\n\\t\\npartial\\n(\\ntf\\n.\\nlayers\\n.\\ndense\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\nhe_init\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_regularizer\\n=\\nl2_regularizer\\n)\\nhidden1\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n)\\nhidden2\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n)\\n\\t\\t\\n#\\tcodings\\nhidden3\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden2\\n,\\n\\t\\nn_hidden3\\n)\\noutputs\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden3\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nactivation\\n=\\nNone\\n)\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\t\\n#\\tMSE\\nreg_losses\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nREGULARIZATION_LOSSES\\n)\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n([\\nreconstruction_loss\\n]\\n\\t\\n+\\n\\t\\nreg_losses\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nYou\\tcan\\tthen\\ttrain\\t\\nthe\\tmodel\\tnormally.\\tNote\\tthat\\tthe\\tdigit\\tlabels\\t(\\ny_batch\\n)\\tare\\tunused:\\nn_epochs\\n\\t\\n=\\n\\t\\n5\\nbatch_size\\n\\t\\n=\\n\\t\\n150\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nn_batches\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_batches\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n})', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 511}), Document(page_content='Tying\\tWeights\\nWhen\\t\\nan\\tautoencoder\\tis\\tneatly\\tsymmetrical,\\tlike\\tthe\\tone\\twe\\tjust\\tbuilt,\\ta\\tcommon\\ttechnique\\tis\\tto\\t\\ntie\\tthe\\nweights\\n\\tof\\t\\nthe\\tdecoder\\tlayers\\tto\\tthe\\tweights\\tof\\tthe\\tencoder\\tlayers.\\tThis\\thalves\\tthe\\tnumber\\tof\\tweights\\tin\\nthe\\tmodel,\\tspeeding\\tup\\ttraining\\tand\\tlimiting\\tthe\\trisk\\tof\\toverfitting.\\tSpecifically,\\tif\\tthe\\tautoencoder\\thas\\ta\\ntotal\\tof\\t\\nN\\n\\tlayers\\t(not\\tcounting\\tthe\\tinput\\tlayer),\\tand\\t\\nW\\nL\\n\\trepresents\\tthe\\tconnection\\tweights\\tof\\tthe\\t\\nL\\nth\\n\\tlayer\\n(e.g.,\\tlayer\\t1\\tis\\tthe\\tfirst\\thidden\\tlayer,\\tlayer\\t\\n\\tis\\tthe\\tcoding\\tlayer,\\tand\\tlayer\\t\\nN\\n\\tis\\tthe\\toutput\\tlayer),\\tthen\\tthe\\ndecoder\\tlayer\\tweights\\tcan\\tbe\\tdefined\\tsimply\\tas:\\t\\nW\\nN–L\\n+1\\n\\t=\\t\\nW\\nL\\nT\\n\\t(with\\t\\nL\\n\\t=\\t1,\\t2,\\t\\n).\\nUnfortunately,\\timplementing\\ttied\\tweights\\tin\\tTensorFlow\\tusing\\tthe\\t\\ndense()\\n\\t\\nfunction\\tis\\ta\\tbit\\tcumbersome;\\nit’s\\tactually\\teasier\\tto\\tjust\\tdefine\\tthe\\tlayers\\tmanually.\\tThe\\tcode\\tends\\tup\\tsignificantly\\tmore\\tverbose:\\nactivation\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nelu\\nregularizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nl2_regularizer\\n(\\nl2_reg\\n)\\ninitializer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nweights1_init\\n\\t\\n=\\n\\t\\ninitializer\\n([\\nn_inputs\\n,\\n\\t\\nn_hidden1\\n])\\nweights2_init\\n\\t\\n=\\n\\t\\ninitializer\\n([\\nn_hidden1\\n,\\n\\t\\nn_hidden2\\n])\\nweights1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\nweights1_init\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"weights1\"\\n)\\nweights2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\nweights2_init\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"weights2\"\\n)\\nweights3\\n\\t\\n=\\n\\t\\ntf\\n.\\ntranspose\\n(\\nweights2\\n,\\n\\t\\nname\\n=\\n\"weights3\"\\n)\\n\\t\\t\\n#\\ttied\\tweights\\nweights4\\n\\t\\n=\\n\\t\\ntf\\n.\\ntranspose\\n(\\nweights1\\n,\\n\\t\\nname\\n=\\n\"weights4\"\\n)\\n\\t\\t\\n#\\ttied\\tweights\\nbiases1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n(\\nn_hidden1\\n),\\n\\t\\nname\\n=\\n\"biases1\"\\n)\\nbiases2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n(\\nn_hidden2\\n),\\n\\t\\nname\\n=\\n\"biases2\"\\n)\\nbiases3\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n(\\nn_hidden3\\n),\\n\\t\\nname\\n=\\n\"biases3\"\\n)\\nbiases4\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n(\\nn_outputs\\n),\\n\\t\\nname\\n=\\n\"biases4\"\\n)\\nhidden1\\n\\t\\n=\\n\\t\\nactivation\\n(\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\nweights1\\n)\\n\\t\\n+\\n\\t\\nbiases1\\n)\\nhidden2\\n\\t\\n=\\n\\t\\nactivation\\n(\\ntf\\n.\\nmatmul\\n(\\nhidden1\\n,\\n\\t\\nweights2\\n)\\n\\t\\n+\\n\\t\\nbiases2\\n)\\nhidden3\\n\\t\\n=\\n\\t\\nactivation\\n(\\ntf\\n.\\nmatmul\\n(\\nhidden2\\n,\\n\\t\\nweights3\\n)\\n\\t\\n+\\n\\t\\nbiases3\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nmatmul\\n(\\nhidden3\\n,\\n\\t\\nweights4\\n)\\n\\t\\n+\\n\\t\\nbiases4\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\nreg_loss\\n\\t\\n=\\n\\t\\nregularizer\\n(\\nweights1\\n)\\n\\t\\n+\\n\\t\\nregularizer\\n(\\nweights2\\n)\\nloss\\n\\t\\n=\\n\\t\\nreconstruction_loss\\n\\t\\n+\\n\\t\\nreg_loss\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nThis\\tcode\\tis\\tfairly\\tstraightforward,\\t\\nbut\\tthere\\tare\\ta\\tfew\\timportant\\tthings\\tto\\tnote:\\nFirst,\\t\\nweight3\\n\\tand\\t\\nweights4\\n\\tare\\tnot\\tvariables,\\tthey\\tare\\trespectively\\tthe\\ttranspose\\tof\\t\\nweights2\\n\\tand\\nweights1\\n\\t(they\\tare\\t“tied”\\tto\\tthem).\\nSecond,\\tsince\\tthey\\tare\\tnot\\tvariables,\\tit’s\\tno\\tuse\\tregularizing\\tthem:\\twe\\tonly\\tregularize\\t\\nweights1\\n\\tand\\nweights2\\n.\\nThird,\\tbiases\\tare\\tnever\\ttied,\\tand\\tnever\\t\\nregularized.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 512}), Document(page_content='Training\\tOne\\tAutoencoder\\tat\\ta\\tTime\\nRather\\t\\nthan\\ttraining\\tthe\\twhole\\tstacked\\tautoencoder\\tin\\tone\\tgo\\tlike\\twe\\tjust\\tdid,\\tit\\tis\\toften\\tmuch\\tfaster\\tto\\ntrain\\tone\\tshallow\\tautoencoder\\tat\\ta\\ttime,\\tthen\\tstack\\tall\\tof\\tthem\\tinto\\ta\\tsingle\\tstacked\\tautoencoder\\t(hence\\nthe\\tname),\\tas\\tshown\\ton\\t\\nFigure\\t15-4\\n.\\tThis\\tis\\tespecially\\tuseful\\tfor\\tvery\\tdeep\\tautoencoders.\\nFigure\\t15-4.\\t\\nTraining\\tone\\tautoencoder\\tat\\ta\\ttime\\nDuring\\tthe\\tfirst\\tphase\\tof\\ttraining,\\tthe\\tfirst\\tautoencoder\\tlearns\\tto\\treconstruct\\tthe\\tinputs.\\tDuring\\tthe\\tsecond\\nphase,\\tthe\\tsecond\\tautoencoder\\tlearns\\tto\\treconstruct\\tthe\\toutput\\tof\\tthe\\tfirst\\tautoencoder’s\\thidden\\tlayer.\\nFinally,\\tyou\\tjust\\tbuild\\ta\\tbig\\tsandwich\\tusing\\tall\\tthese\\tautoencoders,\\tas\\tshown\\tin\\t\\nFigure\\t15-4\\n\\t(i.e.,\\tyou\\tfirst\\nstack\\tthe\\thidden\\tlayers\\tof\\teach\\tautoencoder,\\tthen\\tthe\\toutput\\tlayers\\tin\\treverse\\torder).\\tThis\\tgives\\tyou\\tthe\\nfinal\\tstacked\\tautoencoder.\\tYou\\tcould\\teasily\\ttrain\\tmore\\tautoencoders\\tthis\\tway,\\tbuilding\\ta\\tvery\\tdeep\\nstacked\\tautoencoder.\\nTo\\timplement\\tthis\\tmultiphase\\ttraining\\talgorithm,\\tthe\\tsimplest\\tapproach\\tis\\tto\\tuse\\ta\\tdifferent\\tTensorFlow\\ngraph\\tfor\\teach\\tphase.\\tAfter\\ttraining\\tan\\tautoencoder,\\tyou\\tjust\\trun\\tthe\\ttraining\\tset\\tthrough\\tit\\tand\\tcapture\\tthe\\noutput\\tof\\tthe\\thidden\\tlayer.\\tThis\\toutput\\tthen\\tserves\\tas\\tthe\\ttraining\\tset\\tfor\\tthe\\tnext\\tautoencoder.\\tOnce\\tall\\nautoencoders\\thave\\tbeen\\ttrained\\tthis\\tway,\\tyou\\tsimply\\tcopy\\tthe\\tweights\\tand\\tbiases\\tfrom\\teach\\tautoencoder\\nand\\tuse\\tthem\\tto\\tbuild\\tthe\\tstacked\\tautoencoder.\\tImplementing\\tthis\\tapproach\\tis\\tquite\\tstraightforward,\\tso\\twe\\nwon’t\\tdetail\\tit\\there,\\tbut\\tplease\\tcheck\\tout\\tthe\\tcode\\tin\\tthe\\t\\nJupyter\\tnotebooks\\n\\tfor\\tan\\texample.\\nAnother\\tapproach\\tis\\tto\\tuse\\ta\\tsingle\\tgraph\\tcontaining\\tthe\\twhole\\tstacked\\tautoencoder,\\tplus\\tsome\\textra\\noperations\\tto\\tperform\\teach\\ttraining\\tphase,\\tas\\tshown\\tin\\t\\nFigure\\t15-5\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 513}), Document(page_content='Figure\\t15-5.\\t\\nA\\tsingle\\tgraph\\tto\\ttrain\\ta\\tstacked\\tautoencoder\\nThis\\tdeserves\\ta\\tbit\\tof\\texplanation:\\nThe\\tcentral\\tcolumn\\tin\\tthe\\tgraph\\tis\\tthe\\tfull\\tstacked\\tautoencoder.\\tThis\\tpart\\tcan\\tbe\\tused\\tafter\\ttraining.\\nThe\\tleft\\tcolumn\\tis\\tthe\\tset\\tof\\toperations\\tneeded\\tto\\trun\\tthe\\tfirst\\tphase\\tof\\ttraining.\\tIt\\tcreates\\tan\\toutput\\nlayer\\tthat\\tbypasses\\thidden\\tlayers\\t2\\tand\\t3.\\tThis\\toutput\\tlayer\\tshares\\tthe\\tsame\\tweights\\tand\\tbiases\\tas\\nthe\\tstacked\\tautoencoder’s\\toutput\\tlayer.\\tOn\\ttop\\tof\\tthat\\tare\\tthe\\ttraining\\toperations\\tthat\\twill\\taim\\tat\\nmaking\\tthe\\toutput\\tas\\tclose\\tas\\tpossible\\tto\\tthe\\tinputs.\\tThus,\\tthis\\tphase\\twill\\ttrain\\tthe\\tweights\\tand\\nbiases\\tfor\\tthe\\thidden\\tlayer\\t1\\tand\\tthe\\toutput\\tlayer\\t(i.e.,\\tthe\\tfirst\\tautoencoder).\\nThe\\tright\\tcolumn\\tin\\tthe\\tgraph\\tis\\tthe\\tset\\tof\\toperations\\tneeded\\tto\\trun\\tthe\\tsecond\\tphase\\tof\\ttraining.\\tIt\\nadds\\tthe\\ttraining\\toperation\\tthat\\twill\\taim\\tat\\tmaking\\tthe\\toutput\\tof\\thidden\\tlayer\\t3\\tas\\tclose\\tas\\tpossible\\nto\\tthe\\toutput\\tof\\thidden\\tlayer\\t1.\\tNote\\tthat\\twe\\tmust\\tfreeze\\thidden\\tlayer\\t1\\twhile\\trunning\\tphase\\t2.\\tThis\\nphase\\twill\\ttrain\\tthe\\tweights\\tand\\tbiases\\tfor\\thidden\\tlayers\\t2\\tand\\t3\\t(i.e.,\\tthe\\tsecond\\tautoencoder).\\nThe\\tTensorFlow\\tcode\\tlooks\\t\\nlike\\tthis:\\n[\\n...\\n]\\n\\t\\n#\\tBuild\\tthe\\twhole\\tstacked\\tautoencoder\\tnormally.\\n\\t\\t\\t\\t\\t\\t\\n#\\tIn\\tthis\\texample,\\tthe\\tweights\\tare\\tnot\\ttied.\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"phase1\"\\n):\\n\\t\\t\\t\\t\\nphase1_outputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nmatmul\\n(\\nhidden1\\n,\\n\\t\\nweights4\\n)\\n\\t\\n+\\n\\t\\nbiases4\\n\\t\\t\\t\\t\\nphase1_reconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\nphase1_outputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\t\\t\\t\\nphase1_reg_loss\\n\\t\\n=\\n\\t\\nregularizer\\n(\\nweights1\\n)\\n\\t\\n+\\n\\t\\nregularizer\\n(\\nweights4\\n)\\n\\t\\t\\t\\t\\nphase1_loss\\n\\t\\n=\\n\\t\\nphase1_reconstruction_loss\\n\\t\\n+\\n\\t\\nphase1_reg_loss\\n\\t\\t\\t\\t\\nphase1_training_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nphase1_loss\\n)\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"phase2\"\\n):\\n\\t\\t\\t\\t\\nphase2_reconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\nhidden3\\n\\t\\n-\\n\\t\\nhidden1\\n))\\n\\t\\t\\t\\t\\nphase2_reg_loss\\n\\t\\n=\\n\\t\\nregularizer\\n(\\nweights2\\n)\\n\\t\\n+\\n\\t\\nregularizer\\n(\\nweights3\\n)\\n\\t\\t\\t\\t\\nphase2_loss\\n\\t\\n=\\n\\t\\nphase2_reconstruction_loss\\n\\t\\n+\\n\\t\\nphase2_reg_loss\\n\\t\\t\\t\\t\\ntrain_vars\\n\\t\\n=\\n\\t\\n[\\nweights2\\n,\\n\\t\\nbiases2\\n,\\n\\t\\nweights3\\n,\\n\\t\\nbiases3\\n]\\n\\t\\t\\t\\t\\nphase2_training_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nphase2_loss\\n,\\n\\t\\nvar_list\\n=\\ntrain_vars\\n)\\nThe\\tfirst\\tphase\\tis\\trather\\tstraightforward:\\twe\\t\\njust\\tcreate\\tan\\toutput\\tlayer\\tthat\\tskips\\thidden\\tlayers\\t2\\tand\\t3,\\nthen\\tbuild\\tthe\\ttraining\\toperations\\tto\\tminimize\\tthe\\tdistance\\tbetween\\tthe\\toutputs\\tand\\tthe\\tinputs\\t(plus\\tsome', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 514}), Document(page_content='regularization).\\nThe\\tsecond\\tphase\\tjust\\tadds\\tthe\\toperations\\tneeded\\tto\\tminimize\\tthe\\tdistance\\tbetween\\tthe\\toutput\\tof\\thidden\\nlayer\\t3\\tand\\thidden\\tlayer\\t1\\t(also\\twith\\tsome\\tregularization).\\tMost\\timportantly,\\twe\\tprovide\\tthe\\tlist\\tof\\ntrainable\\tvariables\\tto\\tthe\\t\\nminimize()\\n\\tmethod,\\tmaking\\tsure\\tto\\tleave\\tout\\t\\nweights1\\n\\tand\\t\\nbiases1\\n;\\tthis\\neffectively\\tfreezes\\thidden\\tlayer\\t1\\tduring\\tphase\\t2.\\nDuring\\tthe\\texecution\\tphase,\\tall\\tyou\\tneed\\tto\\tdo\\tis\\trun\\tthe\\tphase\\t1\\ttraining\\top\\tfor\\ta\\tnumber\\tof\\tepochs,\\tthen\\nthe\\tphase\\t2\\ttraining\\top\\tfor\\tsome\\tmore\\tepochs.\\nTIP\\nSince\\thidden\\tlayer\\t1\\tis\\tfrozen\\tduring\\tphase\\t2,\\tits\\toutput\\twill\\talways\\tbe\\tthe\\tsame\\tfor\\tany\\tgiven\\ttraining\\tinstance.\\tTo\\tavoid\\thaving\\nto\\trecompute\\tthe\\toutput\\tof\\thidden\\tlayer\\t1\\tat\\tevery\\tsingle\\tepoch,\\tyou\\tcan\\tcompute\\tit\\tfor\\tthe\\twhole\\ttraining\\tset\\tat\\tthe\\tend\\tof\\tphase\\n1,\\tthen\\tdirectly\\tfeed\\tthe\\tcached\\toutput\\tof\\thidden\\tlayer\\t1\\tduring\\tphase\\t2.\\tThis\\tcan\\tgive\\tyou\\ta\\tnice\\tperformance\\t\\nboost.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 515}), Document(page_content='Visualizing\\tthe\\tReconstructions\\nOne\\t\\nway\\tto\\tensure\\tthat\\tan\\tautoencoder\\tis\\tproperly\\ttrained\\tis\\tto\\tcompare\\tthe\\tinputs\\tand\\tthe\\toutputs.\\tThey\\nmust\\tbe\\tfairly\\tsimilar,\\tand\\tthe\\tdifferences\\tshould\\tbe\\tunimportant\\tdetails.\\tLet’s\\tplot\\ttwo\\trandom\\tdigits\\tand\\ntheir\\treconstructions:\\nn_test_digits\\n\\t\\n=\\n\\t\\n2\\nX_test\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntest\\n.\\nimages\\n[:\\nn_test_digits\\n]\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tTrain\\tthe\\tAutoencoder\\n\\t\\t\\t\\t\\noutputs_val\\n\\t\\n=\\n\\t\\noutputs\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_test\\n})\\ndef\\n\\t\\nplot_image\\n(\\nimage\\n,\\n\\t\\nshape\\n=\\n[\\n28\\n,\\n\\t\\n28\\n]):\\n\\t\\t\\t\\t\\nplt\\n.\\nimshow\\n(\\nimage\\n.\\nreshape\\n(\\nshape\\n),\\n\\t\\ncmap\\n=\\n\"Greys\"\\n,\\n\\t\\ninterpolation\\n=\\n\"nearest\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\naxis\\n(\\n\"off\"\\n)\\nfor\\n\\t\\ndigit_index\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_test_digits\\n):\\n\\t\\t\\t\\t\\nplt\\n.\\nsubplot\\n(\\nn_test_digits\\n,\\n\\t\\n2\\n,\\n\\t\\ndigit_index\\n\\t\\n*\\n\\t\\n2\\n\\t\\n+\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\nplot_image\\n(\\nX_test\\n[\\ndigit_index\\n])\\n\\t\\t\\t\\t\\nplt\\n.\\nsubplot\\n(\\nn_test_digits\\n,\\n\\t\\n2\\n,\\n\\t\\ndigit_index\\n\\t\\n*\\n\\t\\n2\\n\\t\\n+\\n\\t\\n2\\n)\\n\\t\\t\\t\\t\\nplot_image\\n(\\noutputs_val\\n[\\ndigit_index\\n])\\nFigure\\t15-6\\n\\tshows\\tthe\\tresulting\\timages.\\nFigure\\t15-6.\\t\\nOriginal\\tdigits\\t(left)\\tand\\ttheir\\treconstructions\\t(right)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 516}), Document(page_content='Looks\\tclose\\tenough.\\tSo\\tthe\\tautoencoder\\thas\\tproperly\\tlearned\\tto\\treproduce\\tits\\tinputs,\\tbut\\thas\\tit\\tlearned\\nuseful\\tfeatures?\\t\\nLet’s\\ttake\\ta\\tlook.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 517}), Document(page_content='Visualizing\\tFeatures\\nOnce\\t\\nyour\\tautoencoder\\thas\\tlearned\\tsome\\tfeatures,\\tyou\\tmay\\twant\\tto\\ttake\\ta\\tlook\\tat\\tthem.\\tThere\\tare\\tvarious\\ntechniques\\tfor\\tthis.\\tArguably\\tthe\\tsimplest\\ttechnique\\tis\\tto\\tconsider\\teach\\tneuron\\tin\\tevery\\thidden\\tlayer,\\tand\\nfind\\tthe\\ttraining\\tinstances\\tthat\\tactivate\\tit\\tthe\\tmost.\\tThis\\tis\\tespecially\\tuseful\\tfor\\tthe\\ttop\\thidden\\tlayers\\tsince\\nthey\\toften\\tcapture\\trelatively\\tlarge\\tfeatures\\tthat\\tyou\\tcan\\teasily\\tspot\\tin\\ta\\tgroup\\tof\\ttraining\\tinstances\\tthat\\ncontain\\tthem.\\tFor\\texample,\\tif\\ta\\tneuron\\tstrongly\\tactivates\\twhen\\tit\\tsees\\ta\\tcat\\tin\\ta\\tpicture,\\tit\\twill\\tbe\\tpretty\\nobvious\\tthat\\tthe\\tpictures\\tthat\\tactivate\\tit\\tthe\\tmost\\tall\\tcontain\\tcats.\\tHowever,\\tfor\\tlower\\tlayers,\\tthis\\ntechnique\\tdoes\\tnot\\twork\\tso\\twell,\\tas\\tthe\\tfeatures\\tare\\tsmaller\\tand\\tmore\\tabstract,\\tso\\tit’s\\toften\\thard\\tto\\nunderstand\\texactly\\twhat\\tthe\\tneuron\\tis\\tgetting\\tall\\texcited\\tabout.\\nLet’s\\tlook\\tat\\tanother\\ttechnique.\\tFor\\teach\\tneuron\\tin\\tthe\\tfirst\\thidden\\tlayer,\\tyou\\tcan\\tcreate\\tan\\timage\\twhere\\ta\\npixel’s\\tintensity\\tcorresponds\\tto\\tthe\\tweight\\tof\\tthe\\tconnection\\tto\\tthe\\tgiven\\tneuron.\\tFor\\texample,\\tthe\\nfollowing\\tcode\\tplots\\tthe\\tfeatures\\tlearned\\tby\\tfive\\tneurons\\tin\\tthe\\tfirst\\thidden\\tlayer:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\ttrain\\tautoencoder\\n\\t\\t\\t\\t\\nweights1_val\\n\\t\\n=\\n\\t\\nweights1\\n.\\neval\\n()\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n):\\n\\t\\t\\t\\t\\nplt\\n.\\nsubplot\\n(\\n1\\n,\\n\\t\\n5\\n,\\n\\t\\ni\\n\\t\\n+\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\nplot_image\\n(\\nweights1_val\\n.\\nT\\n[\\ni\\n])\\nYou\\tmay\\tget\\tlow-level\\tfeatures\\tsuch\\tas\\tthe\\tones\\tshown\\tin\\t\\nFigure\\t15-7\\n.\\nFigure\\t15-7.\\t\\nFeatures\\tlearned\\tby\\tfive\\tneurons\\tfrom\\tthe\\tfirst\\thidden\\tlayer\\nThe\\tfirst\\tfour\\tfeatures\\tseem\\tto\\tcorrespond\\tto\\tsmall\\tpatches,\\twhile\\tthe\\tfifth\\tfeature\\tseems\\tto\\tlook\\tfor\\nvertical\\tstrokes\\t(note\\tthat\\tthese\\tfeatures\\tcome\\tfrom\\tthe\\t\\nstacked\\tdenoising\\tautoencoder\\tthat\\twe\\twill\\ndiscuss\\tlater).\\nAnother\\ttechnique\\tis\\tto\\tfeed\\tthe\\tautoencoder\\ta\\trandom\\tinput\\timage,\\tmeasure\\tthe\\tactivation\\tof\\tthe\\tneuron\\nyou\\tare\\tinterested\\tin,\\tand\\tthen\\tperform\\tbackpropagation\\t\\nto\\ttweak\\tthe\\timage\\tin\\tsuch\\ta\\tway\\tthat\\tthe\\tneuron\\nwill\\tactivate\\teven\\tmore.\\tIf\\tyou\\titerate\\tseveral\\ttimes\\t(performing\\tgradient\\tascent),\\tthe\\timage\\twill\\ngradually\\tturn\\tinto\\tthe\\tmost\\texciting\\timage\\t(for\\tthe\\tneuron).\\tThis\\tis\\ta\\tuseful\\ttechnique\\tto\\tvisualize\\tthe\\nkinds\\tof\\tinputs\\tthat\\ta\\tneuron\\tis\\tlooking\\tfor.\\nFinally,\\tif\\tyou\\tare\\tusing\\tan\\tautoencoder\\tto\\tperform\\tunsupervised\\tpretraining\\t—\\tfor\\texample,\\tfor\\ta\\nclassification\\ttask\\t—\\ta\\tsimple\\tway\\tto\\tverify\\tthat\\tthe\\tfeatures\\tlearned\\tby\\tthe\\tautoencoder\\tare\\tuseful\\tis\\tto\\nmeasure\\tthe\\tperformance\\tof\\tthe\\t\\nclassifier.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 518}), Document(page_content='Unsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\nAs\\t\\nwe\\tdiscussed\\tin\\t\\nChapter\\t11\\n,\\tif\\tyou\\tare\\ttackling\\ta\\tcomplex\\tsupervised\\ttask\\tbut\\tyou\\tdo\\tnot\\thave\\ta\\tlot\\tof\\nlabeled\\ttraining\\tdata,\\tone\\tsolution\\tis\\tto\\tfind\\ta\\tneural\\tnetwork\\tthat\\tperforms\\ta\\tsimilar\\ttask,\\tand\\tthen\\treuse\\nits\\tlower\\tlayers.\\tThis\\tmakes\\tit\\tpossible\\tto\\ttrain\\ta\\thigh-performance\\tmodel\\tusing\\tonly\\tlittle\\ttraining\\tdata\\nbecause\\tyour\\tneural\\tnetwork\\twon’t\\thave\\tto\\tlearn\\tall\\tthe\\tlow-level\\tfeatures;\\tit\\twill\\tjust\\treuse\\tthe\\tfeature\\ndetectors\\tlearned\\tby\\tthe\\texisting\\tnet.\\nSimilarly,\\tif\\tyou\\thave\\ta\\tlarge\\tdataset\\tbut\\tmost\\tof\\tit\\tis\\tunlabeled,\\tyou\\tcan\\tfirst\\ttrain\\ta\\tstacked\\tautoencoder\\nusing\\tall\\tthe\\tdata,\\tthen\\treuse\\tthe\\tlower\\tlayers\\tto\\tcreate\\ta\\tneural\\tnetwork\\tfor\\tyour\\tactual\\ttask,\\tand\\ttrain\\tit\\nusing\\tthe\\tlabeled\\tdata.\\tFor\\texample,\\t\\nFigure\\t15-8\\n\\tshows\\thow\\tto\\tuse\\ta\\tstacked\\tautoencoder\\tto\\tperform\\nunsupervised\\tpretraining\\tfor\\ta\\tclassification\\tneural\\tnetwork.\\tThe\\tstacked\\tautoencoder\\titself\\tis\\ttypically\\ntrained\\tone\\tautoencoder\\tat\\ta\\ttime,\\tas\\tdiscussed\\tearlier.\\tWhen\\ttraining\\tthe\\tclassifier,\\tif\\tyou\\treally\\tdon’t\\nhave\\tmuch\\tlabeled\\ttraining\\tdata,\\tyou\\tmay\\twant\\tto\\tfreeze\\tthe\\tpretrained\\tlayers\\t(at\\tleast\\tthe\\tlower\\tones).\\nFigure\\t15-8.\\t\\nUnsupervised\\tpretraining\\tusing\\tautoencoders\\nNOTE\\nThis\\tsituation\\tis\\tactually\\tquite\\tcommon,\\tbecause\\tbuilding\\ta\\tlarge\\tunlabeled\\tdataset\\tis\\toften\\tcheap\\t(e.g.,\\ta\\tsimple\\tscript\\tcan\\ndownload\\tmillions\\tof\\timages\\toff\\tthe\\tinternet),\\tbut\\tlabeling\\tthem\\tcan\\tonly\\tbe\\tdone\\treliably\\tby\\thumans\\t(e.g.,\\tclassifying\\timages\\tas\\ncute\\tor\\tnot).\\tLabeling\\tinstances\\tis\\ttime-consuming\\tand\\tcostly,\\tso\\tit\\tis\\tquite\\tcommon\\tto\\thave\\tonly\\ta\\tfew\\tthousand\\tlabeled\\ninstances.\\nAs\\twe\\tdiscussed\\tearlier,\\tone\\tof\\tthe\\ttriggers\\tof\\tthe\\tcurrent\\tDeep\\tLearning\\ttsunami\\tis\\tthe\\tdiscovery\\tin\\t2006\\nby\\tGeoffrey\\tHinton\\tet\\tal.\\tthat\\tdeep\\tneural\\tnetworks\\tcan\\tbe\\tpretrained\\tin\\tan\\tunsupervised\\tfashion.\\tThey\\nused\\trestricted\\tBoltzmann\\tmachines\\tfor\\tthat\\t(see\\t\\nAppendix\\tE\\n),\\tbut\\tin\\t\\n2007\\tYoshua\\tBengio\\tet\\tal.\\tshowed\\n2\\nthat\\tautoencoders\\tworked\\tjust\\tas\\twell.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 519}), Document(page_content='There\\tis\\tnothing\\tspecial\\tabout\\tthe\\tTensorFlow\\timplementation:\\tjust\\ttrain\\tan\\tautoencoder\\tusing\\tall\\tthe\\ntraining\\tdata,\\tthen\\treuse\\tits\\tencoder\\tlayers\\tto\\tcreate\\ta\\tnew\\tneural\\tnetwork\\t(see\\t\\nChapter\\t11\\n\\tfor\\tmore\\ndetails\\ton\\thow\\tto\\treuse\\tpretrained\\tlayers,\\tor\\tcheck\\tout\\tthe\\tcode\\texamples\\tin\\tthe\\tJupyter\\tnotebooks).\\nUp\\tto\\tnow,\\tin\\torder\\tto\\tforce\\tthe\\tautoencoder\\tto\\tlearn\\tinteresting\\tfeatures,\\twe\\thave\\tlimited\\tthe\\tsize\\tof\\tthe\\ncoding\\tlayer,\\tmaking\\tit\\tundercomplete.\\tThere\\tare\\tactually\\tmany\\tother\\tkinds\\tof\\tconstraints\\tthat\\tcan\\tbe\\nused,\\tincluding\\tones\\tthat\\tallow\\tthe\\tcoding\\tlayer\\tto\\tbe\\tjust\\tas\\tlarge\\tas\\tthe\\tinputs,\\tor\\teven\\tlarger,\\tresulting\\nin\\t\\nan\\t\\novercomplete\\tautoencoder\\n.\\tLet’s\\tlook\\tat\\tsome\\tof\\tthose\\t\\napproaches\\tnow.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 520}), Document(page_content='Denoising\\tAutoencoders\\nAnother\\t\\nway\\tto\\tforce\\tthe\\tautoencoder\\tto\\tlearn\\tuseful\\tfeatures\\tis\\tto\\tadd\\tnoise\\tto\\tits\\tinputs,\\ttraining\\tit\\tto\\nrecover\\tthe\\toriginal,\\tnoise-free\\tinputs.\\tThis\\tprevents\\tthe\\tautoencoder\\tfrom\\ttrivially\\tcopying\\tits\\tinputs\\tto\\nits\\toutputs,\\tso\\tit\\tends\\tup\\thaving\\tto\\tfind\\tpatterns\\tin\\tthe\\tdata.\\nThe\\tidea\\tof\\tusing\\tautoencoders\\tto\\tremove\\tnoise\\thas\\tbeen\\taround\\tsince\\tthe\\t1980s\\t(e.g.,\\tit\\tis\\tmentioned\\tin\\nYann\\tLeCun’s\\t1987\\tmaster’s\\tthesis).\\tIn\\ta\\t\\n2008\\tpaper\\n,\\n3\\n\\tPascal\\tVincent\\tet\\tal.\\tshowed\\tthat\\tautoencoders\\ncould\\talso\\tbe\\tused\\tfor\\tfeature\\textraction.\\tIn\\ta\\t\\n2010\\tpaper\\n,\\n4\\n\\tVincent\\tet\\tal.\\t\\nintroduced\\t\\nstacked\\tdenoising\\nautoencoders\\n.\\nThe\\t\\nnoise\\tcan\\tbe\\tpure\\tGaussian\\tnoise\\tadded\\tto\\tthe\\tinputs,\\tor\\tit\\tcan\\tbe\\trandomly\\tswitched\\toff\\tinputs,\\tjust\\nlike\\tin\\tdropout\\t(introduced\\tin\\t\\nChapter\\t11\\n).\\t\\nFigure\\t15-9\\n\\tshows\\tboth\\toptions.\\nFigure\\t15-9.\\t\\nDenoising\\tautoencoders,\\twith\\tGaussian\\tnoise\\t(left)\\tor\\tdropout\\t(right)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 521}), Document(page_content='TensorFlow\\tImplementation\\nImplementing\\t\\ndenoising\\tautoencoders\\tin\\tTensorFlow\\tis\\tnot\\ttoo\\thard.\\tLet’s\\tstart\\twith\\tGaussian\\tnoise.\\tIt’s\\nreally\\tjust\\tlike\\ttraining\\ta\\tregular\\tautoencoder,\\texcept\\tyou\\tadd\\tnoise\\tto\\tthe\\tinputs,\\tand\\tthe\\treconstruction\\nloss\\tis\\tcalculated\\tbased\\ton\\tthe\\toriginal\\tinputs:\\nnoise_level\\n\\t\\n=\\n\\t\\n1.0\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nX_noisy\\n\\t\\n=\\n\\t\\nX\\n\\t\\n+\\n\\t\\nnoise_level\\n\\t\\n*\\n\\t\\ntf\\n.\\nrandom_normal\\n(\\ntf\\n.\\nshape\\n(\\nX\\n))\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX_noisy\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden1\"\\n)\\n[\\n...\\n]\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\n#\\tMSE\\n[\\n...\\n]\\nWARNING\\nSince\\tthe\\tshape\\tof\\t\\nX\\n\\tis\\tonly\\tpartially\\tdefined\\tduring\\tthe\\tconstruction\\tphase,\\twe\\tcannot\\tknow\\tin\\tadvance\\tthe\\tshape\\tof\\tthe\\tnoise\\nthat\\twe\\tmust\\tadd\\tto\\t\\nX\\n.\\tWe\\tcannot\\tcall\\t\\nX.get_shape()\\n\\tbecause\\tthis\\twould\\tjust\\treturn\\tthe\\tpartially\\tdefined\\tshape\\tof\\t\\nX\\n\\t(\\n[None,\\nn_inputs]\\n),\\tand\\t\\nrandom_normal()\\n\\texpects\\ta\\tfully\\tdefined\\tshape\\tso\\tit\\twould\\traise\\tan\\texception.\\tInstead,\\twe\\tcall\\t\\ntf.shape(X)\\n,\\nwhich\\tcreates\\tan\\toperation\\tthat\\twill\\treturn\\tthe\\tshape\\tof\\t\\nX\\n\\tat\\truntime,\\twhich\\twill\\tbe\\tfully\\tdefined\\tat\\tthat\\tpoint.\\nImplementing\\tthe\\tdropout\\tversion,\\t\\nwhich\\tis\\tmore\\tcommon,\\tis\\tnot\\tmuch\\tharder:\\ndropout_rate\\n\\t\\n=\\n\\t\\n0.3\\ntraining\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder_with_default\\n(\\nFalse\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\nname\\n=\\n\\'training\\'\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nX_drop\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndropout\\n(\\nX\\n,\\n\\t\\ndropout_rate\\n,\\n\\t\\ntraining\\n=\\ntraining\\n)\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX_drop\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden1\"\\n)\\n[\\n...\\n]\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\n#\\tMSE\\n[\\n...\\n]\\nDuring\\ttraining\\twe\\tmust\\tset\\t\\ntraining\\n\\tto\\t\\nTrue\\n\\t(as\\texplained\\tin\\t\\nChapter\\t11\\n)\\tusing\\tthe\\t\\nfeed_dict\\n:\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ntraining\\n:\\n\\t\\nTrue\\n})\\nDuring\\ttesting\\tit\\tis\\tnot\\tnecessary\\tto\\tset\\t\\ntraining\\n\\tto\\t\\nFalse\\n,\\tsince\\twe\\tset\\tthat\\tas\\tthe\\tdefault\\tin\\tthe\\tcall\\tto\\nthe\\t\\nplaceholder_with_default()\\n\\t\\nfunction.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 522}), Document(page_content='Sparse\\tAutoencoders\\nAnother\\t\\nkind\\tof\\tconstraint\\tthat\\toften\\tleads\\tto\\tgood\\tfeature\\textraction\\tis\\t\\nsparsity\\n:\\tby\\tadding\\tan\\tappropriate\\nterm\\tto\\tthe\\tcost\\tfunction,\\tthe\\tautoencoder\\tis\\tpushed\\tto\\treduce\\tthe\\tnumber\\tof\\tactive\\tneurons\\tin\\tthe\\tcoding\\nlayer.\\tFor\\texample,\\tit\\tmay\\tbe\\tpushed\\tto\\thave\\ton\\taverage\\tonly\\t5%\\tsignificantly\\tactive\\tneurons\\tin\\tthe\\ncoding\\tlayer.\\tThis\\tforces\\tthe\\tautoencoder\\tto\\trepresent\\teach\\tinput\\tas\\ta\\tcombination\\tof\\ta\\tsmall\\tnumber\\tof\\nactivations.\\tAs\\ta\\tresult,\\teach\\tneuron\\tin\\tthe\\tcoding\\tlayer\\ttypically\\tends\\tup\\trepresenting\\ta\\tuseful\\tfeature\\t(if\\nyou\\tcould\\tspeak\\tonly\\ta\\tfew\\twords\\tper\\tmonth,\\tyou\\twould\\tprobably\\ttry\\tto\\tmake\\tthem\\tworth\\tlistening\\tto).\\nIn\\torder\\tto\\tfavor\\tsparse\\tmodels,\\twe\\tmust\\tfirst\\tmeasure\\tthe\\tactual\\tsparsity\\tof\\tthe\\tcoding\\tlayer\\tat\\teach\\ntraining\\titeration.\\tWe\\tdo\\tso\\tby\\tcomputing\\tthe\\taverage\\tactivation\\tof\\teach\\tneuron\\tin\\tthe\\tcoding\\tlayer,\\tover\\nthe\\twhole\\ttraining\\tbatch.\\tThe\\tbatch\\tsize\\tmust\\tnot\\tbe\\ttoo\\tsmall,\\tor\\telse\\tthe\\tmean\\twill\\tnot\\tbe\\taccurate.\\nOnce\\twe\\thave\\tthe\\tmean\\tactivation\\tper\\tneuron,\\twe\\twant\\tto\\tpenalize\\tthe\\tneurons\\tthat\\tare\\ttoo\\tactive\\tby\\nadding\\t\\na\\t\\nsparsity\\tloss\\n\\tto\\tthe\\tcost\\tfunction.\\tFor\\texample,\\tif\\twe\\tmeasure\\tthat\\ta\\tneuron\\thas\\tan\\taverage\\nactivation\\tof\\t0.3,\\tbut\\tthe\\ttarget\\tsparsity\\tis\\t0.1,\\tit\\tmust\\tbe\\tpenalized\\tto\\tactivate\\tless.\\tOne\\tapproach\\tcould\\nbe\\tsimply\\tadding\\tthe\\tsquared\\terror\\t(0.3\\t–\\t0.1)\\n2\\n\\tto\\tthe\\tcost\\tfunction,\\tbut\\tin\\tpractice\\ta\\tbetter\\tapproach\\tis\\tto\\nuse\\tthe\\t\\nKullback–Leibler\\tdivergence\\t(briefly\\tdiscussed\\tin\\t\\nChapter\\t4\\n),\\twhich\\thas\\tmuch\\tstronger\\tgradients\\nthan\\tthe\\tMean\\tSquared\\tError,\\t\\nas\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t15-10\\n.\\nFigure\\t15-10.\\t\\nSparsity\\tloss\\nGiven\\ttwo\\tdiscrete\\tprobability\\tdistributions\\t\\nP\\n\\tand\\t\\nQ\\n,\\tthe\\tKL\\tdivergence\\tbetween\\tthese\\tdistributions,\\nnoted\\t\\nD\\nKL\\n(\\nP\\n\\t\\t\\nQ\\n),\\tcan\\tbe\\tcomputed\\tusing\\t\\nEquation\\t15-1\\n.\\nEquation\\t15-1.\\t\\nKullback–Leibler\\tdivergence', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 523}), Document(page_content='In\\tour\\tcase,\\twe\\twant\\tto\\tmeasure\\tthe\\tdivergence\\tbetween\\tthe\\ttarget\\tprobability\\t\\np\\n\\tthat\\ta\\tneuron\\tin\\tthe\\ncoding\\tlayer\\twill\\tactivate,\\tand\\tthe\\tactual\\tprobability\\t\\nq\\n\\t(i.e.,\\tthe\\tmean\\tactivation\\tover\\tthe\\ttraining\\tbatch).\\nSo\\tthe\\tKL\\tdivergence\\tsimplifies\\tto\\t\\nEquation\\t15-2\\n.\\nEquation\\t15-2.\\t\\nKL\\tdivergence\\tbetween\\tthe\\ttarget\\tsparsity\\t\\np\\n\\tand\\tthe\\tactual\\tsparsity\\t\\nq\\nOnce\\twe\\thave\\tcomputed\\tthe\\tsparsity\\tloss\\tfor\\teach\\tneuron\\tin\\tthe\\tcoding\\tlayer,\\twe\\tjust\\tsum\\tup\\tthese\\tlosses,\\nand\\tadd\\tthe\\tresult\\tto\\tthe\\tcost\\tfunction.\\tIn\\torder\\tto\\tcontrol\\tthe\\trelative\\timportance\\tof\\tthe\\tsparsity\\tloss\\tand\\nthe\\treconstruction\\tloss,\\twe\\tcan\\tmultiply\\tthe\\tsparsity\\tloss\\tby\\ta\\tsparsity\\tweight\\thyperparameter.\\tIf\\tthis\\nweight\\tis\\ttoo\\thigh,\\tthe\\tmodel\\twill\\tstick\\tclosely\\tto\\tthe\\ttarget\\tsparsity,\\tbut\\tit\\tmay\\tnot\\treconstruct\\tthe\\tinputs\\nproperly,\\tmaking\\tthe\\tmodel\\tuseless.\\tConversely,\\tif\\tit\\tis\\ttoo\\tlow,\\tthe\\tmodel\\twill\\tmostly\\tignore\\tthe\\tsparsity\\nobjective\\tand\\tit\\twill\\tnot\\tlearn\\tany\\tinteresting\\tfeatures.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 524}), Document(page_content='TensorFlow\\tImplementation\\nWe\\t\\nnow\\thave\\tall\\twe\\tneed\\tto\\timplement\\ta\\tsparse\\tautoencoder\\t\\nusing\\tTensorFlow:\\ndef\\n\\t\\nkl_divergence\\n(\\np\\n,\\n\\t\\nq\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\np\\n\\t\\n*\\n\\t\\ntf\\n.\\nlog\\n(\\np\\n\\t\\n/\\n\\t\\nq\\n)\\n\\t\\n+\\n\\t\\n(\\n1\\n\\t\\n-\\n\\t\\np\\n)\\n\\t\\n*\\n\\t\\ntf\\n.\\nlog\\n((\\n1\\n\\t\\n-\\n\\t\\np\\n)\\n\\t\\n/\\n\\t\\n(\\n1\\n\\t\\n-\\n\\t\\nq\\n))\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nsparsity_target\\n\\t\\n=\\n\\t\\n0.1\\nsparsity_weight\\n\\t\\n=\\n\\t\\n0.2\\n[\\n...\\n]\\n\\t\\n#\\tBuild\\ta\\tnormal\\tautoencoder\\t(in\\tthis\\texample\\tthe\\tcoding\\tlayer\\tis\\thidden1)\\nhidden1_mean\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\nhidden1\\n,\\n\\t\\naxis\\n=\\n0\\n)\\n\\t\\n#\\tbatch\\tmean\\nsparsity_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\nkl_divergence\\n(\\nsparsity_target\\n,\\n\\t\\nhidden1_mean\\n))\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\n#\\tMSE\\nloss\\n\\t\\n=\\n\\t\\nreconstruction_loss\\n\\t\\n+\\n\\t\\nsparsity_weight\\n\\t\\n*\\n\\t\\nsparsity_loss\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\nAn\\timportant\\tdetail\\tis\\tthe\\tfact\\tthat\\t\\nthe\\tactivations\\tof\\tthe\\tcoding\\tlayer\\tmust\\tbe\\tbetween\\t0\\tand\\t1\\t(but\\tnot\\nequal\\tto\\t0\\tor\\t1),\\tor\\telse\\tthe\\tKL\\tdivergence\\twill\\treturn\\tNaN\\t(Not\\ta\\tNumber).\\tA\\tsimple\\tsolution\\tis\\tto\\tuse\\nthe\\tlogistic\\tactivation\\tfunction\\tfor\\tthe\\tcoding\\tlayer:\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nsigmoid\\n)\\nOne\\tsimple\\ttrick\\tcan\\tspeed\\tup\\tconvergence:\\tinstead\\tof\\tusing\\tthe\\tMSE,\\twe\\tcan\\tchoose\\ta\\t\\nreconstruction\\nloss\\tthat\\twill\\thave\\tlarger\\tgradients.\\t\\nCross\\tentropy\\tis\\toften\\ta\\tgood\\tchoice.\\tTo\\tuse\\tit,\\twe\\tmust\\tnormalize\\tthe\\ninputs\\tto\\tmake\\tthem\\ttake\\ton\\tvalues\\tfrom\\t0\\tto\\t1,\\tand\\tuse\\tthe\\tlogistic\\tactivation\\tfunction\\tin\\tthe\\toutput\\tlayer\\nso\\tthe\\toutputs\\talso\\ttake\\ton\\tvalues\\tfrom\\t0\\tto\\t1.\\tTensorFlow’s\\t\\nsigmoid_cross_entropy_with_logits()\\nfunction\\t\\ntakes\\tcare\\tof\\tefficiently\\tapplying\\tthe\\tlogistic\\t(sigmoid)\\tactivation\\tfunction\\tto\\tthe\\toutputs\\tand\\ncomputing\\tthe\\tcross\\t\\nentropy:\\n[\\n...\\n]\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden1\\n,\\n\\t\\nn_outputs\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid\\n(\\nlogits\\n)\\nxentropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid_cross_entropy_with_logits\\n(\\nlabels\\n=\\nX\\n,\\n\\t\\nlogits\\n=\\nlogits\\n)\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\nxentropy\\n)\\nNote\\tthat\\tthe\\t\\noutputs\\n\\toperation\\tis\\tnot\\tneeded\\tduring\\ttraining\\t(we\\tuse\\tit\\tonly\\twhen\\twe\\twant\\tto\\tlook\\tat\\tthe\\nreconstructions).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 525}), Document(page_content='Variational\\tAutoencoders\\nAnother\\t\\nimportant\\tcategory\\tof\\tautoencoders\\twas\\t\\nintroduced\\tin\\t2014\\n\\tby\\tDiederik\\tKingma\\tand\\tMax\\nWelling,\\n5\\n\\tand\\thas\\tquickly\\tbecome\\tone\\tof\\tthe\\tmost\\tpopular\\ttypes\\tof\\tautoencoders:\\t\\nvariational\\nautoencoders\\n.\\nThey\\tare\\tquite\\tdifferent\\tfrom\\tall\\tthe\\tautoencoders\\twe\\thave\\tdiscussed\\tso\\tfar,\\tin\\tparticular:\\nThey\\tare\\t\\nprobabilistic\\tautoencoders\\n,\\t\\nmeaning\\tthat\\ttheir\\toutputs\\tare\\tpartly\\tdetermined\\tby\\tchance,\\neven\\tafter\\ttraining\\t(as\\topposed\\tto\\tdenoising\\tautoencoders,\\twhich\\tuse\\trandomness\\tonly\\tduring\\ntraining).\\nMost\\timportantly,\\tthey\\tare\\t\\ngenerative\\tautoencoders\\n,\\t\\nmeaning\\tthat\\tthey\\tcan\\tgenerate\\tnew\\tinstances\\nthat\\tlook\\tlike\\tthey\\twere\\tsampled\\tfrom\\tthe\\ttraining\\tset.\\nBoth\\tthese\\tproperties\\tmake\\tthem\\trather\\tsimilar\\tto\\tRBMs\\t(see\\t\\nAppendix\\tE\\n),\\tbut\\tthey\\tare\\teasier\\tto\\ttrain\\tand\\nthe\\tsampling\\tprocess\\tis\\tmuch\\tfaster\\t(with\\tRBMs\\tyou\\tneed\\tto\\twait\\tfor\\tthe\\tnetwork\\tto\\tstabilize\\tinto\\ta\\n“thermal\\tequilibrium”\\tbefore\\tyou\\tcan\\tsample\\ta\\tnew\\tinstance).\\nLet’s\\ttake\\ta\\tlook\\tat\\thow\\tthey\\twork.\\t\\nFigure\\t15-11\\n\\t(left)\\tshows\\ta\\tvariational\\tautoencoder.\\tYou\\tcan\\nrecognize,\\tof\\tcourse,\\tthe\\tbasic\\tstructure\\tof\\tall\\tautoencoders,\\twith\\tan\\tencoder\\tfollowed\\tby\\ta\\tdecoder\\t(in\\nthis\\texample,\\tthey\\tboth\\thave\\ttwo\\thidden\\tlayers),\\tbut\\tthere\\tis\\ta\\ttwist:\\tinstead\\tof\\tdirectly\\tproducing\\ta\\ncoding\\tfor\\ta\\tgiven\\tinput,\\tthe\\tencoder\\tproduces\\t\\na\\t\\nmean\\tcoding\\n\\t\\nμ\\n\\tand\\ta\\tstandard\\tdeviation\\t\\nσ\\n.\\tThe\\tactual\\ncoding\\tis\\tthen\\tsampled\\trandomly\\tfrom\\ta\\t\\nGaussian\\tdistribution\\twith\\tmean\\t\\nμ\\n\\tand\\tstandard\\tdeviation\\t\\nσ\\n.\\nAfter\\tthat\\tthe\\tdecoder\\tjust\\tdecodes\\tthe\\tsampled\\tcoding\\tnormally.\\tThe\\tright\\tpart\\tof\\tthe\\tdiagram\\tshows\\ta\\ntraining\\tinstance\\tgoing\\tthrough\\tthis\\tautoencoder.\\tFirst,\\tthe\\tencoder\\tproduces\\t\\nμ\\n\\tand\\t\\nσ\\n,\\tthen\\ta\\tcoding\\tis\\nsampled\\trandomly\\t(notice\\tthat\\tit\\tis\\tnot\\texactly\\tlocated\\tat\\t\\nμ\\n),\\tand\\tfinally\\tthis\\tcoding\\tis\\tdecoded,\\tand\\tthe\\nfinal\\toutput\\tresembles\\tthe\\ttraining\\tinstance.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 526}), Document(page_content='Figure\\t15-11.\\t\\nVariational\\tautoencoder\\t(left),\\tand\\tan\\tinstance\\tgoing\\tthrough\\tit\\t(right)\\nAs\\tyou\\tcan\\tsee\\ton\\tthe\\tdiagram,\\talthough\\tthe\\tinputs\\tmay\\thave\\ta\\tvery\\tconvoluted\\tdistribution,\\ta\\tvariational\\nautoencoder\\ttends\\tto\\tproduce\\tcodings\\tthat\\tlook\\tas\\tthough\\tthey\\twere\\tsampled\\tfrom\\ta\\tsimple\\tGaussian\\ndistribution:\\n6\\n\\tduring\\ttraining,\\tthe\\tcost\\tfunction\\t(discussed\\tnext)\\tpushes\\tthe\\tcodings\\tto\\tgradually\\tmigrate\\nwithin\\tthe\\t\\ncoding\\tspace\\t(also\\tcalled\\t\\nthe\\t\\nlatent\\tspace\\n)\\tto\\toccupy\\ta\\troughly\\t(hyper)spherical\\tregion\\tthat\\nlooks\\tlike\\ta\\tcloud\\tof\\tGaussian\\tpoints.\\tOne\\tgreat\\tconsequence\\tis\\tthat\\tafter\\ttraining\\ta\\tvariational\\nautoencoder,\\tyou\\tcan\\tvery\\teasily\\tgenerate\\ta\\tnew\\tinstance:\\tjust\\tsample\\ta\\trandom\\tcoding\\tfrom\\tthe\\tGaussian\\ndistribution,\\tdecode\\tit,\\t\\nand\\tvoilà!\\nSo\\tlet’s\\tlook\\tat\\tthe\\t\\ncost\\tfunction.\\tIt\\tis\\tcomposed\\tof\\ttwo\\tparts.\\tThe\\tfirst\\tis\\tthe\\tusual\\t\\nreconstruction\\tloss\\tthat\\npushes\\tthe\\tautoencoder\\tto\\treproduce\\tits\\tinputs\\t(we\\tcan\\tuse\\tcross\\tentropy\\tfor\\tthis,\\tas\\tdiscussed\\tearlier).\\nThe\\tsecond\\tis\\tthe\\t\\nlatent\\tloss\\n\\t\\nthat\\tpushes\\tthe\\tautoencoder\\tto\\thave\\tcodings\\tthat\\tlook\\tas\\tthough\\tthey\\twere\\nsampled\\tfrom\\ta\\tsimple\\tGaussian\\tdistribution,\\tfor\\twhich\\twe\\tuse\\tthe\\tKL\\tdivergence\\tbetween\\tthe\\ttarget\\ndistribution\\t(the\\tGaussian\\tdistribution)\\tand\\tthe\\tactual\\tdistribution\\tof\\tthe\\tcodings.\\tThe\\tmath\\tis\\ta\\tbit\\tmore\\ncomplex\\tthan\\tearlier,\\tin\\tparticular\\tbecause\\tof\\tthe\\tGaussian\\tnoise,\\twhich\\tlimits\\tthe\\tamount\\tof\\tinformation\\nthat\\tcan\\tbe\\ttransmitted\\tto\\tthe\\tcoding\\tlayer\\t(thus\\tpushing\\tthe\\tautoencoder\\tto\\tlearn\\tuseful\\tfeatures).\\tLuckily,\\nthe\\tequations\\t\\nsimplify\\tto\\tthe\\tfollowing\\tcode\\tfor\\tthe\\t\\nlatent\\tloss:\\n7\\neps\\n\\t\\n=\\n\\t\\n1e-10\\n\\t\\t\\n#\\tsmoothing\\tterm\\tto\\tavoid\\tcomputing\\tlog(0)\\twhich\\tis\\tNaN\\nlatent_loss\\n\\t\\n=\\n\\t\\n0.5\\n\\t\\n*\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\nsquare\\n(\\nhidden3_sigma\\n)\\n\\t\\n+\\n\\t\\ntf\\n.\\nsquare\\n(\\nhidden3_mean\\n)\\n\\t\\t\\t\\t\\n-\\n\\t\\n1\\n\\t\\n-\\n\\t\\ntf\\n.\\nlog\\n(\\neps\\n\\t\\n+\\n\\t\\ntf\\n.\\nsquare\\n(\\nhidden3_sigma\\n)))\\nOne\\tcommon\\tvariant\\tis\\tto\\ttrain\\tthe\\tencoder\\tto\\toutput\\t\\nγ\\n\\t=\\tlog(\\nσ\\n2\\n)\\trather\\tthan\\t\\nσ\\n.\\tWherever\\twe\\tneed\\t\\nσ\\n\\twe\\ncan\\tjust\\tcompute\\t\\n.\\tThis\\tmakes\\tit\\ta\\tbit\\teasier\\tfor\\tthe\\tencoder\\tto\\tcapture\\tsigmas\\tof\\tdifferent', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 527}), Document(page_content='scales,\\tand\\tthus\\tit\\thelps\\tspeed\\tup\\tconvergence.\\tThe\\tlatent\\t\\nloss\\tends\\tup\\ta\\tbit\\tsimpler:\\nlatent_loss\\n\\t\\n=\\n\\t\\n0.5\\n\\t\\n*\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\nexp\\n(\\nhidden3_gamma\\n)\\n\\t\\n+\\n\\t\\ntf\\n.\\nsquare\\n(\\nhidden3_mean\\n)\\n\\t\\n-\\n\\t\\n1\\n\\t\\n-\\n\\t\\nhidden3_gamma\\n)\\nThe\\tfollowing\\tcode\\tbuilds\\tthe\\tvariational\\tautoencoder\\tshown\\tin\\t\\nFigure\\t15-11\\n\\t(left),\\t\\nusing\\tthe\\tlog(\\nσ\\n2\\n)\\nvariant:\\nfrom\\n\\t\\nfunctools\\n\\t\\nimport\\n\\t\\npartial\\nn_inputs\\n\\t\\n=\\n\\t\\n28\\n\\t\\n*\\n\\t\\n28\\nn_hidden1\\n\\t\\n=\\n\\t\\n500\\nn_hidden2\\n\\t\\n=\\n\\t\\n500\\nn_hidden3\\n\\t\\n=\\n\\t\\n20\\n\\t\\t\\n#\\tcodings\\nn_hidden4\\n\\t\\n=\\n\\t\\nn_hidden2\\nn_hidden5\\n\\t\\n=\\n\\t\\nn_hidden1\\nn_outputs\\n\\t\\n=\\n\\t\\nn_inputs\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.001\\ninitializer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\nmy_dense_layer\\n\\t\\n=\\n\\t\\npartial\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n,\\n\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nelu\\n,\\n\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nhidden1\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n)\\nhidden2\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n)\\nhidden3_mean\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden2\\n,\\n\\t\\nn_hidden3\\n,\\n\\t\\nactivation\\n=\\nNone\\n)\\nhidden3_gamma\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden2\\n,\\n\\t\\nn_hidden3\\n,\\n\\t\\nactivation\\n=\\nNone\\n)\\nnoise\\n\\t\\n=\\n\\t\\ntf\\n.\\nrandom_normal\\n(\\ntf\\n.\\nshape\\n(\\nhidden3_gamma\\n),\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nhidden3\\n\\t\\n=\\n\\t\\nhidden3_mean\\n\\t\\n+\\n\\t\\ntf\\n.\\nexp\\n(\\n0.5\\n\\t\\n*\\n\\t\\nhidden3_gamma\\n)\\n\\t\\n*\\n\\t\\nnoise\\nhidden4\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden3\\n,\\n\\t\\nn_hidden4\\n)\\nhidden5\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden4\\n,\\n\\t\\nn_hidden5\\n)\\nlogits\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden5\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nactivation\\n=\\nNone\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nsigmoid\\n(\\nlogits\\n)\\nxentropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid_cross_entropy_with_logits\\n(\\nlabels\\n=\\nX\\n,\\n\\t\\nlogits\\n=\\nlogits\\n)\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\nxentropy\\n)\\nlatent_loss\\n\\t\\n=\\n\\t\\n0.5\\n\\t\\n*\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\nexp\\n(\\nhidden3_gamma\\n)\\n\\t\\n+\\n\\t\\ntf\\n.\\nsquare\\n(\\nhidden3_mean\\n)\\n\\t\\n-\\n\\t\\n1\\n\\t\\n-\\n\\t\\nhidden3_gamma\\n)\\nloss\\n\\t\\n=\\n\\t\\nreconstruction_loss\\n\\t\\n+\\n\\t\\nlatent_loss\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 528}), Document(page_content='Generating\\tDigits\\nNow\\tlet’s\\tuse\\tthis\\tvariational\\t\\nautoencoder\\tto\\tgenerate\\timages\\tthat\\tlook\\tlike\\thandwritten\\tdigits.\\tAll\\twe\\nneed\\tto\\tdo\\tis\\ttrain\\tthe\\tmodel,\\tthen\\tsample\\trandom\\tcodings\\tfrom\\ta\\t\\nGaussian\\tdistribution\\tand\\tdecode\\tthem.\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nn_digits\\n\\t\\n=\\n\\t\\n60\\nn_epochs\\n\\t\\n=\\n\\t\\n50\\nbatch_size\\n\\t\\n=\\n\\t\\n150\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nn_batches\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_batches\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n})\\n\\t\\t\\t\\t\\ncodings_rnd\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nnormal\\n(\\nsize\\n=\\n[\\nn_digits\\n,\\n\\t\\nn_hidden3\\n])\\n\\t\\t\\t\\t\\noutputs_val\\n\\t\\n=\\n\\t\\noutputs\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nhidden3\\n:\\n\\t\\ncodings_rnd\\n})\\nThat’s\\tit.\\tNow\\twe\\tcan\\tsee\\twhat\\tthe\\t“handwritten”\\tdigits\\tproduced\\tby\\tthe\\tautoencoder\\tlook\\tlike\\t(see\\nFigure\\t15-12\\n):\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_digits\\n):\\n\\t\\t\\t\\t\\nplt\\n.\\nsubplot\\n(\\nn_digits\\n,\\n\\t\\n10\\n,\\n\\t\\niteration\\n\\t\\n+\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\nplot_image\\n(\\noutputs_val\\n[\\niteration\\n])\\nFigure\\t15-12.\\t\\nImages\\tof\\thandwritten\\tdigits\\tgenerated\\tby\\tthe\\tvariational\\tautoencoder\\nA\\tmajority\\tof\\tthese\\tdigits\\tlook\\tpretty\\tconvincing,\\twhile\\ta\\tfew\\tare\\trather\\t“creative.”\\tBut\\tdon’t\\tbe\\ttoo\\tharsh\\non\\tthe\\tautoencoder\\t—\\tit\\tonly\\tstarted\\tlearning\\tless\\tthan\\tan\\thour\\tago.\\tGive\\tit\\ta\\tbit\\tmore\\ttraining\\ttime,\\tand\\nthose\\tdigits\\twill\\tlook\\tbetter\\tand\\t\\nbetter.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 529}), Document(page_content='Other\\tAutoencoders\\nThe\\tamazing\\tsuccesses\\tof\\tsupervised\\tlearning\\tin\\timage\\trecognition,\\tspeech\\trecognition,\\ttext\\ttranslation,\\nand\\tmore\\thave\\tsomewhat\\tovershadowed\\tunsupervised\\tlearning,\\tbut\\tit\\tis\\tactually\\tbooming.\\tNew\\narchitectures\\tfor\\tautoencoders\\tand\\tother\\tunsupervised\\tlearning\\talgorithms\\tare\\tinvented\\tregularly,\\tso\\tmuch\\nso\\tthat\\twe\\tcannot\\tcover\\tthem\\tall\\tin\\tthis\\tbook.\\tHere\\tis\\ta\\tbrief\\t(by\\tno\\tmeans\\texhaustive)\\toverview\\tof\\ta\\tfew\\nmore\\ttypes\\tof\\tautoencoders\\tthat\\tyou\\tmay\\twant\\tto\\tcheck\\tout:\\nContractive\\tautoencoder\\n\\t(CAE)\\n8\\nThe\\tautoencoder\\tis\\tconstrained\\tduring\\ttraining\\tso\\tthat\\tthe\\tderivatives\\tof\\tthe\\tcodings\\twith\\tregards\\tto\\nthe\\tinputs\\tare\\tsmall.\\tIn\\tother\\twords,\\ttwo\\tsimilar\\tinputs\\tmust\\thave\\tsimilar\\tcodings.\\nStacked\\tconvolutional\\tautoencoders\\n9\\nAutoencoders\\tthat\\tlearn\\tto\\textract\\tvisual\\tfeatures\\tby\\treconstructing\\timages\\tprocessed\\tthrough\\nconvolutional\\tlayers.\\nGenerative\\tstochastic\\tnetwork\\n\\t(GSN)\\n10\\nA\\tgeneralization\\tof\\tdenoising\\tautoencoders,\\twith\\tthe\\tadded\\tcapability\\tto\\tgenerate\\tdata.\\nWinner-take-all\\t(WTA)\\tautoencoder\\n11\\nDuring\\ttraining,\\tafter\\tcomputing\\tthe\\tactivations\\tof\\tall\\tthe\\tneurons\\tin\\tthe\\tcoding\\tlayer,\\tonly\\tthe\\ttop\\t\\nk\\n%\\nactivations\\tfor\\teach\\tneuron\\tover\\tthe\\ttraining\\tbatch\\tare\\tpreserved,\\tand\\tthe\\trest\\tare\\tset\\tto\\tzero.\\nNaturally\\tthis\\tleads\\tto\\tsparse\\tcodings.\\tMoreover,\\ta\\tsimilar\\tWTA\\tapproach\\tcan\\tbe\\tused\\tto\\tproduce\\nsparse\\tconvolutional\\tautoencoders.\\nAdversarial\\tautoencoders\\n12\\nOne\\tnetwork\\tis\\ttrained\\tto\\treproduce\\tits\\tinputs,\\tand\\tat\\tthe\\tsame\\ttime\\tanother\\tis\\ttrained\\tto\\tfind\\tinputs\\nthat\\tthe\\tfirst\\tnetwork\\tis\\tunable\\tto\\tproperly\\treconstruct.\\tThis\\tpushes\\tthe\\tfirst\\tautoencoder\\tto\\tlearn\\nrobust\\tcodings.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 530}), Document(page_content='Exercises\\n1\\n.\\t\\nWhat\\tare\\tthe\\tmain\\ttasks\\tthat\\tautoencoders\\tare\\tused\\tfor?\\n2\\n.\\t\\nSuppose\\tyou\\twant\\tto\\ttrain\\ta\\tclassifier\\tand\\tyou\\thave\\tplenty\\tof\\tunlabeled\\ttraining\\tdata,\\tbut\\tonly\\ta\\tfew\\nthousand\\tlabeled\\tinstances.\\tHow\\tcan\\tautoencoders\\thelp?\\tHow\\twould\\tyou\\tproceed?\\n3\\n.\\t\\nIf\\tan\\tautoencoder\\tperfectly\\treconstructs\\tthe\\tinputs,\\tis\\tit\\tnecessarily\\ta\\tgood\\tautoencoder?\\tHow\\tcan\\nyou\\tevaluate\\tthe\\tperformance\\tof\\tan\\tautoencoder?\\n4\\n.\\t\\nWhat\\tare\\tundercomplete\\tand\\tovercomplete\\tautoencoders?\\tWhat\\tis\\tthe\\tmain\\trisk\\tof\\tan\\texcessively\\nundercomplete\\tautoencoder?\\tWhat\\tabout\\tthe\\tmain\\trisk\\tof\\tan\\tovercomplete\\tautoencoder?\\n5\\n.\\t\\nHow\\tdo\\tyou\\ttie\\tweights\\tin\\ta\\tstacked\\tautoencoder?\\tWhat\\tis\\tthe\\tpoint\\tof\\tdoing\\tso?\\n6\\n.\\t\\nWhat\\tis\\ta\\tcommon\\ttechnique\\tto\\tvisualize\\tfeatures\\tlearned\\tby\\tthe\\tlower\\tlayer\\tof\\ta\\tstacked\\nautoencoder?\\tWhat\\tabout\\thigher\\tlayers?\\n7\\n.\\t\\nWhat\\tis\\ta\\tgenerative\\tmodel?\\tCan\\tyou\\tname\\ta\\ttype\\tof\\tgenerative\\tautoencoder?\\n8\\n.\\t\\nLet’s\\tuse\\ta\\tdenoising\\tautoencoder\\tto\\tpretrain\\tan\\timage\\tclassifier:\\nYou\\tcan\\tuse\\tMNIST\\t(simplest),\\tor\\tanother\\tlarge\\tset\\tof\\timages\\tsuch\\tas\\t\\nCIFAR10\\n\\tif\\tyou\\twant\\ta\\nbigger\\tchallenge.\\tIf\\tyou\\tchoose\\tCIFAR10,\\tyou\\tneed\\tto\\twrite\\tcode\\tto\\tload\\tbatches\\tof\\timages\\tfor\\ntraining.\\tIf\\tyou\\twant\\tto\\tskip\\tthis\\tpart,\\tTensorFlow’s\\tmodel\\tzoo\\tcontains\\t\\ntools\\tto\\tdo\\tjust\\tthat\\n.\\nSplit\\tthe\\tdataset\\tinto\\ta\\ttraining\\tset\\tand\\ta\\ttest\\tset.\\tTrain\\ta\\tdeep\\tdenoising\\tautoencoder\\ton\\tthe\\tfull\\ntraining\\tset.\\nCheck\\tthat\\tthe\\timages\\tare\\tfairly\\twell\\treconstructed,\\tand\\tvisualize\\tthe\\tlow-level\\tfeatures.\\nVisualize\\tthe\\timages\\tthat\\tmost\\tactivate\\teach\\tneuron\\tin\\tthe\\tcoding\\tlayer.\\nBuild\\ta\\tclassification\\tdeep\\tneural\\tnetwork,\\treusing\\tthe\\tlower\\tlayers\\tof\\tthe\\tautoencoder.\\tTrain\\tit\\nusing\\tonly\\t10%\\tof\\tthe\\ttraining\\tset.\\tCan\\tyou\\tget\\tit\\tto\\tperform\\tas\\twell\\tas\\tthe\\tsame\\tclassifier\\ntrained\\ton\\tthe\\tfull\\ttraining\\tset?\\n9\\n.\\t\\nSemantic\\thashing\\n,\\t\\nintroduced\\tin\\t2008\\tby\\tRuslan\\tSalakhutdinov\\tand\\tGeoffrey\\tHinton\\n,\\n13\\n\\tis\\t\\na\\ntechnique\\tused\\tfor\\tefficient\\t\\ninformation\\tretrieval\\n:\\ta\\tdocument\\t(e.g.,\\tan\\timage)\\tis\\tpassed\\tthrough\\ta\\nsystem,\\ttypically\\ta\\tneural\\tnetwork,\\twhich\\toutputs\\ta\\tfairly\\tlow-dimensional\\tbinary\\tvector\\t(e.g.,\\t30\\nbits).\\tTwo\\tsimilar\\tdocuments\\tare\\tlikely\\tto\\thave\\tidentical\\tor\\tvery\\tsimilar\\thashes.\\tBy\\tindexing\\teach\\ndocument\\tusing\\tits\\thash,\\tit\\tis\\tpossible\\tto\\tretrieve\\tmany\\tdocuments\\tsimilar\\tto\\ta\\tparticular\\tdocument\\nalmost\\tinstantly,\\teven\\tif\\tthere\\tare\\tbillions\\tof\\tdocuments:\\tjust\\tcompute\\tthe\\thash\\tof\\tthe\\tdocument\\tand\\nlook\\tup\\tall\\tdocuments\\twith\\tthat\\tsame\\thash\\t(or\\thashes\\tdiffering\\tby\\tjust\\tone\\tor\\ttwo\\tbits).\\tLet’s\\nimplement\\tsemantic\\thashing\\tusing\\ta\\tslightly\\ttweaked\\tstacked\\tautoencoder:\\nCreate\\ta\\tstacked\\tautoencoder\\tcontaining\\ttwo\\thidden\\tlayers\\tbelow\\tthe\\tcoding\\tlayer,\\tand\\ttrain\\tit\\non\\tthe\\timage\\tdataset\\tyou\\tused\\tin\\tthe\\tprevious\\texercise.\\tThe\\tcoding\\tlayer\\tshould\\tcontain\\t30\\nneurons\\tand\\tuse\\tthe\\tlogistic\\tactivation\\tfunction\\tto\\toutput\\tvalues\\tbetween\\t0\\tand\\t1.\\tAfter\\ttraining,', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 531}), Document(page_content='to\\tproduce\\tthe\\thash\\tof\\tan\\timage,\\tyou\\tcan\\tsimply\\trun\\tit\\tthrough\\tthe\\tautoencoder,\\ttake\\tthe\\toutput\\nof\\tthe\\tcoding\\tlayer,\\tand\\tround\\tevery\\tvalue\\tto\\tthe\\tclosest\\tinteger\\t(0\\tor\\t1).\\nOne\\tneat\\ttrick\\tproposed\\tby\\tSalakhutdinov\\tand\\tHinton\\tis\\tto\\tadd\\tGaussian\\tnoise\\t(with\\tzero\\nmean)\\tto\\tthe\\tinputs\\tof\\tthe\\tcoding\\tlayer,\\tduring\\ttraining\\tonly.\\tIn\\torder\\tto\\tpreserve\\ta\\thigh\\tsignal-\\nto-noise\\tratio,\\tthe\\tautoencoder\\twill\\tlearn\\tto\\tfeed\\tlarge\\tvalues\\tto\\tthe\\tcoding\\tlayer\\t(so\\tthat\\tthe\\nnoise\\tbecomes\\tnegligible).\\tIn\\tturn,\\tthis\\tmeans\\tthat\\tthe\\tlogistic\\tfunction\\tof\\tthe\\tcoding\\tlayer\\twill\\nlikely\\tsaturate\\tat\\t0\\tor\\t1.\\tAs\\ta\\tresult,\\trounding\\tthe\\tcodings\\tto\\t0\\tor\\t1\\twon’t\\tdistort\\tthem\\ttoo\\tmuch,\\nand\\tthis\\twill\\timprove\\tthe\\treliability\\tof\\tthe\\thashes.\\nCompute\\tthe\\thash\\tof\\tevery\\timage,\\tand\\tsee\\tif\\timages\\twith\\tidentical\\thashes\\tlook\\talike.\\tSince\\nMNIST\\tand\\tCIFAR10\\tare\\tlabeled,\\ta\\tmore\\tobjective\\tway\\tto\\tmeasure\\tthe\\tperformance\\tof\\tthe\\nautoencoder\\tfor\\tsemantic\\thashing\\tis\\tto\\tensure\\tthat\\timages\\twith\\tthe\\tsame\\thash\\tgenerally\\thave\\tthe\\nsame\\tclass.\\tOne\\tway\\tto\\tdo\\tthis\\tis\\tto\\tmeasure\\tthe\\taverage\\tGini\\tpurity\\t(introduced\\tin\\t\\nChapter\\t6\\n)\\nof\\tthe\\tsets\\tof\\timages\\twith\\tidentical\\t(or\\tvery\\tsimilar)\\thashes.\\nTry\\tfine-tuning\\tthe\\thyperparameters\\tusing\\tcross-validation.\\nNote\\tthat\\twith\\ta\\tlabeled\\tdataset,\\tanother\\tapproach\\tis\\tto\\ttrain\\ta\\tconvolutional\\tneural\\tnetwork\\n(see\\t\\nChapter\\t13\\n)\\tfor\\tclassification,\\tthen\\tuse\\tthe\\tlayer\\tbelow\\tthe\\toutput\\tlayer\\tto\\tproduce\\tthe\\nhashes.\\tSee\\tJinma\\tGua\\tand\\tJianmin\\tLi’s\\t\\n2015\\tpaper\\n.\\n14\\n\\tSee\\tif\\tthat\\tperforms\\tbetter.\\n10\\n.\\t\\nTrain\\ta\\tvariational\\tautoencoder\\ton\\tthe\\timage\\tdataset\\tused\\tin\\tthe\\tprevious\\texercises\\t(MNIST\\tor\\nCIFAR10),\\tand\\tmake\\tit\\tgenerate\\timages.\\tAlternatively,\\tyou\\tcan\\ttry\\tto\\tfind\\tan\\tunlabeled\\tdataset\\tthat\\nyou\\tare\\tinterested\\tin\\tand\\tsee\\tif\\tyou\\tcan\\tgenerate\\tnew\\tsamples.\\nSolutions\\tto\\tthese\\texercises\\tare\\t\\navailable\\tin\\t\\nAppendix\\tA\\n.\\n“Perception\\tin\\tchess,”\\tW.\\tChase\\tand\\tH.\\tSimon\\t(1973).\\n“Greedy\\tLayer-Wise\\tTraining\\tof\\tDeep\\tNetworks,”\\tY.\\tBengio\\tet\\tal.\\t(2007).\\n“Extracting\\tand\\tComposing\\tRobust\\tFeatures\\twith\\tDenoising\\tAutoencoders,”\\tP.\\tVincent\\tet\\tal.\\t(2008).\\n“Stacked\\tDenoising\\tAutoencoders:\\tLearning\\tUseful\\tRepresentations\\tin\\ta\\tDeep\\tNetwork\\twith\\ta\\tLocal\\tDenoising\\tCriterion,”\\tP.\\tVincent\\tet\\nal.\\t(2010).\\n“Auto-Encoding\\tVariational\\tBayes,”\\tD.\\tKingma\\tand\\tM.\\tWelling\\t(2014).\\nVariational\\tautoencoders\\tare\\tactually\\tmore\\tgeneral;\\tthe\\tcodings\\tare\\tnot\\tlimited\\tto\\tGaussian\\tdistributions.\\nFor\\tmore\\tmathematical\\tdetails,\\tcheck\\tout\\tthe\\toriginal\\tpaper\\ton\\tvariational\\tautoencoders,\\tor\\tCarl\\tDoersch’s\\t\\ngreat\\ttutorial\\n\\t(2016).\\n“Contractive\\tAuto-Encoders:\\tExplicit\\tInvariance\\tDuring\\tFeature\\tExtraction,”\\tS.\\tRifai\\tet\\tal.\\t(2011).\\n“Stacked\\tConvolutional\\tAuto-Encoders\\tfor\\tHierarchical\\tFeature\\tExtraction,”\\tJ.\\tMasci\\tet\\tal.\\t(2011).\\n“GSNs:\\tGenerative\\tStochastic\\tNetworks,”\\tG.\\tAlain\\tet\\tal.\\t(2015).\\n“Winner-Take-All\\tAutoencoders,”\\tA.\\tMakhzani\\tand\\tB.\\tFrey\\t(2015).\\n“Adversarial\\tAutoencoders,”\\tA.\\tMakhzani\\tet\\tal.\\t(2016).\\n“Semantic\\tHashing,”\\tR.\\tSalakhutdinov\\tand\\tG.\\tHinton\\t(2008).\\n“CNN\\tBased\\tHashing\\tfor\\tImage\\tRetrieval,”\\tJ.\\tGua\\tand\\tJ.\\tLi\\t(2015).\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 532}), Document(page_content='Chapter\\t16.\\t\\nReinforcement\\tLearning\\nReinforcement\\tLearning\\n\\t(RL)\\t\\nis\\tone\\tof\\tthe\\tmost\\texciting\\tfields\\tof\\tMachine\\tLearning\\ttoday,\\tand\\talso\\tone\\nof\\tthe\\toldest.\\tIt\\thas\\tbeen\\taround\\tsince\\tthe\\t1950s,\\tproducing\\tmany\\tinteresting\\tapplications\\tover\\tthe\\tyears,\\n1\\nin\\tparticular\\tin\\tgames\\t(e.g.,\\t\\nTD-Gammon\\n,\\ta\\t\\nBackgammon\\n\\tplaying\\tprogram)\\tand\\tin\\t\\nmachine\\tcontrol,\\tbut\\nseldom\\tmaking\\tthe\\theadline\\tnews.\\tBut\\ta\\trevolution\\ttook\\tplace\\tin\\t2013\\twhen\\tresearchers\\tfrom\\tan\\tEnglish\\nstartup\\tcalled\\t\\nDeepMind\\t\\ndemonstrated\\ta\\tsystem\\tthat\\tcould\\tlearn\\tto\\tplay\\tjust\\tabout\\tany\\tAtari\\tgame\\tfrom\\nscratch\\n,\\n2\\n\\teventually\\t\\noutperforming\\thumans\\n3\\n\\tin\\tmost\\tof\\tthem,\\tusing\\tonly\\traw\\tpixels\\tas\\tinputs\\tand\\twithout\\nany\\tprior\\tknowledge\\tof\\tthe\\trules\\tof\\tthe\\tgames.\\n4\\n\\tThis\\twas\\tthe\\tfirst\\tof\\ta\\tseries\\tof\\tamazing\\tfeats,\\tculminating\\nin\\tMarch\\t2016\\twith\\tthe\\tvictory\\tof\\ttheir\\tsystem\\t\\nAlphaGo\\tagainst\\tLee\\tSedol,\\tthe\\tworld\\tchampion\\tof\\tthe\\ngame\\tof\\t\\nGo\\n.\\tNo\\tprogram\\thad\\tever\\tcome\\tclose\\tto\\tbeating\\ta\\tmaster\\tof\\tthis\\tgame,\\tlet\\talone\\tthe\\tworld\\nchampion.\\tToday\\tthe\\twhole\\tfield\\tof\\tRL\\tis\\tboiling\\twith\\tnew\\tideas,\\twith\\ta\\twide\\trange\\tof\\tapplications.\\nDeepMind\\twas\\tbought\\tby\\tGoogle\\tfor\\tover\\t500\\tmillion\\tdollars\\tin\\t2014.\\nSo\\thow\\tdid\\tthey\\tdo\\tit?\\tWith\\thindsight\\tit\\tseems\\trather\\tsimple:\\tthey\\tapplied\\tthe\\tpower\\tof\\t\\nDeep\\tLearning\\tto\\nthe\\tfield\\tof\\tReinforcement\\tLearning,\\tand\\tit\\tworked\\tbeyond\\ttheir\\twildest\\tdreams.\\tIn\\tthis\\tchapter\\twe\\twill\\nfirst\\texplain\\twhat\\tReinforcement\\tLearning\\tis\\tand\\twhat\\tit\\tis\\tgood\\tat,\\tand\\tthen\\twe\\twill\\tpresent\\ttwo\\tof\\tthe\\nmost\\timportant\\ttechniques\\tin\\tdeep\\tReinforcement\\tLearning:\\t\\npolicy\\tgradients\\n\\tand\\t\\ndeep\\tQ-networks\\n(DQN),\\tincluding\\ta\\tdiscussion\\tof\\t\\nMarkov\\tdecision\\tprocesses\\n\\t(MDP).\\tWe\\twill\\tuse\\tthese\\ttechniques\\tto\\ntrain\\ta\\tmodel\\tto\\tbalance\\ta\\tpole\\ton\\ta\\tmoving\\tcart,\\tand\\tanother\\tto\\tplay\\tAtari\\tgames.\\tThe\\tsame\\ttechniques\\ncan\\tbe\\tused\\tfor\\ta\\twide\\tvariety\\tof\\ttasks,\\tfrom\\twalking\\trobots\\tto\\tself-driving\\tcars.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 533}), Document(page_content='Learning\\tto\\tOptimize\\tRewards\\nIn\\t\\nReinforcement\\tLearning,\\ta\\tsoftware\\t\\nagent\\n\\t\\nmakes\\t\\nobservations\\n\\tand\\ttakes\\t\\nactions\\n\\twithin\\tan\\nenvironment\\n,\\t\\nand\\tin\\treturn\\tit\\treceives\\t\\nrewards\\n.\\tIts\\tobjective\\tis\\tto\\tlearn\\tto\\tact\\tin\\ta\\tway\\tthat\\twill\\tmaximize\\nits\\texpected\\tlong-term\\trewards.\\tIf\\tyou\\tdon’t\\tmind\\ta\\tbit\\tof\\tanthropomorphism,\\tyou\\tcan\\tthink\\tof\\tpositive\\nrewards\\tas\\tpleasure,\\tand\\tnegative\\trewards\\tas\\tpain\\t(the\\tterm\\t“reward”\\tis\\ta\\tbit\\tmisleading\\tin\\tthis\\tcase).\\tIn\\nshort,\\tthe\\tagent\\tacts\\tin\\tthe\\tenvironment\\tand\\tlearns\\tby\\ttrial\\tand\\terror\\tto\\tmaximize\\tits\\tpleasure\\tand\\tminimize\\nits\\tpain.\\nThis\\tis\\t\\nquite\\ta\\tbroad\\tsetting,\\twhich\\tcan\\tapply\\tto\\ta\\twide\\tvariety\\tof\\ttasks.\\tHere\\tare\\ta\\tfew\\texamples\\t(see\\nFigure\\t16-1\\n):\\n1\\n.\\t\\nThe\\tagent\\tcan\\tbe\\tthe\\tprogram\\tcontrolling\\ta\\twalking\\trobot.\\tIn\\tthis\\tcase,\\tthe\\tenvironment\\tis\\tthe\\treal\\nworld,\\tthe\\tagent\\tobserves\\tthe\\tenvironment\\tthrough\\ta\\tset\\tof\\t\\nsensors\\n\\tsuch\\tas\\tcameras\\tand\\ttouch\\nsensors,\\tand\\tits\\tactions\\tconsist\\tof\\tsending\\tsignals\\tto\\tactivate\\tmotors.\\tIt\\tmay\\tbe\\tprogrammed\\tto\\tget\\npositive\\trewards\\twhenever\\tit\\tapproaches\\tthe\\ttarget\\tdestination,\\tand\\tnegative\\trewards\\twhenever\\tit\\nwastes\\ttime,\\tgoes\\tin\\tthe\\twrong\\tdirection,\\tor\\tfalls\\tdown.\\n2\\n.\\t\\nThe\\tagent\\tcan\\tbe\\tthe\\tprogram\\tcontrolling\\tMs.\\tPac-Man.\\tIn\\tthis\\tcase,\\tthe\\tenvironment\\tis\\ta\\tsimulation\\nof\\tthe\\tAtari\\tgame,\\tthe\\tactions\\tare\\tthe\\tnine\\tpossible\\tjoystick\\tpositions\\t(upper\\tleft,\\tdown,\\tcenter,\\tand\\nso\\ton),\\tthe\\tobservations\\tare\\tscreenshots,\\tand\\tthe\\trewards\\tare\\tjust\\tthe\\tgame\\tpoints.\\n3\\n.\\t\\nSimilarly,\\tthe\\tagent\\tcan\\tbe\\tthe\\tprogram\\tplaying\\ta\\tboard\\tgame\\tsuch\\tas\\tthe\\tgame\\tof\\t\\nGo\\n.\\n4\\n.\\t\\nThe\\tagent\\tdoes\\tnot\\thave\\tto\\tcontrol\\ta\\tphysically\\t(or\\tvirtually)\\tmoving\\tthing.\\tFor\\texample,\\tit\\tcan\\tbe\\ta\\nsmart\\tthermostat,\\tgetting\\trewards\\twhenever\\tit\\tis\\tclose\\tto\\tthe\\ttarget\\ttemperature\\tand\\tsaves\\tenergy,\\tand\\nnegative\\trewards\\twhen\\thumans\\tneed\\tto\\ttweak\\tthe\\ttemperature,\\tso\\tthe\\tagent\\tmust\\tlearn\\tto\\tanticipate\\nhuman\\tneeds.\\n5\\n.\\t\\nThe\\tagent\\tcan\\tobserve\\tstock\\tmarket\\tprices\\tand\\tdecide\\thow\\tmuch\\tto\\tbuy\\tor\\tsell\\tevery\\tsecond.\\nRewards\\tare\\tobviously\\tthe\\tmonetary\\tgains\\tand\\tlosses.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 534}), Document(page_content='Figure\\t16-1.\\t\\nReinforcement\\tLearning\\texamples:\\t(a)\\twalking\\trobot,\\t(b)\\tMs.\\tPac-Man,\\t(c)\\tGo\\tplayer,\\t(d)\\tthermostat,\\t(e)\\tautomatic\\ntrader\\n5\\nNote\\tthat\\tthere\\tmay\\tnot\\tbe\\tany\\tpositive\\trewards\\tat\\tall;\\tfor\\texample,\\tthe\\tagent\\tmay\\tmove\\taround\\tin\\ta\\tmaze,\\ngetting\\ta\\tnegative\\treward\\tat\\tevery\\ttime\\tstep,\\tso\\tit\\tbetter\\tfind\\tthe\\texit\\tas\\tquickly\\tas\\tpossible!\\tThere\\tare\\nmany\\tother\\texamples\\tof\\ttasks\\twhere\\tReinforcement\\tLearning\\tis\\twell\\tsuited,\\tsuch\\tas\\tself-driving\\tcars,\\nplacing\\tads\\ton\\ta\\tweb\\tpage,\\tor\\tcontrolling\\twhere\\tan\\timage\\tclassification\\tsystem\\tshould\\tfocus\\tits\\t\\nattention.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 535}), Document(page_content='Policy\\tSearch\\nThe\\t\\nalgorithm\\tused\\tby\\tthe\\tsoftware\\tagent\\tto\\tdetermine\\tits\\tactions\\tis\\tcalled\\tits\\t\\npolicy\\n.\\t\\nFor\\texample,\\tthe\\npolicy\\tcould\\tbe\\ta\\tneural\\tnetwork\\ttaking\\tobservations\\tas\\tinputs\\tand\\toutputting\\tthe\\taction\\tto\\ttake\\t(see\\nFigure\\t16-2\\n).\\nFigure\\t16-2.\\t\\nReinforcement\\tLearning\\tusing\\ta\\tneural\\tnetwork\\tpolicy\\nThe\\tpolicy\\tcan\\tbe\\tany\\talgorithm\\tyou\\tcan\\tthink\\tof,\\tand\\tit\\tdoes\\tnot\\teven\\thave\\tto\\tbe\\tdeterministic.\\tFor\\nexample,\\tconsider\\ta\\trobotic\\tvacuum\\tcleaner\\twhose\\treward\\tis\\tthe\\tamount\\tof\\tdust\\tit\\tpicks\\tup\\tin\\t30\\tminutes.\\nIts\\tpolicy\\tcould\\tbe\\tto\\tmove\\tforward\\twith\\tsome\\tprobability\\t\\np\\n\\tevery\\tsecond,\\tor\\trandomly\\trotate\\tleft\\tor\\nright\\twith\\tprobability\\t1\\t–\\t\\np\\n.\\tThe\\trotation\\tangle\\twould\\tbe\\ta\\trandom\\tangle\\tbetween\\t–r\\tand\\t+r.\\tSince\\tthis\\npolicy\\tinvolves\\tsome\\trandomness,\\tit\\tis\\tcalled\\t\\na\\t\\nstochastic\\tpolicy\\n.\\tThe\\trobot\\twill\\thave\\tan\\terratic\\ntrajectory,\\twhich\\tguarantees\\tthat\\tit\\twill\\teventually\\tget\\tto\\tany\\tplace\\tit\\tcan\\treach\\tand\\tpick\\tup\\tall\\tthe\\tdust.\\nThe\\tquestion\\tis:\\thow\\tmuch\\tdust\\twill\\tit\\tpick\\tup\\tin\\t30\\tminutes?\\nHow\\twould\\tyou\\ttrain\\tsuch\\ta\\trobot?\\tThere\\tare\\tjust\\ttwo\\t\\npolicy\\tparameters\\n\\tyou\\tcan\\ttweak:\\tthe\\tprobability\\np\\n\\tand\\tthe\\tangle\\trange\\t\\nr\\n.\\tOne\\tpossible\\tlearning\\talgorithm\\tcould\\tbe\\tto\\ttry\\tout\\tmany\\tdifferent\\tvalues\\tfor\\nthese\\tparameters,\\tand\\tpick\\tthe\\tcombination\\tthat\\tperforms\\tbest\\t(see\\t\\nFigure\\t16-3\\n).\\tThis\\tis\\tan\\texample\\tof\\npolicy\\tsearch\\n,\\tin\\tthis\\tcase\\tusing\\ta\\tbrute\\tforce\\tapproach.\\tHowever,\\twhen\\tthe\\t\\npolicy\\tspace\\n\\t\\nis\\ttoo\\tlarge\\n(which\\tis\\tgenerally\\tthe\\tcase),\\tfinding\\ta\\tgood\\tset\\tof\\tparameters\\tthis\\tway\\tis\\tlike\\tsearching\\tfor\\ta\\tneedle\\tin\\ta\\ngigantic\\thaystack.\\nAnother\\tway\\tto\\texplore\\tthe\\tpolicy\\tspace\\tis\\tto\\t\\nuse\\t\\ngenetic\\talgorithms\\n.\\tFor\\texample,\\tyou\\tcould\\trandomly\\ncreate\\ta\\tfirst\\tgeneration\\tof\\t100\\tpolicies\\tand\\ttry\\tthem\\tout,\\tthen\\t“kill”\\tthe\\t80\\tworst\\tpolicies\\n6\\n\\tand\\tmake\\tthe\\n20\\tsurvivors\\tproduce\\t4\\toffspring\\teach.\\tAn\\toffspring\\tis\\tjust\\ta\\tcopy\\tof\\tits\\tparent\\n7\\n\\tplus\\tsome\\trandom\\nvariation.\\tThe\\tsurviving\\tpolicies\\tplus\\ttheir\\toffspring\\ttogether\\tconstitute\\tthe\\tsecond\\tgeneration.\\tYou\\tcan\\ncontinue\\tto\\titerate\\tthrough\\tgenerations\\tthis\\tway,\\tuntil\\tyou\\tfind\\ta\\tgood\\tpolicy.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 536}), Document(page_content='Figure\\t16-3.\\t\\nFour\\tpoints\\tin\\tpolicy\\tspace\\tand\\tthe\\tagent’s\\tcorresponding\\tbehavior\\nYet\\tanother\\tapproach\\tis\\tto\\tuse\\toptimization\\ttechniques,\\tby\\tevaluating\\tthe\\tgradients\\tof\\tthe\\trewards\\twith\\nregards\\tto\\tthe\\tpolicy\\tparameters,\\tthen\\ttweaking\\tthese\\tparameters\\tby\\tfollowing\\tthe\\tgradient\\ttoward\\thigher\\nrewards\\t(\\ngradient\\tascent\\n).\\t\\nThis\\tapproach\\tis\\tcalled\\t\\npolicy\\tgradients\\n\\t(PG),\\twhich\\twe\\twill\\tdiscuss\\tin\\nmore\\tdetail\\tlater\\tin\\tthis\\tchapter.\\tFor\\texample,\\tgoing\\tback\\tto\\tthe\\tvacuum\\tcleaner\\trobot,\\tyou\\tcould\\tslightly\\nincrease\\t\\np\\n\\tand\\tevaluate\\twhether\\tthis\\tincreases\\tthe\\tamount\\tof\\tdust\\tpicked\\tup\\tby\\tthe\\trobot\\tin\\t30\\tminutes;\\tif\\nit\\tdoes,\\tthen\\tincrease\\t\\np\\n\\tsome\\tmore,\\tor\\telse\\treduce\\t\\np\\n.\\tWe\\twill\\timplement\\ta\\tpopular\\tPG\\talgorithm\\tusing\\nTensorFlow,\\tbut\\tbefore\\twe\\tdo\\twe\\tneed\\tto\\tcreate\\tan\\tenvironment\\tfor\\tthe\\tagent\\tto\\tlive\\tin,\\tso\\tit’s\\ttime\\tto\\nintroduce\\tOpenAI\\tgym.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 537}), Document(page_content='Introduction\\tto\\tOpenAI\\tGym\\nOne\\tof\\t\\nthe\\tchallenges\\tof\\tReinforcement\\tLearning\\tis\\tthat\\tin\\torder\\tto\\ttrain\\tan\\tagent,\\tyou\\tfirst\\tneed\\tto\\thave\\ta\\nworking\\tenvironment.\\tIf\\tyou\\twant\\tto\\tprogram\\tan\\tagent\\tthat\\twill\\tlearn\\tto\\tplay\\tan\\tAtari\\tgame,\\tyou\\twill\\tneed\\nan\\tAtari\\tgame\\tsimulator.\\tIf\\tyou\\twant\\tto\\tprogram\\ta\\twalking\\trobot,\\tthen\\tthe\\tenvironment\\tis\\tthe\\treal\\tworld\\nand\\tyou\\tcan\\tdirectly\\ttrain\\tyour\\trobot\\tin\\tthat\\tenvironment,\\tbut\\tthis\\thas\\tits\\tlimits:\\tif\\tthe\\trobot\\tfalls\\toff\\ta\\tcliff,\\nyou\\tcan’t\\tjust\\tclick\\t“undo.”\\tYou\\tcan’t\\tspeed\\tup\\ttime\\teither;\\tadding\\tmore\\tcomputing\\tpower\\twon’t\\tmake\\tthe\\nrobot\\tmove\\tany\\tfaster.\\tAnd\\tit’s\\tgenerally\\ttoo\\texpensive\\tto\\ttrain\\t1,000\\trobots\\tin\\tparallel.\\tIn\\tshort,\\ttraining\\nis\\thard\\tand\\tslow\\tin\\tthe\\treal\\tworld,\\tso\\tyou\\tgenerally\\tneed\\ta\\t\\nsimulated\\tenvironment\\n\\t\\nat\\tleast\\tto\\t\\nbootstrap\\ntraining.\\nOpenAI\\tgym\\n8\\n\\tis\\ta\\ttoolkit\\tthat\\tprovides\\ta\\twide\\tvariety\\tof\\tsimulated\\tenvironments\\t(Atari\\tgames,\\tboard\\ngames,\\t2D\\tand\\t3D\\tphysical\\tsimulations,\\tand\\tso\\ton),\\tso\\tyou\\tcan\\ttrain\\tagents,\\tcompare\\tthem,\\tor\\tdevelop\\nnew\\tRL\\talgorithms.\\nLet’s\\tinstall\\tOpenAI\\tgym.\\tFor\\ta\\tminimal\\tOpenAI\\tgym\\tinstallation,\\tsimply\\tuse\\tpip:\\n$\\tpip3\\tinstall\\t--upgrade\\tgym\\nNext\\topen\\tup\\ta\\tPython\\tshell\\tor\\ta\\tJupyter\\tnotebook\\tand\\tcreate\\tyour\\tfirst\\tenvironment:\\n>>>\\t\\nimport\\n\\t\\ngym\\n>>>\\t\\nenv\\n\\t\\n=\\n\\t\\ngym\\n.\\nmake\\n(\\n\"CartPole-v0\"\\n)\\n[2016-10-14\\t16:03:23,199]\\tMaking\\tnew\\tenv:\\tMsPacman-v0\\n>>>\\t\\nobs\\n\\t\\n=\\n\\t\\nenv\\n.\\nreset\\n()\\n>>>\\t\\nobs\\narray([-0.03799846,\\t-0.03288115,\\t\\t0.02337094,\\t\\t0.00720711])\\n>>>\\t\\nenv\\n.\\nrender\\n()\\nThe\\t\\nmake()\\n\\t\\nfunction\\tcreates\\tan\\tenvironment,\\tin\\tthis\\tcase\\ta\\tCartPole\\tenvironment.\\tThis\\tis\\ta\\t2D\\tsimulation\\nin\\twhich\\ta\\tcart\\tcan\\tbe\\taccelerated\\tleft\\tor\\tright\\tin\\torder\\tto\\tbalance\\ta\\tpole\\tplaced\\ton\\ttop\\tof\\tit\\t(see\\nFigure\\t16-4\\n).\\tAfter\\tthe\\tenvironment\\tis\\tcreated,\\twe\\tmust\\tinitialize\\tit\\tusing\\tthe\\t\\nreset()\\n\\t\\nmethod.\\tThis\\nreturns\\tthe\\tfirst\\tobservation.\\tObservations\\tdepend\\ton\\tthe\\ttype\\tof\\tenvironment.\\tFor\\tthe\\tCartPole\\nenvironment,\\teach\\tobservation\\tis\\ta\\t1D\\tNumPy\\tarray\\tcontaining\\tfour\\tfloats:\\tthese\\tfloats\\trepresent\\tthe\\ncart’s\\thorizontal\\tposition\\t(\\n0.0\\n\\t=\\tcenter),\\tits\\tvelocity,\\tthe\\tangle\\tof\\tthe\\tpole\\t(\\n0.0\\n\\t=\\tvertical),\\tand\\tits\\nangular\\tvelocity.\\tFinally,\\tthe\\t\\nrender()\\n\\t\\nmethod\\tdisplays\\tthe\\tenvironment\\tas\\tshown\\tin\\t\\nFigure\\t16-4\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 538}), Document(page_content='Figure\\t16-4.\\t\\nThe\\tCartPole\\tenvironment\\nIf\\tyou\\twant\\t\\nrender()\\n\\tto\\treturn\\tthe\\trendered\\timage\\tas\\ta\\tNumPy\\tarray,\\tyou\\tcan\\tset\\tthe\\t\\nmode\\n\\tparameter\\tto\\nrgb_array\\n\\t\\n(note\\tthat\\tother\\tenvironments\\tmay\\tsupport\\tdifferent\\tmodes):\\n>>>\\t\\nimg\\n\\t\\n=\\n\\t\\nenv\\n.\\nrender\\n(\\nmode\\n=\\n\"rgb_array\"\\n)\\n>>>\\t\\nimg\\n.\\nshape\\n\\t\\t\\n#\\theight,\\twidth,\\tchannels\\t(3=RGB)\\n(400,\\t600,\\t3)\\nTIP\\nUnfortunately,\\tthe\\tCartPole\\t(and\\ta\\tfew\\tother\\tenvironments)\\trenders\\tthe\\timage\\tto\\tthe\\tscreen\\teven\\tif\\tyou\\tset\\tthe\\tmode\\tto\\n\"rgb_array\"\\n.\\tThe\\tonly\\tway\\tto\\tavoid\\tthis\\tis\\tto\\tuse\\ta\\tfake\\tX\\tserver\\n\\tsuch\\tas\\tXvfb\\tor\\tXdummy.\\tFor\\texample,\\tyou\\tcan\\tinstall\\tXvfb\\nand\\tstart\\tPython\\tusing\\tthe\\tfollowing\\tcommand:\\t\\nxvfb-run\\t-s\\t\"-screen\\t0\\t1400x900x24\"\\tpython\\n.\\tOr\\tuse\\tthe\\t\\nxvfbwrapper\\npackage\\n.\\nLet’s\\task\\tthe\\tenvironment\\twhat\\tactions\\tare\\tpossible:\\n>>>\\t\\nenv\\n.\\naction_space\\nDiscrete(2)\\nDiscrete(2)\\n\\tmeans\\tthat\\tthe\\tpossible\\tactions\\tare\\tintegers\\t0\\tand\\t1,\\twhich\\trepresent\\taccelerating\\tleft\\t(0)\\nor\\tright\\t(1).\\tOther\\tenvironments\\tmay\\thave\\tmore\\tdiscrete\\tactions,\\tor\\tother\\tkinds\\tof\\tactions\\t(e.g.,\\ncontinuous).\\tSince\\tthe\\tpole\\tis\\tleaning\\ttoward\\tthe\\tright,\\tlet’s\\taccelerate\\tthe\\tcart\\ttoward\\tthe\\tright:\\n>>>\\t\\naction\\n\\t\\n=\\n\\t\\n1\\n\\t\\t\\n#\\taccelerate\\tright\\n>>>\\t\\nobs\\n,\\n\\t\\nreward\\n,\\n\\t\\ndone\\n,\\n\\t\\ninfo\\n\\t\\n=\\n\\t\\nenv\\n.\\nstep\\n(\\naction\\n)\\n>>>\\t\\nobs\\narray([-0.03865608,\\t\\t0.16189797,\\t\\t0.02351508,\\t-0.27801135])\\n>>>\\t\\nreward\\n1.0\\n>>>\\t\\ndone\\nFalse\\n>>>\\t\\ninfo', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 539}), Document(page_content=\"{}\\nThe\\t\\nstep()\\n\\tmethod\\t\\nexecutes\\tthe\\tgiven\\taction\\tand\\treturns\\tfour\\tvalues:\\nobs\\nThis\\tis\\tthe\\tnew\\tobservation.\\tThe\\tcart\\tis\\tnow\\tmoving\\ttoward\\tthe\\tright\\t(\\nobs[1]>0\\n).\\tThe\\tpole\\tis\\tstill\\ntilted\\ttoward\\tthe\\tright\\t(\\nobs[2]>0\\n),\\tbut\\tits\\tangular\\tvelocity\\tis\\tnow\\tnegative\\t(\\nobs[3]<0\\n),\\tso\\tit\\twill\\nlikely\\tbe\\ttilted\\ttoward\\tthe\\tleft\\tafter\\tthe\\tnext\\tstep.\\nreward\\nIn\\tthis\\tenvironment,\\tyou\\tget\\ta\\treward\\tof\\t1.0\\tat\\tevery\\tstep,\\tno\\tmatter\\twhat\\tyou\\tdo,\\tso\\tthe\\tgoal\\tis\\tto\\nkeep\\trunning\\tas\\tlong\\tas\\tpossible.\\ndone\\nThis\\tvalue\\twill\\tbe\\t\\nTrue\\n\\twhen\\t\\nthe\\t\\nepisode\\n\\tis\\tover.\\tThis\\twill\\thappen\\twhen\\tthe\\tpole\\ttilts\\ttoo\\tmuch.\\nAfter\\tthat,\\tthe\\tenvironment\\tmust\\tbe\\treset\\tbefore\\tit\\tcan\\tbe\\tused\\tagain.\\ninfo\\nThis\\tdictionary\\tmay\\tprovide\\textra\\tdebug\\tinformation\\tin\\tother\\tenvironments.\\tThis\\tdata\\tshould\\tnot\\tbe\\nused\\tfor\\ttraining\\t(it\\twould\\tbe\\tcheating).\\nLet’s\\thardcode\\ta\\tsimple\\tpolicy\\tthat\\taccelerates\\tleft\\twhen\\tthe\\tpole\\tis\\tleaning\\ttoward\\tthe\\tleft\\tand\\naccelerates\\tright\\twhen\\tthe\\tpole\\tis\\tleaning\\ttoward\\tthe\\tright.\\tWe\\twill\\trun\\tthis\\tpolicy\\tto\\tsee\\tthe\\taverage\\nrewards\\tit\\tgets\\tover\\t500\\tepisodes:\\ndef\\n\\t\\nbasic_policy\\n(\\nobs\\n):\\n\\t\\t\\t\\t\\nangle\\n\\t\\n=\\n\\t\\nobs\\n[\\n2\\n]\\n\\t\\t\\t\\t\\nreturn\\n\\t\\n0\\n\\t\\nif\\n\\t\\nangle\\n\\t\\n<\\n\\t\\n0\\n\\t\\nelse\\n\\t\\n1\\ntotals\\n\\t\\n=\\n\\t\\n[]\\nfor\\n\\t\\nepisode\\n\\t\\nin\\n\\t\\nrange\\n(\\n500\\n):\\n\\t\\t\\t\\t\\nepisode_rewards\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\t\\t\\nobs\\n\\t\\n=\\n\\t\\nenv\\n.\\nreset\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nstep\\n\\t\\nin\\n\\t\\nrange\\n(\\n1000\\n):\\n\\t\\n#\\t1000\\tsteps\\tmax,\\twe\\tdon't\\twant\\tto\\trun\\tforever\\n\\t\\t\\t\\t\\t\\t\\t\\t\\naction\\n\\t\\n=\\n\\t\\nbasic_policy\\n(\\nobs\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n,\\n\\t\\nreward\\n,\\n\\t\\ndone\\n,\\n\\t\\ninfo\\n\\t\\n=\\n\\t\\nenv\\n.\\nstep\\n(\\naction\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nepisode_rewards\\n\\t\\n+=\\n\\t\\nreward\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\ndone\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbreak\\n\\t\\t\\t\\t\\ntotals\\n.\\nappend\\n(\\nepisode_rewards\\n)\\nThis\\tcode\\tis\\thopefully\\tself-explanatory.\\tLet’s\\tlook\\tat\\tthe\\tresult:\\n>>>\\t\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\n>>>\\t\\nnp\\n.\\nmean\\n(\\ntotals\\n),\\n\\t\\nnp\\n.\\nstd\\n(\\ntotals\\n),\\n\\t\\nnp\\n.\\nmin\\n(\\ntotals\\n),\\n\\t\\nnp\\n.\\nmax\\n(\\ntotals\\n)\\n(42.125999999999998,\\t9.1237121830974033,\\t24.0,\\t68.0)\\nEven\\twith\\t500\\ttries,\\tthis\\tpolicy\\tnever\\tmanaged\\tto\\tkeep\\tthe\\tpole\\tupright\\tfor\\tmore\\tthan\\t68\\tconsecutive\\nsteps.\\tNot\\tgreat.\\tIf\\tyou\\tlook\\tat\\tthe\\tsimulation\\tin\\tthe\\t\\nJupyter\\tnotebooks\\n,\\tyou\\twill\\tsee\\tthat\\tthe\\tcart\\toscillates\\nleft\\tand\\tright\\tmore\\tand\\tmore\\tstrongly\\tuntil\\tthe\\tpole\\ttilts\\ttoo\\tmuch.\\tLet’s\\tsee\\tif\\ta\\tneural\\tnetwork\\tcan\\tcome\\nup\\twith\\ta\\tbetter\\t\\npolicy.\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 540}), Document(page_content='Neural\\tNetwork\\tPolicies\\nLet’s\\t\\ncreate\\ta\\tneural\\tnetwork\\tpolicy.\\tJust\\tlike\\tthe\\tpolicy\\twe\\thardcoded\\tearlier,\\tthis\\tneural\\tnetwork\\twill\\ntake\\tan\\tobservation\\tas\\tinput,\\tand\\tit\\twill\\toutput\\tthe\\taction\\tto\\tbe\\texecuted.\\tMore\\tprecisely,\\tit\\twill\\testimate\\ta\\nprobability\\tfor\\teach\\taction,\\tand\\tthen\\twe\\twill\\tselect\\tan\\taction\\trandomly\\taccording\\tto\\tthe\\testimated\\nprobabilities\\t(see\\t\\nFigure\\t16-5\\n).\\tIn\\tthe\\tcase\\tof\\tthe\\tCartPole\\tenvironment,\\tthere\\tare\\tjust\\ttwo\\tpossible\\nactions\\t(left\\tor\\tright),\\tso\\twe\\tonly\\tneed\\tone\\toutput\\tneuron.\\tIt\\twill\\toutput\\tthe\\tprobability\\t\\np\\n\\tof\\taction\\t0\\t(left),\\nand\\tof\\tcourse\\tthe\\tprobability\\tof\\taction\\t1\\t(right)\\twill\\tbe\\t1\\t–\\t\\np\\n.\\tFor\\texample,\\tif\\tit\\toutputs\\t0.7,\\tthen\\twe\\twill\\npick\\taction\\t0\\twith\\t70%\\tprobability,\\tand\\taction\\t1\\twith\\t30%\\tprobability.\\nFigure\\t16-5.\\t\\nNeural\\tnetwork\\tpolicy\\nYou\\tmay\\twonder\\twhy\\twe\\tare\\tpicking\\ta\\trandom\\taction\\tbased\\ton\\tthe\\tprobability\\tgiven\\tby\\tthe\\tneural\\nnetwork,\\trather\\tthan\\tjust\\tpicking\\tthe\\taction\\twith\\tthe\\thighest\\tscore.\\tThis\\tapproach\\tlets\\tthe\\tagent\\tfind\\tthe\\nright\\tbalance\\tbetween\\t\\nexploring\\n\\tnew\\tactions\\tand\\t\\nexploiting\\n\\tthe\\tactions\\tthat\\tare\\tknown\\tto\\twork\\twell.\\nHere’s\\tan\\tanalogy:\\tsuppose\\tyou\\tgo\\tto\\ta\\trestaurant\\tfor\\tthe\\tfirst\\ttime,\\tand\\tall\\tthe\\tdishes\\tlook\\tequally\\nappealing\\tso\\tyou\\trandomly\\tpick\\tone.\\tIf\\tit\\tturns\\tout\\tto\\tbe\\tgood,\\tyou\\tcan\\tincrease\\tthe\\tprobability\\tto\\torder\\tit\\nnext\\ttime,\\tbut\\tyou\\tshouldn’t\\tincrease\\tthat\\tprobability\\tup\\tto\\t100%,\\tor\\telse\\tyou\\twill\\tnever\\ttry\\tout\\tthe\\tother\\ndishes,\\tsome\\tof\\twhich\\tmay\\tbe\\teven\\tbetter\\tthan\\tthe\\tone\\tyou\\ttried.\\nAlso\\tnote\\tthat\\tin\\tthis\\tparticular\\tenvironment,\\tthe\\tpast\\tactions\\tand\\tobservations\\tcan\\tsafely\\tbe\\tignored,', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 541}), Document(page_content=\"since\\teach\\tobservation\\tcontains\\tthe\\tenvironment’s\\tfull\\tstate.\\tIf\\tthere\\twere\\tsome\\thidden\\tstate,\\tthen\\tyou\\tmay\\nneed\\tto\\tconsider\\tpast\\tactions\\tand\\tobservations\\tas\\twell.\\tFor\\texample,\\tif\\tthe\\tenvironment\\tonly\\trevealed\\tthe\\nposition\\tof\\tthe\\tcart\\tbut\\tnot\\tits\\tvelocity,\\tyou\\twould\\thave\\tto\\tconsider\\tnot\\tonly\\tthe\\tcurrent\\tobservation\\tbut\\nalso\\tthe\\tprevious\\tobservation\\tin\\torder\\tto\\testimate\\tthe\\tcurrent\\tvelocity.\\tAnother\\texample\\tis\\twhen\\tthe\\nobservations\\tare\\tnoisy;\\tin\\tthat\\tcase,\\tyou\\tgenerally\\twant\\tto\\tuse\\tthe\\tpast\\tfew\\tobservations\\tto\\testimate\\tthe\\nmost\\tlikely\\tcurrent\\tstate.\\tThe\\tCartPole\\tproblem\\tis\\tthus\\tas\\tsimple\\tas\\tcan\\tbe;\\tthe\\tobservations\\tare\\tnoise-\\nfree\\tand\\tthey\\tcontain\\tthe\\tenvironment’s\\tfull\\tstate.\\nHere\\tis\\tthe\\tcode\\tto\\tbuild\\tthis\\tneural\\tnetwork\\tpolicy\\tusing\\t\\nTensorFlow:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\n#\\t1.\\tSpecify\\tthe\\tneural\\tnetwork\\tarchitecture\\nn_inputs\\n\\t\\n=\\n\\t\\n4\\n\\t\\t\\n#\\t==\\tenv.observation_space.shape[0]\\nn_hidden\\n\\t\\n=\\n\\t\\n4\\n\\t\\t\\n#\\tit's\\ta\\tsimple\\ttask,\\twe\\tdon't\\tneed\\tmore\\thidden\\tneurons\\nn_outputs\\n\\t\\n=\\n\\t\\n1\\n\\t\\n#\\tonly\\toutputs\\tthe\\tprobability\\tof\\taccelerating\\tleft\\ninitializer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\n#\\t2.\\tBuild\\tthe\\tneural\\tnetwork\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nhidden\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid\\n(\\nlogits\\n)\\n#\\t3.\\tSelect\\ta\\trandom\\taction\\tbased\\ton\\tthe\\testimated\\tprobabilities\\np_left_and_right\\n\\t\\n=\\n\\t\\ntf\\n.\\nconcat\\n(\\naxis\\n=\\n1\\n,\\n\\t\\nvalues\\n=\\n[\\noutputs\\n,\\n\\t\\n1\\n\\t\\n-\\n\\t\\noutputs\\n])\\naction\\n\\t\\n=\\n\\t\\ntf\\n.\\nmultinomial\\n(\\ntf\\n.\\nlog\\n(\\np_left_and_right\\n),\\n\\t\\nnum_samples\\n=\\n1\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nLet’s\\tgo\\tthrough\\tthis\\tcode:\\n1\\n.\\t\\nAfter\\tthe\\timports,\\twe\\tdefine\\tthe\\tneural\\tnetwork\\tarchitecture.\\tThe\\tnumber\\tof\\tinputs\\tis\\tthe\\tsize\\tof\\tthe\\nobservation\\tspace\\t(which\\tin\\tthe\\tcase\\tof\\tthe\\tCartPole\\tis\\tfour),\\twe\\tjust\\thave\\tfour\\thidden\\tunits\\tand\\tno\\nneed\\tfor\\tmore,\\tand\\twe\\thave\\tjust\\tone\\toutput\\tprobability\\t(the\\tprobability\\tof\\tgoing\\tleft).\\n2\\n.\\t\\nNext\\twe\\tbuild\\tthe\\tneural\\tnetwork.\\tIn\\tthis\\texample,\\tit’s\\ta\\tvanilla\\tMulti-Layer\\tPerceptron,\\t\\nwith\\ta\\nsingle\\toutput.\\tNote\\tthat\\tthe\\toutput\\tlayer\\tuses\\tthe\\tlogistic\\t(sigmoid)\\tactivation\\tfunction\\tin\\torder\\tto\\noutput\\ta\\tprobability\\tfrom\\t0.0\\tto\\t1.0.\\tIf\\tthere\\twere\\tmore\\tthan\\ttwo\\tpossible\\tactions,\\tthere\\twould\\tbe\\none\\toutput\\tneuron\\tper\\taction,\\tand\\tyou\\twould\\tuse\\tthe\\tsoftmax\\tactivation\\tfunction\\tinstead.\\n3\\n.\\t\\nLastly,\\twe\\tcall\\tthe\\t\\nmultinomial()\\n\\t\\nfunction\\tto\\tpick\\ta\\trandom\\taction.\\tThis\\tfunction\\tindependently\\nsamples\\tone\\t(or\\tmore)\\tintegers,\\tgiven\\tthe\\tlog\\tprobability\\tof\\teach\\tinteger.\\tFor\\texample,\\tif\\tyou\\tcall\\tit\\nwith\\tthe\\tarray\\t\\n[np.log(0.5),\\tnp.log(0.2),\\tnp.log(0.3)]\\n\\tand\\twith\\t\\nnum_samples=5\\n,\\tthen\\tit\\nwill\\toutput\\tfive\\tintegers,\\teach\\tof\\twhich\\twill\\thave\\ta\\t50%\\tprobability\\tof\\tbeing\\t0,\\t20%\\tof\\tbeing\\t1,\\tand\\n30%\\tof\\tbeing\\t2.\\tIn\\tour\\tcase\\twe\\tjust\\tneed\\tone\\tinteger\\trepresenting\\tthe\\taction\\tto\\ttake.\\tSince\\tthe\\noutputs\\n\\ttensor\\tonly\\tcontains\\tthe\\tprobability\\tof\\tgoing\\tleft,\\twe\\tmust\\tfirst\\tconcatenate\\t\\n1-outputs\\n\\tto\\tit\\nto\\thave\\ta\\ttensor\\tcontaining\\tthe\\tprobability\\tof\\tboth\\tleft\\tand\\tright\\tactions.\\tNote\\tthat\\tif\\tthere\\twere\\tmore\\nthan\\ttwo\\tpossible\\tactions,\\tthe\\tneural\\tnetwork\\twould\\thave\\tto\\toutput\\tone\\tprobability\\tper\\taction\\tso\\tyou\\nwould\\tnot\\tneed\\tthe\\tconcatenation\\tstep.\\nOkay,\\twe\\tnow\\thave\\ta\\tneural\\tnetwork\\tpolicy\\tthat\\twill\\ttake\\tobservations\\tand\\toutput\\tactions.\\tBut\\thow\\tdo\\nwe\\t\\ntrain\\tit?\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 542}), Document(page_content='Evaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\nIf\\twe\\t\\nknew\\twhat\\tthe\\tbest\\taction\\twas\\tat\\teach\\tstep,\\twe\\tcould\\ttrain\\tthe\\tneural\\tnetwork\\tas\\tusual,\\tby\\nminimizing\\tthe\\tcross\\tentropy\\tbetween\\tthe\\testimated\\tprobability\\tand\\tthe\\ttarget\\tprobability.\\tIt\\twould\\tjust\\tbe\\nregular\\tsupervised\\tlearning.\\tHowever,\\tin\\tReinforcement\\tLearning\\tthe\\tonly\\tguidance\\tthe\\tagent\\tgets\\tis\\nthrough\\trewards,\\tand\\trewards\\tare\\ttypically\\tsparse\\tand\\tdelayed.\\tFor\\texample,\\tif\\tthe\\tagent\\tmanages\\tto\\nbalance\\tthe\\tpole\\tfor\\t100\\tsteps,\\thow\\tcan\\tit\\tknow\\twhich\\tof\\tthe\\t100\\tactions\\tit\\ttook\\twere\\tgood,\\tand\\twhich\\tof\\nthem\\twere\\tbad?\\tAll\\tit\\tknows\\tis\\tthat\\tthe\\tpole\\tfell\\tafter\\tthe\\tlast\\taction,\\tbut\\tsurely\\tthis\\tlast\\taction\\tis\\tnot\\nentirely\\tresponsible.\\tThis\\tis\\tcalled\\tthe\\t\\ncredit\\tassignment\\tproblem\\n:\\twhen\\tthe\\tagent\\tgets\\ta\\treward,\\tit\\tis\\nhard\\tfor\\tit\\tto\\tknow\\twhich\\tactions\\tshould\\tget\\tcredited\\t(or\\tblamed)\\tfor\\tit.\\tThink\\tof\\ta\\tdog\\tthat\\tgets\\trewarded\\nhours\\tafter\\tit\\tbehaved\\twell;\\twill\\tit\\tunderstand\\twhat\\tit\\tis\\trewarded\\tfor?\\nTo\\ttackle\\tthis\\tproblem,\\ta\\tcommon\\tstrategy\\tis\\tto\\tevaluate\\tan\\taction\\tbased\\ton\\tthe\\tsum\\tof\\tall\\tthe\\trewards\\tthat\\ncome\\tafter\\tit,\\tusually\\tapplying\\ta\\t\\ndiscount\\trate\\n\\t\\nr\\n\\tat\\teach\\tstep.\\tFor\\texample\\t(see\\t\\nFigure\\t16-6\\n),\\tif\\tan\\tagent\\ndecides\\tto\\tgo\\tright\\tthree\\ttimes\\tin\\ta\\trow\\tand\\tgets\\t+10\\treward\\tafter\\tthe\\tfirst\\tstep,\\t0\\tafter\\tthe\\tsecond\\tstep,\\nand\\tfinally\\t–50\\tafter\\tthe\\tthird\\tstep,\\tthen\\tassuming\\twe\\tuse\\ta\\tdiscount\\trate\\t\\nr\\n\\t=\\t0.8,\\tthe\\tfirst\\taction\\twill\\thave\\na\\ttotal\\tscore\\tof\\t10\\t+\\t\\nr\\n\\t×\\t0\\t+\\t\\nr\\n2\\n\\t×\\t(–50)\\t=\\t–22.\\tIf\\tthe\\tdiscount\\trate\\tis\\tclose\\tto\\t0,\\tthen\\tfuture\\trewards\\twon’t\\ncount\\tfor\\tmuch\\tcompared\\tto\\timmediate\\trewards.\\tConversely,\\tif\\tthe\\t\\ndiscount\\trate\\tis\\tclose\\tto\\t1,\\tthen\\nrewards\\tfar\\tinto\\tthe\\tfuture\\twill\\tcount\\talmost\\tas\\tmuch\\tas\\timmediate\\trewards.\\tTypical\\tdiscount\\trates\\tare\\n0.95\\tor\\t0.99.\\tWith\\ta\\tdiscount\\trate\\tof\\t0.95,\\trewards\\t13\\tsteps\\tinto\\tthe\\tfuture\\tcount\\troughly\\tfor\\thalf\\tas\\tmuch\\nas\\timmediate\\trewards\\t(since\\t0.95\\n13\\n\\t≈\\t0.5),\\twhile\\twith\\ta\\tdiscount\\trate\\tof\\t0.99,\\trewards\\t69\\tsteps\\tinto\\tthe\\nfuture\\tcount\\tfor\\thalf\\tas\\tmuch\\tas\\timmediate\\trewards.\\tIn\\tthe\\tCartPole\\tenvironment,\\tactions\\thave\\tfairly\\nshort-term\\teffects,\\tso\\tchoosing\\ta\\tdiscount\\trate\\tof\\t0.95\\tseems\\treasonable\\n.\\nFigure\\t16-6.\\t\\nDiscounted\\trewards\\nOf\\tcourse,\\ta\\tgood\\taction\\tmay\\tbe\\tfollowed\\tby\\tseveral\\tbad\\tactions\\tthat\\tcause\\tthe\\tpole\\tto\\tfall\\tquickly,\\nresulting\\tin\\tthe\\tgood\\taction\\tgetting\\ta\\tlow\\tscore\\t(similarly,\\ta\\tgood\\tactor\\tmay\\tsometimes\\tstar\\tin\\ta\\tterrible\\nmovie).\\tHowever,\\tif\\twe\\tplay\\tthe\\tgame\\tenough\\ttimes,\\ton\\taverage\\tgood\\tactions\\twill\\tget\\ta\\tbetter\\tscore\\tthan\\nbad\\tones.\\tSo,\\tto\\tget\\tfairly\\treliable\\taction\\tscores,\\twe\\tmust\\trun\\tmany\\t\\nepisodes\\tand\\tnormalize\\tall\\tthe\\taction', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 543}), Document(page_content='scores\\t(by\\tsubtracting\\tthe\\tmean\\tand\\tdividing\\tby\\tthe\\tstandard\\tdeviation).\\tAfter\\tthat,\\twe\\tcan\\treasonably\\nassume\\tthat\\tactions\\twith\\ta\\tnegative\\tscore\\twere\\tbad\\twhile\\tactions\\twith\\ta\\tpositive\\tscore\\twere\\tgood.\\nPerfect\\t—\\tnow\\tthat\\twe\\thave\\ta\\tway\\tto\\tevaluate\\teach\\taction,\\twe\\tare\\tready\\tto\\ttrain\\tour\\tfirst\\tagent\\tusing\\npolicy\\tgradients.\\t\\nLet’s\\tsee\\thow.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 544}), Document(page_content='Policy\\tGradients\\nAs\\t\\ndiscussed\\tearlier,\\tPG\\talgorithms\\toptimize\\tthe\\tparameters\\tof\\ta\\tpolicy\\tby\\tfollowing\\tthe\\tgradients\\ntoward\\thigher\\trewards.\\tOne\\tpopular\\tclass\\tof\\tPG\\talgorithms,\\tcalled\\t\\nREINFORCE\\talgorithms\\n,\\t\\nwas\\nintroduced\\tback\\tin\\t1992\\n9\\n\\tby\\tRonald\\tWilliams.\\tHere\\tis\\tone\\tcommon\\tvariant:\\n1\\n.\\t\\nFirst,\\tlet\\tthe\\tneural\\tnetwork\\tpolicy\\tplay\\tthe\\tgame\\tseveral\\ttimes\\tand\\tat\\teach\\tstep\\tcompute\\tthe\\ngradients\\tthat\\twould\\tmake\\tthe\\tchosen\\taction\\teven\\tmore\\tlikely,\\tbut\\tdon’t\\tapply\\tthese\\tgradients\\tyet.\\n2\\n.\\t\\nOnce\\tyou\\thave\\trun\\tseveral\\tepisodes,\\tcompute\\teach\\taction’s\\tscore\\t(using\\tthe\\tmethod\\tdescribed\\tin\\tthe\\nprevious\\tparagraph).\\n3\\n.\\t\\nIf\\tan\\taction’s\\tscore\\tis\\tpositive,\\tit\\tmeans\\tthat\\tthe\\taction\\twas\\tgood\\tand\\tyou\\twant\\tto\\tapply\\tthe\\tgradients\\ncomputed\\tearlier\\tto\\tmake\\tthe\\taction\\teven\\tmore\\tlikely\\tto\\tbe\\tchosen\\tin\\tthe\\tfuture.\\tHowever,\\tif\\tthe\\nscore\\tis\\tnegative,\\tit\\tmeans\\tthe\\taction\\twas\\tbad\\tand\\tyou\\twant\\tto\\tapply\\tthe\\topposite\\tgradients\\tto\\tmake\\nthis\\taction\\tslightly\\t\\nless\\n\\tlikely\\tin\\tthe\\tfuture.\\tThe\\tsolution\\tis\\tsimply\\tto\\tmultiply\\teach\\tgradient\\tvector\\tby\\nthe\\tcorresponding\\taction’s\\tscore.\\n4\\n.\\t\\nFinally,\\tcompute\\tthe\\tmean\\tof\\tall\\tthe\\tresulting\\tgradient\\tvectors,\\tand\\tuse\\tit\\tto\\tperform\\ta\\tGradient\\nDescent\\tstep.\\nLet’s\\timplement\\tthis\\talgorithm\\tusing\\tTensorFlow.\\tWe\\twill\\ttrain\\tthe\\tneural\\tnetwork\\tpolicy\\twe\\tbuilt\\tearlier\\nso\\tthat\\tit\\tlearns\\tto\\tbalance\\tthe\\tpole\\ton\\tthe\\tcart.\\tLet’s\\tstart\\tby\\tcompleting\\tthe\\tconstruction\\tphase\\twe\\tcoded\\nearlier\\tto\\tadd\\tthe\\ttarget\\tprobability,\\tthe\\t\\ncost\\tfunction,\\tand\\tthe\\ttraining\\toperation.\\tSince\\twe\\tare\\tacting\\tas\\nthough\\tthe\\tchosen\\taction\\tis\\tthe\\tbest\\tpossible\\taction,\\tthe\\ttarget\\tprobability\\tmust\\tbe\\t1.0\\tif\\tthe\\tchosen\\taction\\nis\\taction\\t0\\t(left)\\tand\\t0.0\\tif\\tit\\tis\\t\\naction\\t1\\t(right):\\ny\\n\\t\\n=\\n\\t\\n1.\\n\\t\\n-\\n\\t\\ntf\\n.\\nto_float\\n(\\naction\\n)\\nNow\\tthat\\twe\\thave\\ta\\ttarget\\tprobability,\\twe\\tcan\\tdefine\\tthe\\tcost\\tfunction\\t(cross\\tentropy)\\t\\nand\\tcompute\\tthe\\ngradients:\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\ncross_entropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid_cross_entropy_with_logits\\n(\\nlabels\\n=\\ny\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlogits\\n=\\nlogits\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ngrads_and_vars\\n\\t\\n=\\n\\t\\noptimizer\\n.\\ncompute_gradients\\n(\\ncross_entropy\\n)\\nNote\\tthat\\twe\\tare\\tcalling\\tthe\\toptimizer’s\\t\\ncompute_gradients()\\n\\t\\nmethod\\tinstead\\tof\\tthe\\t\\nminimize()\\nmethod.\\tThis\\tis\\tbecause\\twe\\twant\\tto\\ttweak\\tthe\\tgradients\\tbefore\\twe\\tapply\\tthem.\\n10\\n\\tThe\\ncompute_gradients()\\n\\tmethod\\treturns\\ta\\tlist\\tof\\tgradient\\tvector/variable\\tpairs\\t(one\\tpair\\tper\\ttrainable\\nvariable).\\tLet’s\\tput\\tall\\tthe\\tgradients\\tin\\ta\\tlist,\\tto\\tmake\\tit\\tmore\\tconvenient\\tto\\tobtain\\ttheir\\tvalues:\\ngradients\\n\\t\\n=\\n\\t\\n[\\ngrad\\n\\t\\nfor\\n\\t\\ngrad\\n,\\n\\t\\nvariable\\n\\t\\nin\\n\\t\\ngrads_and_vars\\n]\\nOkay,\\tnow\\tcomes\\tthe\\ttricky\\tpart.\\tDuring\\tthe\\texecution\\tphase,\\tthe\\talgorithm\\twill\\trun\\tthe\\tpolicy\\tand\\tat\\teach', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 545}), Document(page_content='step\\tit\\twill\\tevaluate\\tthese\\tgradient\\ttensors\\tand\\tstore\\ttheir\\tvalues.\\t\\nAfter\\ta\\tnumber\\tof\\tepisodes\\tit\\twill\\ttweak\\nthese\\tgradients\\tas\\texplained\\tearlier\\t(i.e.,\\tmultiply\\tthem\\tby\\tthe\\taction\\tscores\\tand\\tnormalize\\tthem)\\tand\\ncompute\\tthe\\tmean\\tof\\tthe\\ttweaked\\tgradients.\\tNext,\\tit\\twill\\tneed\\tto\\tfeed\\tthe\\tresulting\\tgradients\\tback\\tto\\tthe\\noptimizer\\tso\\tthat\\tit\\tcan\\tperform\\tan\\toptimization\\tstep.\\tThis\\tmeans\\twe\\tneed\\tone\\tplaceholder\\tper\\tgradient\\nvector.\\tMoreover,\\twe\\tmust\\tcreate\\tthe\\toperation\\tthat\\twill\\tapply\\tthe\\tupdated\\tgradients.\\tFor\\tthis\\twe\\twill\\ncall\\tthe\\toptimizer’s\\t\\napply_gradients()\\n\\t\\nfunction,\\twhich\\ttakes\\ta\\tlist\\tof\\tgradient\\tvector/variable\\tpairs.\\nInstead\\tof\\tgiving\\tit\\tthe\\toriginal\\tgradient\\tvectors,\\twe\\twill\\tgive\\tit\\ta\\tlist\\tcontaining\\tthe\\tupdated\\tgradients\\n(i.e.,\\tthe\\tones\\tfed\\tthrough\\tthe\\tgradient\\tplaceholders):\\ngradient_placeholders\\n\\t\\n=\\n\\t\\n[]\\ngrads_and_vars_feed\\n\\t\\n=\\n\\t\\n[]\\nfor\\n\\t\\ngrad\\n,\\n\\t\\nvariable\\n\\t\\nin\\n\\t\\ngrads_and_vars\\n:\\n\\t\\t\\t\\t\\ngradient_placeholder\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\ngrad\\n.\\nget_shape\\n())\\n\\t\\t\\t\\t\\ngradient_placeholders\\n.\\nappend\\n(\\ngradient_placeholder\\n)\\n\\t\\t\\t\\t\\ngrads_and_vars_feed\\n.\\nappend\\n((\\ngradient_placeholder\\n,\\n\\t\\nvariable\\n))\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\napply_gradients\\n(\\ngrads_and_vars_feed\\n)\\nLet’s\\tstep\\tback\\tand\\ttake\\ta\\tlook\\tat\\tthe\\tfull\\t\\nconstruction\\tphase:\\nn_inputs\\n\\t\\n=\\n\\t\\n4\\nn_hidden\\n\\t\\n=\\n\\t\\n4\\nn_outputs\\n\\t\\n=\\n\\t\\n1\\ninitializer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nhidden\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid\\n(\\nlogits\\n)\\np_left_and_right\\n\\t\\n=\\n\\t\\ntf\\n.\\nconcat\\n(\\naxis\\n=\\n1\\n,\\n\\t\\nvalues\\n=\\n[\\noutputs\\n,\\n\\t\\n1\\n\\t\\n-\\n\\t\\noutputs\\n])\\naction\\n\\t\\n=\\n\\t\\ntf\\n.\\nmultinomial\\n(\\ntf\\n.\\nlog\\n(\\np_left_and_right\\n),\\n\\t\\nnum_samples\\n=\\n1\\n)\\ny\\n\\t\\n=\\n\\t\\n1.\\n\\t\\n-\\n\\t\\ntf\\n.\\nto_float\\n(\\naction\\n)\\ncross_entropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid_cross_entropy_with_logits\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlabels\\n=\\ny\\n,\\n\\t\\nlogits\\n=\\nlogits\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ngrads_and_vars\\n\\t\\n=\\n\\t\\noptimizer\\n.\\ncompute_gradients\\n(\\ncross_entropy\\n)\\ngradients\\n\\t\\n=\\n\\t\\n[\\ngrad\\n\\t\\nfor\\n\\t\\ngrad\\n,\\n\\t\\nvariable\\n\\t\\nin\\n\\t\\ngrads_and_vars\\n]\\ngradient_placeholders\\n\\t\\n=\\n\\t\\n[]\\ngrads_and_vars_feed\\n\\t\\n=\\n\\t\\n[]\\nfor\\n\\t\\ngrad\\n,\\n\\t\\nvariable\\n\\t\\nin\\n\\t\\ngrads_and_vars\\n:\\n\\t\\t\\t\\t\\ngradient_placeholder\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\ngrad\\n.\\nget_shape\\n())\\n\\t\\t\\t\\t\\ngradient_placeholders\\n.\\nappend\\n(\\ngradient_placeholder\\n)\\n\\t\\t\\t\\t\\ngrads_and_vars_feed\\n.\\nappend\\n((\\ngradient_placeholder\\n,\\n\\t\\nvariable\\n))\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\napply_gradients\\n(\\ngrads_and_vars_feed\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\nOn\\tto\\tthe\\texecution\\tphase!\\tWe\\twill\\tneed\\ta\\tcouple\\tof\\tfunctions\\tto\\tcompute\\tthe\\ttotal\\tdiscounted\\trewards,\\ngiven\\tthe\\traw\\trewards,\\tand\\tto\\tnormalize\\tthe\\tresults\\tacross\\tmultiple\\t\\nepisodes:\\ndef\\n\\t\\ndiscount_rewards\\n(\\nrewards\\n,\\n\\t\\ndiscount_rate\\n):\\n\\t\\t\\t\\t\\ndiscounted_rewards\\n\\t\\n=\\n\\t\\nnp\\n.\\nempty\\n(\\nlen\\n(\\nrewards\\n))\\n\\t\\t\\t\\t\\ncumulative_rewards\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\t\\t\\nfor\\n\\t\\nstep\\n\\t\\nin\\n\\t\\nreversed\\n(\\nrange\\n(\\nlen\\n(\\nrewards\\n))):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ncumulative_rewards\\n\\t\\n=\\n\\t\\nrewards\\n[\\nstep\\n]\\n\\t\\n+\\n\\t\\ncumulative_rewards\\n\\t\\n*\\n\\t\\ndiscount_rate\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ndiscounted_rewards\\n[\\nstep\\n]\\n\\t\\n=\\n\\t\\ncumulative_rewards\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ndiscounted_rewards', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 546}), Document(page_content='def\\n\\t\\ndiscount_and_normalize_rewards\\n(\\nall_rewards\\n,\\n\\t\\ndiscount_rate\\n):\\n\\t\\t\\t\\t\\nall_discounted_rewards\\n\\t\\n=\\n\\t\\n[\\ndiscount_rewards\\n(\\nrewards\\n,\\n\\t\\ndiscount_rate\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nrewards\\n\\t\\nin\\n\\t\\nall_rewards\\n]\\n\\t\\t\\t\\t\\nflat_rewards\\n\\t\\n=\\n\\t\\nnp\\n.\\nconcatenate\\n(\\nall_discounted_rewards\\n)\\n\\t\\t\\t\\t\\nreward_mean\\n\\t\\n=\\n\\t\\nflat_rewards\\n.\\nmean\\n()\\n\\t\\t\\t\\t\\nreward_std\\n\\t\\n=\\n\\t\\nflat_rewards\\n.\\nstd\\n()\\n\\t\\t\\t\\t\\nreturn\\n\\t\\n[(\\ndiscounted_rewards\\n\\t\\n-\\n\\t\\nreward_mean\\n)\\n/\\nreward_std\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ndiscounted_rewards\\n\\t\\nin\\n\\t\\nall_discounted_rewards\\n]\\nLet’s\\tcheck\\tthat\\tthis\\tworks:\\n>>>\\t\\ndiscount_rewards\\n([\\n10\\n,\\n\\t\\n0\\n,\\n\\t\\n-\\n50\\n],\\n\\t\\ndiscount_rate\\n=\\n0.8\\n)\\narray([-22.,\\t-40.,\\t-50.])\\n>>>\\t\\ndiscount_and_normalize_rewards\\n([[\\n10\\n,\\n\\t\\n0\\n,\\n\\t\\n-\\n50\\n],\\n\\t\\n[\\n10\\n,\\n\\t\\n20\\n]],\\n\\t\\ndiscount_rate\\n=\\n0.8\\n)\\n[array([-0.28435071,\\t-0.86597718,\\t-1.18910299]),\\n\\tarray([\\t1.26665318,\\t\\t1.0727777\\t])]\\nThe\\tcall\\tto\\t\\ndiscount_rewards()\\n\\treturns\\texactly\\twhat\\twe\\texpect\\t(see\\t\\nFigure\\t16-6\\n).\\tYou\\tcan\\tverify\\tthat\\nthe\\tfunction\\t\\ndiscount_and_normalize_rewards()\\n\\tdoes\\tindeed\\treturn\\tthe\\tnormalized\\tscores\\tfor\\teach\\naction\\tin\\tboth\\tepisodes.\\tNotice\\tthat\\tthe\\tfirst\\tepisode\\twas\\tmuch\\tworse\\tthan\\tthe\\tsecond,\\tso\\tits\\tnormalized\\nscores\\tare\\tall\\tnegative;\\tall\\tactions\\tfrom\\tthe\\tfirst\\tepisode\\twould\\tbe\\tconsidered\\tbad,\\tand\\tconversely\\tall\\nactions\\tfrom\\tthe\\tsecond\\tepisode\\twould\\tbe\\tconsidered\\tgood.\\nWe\\tnow\\thave\\tall\\twe\\tneed\\tto\\ttrain\\tthe\\tpolicy:\\nn_iterations\\n\\t\\n=\\n\\t\\n250\\n\\t\\t\\t\\t\\t\\t\\n#\\tnumber\\tof\\ttraining\\titerations\\nn_max_steps\\n\\t\\n=\\n\\t\\n1000\\n\\t\\t\\t\\t\\t\\t\\n#\\tmax\\tsteps\\tper\\tepisode\\nn_games_per_update\\n\\t\\n=\\n\\t\\n10\\n\\t\\n#\\ttrain\\tthe\\tpolicy\\tevery\\t10\\tepisodes\\nsave_iterations\\n\\t\\n=\\n\\t\\n10\\n\\t\\t\\t\\t\\n#\\tsave\\tthe\\tmodel\\tevery\\t10\\ttraining\\titerations\\ndiscount_rate\\n\\t\\n=\\n\\t\\n0.95\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nall_rewards\\n\\t\\n=\\n\\t\\n[]\\n\\t\\t\\t\\t\\n#\\tall\\tsequences\\tof\\traw\\trewards\\tfor\\teach\\tepisode\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nall_gradients\\n\\t\\n=\\n\\t\\n[]\\n\\t\\t\\n#\\tgradients\\tsaved\\tat\\teach\\tstep\\tof\\teach\\tepisode\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ngame\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_games_per_update\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncurrent_rewards\\n\\t\\n=\\n\\t\\n[]\\n\\t\\t\\t\\n#\\tall\\traw\\trewards\\tfrom\\tthe\\tcurrent\\tepisode\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncurrent_gradients\\n\\t\\n=\\n\\t\\n[]\\n\\t\\n#\\tall\\tgradients\\tfrom\\tthe\\tcurrent\\tepisode\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n\\t\\n=\\n\\t\\nenv\\n.\\nreset\\n()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nstep\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_max_steps\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\naction_val\\n,\\n\\t\\ngradients_val\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\naction\\n,\\n\\t\\ngradients\\n],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nobs\\n.\\nreshape\\n(\\n1\\n,\\n\\t\\nn_inputs\\n)})\\n\\t\\n#\\tone\\tobs\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n,\\n\\t\\nreward\\n,\\n\\t\\ndone\\n,\\n\\t\\ninfo\\n\\t\\n=\\n\\t\\nenv\\n.\\nstep\\n(\\naction_val\\n[\\n0\\n][\\n0\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncurrent_rewards\\n.\\nappend\\n(\\nreward\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncurrent_gradients\\n.\\nappend\\n(\\ngradients_val\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\ndone\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbreak\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nall_rewards\\n.\\nappend\\n(\\ncurrent_rewards\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nall_gradients\\n.\\nappend\\n(\\ncurrent_gradients\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tAt\\tthis\\tpoint\\twe\\thave\\trun\\tthe\\tpolicy\\tfor\\t10\\tepisodes,\\tand\\twe\\tare\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tready\\tfor\\ta\\tpolicy\\tupdate\\tusing\\tthe\\talgorithm\\tdescribed\\tearlier.\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nall_rewards\\n\\t\\n=\\n\\t\\ndiscount_and_normalize_rewards\\n(\\nall_rewards\\n,\\ndiscount_rate\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n\\t\\n=\\n\\t\\n{}\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nvar_index\\n,\\n\\t\\ngrad_placeholder\\n\\t\\nin\\n\\t\\nenumerate\\n(\\ngradient_placeholders\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tmultiply\\tthe\\tgradients\\tby\\tthe\\taction\\tscores,\\tand\\tcompute\\tthe\\tmean\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmean_gradients\\n\\t\\n=\\n\\t\\nnp\\n.\\nmean\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\nreward\\n\\t\\n*\\n\\t\\nall_gradients\\n[\\ngame_index\\n][\\nstep\\n][\\nvar_index\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ngame_index\\n,\\n\\t\\nrewards\\n\\t\\nin\\n\\t\\nenumerate\\n(\\nall_rewards\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nstep\\n,\\n\\t\\nreward\\n\\t\\nin\\n\\t\\nenumerate\\n(\\nrewards\\n)],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\naxis\\n=\\n0\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n[\\ngrad_placeholder\\n]\\n\\t\\n=\\n\\t\\nmean_gradients\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\nfeed_dict\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\niteration\\n\\t\\n%\\n\\t\\nsave_iterations\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"./my_policy_net_pg.ckpt\"\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 547}), Document(page_content='Each\\ttraining\\titeration\\tstarts\\tby\\trunning\\tthe\\tpolicy\\tfor\\t10\\tepisodes\\t(with\\tmaximum\\t1,000\\tsteps\\tper\\nepisode,\\tto\\tavoid\\trunning\\tforever).\\tAt\\teach\\tstep,\\twe\\talso\\tcompute\\tthe\\tgradients,\\tpretending\\tthat\\tthe\\nchosen\\taction\\twas\\tthe\\tbest.\\tAfter\\tthese\\t10\\tepisodes\\thave\\tbeen\\trun,\\twe\\tcompute\\tthe\\taction\\tscores\\tusing\\tthe\\ndiscount_and_normalize_rewards()\\n\\tfunction;\\twe\\tgo\\tthrough\\teach\\ttrainable\\tvariable,\\tacross\\tall\\nepisodes\\tand\\tall\\tsteps,\\tto\\tmultiply\\teach\\tgradient\\tvector\\tby\\tits\\tcorresponding\\taction\\tscore;\\tand\\twe\\ncompute\\tthe\\tmean\\tof\\tthe\\tresulting\\tgradients.\\tFinally,\\twe\\trun\\tthe\\ttraining\\toperation,\\tfeeding\\tit\\tthese\\tmean\\ngradients\\t(one\\tper\\ttrainable\\tvariable).\\tWe\\talso\\tsave\\tthe\\tmodel\\tevery\\t10\\t\\ntraining\\toperations.\\nAnd\\twe’re\\tdone!\\tThis\\tcode\\twill\\ttrain\\tthe\\tneural\\tnetwork\\tpolicy,\\tand\\tit\\twill\\tsuccessfully\\tlearn\\tto\\tbalance\\nthe\\tpole\\ton\\tthe\\tcart\\t(you\\tcan\\ttry\\tit\\tout\\tin\\tthe\\tJupyter\\tnotebooks).\\tNote\\tthat\\tthere\\tare\\tactually\\ttwo\\tways\\tthe\\nagent\\tcan\\tlose\\tthe\\tgame:\\teither\\tthe\\tpole\\tcan\\ttilt\\ttoo\\tmuch,\\tor\\tthe\\tcart\\tcan\\tgo\\tcompletely\\toff\\tthe\\tscreen.\\nWith\\t250\\ttraining\\titerations,\\tthe\\tpolicy\\tlearns\\tto\\tbalance\\tthe\\tpole\\tquite\\twell,\\tbut\\tit\\tis\\tnot\\tyet\\tgood\\tenough\\nat\\tavoiding\\tgoing\\toff\\tthe\\tscreen.\\tA\\tfew\\thundred\\tmore\\ttraining\\titerations\\twill\\tfix\\tthat.\\nTIP\\nResearchers\\ttry\\tto\\tfind\\talgorithms\\tthat\\twork\\twell\\teven\\twhen\\tthe\\tagent\\tinitially\\tknows\\tnothing\\tabout\\tthe\\tenvironment.\\tHowever,\\nunless\\tyou\\tare\\twriting\\ta\\tpaper,\\tyou\\tshould\\tinject\\tas\\tmuch\\tprior\\tknowledge\\tas\\tpossible\\tinto\\tthe\\tagent,\\tas\\tit\\twill\\tspeed\\tup\\ttraining\\ndramatically.\\tFor\\texample,\\tyou\\tcould\\tadd\\tnegative\\trewards\\tproportional\\tto\\tthe\\tdistance\\tfrom\\tthe\\tcenter\\tof\\tthe\\tscreen,\\tand\\tto\\tthe\\npole’s\\tangle.\\tAlso,\\tif\\tyou\\talready\\thave\\ta\\treasonably\\tgood\\tpolicy\\t(e.g.,\\thardcoded),\\tyou\\tmay\\twant\\tto\\ttrain\\tthe\\tneural\\tnetwork\\tto\\nimitate\\tit\\tbefore\\tusing\\tpolicy\\tgradients\\tto\\timprove\\tit.\\nDespite\\tits\\trelative\\tsimplicity,\\tthis\\talgorithm\\tis\\tquite\\tpowerful.\\tYou\\tcan\\tuse\\tit\\tto\\ttackle\\tmuch\\tharder\\nproblems\\tthan\\tbalancing\\ta\\tpole\\ton\\ta\\tcart.\\tIn\\tfact,\\t\\nAlphaGo\\twas\\tbased\\ton\\ta\\tsimilar\\tPG\\talgorithm\\t(plus\\nMonte\\tCarlo\\tTree\\tSearch\\n,\\t\\nwhich\\tis\\tbeyond\\tthe\\tscope\\tof\\tthis\\tbook).\\nWe\\twill\\tnow\\tlook\\tat\\tanother\\tpopular\\tfamily\\tof\\talgorithms.\\tWhereas\\tPG\\talgorithms\\tdirectly\\ttry\\tto\\toptimize\\nthe\\tpolicy\\tto\\tincrease\\trewards,\\tthe\\talgorithms\\twe\\twill\\tlook\\tat\\tnow\\tare\\tless\\tdirect:\\tthe\\tagent\\tlearns\\tto\\nestimate\\tthe\\texpected\\tsum\\tof\\tdiscounted\\tfuture\\trewards\\tfor\\teach\\tstate,\\tor\\tthe\\texpected\\tsum\\tof\\tdiscounted\\nfuture\\trewards\\tfor\\teach\\taction\\tin\\teach\\tstate,\\tthen\\tuses\\tthis\\tknowledge\\tto\\tdecide\\thow\\tto\\tact.\\tTo\\tunderstand\\nthese\\talgorithms,\\t\\nwe\\tmust\\tfirst\\tintroduce\\t\\nMarkov\\tdecision\\tprocesses\\n\\t(MDP).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 548}), Document(page_content='Markov\\tDecision\\tProcesses\\nIn\\tthe\\t\\nearly\\t20\\nth\\n\\tcentury,\\tthe\\tmathematician\\tAndrey\\tMarkov\\tstudied\\tstochastic\\tprocesses\\twith\\tno\\tmemory,\\ncalled\\t\\nMarkov\\tchains\\n.\\t\\nSuch\\ta\\tprocess\\thas\\ta\\tfixed\\tnumber\\tof\\tstates,\\tand\\tit\\trandomly\\tevolves\\tfrom\\tone\\nstate\\tto\\tanother\\tat\\teach\\tstep.\\tThe\\tprobability\\tfor\\tit\\tto\\tevolve\\tfrom\\ta\\tstate\\t\\ns\\n\\tto\\ta\\tstate\\t\\ns\\n′\\tis\\tfixed,\\tand\\tit\\ndepends\\tonly\\ton\\tthe\\tpair\\t(\\ns\\n,\\ns\\n′),\\tnot\\ton\\tpast\\tstates\\t(the\\tsystem\\thas\\tno\\tmemory).\\nFigure\\t16-7\\n\\tshows\\tan\\texample\\tof\\ta\\tMarkov\\tchain\\twith\\tfour\\tstates.\\tSuppose\\tthat\\tthe\\tprocess\\tstarts\\tin\\tstate\\ns\\n0\\n,\\tand\\tthere\\tis\\ta\\t70%\\tchance\\tthat\\tit\\twill\\tremain\\tin\\tthat\\tstate\\tat\\tthe\\tnext\\tstep.\\tEventually\\tit\\tis\\tbound\\tto\\nleave\\tthat\\tstate\\tand\\tnever\\tcome\\tback\\tsince\\tno\\tother\\tstate\\tpoints\\tback\\tto\\t\\ns\\n0\\n.\\tIf\\tit\\tgoes\\tto\\tstate\\t\\ns\\n1\\n,\\tit\\twill\\tthen\\nmost\\tlikely\\tgo\\tto\\tstate\\t\\ns\\n2\\n\\t(90%\\tprobability),\\tthen\\timmediately\\tback\\tto\\tstate\\t\\ns\\n1\\n\\t(with\\t100%\\tprobability).\\tIt\\nmay\\talternate\\ta\\tnumber\\tof\\ttimes\\tbetween\\tthese\\ttwo\\tstates,\\tbut\\teventually\\tit\\twill\\tfall\\tinto\\tstate\\t\\ns\\n3\\n\\tand\\nremain\\tthere\\tforever\\t(this\\tis\\ta\\t\\nterminal\\tstate\\n).\\tMarkov\\tchains\\tcan\\thave\\tvery\\tdifferent\\tdynamics,\\tand\\tthey\\nare\\theavily\\tused\\tin\\tthermodynamics,\\tchemistry,\\tstatistics,\\tand\\tmuch\\tmore.\\nFigure\\t16-7.\\t\\nExample\\tof\\ta\\tMarkov\\tchain\\nMarkov\\tdecision\\tprocesses\\twere\\t\\nfirst\\tdescribed\\tin\\tthe\\t1950s\\tby\\tRichard\\tBellman\\n.\\n11\\n\\tThey\\tresemble\\nMarkov\\tchains\\tbut\\twith\\ta\\ttwist:\\tat\\teach\\tstep,\\tan\\tagent\\tcan\\tchoose\\tone\\tof\\tseveral\\tpossible\\tactions,\\tand\\tthe\\ntransition\\tprobabilities\\tdepend\\ton\\tthe\\tchosen\\taction.\\tMoreover,\\tsome\\tstate\\ttransitions\\treturn\\tsome\\treward\\n(positive\\tor\\tnegative),\\tand\\tthe\\tagent’s\\tgoal\\tis\\tto\\tfind\\ta\\tpolicy\\tthat\\twill\\tmaximize\\trewards\\tover\\ttime.\\nFor\\texample,\\tthe\\tMDP\\trepresented\\tin\\t\\nFigure\\t16-8\\n\\thas\\tthree\\tstates\\tand\\tup\\tto\\tthree\\tpossible\\tdiscrete\\nactions\\tat\\teach\\tstep.\\tIf\\tit\\tstarts\\tin\\tstate\\t\\ns\\n0\\n,\\tthe\\tagent\\tcan\\tchoose\\tbetween\\tactions\\t\\na\\n0\\n,\\t\\na\\n1\\n,\\tor\\t\\na\\n2\\n.\\tIf\\tit\\tchooses\\naction\\t\\na\\n1\\n,\\tit\\tjust\\tremains\\tin\\tstate\\t\\ns\\n0\\n\\twith\\tcertainty,\\tand\\twithout\\tany\\treward.\\tIt\\tcan\\tthus\\tdecide\\tto\\tstay\\tthere\\nforever\\tif\\tit\\twants.\\tBut\\tif\\tit\\tchooses\\taction\\t\\na\\n0\\n,\\tit\\thas\\ta\\t70%\\tprobability\\tof\\tgaining\\ta\\treward\\tof\\t+10,\\tand\\nremaining\\tin\\tstate\\t\\ns\\n0\\n.\\tIt\\tcan\\tthen\\ttry\\tagain\\tand\\tagain\\tto\\tgain\\tas\\tmuch\\treward\\tas\\tpossible.\\tBut\\tat\\tone\\tpoint\\tit', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 549}), Document(page_content='is\\tgoing\\tto\\tend\\tup\\tinstead\\tin\\tstate\\t\\ns\\n1\\n.\\tIn\\tstate\\t\\ns\\n1\\n\\tit\\thas\\tonly\\ttwo\\tpossible\\tactions:\\t\\na\\n0\\n\\tor\\t\\na\\n2\\n.\\tIt\\tcan\\tchoose\\tto\\nstay\\tput\\tby\\trepeatedly\\tchoosing\\taction\\t\\na\\n0\\n,\\tor\\tit\\tcan\\tchoose\\tto\\tmove\\ton\\tto\\tstate\\t\\ns\\n2\\n\\tand\\tget\\ta\\tnegative\\nreward\\tof\\t–50\\t(ouch).\\tIn\\tstate\\t\\ns\\n2\\n\\tit\\thas\\tno\\tother\\tchoice\\tthan\\tto\\ttake\\taction\\t\\na\\n1\\n,\\twhich\\twill\\tmost\\tlikely\\tlead\\nit\\tback\\tto\\tstate\\t\\ns\\n0\\n,\\tgaining\\ta\\treward\\tof\\t+40\\ton\\tthe\\tway.\\tYou\\tget\\tthe\\tpicture.\\tBy\\tlooking\\tat\\tthis\\tMDP,\\tcan\\nyou\\tguess\\twhich\\tstrategy\\twill\\tgain\\tthe\\tmost\\treward\\tover\\ttime?\\tIn\\tstate\\t\\ns\\n0\\n\\tit\\tis\\tclear\\tthat\\taction\\t\\na\\n0\\n\\tis\\tthe\\nbest\\toption,\\tand\\tin\\tstate\\t\\ns\\n2\\n\\tthe\\tagent\\thas\\tno\\tchoice\\tbut\\tto\\ttake\\taction\\t\\na\\n1\\n,\\tbut\\tin\\tstate\\t\\ns\\n1\\n\\tit\\tis\\tnot\\tobvious\\nwhether\\tthe\\tagent\\tshould\\tstay\\tput\\t(\\na\\n0\\n)\\tor\\tgo\\tthrough\\tthe\\tfire\\t(\\na\\n2\\n).\\nFigure\\t16-8.\\t\\nExample\\tof\\ta\\tMarkov\\tdecision\\tprocess\\nBellman\\tfound\\ta\\tway\\tto\\testimate\\t\\nthe\\t\\noptimal\\tstate\\tvalue\\n\\tof\\tany\\tstate\\t\\ns\\n,\\tnoted\\t\\nV\\n*(\\ns\\n),\\twhich\\tis\\tthe\\tsum\\tof\\nall\\tdiscounted\\tfuture\\trewards\\tthe\\tagent\\tcan\\texpect\\ton\\taverage\\tafter\\tit\\treaches\\ta\\tstate\\t\\ns\\n,\\tassuming\\tit\\tacts\\noptimally.\\tHe\\tshowed\\tthat\\tif\\tthe\\tagent\\tacts\\toptimally,\\tthen\\t\\nthe\\t\\nBellman\\tOptimality\\tEquation\\n\\tapplies\\t(see\\nEquation\\t16-1\\n).\\tThis\\trecursive\\tequation\\tsays\\tthat\\tif\\tthe\\tagent\\tacts\\toptimally,\\tthen\\tthe\\toptimal\\tvalue\\tof\\tthe\\ncurrent\\tstate\\tis\\tequal\\tto\\tthe\\treward\\tit\\twill\\tget\\ton\\taverage\\tafter\\ttaking\\tone\\toptimal\\taction,\\tplus\\tthe\\texpected\\noptimal\\tvalue\\tof\\tall\\tpossible\\tnext\\tstates\\tthat\\tthis\\taction\\tcan\\tlead\\tto.\\nEquation\\t16-1.\\t\\nBellman\\tOptimality\\tEquation\\nT\\n(\\ns\\n,\\t\\na\\n,\\t\\ns\\n′)\\tis\\tthe\\ttransition\\tprobability\\tfrom\\tstate\\t\\ns\\n\\tto\\tstate\\t\\ns\\n′,\\tgiven\\tthat\\tthe\\tagent\\tchose\\taction\\t\\na\\n.\\nR\\n(\\ns\\n,\\t\\na\\n,\\t\\ns\\n′)\\tis\\tthe\\treward\\tthat\\tthe\\tagent\\tgets\\twhen\\tit\\tgoes\\tfrom\\tstate\\t\\ns\\n\\tto\\tstate\\t\\ns\\n′,\\tgiven\\tthat\\tthe\\tagent\\nchose\\taction\\t\\na\\n.\\nγ\\n\\tis\\tthe\\tdiscount\\trate.\\nThis\\tequation\\tleads\\tdirectly\\tto\\tan\\talgorithm\\tthat\\tcan\\tprecisely\\testimate\\tthe\\toptimal\\tstate\\tvalue\\tof\\tevery\\npossible\\tstate:\\tyou\\tfirst\\tinitialize\\tall\\tthe\\tstate\\tvalue\\testimates\\tto\\tzero,\\tand\\tthen\\tyou\\titeratively\\tupdate\\tthem\\nusing\\tthe\\t\\nValue\\tIteration\\n\\t\\nalgorithm\\t(see\\t\\nEquation\\t16-2\\n).\\tA\\tremarkable\\tresult\\tis\\tthat,\\tgiven\\tenough\\ttime,\\nthese\\testimates\\tare\\tguaranteed\\tto\\tconverge\\tto\\tthe\\toptimal\\tstate\\tvalues,\\tcorresponding\\tto\\tthe\\toptimal', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 550}), Document(page_content=\"policy.\\nEquation\\t16-2.\\t\\nValue\\tIteration\\talgorithm\\nV\\nk\\n(\\ns\\n)\\tis\\tthe\\testimated\\tvalue\\tof\\tstate\\t\\ns\\n\\tat\\tthe\\t\\nk\\nth\\n\\titeration\\tof\\tthe\\talgorithm.\\nNOTE\\nThis\\talgorithm\\tis\\tan\\texample\\tof\\t\\nDynamic\\tProgramming\\n,\\t\\nwhich\\tbreaks\\tdown\\ta\\tcomplex\\tproblem\\t(in\\tthis\\tcase\\testimating\\ta\\npotentially\\tinfinite\\tsum\\tof\\tdiscounted\\tfuture\\trewards)\\tinto\\ttractable\\tsub-problems\\tthat\\tcan\\tbe\\ttackled\\titeratively\\t(in\\tthis\\tcase\\nfinding\\tthe\\taction\\tthat\\tmaximizes\\tthe\\taverage\\treward\\tplus\\tthe\\tdiscounted\\tnext\\tstate\\tvalue).\\nKnowing\\tthe\\toptimal\\tstate\\tvalues\\tcan\\tbe\\tuseful,\\tin\\tparticular\\tto\\tevaluate\\ta\\tpolicy,\\tbut\\tit\\tdoes\\tnot\\ttell\\tthe\\nagent\\texplicitly\\twhat\\tto\\tdo.\\tLuckily,\\tBellman\\tfound\\ta\\tvery\\tsimilar\\talgorithm\\tto\\testimate\\tthe\\toptimal\\t\\nstate-\\naction\\tvalues\\n,\\t\\ngenerally\\tcalled\\t\\nQ-Values\\n.\\t\\nThe\\toptimal\\tQ-Value\\tof\\tthe\\tstate-action\\tpair\\t(\\ns\\n,\\na\\n),\\tnoted\\t\\nQ\\n*\\n(\\ns\\n,\\na\\n),\\tis\\tthe\\tsum\\tof\\tdiscounted\\tfuture\\trewards\\tthe\\tagent\\tcan\\texpect\\ton\\taverage\\tafter\\tit\\treaches\\tthe\\tstate\\t\\ns\\nand\\tchooses\\taction\\t\\na\\n,\\tbut\\tbefore\\tit\\tsees\\tthe\\toutcome\\tof\\tthis\\taction,\\tassuming\\tit\\tacts\\toptimally\\tafter\\tthat\\naction.\\nHere\\tis\\thow\\tit\\tworks:\\tonce\\tagain,\\tyou\\tstart\\tby\\tinitializing\\tall\\tthe\\tQ-Value\\testimates\\tto\\tzero,\\tthen\\tyou\\nupdate\\tthem\\tusing\\t\\nthe\\t\\nQ-Value\\tIteration\\n\\talgorithm\\t(see\\t\\nEquation\\t16-3\\n).\\nEquation\\t16-3.\\t\\nQ-Value\\tIteration\\talgorithm\\nOnce\\tyou\\thave\\tthe\\toptimal\\tQ-Values,\\tdefining\\tthe\\toptimal\\tpolicy,\\tnoted\\t\\nπ\\n*(\\ns\\n),\\tis\\ttrivial:\\twhen\\tthe\\tagent\\tis\\nin\\tstate\\t\\ns\\n,\\tit\\tshould\\tchoose\\tthe\\taction\\twith\\tthe\\thighest\\tQ-Value\\tfor\\tthat\\tstate:\\t\\n.\\nLet’s\\tapply\\tthis\\talgorithm\\tto\\tthe\\tMDP\\trepresented\\tin\\t\\nFigure\\t16-8\\n.\\tFirst,\\twe\\tneed\\tto\\tdefine\\tthe\\tMDP:\\nnan\\n=\\nnp\\n.\\nnan\\n\\t\\t\\n#\\trepresents\\timpossible\\tactions\\nT\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n\\t\\t\\n#\\tshape=[s,\\ta,\\ts']\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n0.7\\n,\\n\\t\\n0.3\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\n1.0\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\n0.8\\n,\\n\\t\\n0.2\\n,\\n\\t\\n0.0\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n0.0\\n,\\n\\t\\n1.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n],\\n\\t\\n[\\n0.0\\n,\\n\\t\\n0.0\\n,\\n\\t\\n1.0\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n],\\n\\t\\n[\\n0.8\\n,\\n\\t\\n0.1\\n,\\n\\t\\n0.1\\n],\\n\\t\\n[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n]],\\n\\t\\t\\t\\t\\n])\\nR\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n\\t\\t\\n#\\tshape=[s,\\ta,\\ts']\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n10.\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\n0.0\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\n0.0\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n10.\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n],\\n\\t\\n[\\n0.0\\n,\\n\\t\\n0.0\\n,\\n\\t\\n-\\n50.\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n],\\n\\t\\n[\\n40.\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n]],\\n\\t\\t\\t\\t\\n])\\npossible_actions\\n\\t\\n=\\n\\t\\n[[\\n0\\n,\\n\\t\\n1\\n,\\n\\t\\n2\\n],\\n\\t\\n[\\n0\\n,\\n\\t\\n2\\n],\\n\\t\\n[\\n1\\n]]\\nNow\\tlet’s\\trun\\tthe\\tQ-Value\\tIteration\\talgorithm:\\nQ\\n\\t\\n=\\n\\t\\nnp\\n.\\nfull\\n((\\n3\\n,\\n\\t\\n3\\n),\\n\\t\\n-\\nnp\\n.\\ninf\\n)\\n\\t\\t\\n#\\t-inf\\tfor\\timpossible\\tactions\\nfor\\n\\t\\nstate\\n,\\n\\t\\nactions\\n\\t\\nin\\n\\t\\nenumerate\\n(\\npossible_actions\\n):\\n\\t\\t\\t\\t\\nQ\\n[\\nstate\\n,\\n\\t\\nactions\\n]\\n\\t\\n=\\n\\t\\n0.0\\n\\t\\t\\n#\\tInitial\\tvalue\\t=\\t0.0,\\tfor\\tall\\tpossible\\tactions\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 551}), Document(page_content='learning_rate\\n\\t\\n=\\n\\t\\n0.01\\ndiscount_rate\\n\\t\\n=\\n\\t\\n0.95\\nn_iterations\\n\\t\\n=\\n\\t\\n100\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\nQ_prev\\n\\t\\n=\\n\\t\\nQ\\n.\\ncopy\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\ns\\n\\t\\nin\\n\\t\\nrange\\n(\\n3\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\na\\n\\t\\nin\\n\\t\\npossible_actions\\n[\\ns\\n]:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nQ\\n[\\ns\\n,\\n\\t\\na\\n]\\n\\t\\n=\\n\\t\\nnp\\n.\\nsum\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nT\\n[\\ns\\n,\\n\\t\\na\\n,\\n\\t\\nsp\\n]\\n\\t\\n*\\n\\t\\n(\\nR\\n[\\ns\\n,\\n\\t\\na\\n,\\n\\t\\nsp\\n]\\n\\t\\n+\\n\\t\\ndiscount_rate\\n\\t\\n*\\n\\t\\nnp\\n.\\nmax\\n(\\nQ_prev\\n[\\nsp\\n]))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nsp\\n\\t\\nin\\n\\t\\nrange\\n(\\n3\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n])\\nThe\\tresulting\\tQ-Values\\tlook\\tlike\\tthis:\\n>>>\\t\\nQ\\narray([[\\t21.89498982,\\t\\t20.80024033,\\t\\t16.86353093],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t1.11669335,\\t\\t\\t\\t\\t\\t\\t\\t\\t-inf,\\t\\t\\t1.17573546],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t\\t\\t\\t\\t\\t\\t-inf,\\t\\t53.86946068,\\t\\t\\t\\t\\t\\t\\t\\t\\t-inf]])\\n>>>\\t\\nnp\\n.\\nargmax\\n(\\nQ\\n,\\n\\t\\naxis\\n=\\n1\\n)\\n\\t\\t\\n#\\toptimal\\taction\\tfor\\teach\\tstate\\narray([0,\\t2,\\t1])\\nThis\\tgives\\tus\\tthe\\toptimal\\tpolicy\\tfor\\tthis\\tMDP,\\twhen\\tusing\\ta\\tdiscount\\trate\\tof\\t0.95:\\tin\\tstate\\t\\ns\\n0\\n\\tchoose\\naction\\t\\na\\n0\\n,\\tin\\tstate\\t\\ns\\n1\\n\\tchoose\\taction\\t\\na\\n2\\n\\t(go\\tthrough\\tthe\\tfire!),\\tand\\tin\\tstate\\t\\ns\\n2\\n\\tchoose\\taction\\t\\na\\n1\\n\\t(the\\tonly\\npossible\\taction).\\tInterestingly,\\tif\\tyou\\treduce\\tthe\\tdiscount\\trate\\tto\\t0.9,\\tthe\\toptimal\\tpolicy\\tchanges:\\tin\\tstate\\ns\\n1\\n\\tthe\\tbest\\taction\\tbecomes\\t\\na\\n0\\n\\t(stay\\tput;\\tdon’t\\tgo\\tthrough\\tthe\\tfire).\\tIt\\tmakes\\tsense\\tbecause\\tif\\tyou\\tvalue\\tthe\\npresent\\tmuch\\tmore\\tthan\\tthe\\tfuture,\\tthen\\tthe\\tprospect\\tof\\tfuture\\trewards\\tis\\tnot\\tworth\\t\\nimmediate\\tpain.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 552}), Document(page_content='Temporal\\tDifference\\tLearning\\tand\\tQ-Learning\\nReinforcement\\t\\nLearning\\tproblems\\twith\\tdiscrete\\tactions\\tcan\\toften\\tbe\\tmodeled\\tas\\tMarkov\\tdecision\\nprocesses,\\tbut\\tthe\\tagent\\tinitially\\thas\\tno\\tidea\\twhat\\tthe\\ttransition\\tprobabilities\\tare\\t(it\\tdoes\\tnot\\tknow\\t\\nT\\n(\\ns\\n,\\t\\na\\n,\\ns\\n′)),\\tand\\tit\\tdoes\\tnot\\tknow\\twhat\\tthe\\trewards\\tare\\tgoing\\tto\\tbe\\teither\\t(it\\tdoes\\tnot\\tknow\\t\\nR\\n(\\ns\\n,\\t\\na\\n,\\t\\ns\\n′)).\\tIt\\tmust\\nexperience\\teach\\tstate\\tand\\teach\\ttransition\\tat\\tleast\\tonce\\tto\\tknow\\tthe\\trewards,\\tand\\tit\\tmust\\texperience\\tthem\\nmultiple\\ttimes\\tif\\tit\\tis\\tto\\thave\\ta\\treasonable\\testimate\\tof\\tthe\\ttransition\\tprobabilities.\\nThe\\t\\nTemporal\\tDifference\\tLearning\\n\\t(TD\\tLearning)\\talgorithm\\tis\\tvery\\tsimilar\\tto\\tthe\\tValue\\tIteration\\nalgorithm,\\tbut\\ttweaked\\tto\\ttake\\tinto\\taccount\\tthe\\tfact\\tthat\\tthe\\tagent\\thas\\tonly\\tpartial\\tknowledge\\tof\\tthe\\tMDP.\\nIn\\tgeneral\\twe\\tassume\\tthat\\tthe\\tagent\\tinitially\\tknows\\tonly\\tthe\\tpossible\\tstates\\tand\\tactions,\\tand\\tnothing\\tmore.\\nThe\\tagent\\tuses\\tan\\t\\nexploration\\tpolicy\\n\\t—\\tfor\\texample,\\ta\\tpurely\\trandom\\tpolicy\\t—\\tto\\texplore\\tthe\\tMDP,\\tand\\nas\\tit\\tprogresses\\tthe\\tTD\\tLearning\\talgorithm\\tupdates\\tthe\\testimates\\tof\\tthe\\tstate\\tvalues\\tbased\\ton\\tthe\\ntransitions\\tand\\trewards\\tthat\\tare\\tactually\\tobserved\\t(see\\t\\nEquation\\t16-4\\n).\\nEquation\\t16-4.\\t\\nTD\\tLearning\\talgorithm\\nα\\n\\tis\\tthe\\tlearning\\trate\\t(e.g.,\\t0.01).\\nTIP\\nTD\\tLearning\\thas\\tmany\\tsimilarities\\twith\\tStochastic\\tGradient\\tDescent,\\tin\\tparticular\\tthe\\tfact\\tthat\\tit\\thandles\\tone\\tsample\\tat\\ta\\ttime.\\nJust\\tlike\\tSGD,\\tit\\tcan\\tonly\\ttruly\\tconverge\\tif\\tyou\\tgradually\\treduce\\tthe\\tlearning\\trate\\t(otherwise\\tit\\twill\\tkeep\\tbouncing\\taround\\tthe\\noptimum).\\nFor\\teach\\tstate\\t\\ns\\n,\\tthis\\talgorithm\\tsimply\\tkeeps\\ttrack\\tof\\ta\\trunning\\taverage\\tof\\tthe\\timmediate\\trewards\\tthe\\nagent\\tgets\\tupon\\tleaving\\tthat\\tstate,\\tplus\\tthe\\trewards\\tit\\texpects\\tto\\tget\\tlater\\t\\n(assuming\\tit\\tacts\\toptimally).\\nSimilarly,\\tthe\\t\\nQ-Learning\\talgorithm\\tis\\tan\\tadaptation\\tof\\tthe\\tQ-Value\\tIteration\\talgorithm\\tto\\tthe\\tsituation\\nwhere\\tthe\\ttransition\\tprobabilities\\tand\\tthe\\trewards\\tare\\tinitially\\tunknown\\t(see\\t\\nEquation\\t16-5\\n).\\nEquation\\t16-5.\\t\\nQ-Learning\\talgorithm\\nFor\\teach\\tstate-action\\tpair\\t(\\ns\\n,\\t\\na\\n),\\tthis\\talgorithm\\tkeeps\\ttrack\\tof\\ta\\trunning\\taverage\\tof\\tthe\\trewards\\t\\nr\\n\\tthe\\tagent\\ngets\\tupon\\tleaving\\tthe\\tstate\\t\\ns\\n\\twith\\taction\\t\\na\\n,\\tplus\\tthe\\trewards\\tit\\texpects\\tto\\tget\\tlater.\\tSince\\tthe\\ttarget\\tpolicy\\nwould\\tact\\toptimally,\\twe\\ttake\\tthe\\tmaximum\\tof\\tthe\\tQ-Value\\testimates\\tfor\\tthe\\tnext\\tstate.\\nHere\\tis\\thow\\tQ-Learning\\tcan\\tbe\\timplemented:\\nimport\\n\\t\\nnumpy.random\\n\\t\\nas\\n\\t\\nrnd\\nlearning_rate0\\n\\t\\n=\\n\\t\\n0.05\\nlearning_rate_decay\\n\\t\\n=\\n\\t\\n0.1', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 553}), Document(page_content='n_iterations\\n\\t\\n=\\n\\t\\n20000\\ns\\n\\t\\n=\\n\\t\\n0\\n\\t\\n#\\tstart\\tin\\tstate\\t0\\nQ\\n\\t\\n=\\n\\t\\nnp\\n.\\nfull\\n((\\n3\\n,\\n\\t\\n3\\n),\\n\\t\\n-\\nnp\\n.\\ninf\\n)\\n\\t\\t\\n#\\t-inf\\tfor\\timpossible\\tactions\\nfor\\n\\t\\nstate\\n,\\n\\t\\nactions\\n\\t\\nin\\n\\t\\nenumerate\\n(\\npossible_actions\\n):\\n\\t\\t\\t\\t\\nQ\\n[\\nstate\\n,\\n\\t\\nactions\\n]\\n\\t\\n=\\n\\t\\n0.0\\n\\t\\t\\n#\\tInitial\\tvalue\\t=\\t0.0,\\tfor\\tall\\tpossible\\tactions\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\na\\n\\t\\n=\\n\\t\\nrnd\\n.\\nchoice\\n(\\npossible_actions\\n[\\ns\\n])\\n\\t\\t\\n#\\tchoose\\tan\\taction\\t(randomly)\\n\\t\\t\\t\\t\\nsp\\n\\t\\n=\\n\\t\\nrnd\\n.\\nchoice\\n(\\nrange\\n(\\n3\\n),\\n\\t\\np\\n=\\nT\\n[\\ns\\n,\\n\\t\\na\\n])\\n\\t\\n#\\tpick\\tnext\\tstate\\tusing\\tT[s,\\ta]\\n\\t\\t\\t\\t\\nreward\\n\\t\\n=\\n\\t\\nR\\n[\\ns\\n,\\n\\t\\na\\n,\\n\\t\\nsp\\n]\\n\\t\\t\\t\\t\\nlearning_rate\\n\\t\\n=\\n\\t\\nlearning_rate0\\n\\t\\n/\\n\\t\\n(\\n1\\n\\t\\n+\\n\\t\\niteration\\n\\t\\n*\\n\\t\\nlearning_rate_decay\\n)\\n\\t\\t\\t\\t\\nQ\\n[\\ns\\n,\\n\\t\\na\\n]\\n\\t\\n=\\n\\t\\nlearning_rate\\n\\t\\n*\\n\\t\\nQ\\n[\\ns\\n,\\n\\t\\na\\n]\\n\\t\\n+\\n\\t\\n(\\n1\\n\\t\\n-\\n\\t\\nlearning_rate\\n)\\n\\t\\n*\\n\\t\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nreward\\n\\t\\n+\\n\\t\\ndiscount_rate\\n\\t\\n*\\n\\t\\nnp\\n.\\nmax\\n(\\nQ\\n[\\nsp\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n)\\n\\t\\t\\t\\t\\ns\\n\\t\\n=\\n\\t\\nsp\\n\\t\\n#\\tmove\\tto\\tnext\\tstate\\nGiven\\tenough\\titerations,\\tthis\\talgorithm\\twill\\tconverge\\tto\\tthe\\toptimal\\tQ-Values.\\tThis\\tis\\tcalled\\tan\\t\\noff-policy\\nalgorithm\\tbecause\\tthe\\tpolicy\\tbeing\\ttrained\\tis\\tnot\\tthe\\tone\\tbeing\\texecuted.\\tIt\\tis\\tsomewhat\\tsurprising\\tthat\\nthis\\talgorithm\\tis\\tcapable\\tof\\tlearning\\tthe\\toptimal\\tpolicy\\tby\\tjust\\twatching\\tan\\tagent\\tact\\trandomly\\t\\n(imagine\\nlearning\\tto\\tplay\\tgolf\\twhen\\tyour\\tteacher\\tis\\ta\\tdrunken\\tmonkey).\\tCan\\twe\\tdo\\tbetter?', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 554}), Document(page_content='Exploration\\tPolicies\\nOf\\t\\ncourse\\tQ-Learning\\tcan\\twork\\tonly\\tif\\tthe\\texploration\\tpolicy\\texplores\\tthe\\tMDP\\tthoroughly\\tenough.\\nAlthough\\ta\\tpurely\\trandom\\tpolicy\\tis\\tguaranteed\\tto\\teventually\\tvisit\\tevery\\tstate\\tand\\tevery\\ttransition\\tmany\\ntimes,\\tit\\tmay\\ttake\\tan\\textremely\\tlong\\ttime\\tto\\tdo\\tso.\\tTherefore,\\ta\\tbetter\\toption\\tis\\tto\\tuse\\tthe\\t\\nε-greedy\\tpolicy\\n:\\nat\\teach\\tstep\\tit\\tacts\\trandomly\\twith\\tprobability\\tε,\\tor\\tgreedily\\t(choosing\\tthe\\taction\\twith\\tthe\\thighest\\tQ-Value)\\nwith\\tprobability\\t1-ε.\\tThe\\tadvantage\\tof\\tthe\\tε-greedy\\tpolicy\\t(compared\\tto\\ta\\tcompletely\\trandom\\tpolicy)\\tis\\nthat\\tit\\twill\\tspend\\tmore\\tand\\tmore\\ttime\\texploring\\tthe\\tinteresting\\tparts\\tof\\tthe\\t\\nenvironment,\\tas\\tthe\\tQ-Value\\nestimates\\tget\\tbetter\\tand\\tbetter,\\twhile\\tstill\\tspending\\tsome\\ttime\\tvisiting\\tunknown\\tregions\\tof\\tthe\\tMDP.\\tIt\\tis\\nquite\\tcommon\\tto\\tstart\\twith\\ta\\thigh\\tvalue\\tfor\\tε\\t(e.g.,\\t1.0)\\tand\\tthen\\tgradually\\treduce\\tit\\t(e.g.,\\tdown\\tto\\t0.05).\\nAlternatively,\\trather\\tthan\\trelying\\ton\\tchance\\tfor\\texploration,\\tanother\\tapproach\\tis\\tto\\tencourage\\tthe\\nexploration\\tpolicy\\tto\\ttry\\tactions\\tthat\\tit\\thas\\tnot\\ttried\\tmuch\\tbefore.\\tThis\\tcan\\tbe\\timplemented\\tas\\ta\\tbonus\\nadded\\tto\\tthe\\tQ-Value\\testimates,\\tas\\tshown\\tin\\t\\nEquation\\t16-6\\n.\\nEquation\\t16-6.\\t\\nQ-Learning\\tusing\\tan\\texploration\\tfunction\\nN\\n(\\ns\\n′,\\t\\na\\n′)\\tcounts\\tthe\\tnumber\\tof\\ttimes\\tthe\\taction\\t\\na\\n′\\twas\\tchosen\\tin\\tstate\\t\\ns\\n′.\\nf\\n(\\nq\\n,\\t\\nn\\n)\\tis\\tan\\t\\nexploration\\tfunction\\n,\\tsuch\\tas\\t\\nf\\n(\\nq\\n,\\t\\nn\\n)\\t=\\t\\nq\\n\\t+\\t\\nK\\n/(1\\t+\\t\\nn\\n),\\twhere\\t\\nK\\n\\tis\\ta\\tcuriosity\\nhyperparameter\\tthat\\tmeasures\\thow\\tmuch\\tthe\\tagent\\tis\\tattracted\\tto\\tto\\tthe\\tunknown.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 555}), Document(page_content='Approximate\\tQ-Learning\\nThe\\t\\nmain\\tproblem\\twith\\tQ-Learning\\tis\\tthat\\tit\\tdoes\\tnot\\tscale\\twell\\tto\\tlarge\\t(or\\teven\\tmedium)\\tMDPs\\twith\\nmany\\tstates\\tand\\tactions.\\tConsider\\ttrying\\tto\\tuse\\tQ-Learning\\tto\\ttrain\\tan\\tagent\\tto\\tplay\\tMs.\\tPac-Man.\\tThere\\nare\\tover\\t250\\tpellets\\tthat\\tMs.\\tPac-Man\\tcan\\teat,\\teach\\tof\\twhich\\tcan\\tbe\\tpresent\\tor\\tabsent\\t(i.e.,\\talready\\neaten).\\tSo\\tthe\\tnumber\\tof\\tpossible\\tstates\\tis\\tgreater\\tthan\\t2\\n250\\n\\t≈\\t10\\n75\\n\\t(and\\tthat’s\\tconsidering\\tthe\\tpossible\\nstates\\tonly\\tof\\tthe\\tpellets).\\tThis\\tis\\tway\\tmore\\tthan\\tatoms\\tin\\tthe\\tobservable\\tuniverse,\\tso\\tthere’s\\tabsolutely\\nno\\tway\\tyou\\tcan\\tkeep\\ttrack\\tof\\tan\\testimate\\tfor\\tevery\\tsingle\\tQ-Value.\\nThe\\tsolution\\tis\\tto\\tfind\\ta\\tfunction\\tthat\\tapproximates\\tthe\\tQ-Values\\tusing\\ta\\tmanageable\\tnumber\\tof\\nparameters.\\tThis\\tis\\tcalled\\t\\nApproximate\\tQ-Learning\\n.\\tFor\\tyears\\tit\\twas\\trecommended\\tto\\tuse\\tlinear\\ncombinations\\tof\\thand-crafted\\tfeatures\\textracted\\tfrom\\tthe\\tstate\\t(e.g.,\\tdistance\\tof\\tthe\\tclosest\\tghosts,\\ttheir\\ndirections,\\tand\\tso\\ton)\\tto\\testimate\\tQ-Values,\\tbut\\t\\nDeepMind\\tshowed\\tthat\\tusing\\tdeep\\tneural\\tnetworks\\tcan\\nwork\\tmuch\\tbetter,\\tespecially\\tfor\\tcomplex\\tproblems,\\tand\\tit\\tdoes\\tnot\\trequire\\tany\\tfeature\\tengineering.\\tA\\nDNN\\tused\\tto\\testimate\\tQ-Values\\tis\\t\\ncalled\\ta\\t\\ndeep\\tQ-network\\n\\t(DQN),\\tand\\tusing\\ta\\tDQN\\tfor\\tApproximate\\nQ-Learning\\tis\\t\\ncalled\\t\\nDeep\\tQ-Learning\\n.\\nIn\\tthe\\trest\\tof\\tthis\\tchapter,\\twe\\twill\\tuse\\tDeep\\tQ-Learning\\tto\\ttrain\\tan\\tagent\\tto\\tplay\\tMs.\\tPac-Man,\\tmuch\\tlike\\nDeepMind\\tdid\\tin\\t2013.\\tThe\\tcode\\tcan\\teasily\\tbe\\ttweaked\\tto\\tlearn\\tto\\tplay\\tthe\\tmajority\\tof\\tAtari\\tgames\\tquite\\nwell.\\tIt\\tcan\\tachieve\\tsuperhuman\\tskill\\tat\\tmost\\taction\\tgames,\\tbut\\tit\\tis\\tnot\\tso\\tgood\\tat\\tgames\\twith\\tlong-\\nrunning\\tstorylines.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 556}), Document(page_content='Learning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nSince\\t\\nwe\\twill\\tbe\\tusing\\tan\\tAtari\\tenvironment,\\twe\\tmust\\tfirst\\tinstall\\tOpenAI\\tgym’s\\tAtari\\tdependencies.\\nWhile\\twe’re\\tat\\tit,\\twe\\twill\\talso\\tinstall\\tdependencies\\tfor\\tother\\tOpenAI\\tgym\\tenvironments\\tthat\\tyou\\tmay\\nwant\\tto\\tplay\\twith.\\tOn\\tmacOS,\\tassuming\\tyou\\thave\\tinstalled\\t\\nHomebrew\\n,\\tyou\\tneed\\tto\\trun:\\n$\\tbrew\\tinstall\\tcmake\\tboost\\tboost-python\\tsdl2\\tswig\\twget\\nOn\\tUbuntu,\\ttype\\tthe\\tfollowing\\tcommand\\t(replacing\\t\\npython3\\n\\twith\\t\\npython\\n\\tif\\tyou\\tare\\tusing\\tPython\\t2):\\n$\\tapt-get\\tinstall\\t-y\\tpython3-numpy\\tpython3-dev\\tcmake\\tzlib1g-dev\\tlibjpeg-dev\\\\\\n\\t\\t\\t\\txvfb\\tlibav-tools\\txorg-dev\\tpython3-opengl\\tlibboost-all-dev\\tlibsdl2-dev\\tswig\\nThen\\tinstall\\tthe\\textra\\tPython\\tmodules:\\n$\\tpip3\\tinstall\\t--upgrade\\t\\'gym[all]\\'\\nIf\\teverything\\twent\\twell,\\tyou\\tshould\\tbe\\table\\tto\\tcreate\\ta\\tMs.\\tPac-Man\\tenvironment:\\n>>>\\t\\nenv\\n\\t\\n=\\n\\t\\ngym\\n.\\nmake\\n(\\n\"MsPacman-v0\"\\n)\\n>>>\\t\\nobs\\n\\t\\n=\\n\\t\\nenv\\n.\\nreset\\n()\\n>>>\\t\\nobs\\n.\\nshape\\n\\t\\t\\n#\\t[height,\\twidth,\\tchannels]\\n(210,\\t160,\\t3)\\n>>>\\t\\nenv\\n.\\naction_space\\nDiscrete(9)\\nAs\\tyou\\tcan\\tsee,\\tthere\\tare\\tnine\\tdiscrete\\tactions\\tavailable,\\twhich\\tcorrespond\\tto\\tthe\\tnine\\tpossible\\tpositions\\nof\\tthe\\tjoystick\\t(left,\\tright,\\tup,\\tdown,\\tcenter,\\tupper\\tleft,\\tand\\tso\\ton),\\tand\\tthe\\tobservations\\tare\\tsimply\\nscreenshots\\tof\\tthe\\tAtari\\tscreen\\t(see\\t\\nFigure\\t16-9\\n,\\tleft),\\trepresented\\tas\\t3D\\tNumPy\\tarrays.\\tThese\\timages\\nare\\ta\\tbit\\tlarge,\\tso\\twe\\twill\\tcreate\\ta\\tsmall\\tpreprocessing\\tfunction\\tthat\\twill\\tcrop\\tthe\\timage\\tand\\tshrink\\tit\\ndown\\tto\\t88\\t×\\t80\\tpixels,\\tconvert\\tit\\tto\\tgrayscale,\\tand\\timprove\\tthe\\tcontrast\\tof\\tMs.\\tPac-Man.\\tThis\\twill\\nreduce\\tthe\\tamount\\tof\\tcomputations\\trequired\\tby\\tthe\\tDQN,\\tand\\tspeed\\tup\\ttraining.\\nmspacman_color\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n210\\n,\\n\\t\\n164\\n,\\n\\t\\n74\\n])\\n.\\nmean\\n()\\ndef\\n\\t\\npreprocess_observation\\n(\\nobs\\n):\\n\\t\\t\\t\\t\\nimg\\n\\t\\n=\\n\\t\\nobs\\n[\\n1\\n:\\n176\\n:\\n2\\n,\\n\\t\\n::\\n2\\n]\\n\\t\\n#\\tcrop\\tand\\tdownsize\\n\\t\\t\\t\\t\\nimg\\n\\t\\n=\\n\\t\\nimg\\n.\\nmean\\n(\\naxis\\n=\\n2\\n)\\n\\t\\n#\\tto\\tgreyscale\\n\\t\\t\\t\\t\\nimg\\n[\\nimg\\n==\\nmspacman_color\\n]\\n\\t\\n=\\n\\t\\n0\\n\\t\\n#\\timprove\\tcontrast\\n\\t\\t\\t\\t\\nimg\\n\\t\\n=\\n\\t\\n(\\nimg\\n\\t\\n-\\n\\t\\n128\\n)\\n\\t\\n/\\n\\t\\n128\\n\\t\\n-\\n\\t\\n1\\n\\t\\n#\\tnormalize\\tfrom\\t-1.\\tto\\t1.\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nimg\\n.\\nreshape\\n(\\n88\\n,\\n\\t\\n80\\n,\\n\\t\\n1\\n)\\nThe\\tresult\\tof\\tpreprocessing\\tis\\tshown\\tin\\t\\nFigure\\t16-9\\n\\t(right).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 557}), Document(page_content='Figure\\t16-9.\\t\\nMs.\\tPac-Man\\tobservation,\\toriginal\\t(left)\\tand\\tafter\\tpreprocessing\\t(right)\\nNext,\\tlet’s\\tcreate\\tthe\\tDQN.\\tIt\\tcould\\tjust\\ttake\\ta\\tstate-action\\tpair\\t(\\ns\\n,\\na\\n)\\tas\\tinput,\\tand\\toutput\\tan\\testimate\\tof\\nthe\\tcorresponding\\tQ-Value\\t\\nQ\\n(\\ns\\n,\\na\\n),\\tbut\\tsince\\tthe\\tactions\\tare\\tdiscrete\\tit\\tis\\tmore\\tconvenient\\tto\\tuse\\ta\\tneural\\nnetwork\\tthat\\ttakes\\tonly\\ta\\tstate\\t\\ns\\n\\tas\\tinput\\tand\\toutputs\\tone\\tQ-Value\\testimate\\tper\\taction.\\tThe\\tDQN\\twill\\tbe\\ncomposed\\tof\\tthree\\tconvolutional\\tlayers,\\tfollowed\\tby\\ttwo\\tfully\\tconnected\\tlayers,\\tincluding\\tthe\\toutput\\nlayer\\t(see\\t\\nFigure\\t16-10\\n).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 558}), Document(page_content='Figure\\t16-10.\\t\\nDeep\\tQ-network\\tto\\tplay\\tMs.\\tPac-Man\\nAs\\twe\\twill\\tsee,\\tthe\\ttraining\\talgorithm\\twe\\twill\\tuse\\trequires\\ttwo\\tDQNs\\twith\\tthe\\tsame\\tarchitecture\\t(but\\ndifferent\\tparameters):\\tone\\twill\\tbe\\tused\\tto\\tdrive\\tMs.\\tPac-Man\\tduring\\ttraining\\t(the\\t\\nactor\\n),\\t\\nand\\tthe\\tother\\nwill\\twatch\\tthe\\tactor\\tand\\tlearn\\tfrom\\tits\\ttrials\\tand\\terrors\\t(the\\t\\ncritic\\n).\\t\\nAt\\tregular\\tintervals\\twe\\twill\\tcopy\\tthe\\ncritic\\tto\\tthe\\tactor.\\tSince\\twe\\tneed\\ttwo\\tidentical\\tDQNs,\\twe\\twill\\tcreate\\ta\\t\\nq_network()\\n\\t\\nfunction\\tto\\tbuild\\nthem:\\ninput_height\\n\\t\\n=\\n\\t\\n88\\ninput_width\\n\\t\\n=\\n\\t\\n80\\ninput_channels\\n\\t\\n=\\n\\t\\n1\\nconv_n_maps\\n\\t\\n=\\n\\t\\n[\\n32\\n,\\n\\t\\n64\\n,\\n\\t\\n64\\n]\\nconv_kernel_sizes\\n\\t\\n=\\n\\t\\n[(\\n8\\n,\\n8\\n),\\n\\t\\n(\\n4\\n,\\n4\\n),\\n\\t\\n(\\n3\\n,\\n3\\n)]\\nconv_strides\\n\\t\\n=\\n\\t\\n[\\n4\\n,\\n\\t\\n2\\n,\\n\\t\\n1\\n]\\nconv_paddings\\n\\t\\n=\\n\\t\\n[\\n\"SAME\"\\n]\\n\\t\\n*\\n\\t\\n3\\nconv_activation\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\nnn\\n.\\nrelu\\n]\\n\\t\\n*\\n\\t\\n3', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 559}), Document(page_content='n_hidden_in\\n\\t\\n=\\n\\t\\n64\\n\\t\\n*\\n\\t\\n11\\n\\t\\n*\\n\\t\\n10\\n\\t\\t\\n#\\tconv3\\thas\\t64\\tmaps\\tof\\t11x10\\teach\\nn_hidden\\n\\t\\n=\\n\\t\\n512\\nhidden_activation\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nrelu\\nn_outputs\\n\\t\\n=\\n\\t\\nenv\\n.\\naction_space\\n.\\nn\\n\\t\\t\\n#\\t9\\tdiscrete\\tactions\\tare\\tavailable\\ninitializer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\ndef\\n\\t\\nq_network\\n(\\nX_state\\n,\\n\\t\\nname\\n):\\n\\t\\t\\t\\t\\nprev_layer\\n\\t\\n=\\n\\t\\nX_state\\n\\t\\t\\t\\t\\nconv_layers\\n\\t\\n=\\n\\t\\n[]\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\nname\\n)\\n\\t\\nas\\n\\t\\nscope\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nn_maps\\n,\\n\\t\\nkernel_size\\n,\\n\\t\\nstride\\n,\\n\\t\\npadding\\n,\\n\\t\\nactivation\\n\\t\\nin\\n\\t\\nzip\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nconv_n_maps\\n,\\n\\t\\nconv_kernel_sizes\\n,\\n\\t\\nconv_strides\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nconv_paddings\\n,\\n\\t\\nconv_activation\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nprev_layer\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\nconv2d\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nprev_layer\\n,\\n\\t\\nfilters\\n=\\nn_maps\\n,\\n\\t\\nkernel_size\\n=\\nkernel_size\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nstride\\n=\\nstride\\n,\\n\\t\\npadding\\n=\\npadding\\n,\\n\\t\\nactivation\\n=\\nactivation\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nconv_layers\\n.\\nappend\\n(\\nprev_layer\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nlast_conv_layer_flat\\n\\t\\n=\\n\\t\\ntf\\n.\\nreshape\\n(\\nprev_layer\\n,\\n\\t\\nshape\\n=\\n[\\n-\\n1\\n,\\n\\t\\nn_hidden_in\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nhidden\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nlast_conv_layer_flat\\n,\\n\\t\\nn_hidden\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\nhidden_activation\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\n\\t\\t\\t\\t\\ntrainable_vars\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nTRAINABLE_VARIABLES\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscope\\n=\\nscope\\n.\\nname\\n)\\n\\t\\t\\t\\t\\ntrainable_vars_by_name\\n\\t\\n=\\n\\t\\n{\\nvar\\n.\\nname\\n[\\nlen\\n(\\nscope\\n.\\nname\\n):]:\\n\\t\\nvar\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nvar\\n\\t\\nin\\n\\t\\ntrainable_vars\\n}\\n\\t\\t\\t\\t\\nreturn\\n\\t\\noutputs\\n,\\n\\t\\ntrainable_vars_by_name\\nThe\\tfirst\\tpart\\tof\\tthis\\tcode\\tdefines\\tthe\\thyperparameters\\tof\\tthe\\tDQN\\tarchitecture.\\tThen\\tthe\\t\\nq_network()\\nfunction\\tcreates\\tthe\\tDQN,\\ttaking\\tthe\\tenvironment’s\\tstate\\t\\nX_state\\n\\tas\\tinput,\\tand\\tthe\\tname\\tof\\tthe\\tvariable\\nscope.\\tNote\\tthat\\twe\\twill\\tjust\\tuse\\tone\\tobservation\\tto\\trepresent\\tthe\\tenvironment’s\\t\\nstate\\tsince\\tthere’s\\talmost\\nno\\thidden\\tstate\\t(except\\tfor\\tblinking\\tobjects\\tand\\tthe\\tghosts’\\tdirections).\\nThe\\t\\ntrainable_vars_by_name\\n\\tdictionary\\tgathers\\tall\\tthe\\ttrainable\\tvariables\\tof\\tthis\\tDQN.\\tIt\\twill\\tbe\\nuseful\\tin\\ta\\tminute\\twhen\\twe\\tcreate\\toperations\\tto\\tcopy\\tthe\\tcritic\\tDQN\\tto\\tthe\\tactor\\tDQN.\\tThe\\tkeys\\tof\\tthe\\ndictionary\\tare\\tthe\\tnames\\tof\\tthe\\tvariables,\\tstripping\\tthe\\tpart\\tof\\tthe\\tprefix\\tthat\\tjust\\tcorresponds\\tto\\tthe\\nscope’s\\tname.\\tIt\\tlooks\\tlike\\tthis:\\n>>>\\t\\ntrainable_vars_by_name\\n{\\'/conv2d/bias:0\\':\\t<tensorflow.python.ops.variables.Variable\\tat\\t0x121cf7b50>,\\n\\t\\'/conv2d/kernel:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/conv2d_1/bias:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/conv2d_1/kernel:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/conv2d_2/bias:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/conv2d_2/kernel:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/dense/bias:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/dense/kernel:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/dense_1/bias:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/dense_1/kernel:0\\':\\t<tensorflow.python.ops.variables.Variable...>}\\nNow\\tlet’s\\tcreate\\tthe\\tinput\\tplaceholder,\\tthe\\ttwo\\tDQNs,\\tand\\tthe\\toperation\\tto\\tcopy\\tthe\\tcritic\\tDQN\\tto\\t\\nthe\\nactor\\tDQN:\\nX_state\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\ninput_height\\n,\\n\\t\\ninput_width\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ninput_channels\\n])\\nactor_q_values\\n,\\n\\t\\nactor_vars\\n\\t\\n=\\n\\t\\nq_network\\n(\\nX_state\\n,\\n\\t\\nname\\n=\\n\"q_networks/actor\"\\n)\\ncritic_q_values\\n,\\n\\t\\ncritic_vars\\n\\t\\n=\\n\\t\\nq_network\\n(\\nX_state\\n,\\n\\t\\nname\\n=\\n\"q_networks/critic\"\\n)\\ncopy_ops\\n\\t\\n=\\n\\t\\n[\\nactor_var\\n.\\nassign\\n(\\ncritic_vars\\n[\\nvar_name\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nvar_name\\n,\\n\\t\\nactor_var\\n\\t\\nin\\n\\t\\nactor_vars\\n.\\nitems\\n()]\\ncopy_critic_to_actor\\n\\t\\n=\\n\\t\\ntf\\n.\\ngroup\\n(\\n*\\ncopy_ops\\n)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 560}), Document(page_content='Let’s\\tstep\\tback\\tfor\\ta\\tsecond:\\twe\\tnow\\thave\\ttwo\\tDQNs\\tthat\\tare\\tboth\\tcapable\\tof\\ttaking\\tan\\tenvironment\\tstate\\n(i.e.,\\ta\\tpreprocessed\\tobservation)\\tas\\tinput\\tand\\toutputting\\tan\\testimated\\tQ-Value\\tfor\\teach\\tpossible\\taction\\tin\\nthat\\tstate.\\tPlus\\twe\\thave\\tan\\toperation\\tcalled\\t\\ncopy_critic_to_actor\\n\\tto\\tcopy\\tall\\tthe\\ttrainable\\tvariables\\nof\\tthe\\tcritic\\tDQN\\tto\\tthe\\tactor\\tDQN.\\tWe\\tuse\\tTensorFlow’s\\t\\ntf.group()\\n\\t\\nfunction\\tto\\tgroup\\tall\\tthe\\nassignment\\toperations\\tinto\\ta\\tsingle\\tconvenient\\toperation.\\nThe\\tactor\\tDQN\\tcan\\tbe\\tused\\tto\\tplay\\tMs.\\tPac-Man\\t(initially\\tvery\\tbadly).\\tAs\\tdiscussed\\tearlier,\\tyou\\twant\\tit\\nto\\texplore\\tthe\\tgame\\tthoroughly\\tenough,\\tso\\tyou\\tgenerally\\twant\\tto\\tcombine\\tit\\twith\\t\\nan\\t\\nε-greedy\\tpolicy\\n\\tor\\nanother\\texploration\\tstrategy.\\nBut\\twhat\\tabout\\tthe\\tcritic\\tDQN?\\tHow\\twill\\tit\\tlearn\\tto\\tplay\\tthe\\tgame?\\tThe\\tshort\\tanswer\\tis\\tthat\\tit\\twill\\ttry\\tto\\nmake\\tits\\tQ-Value\\tpredictions\\tmatch\\tthe\\tQ-Values\\testimated\\tby\\tthe\\tactor\\tthrough\\tits\\texperience\\tof\\tthe\\ngame.\\tSpecifically,\\twe\\twill\\tlet\\tthe\\tactor\\tplay\\tfor\\ta\\twhile,\\tstoring\\tall\\tits\\texperiences\\tin\\ta\\t\\nreplay\\tmemory\\n.\\nEach\\tmemory\\twill\\tbe\\ta\\t5-tuple\\t(state,\\taction,\\tnext\\tstate,\\treward,\\tcontinue),\\twhere\\tthe\\t“continue”\\titem\\twill\\nbe\\tequal\\tto\\t0.0\\twhen\\tthe\\tgame\\tis\\tover,\\tor\\t1.0\\totherwise.\\tNext,\\tat\\tregular\\tintervals\\twe\\twill\\tsample\\ta\\tbatch\\nof\\tmemories\\tfrom\\tthe\\treplay\\tmemory,\\tand\\twe\\twill\\testimate\\tthe\\tQ-Values\\tfrom\\tthese\\tmemories.\\tFinally,\\nwe\\twill\\ttrain\\tthe\\tcritic\\tDQN\\tto\\tpredict\\tthese\\tQ-Values\\tusing\\tregular\\tsupervised\\tlearning\\ttechniques.\\tOnce\\nevery\\tfew\\ttraining\\titerations,\\twe\\twill\\tcopy\\tthe\\tcritic\\tDQN\\tto\\tthe\\tactor\\tDQN.\\tAnd\\tthat’s\\tit!\\t\\nEquation\\t16-7\\nshows\\tthe\\t\\ncost\\tfunction\\tused\\tto\\ttrain\\tthe\\tcritic\\tDQN:\\nEquation\\t16-7.\\t\\nDeep\\tQ-Learning\\tcost\\tfunction\\ns\\n(\\ni\\n)\\n,\\t\\na\\n(\\ni\\n)\\n,\\t\\nr\\n(\\ni\\n)\\n\\tand\\t\\ns\\n′\\n(\\ni\\n)\\n\\tare\\trespectively\\tthe\\tstate,\\taction,\\treward,\\tand\\tnext\\tstate\\tof\\tthe\\ti\\nth\\n\\tmemory\\nsampled\\tfrom\\tthe\\treplay\\tmemory.\\nm\\n\\tis\\tthe\\tsize\\tof\\tthe\\tmemory\\tbatch.\\nθ\\ncritic\\n\\tand\\t\\nθ\\nactor\\n\\tare\\tthe\\tcritic\\tand\\tthe\\tactor’s\\tparameters.\\nQ\\n(\\ns\\n(\\ni\\n)\\n,\\na\\n(\\ni\\n)\\n,\\nθ\\ncritic\\n)\\tis\\tthe\\tcritic\\tDQN’s\\tprediction\\tof\\tthe\\ti\\nth\\n\\tmemorized\\tstate-action’s\\tQ-Value.\\nQ\\n(\\ns\\n′\\n(\\ni\\n)\\n,\\t\\na\\n′,\\t\\nθ\\nactor\\n)\\tis\\tthe\\tactor\\tDQN’s\\tprediction\\tof\\tthe\\tQ-Value\\tit\\tcan\\texpect\\tfrom\\tthe\\tnext\\tstate\\t\\ns\\n′\\n(\\ni\\n)\\n\\tif\\nit\\tchooses\\taction\\t\\na\\n′.\\ny\\n(\\ni\\n)\\n\\tis\\tthe\\ttarget\\tQ-Value\\tfor\\tthe\\ti\\nth\\n\\tmemory.\\tNote\\tthat\\tit\\tis\\tequal\\tto\\tthe\\treward\\tactually\\tobserved\\tby\\nthe\\tactor,\\tplus\\tthe\\tactor’s\\t\\nprediction\\n\\tof\\twhat\\tfuture\\trewards\\tit\\tshould\\texpect\\tif\\tit\\twere\\tto\\tplay\\noptimally\\t(as\\tfar\\tas\\tit\\tknows).\\nJ\\n(\\nθ\\ncritic\\n)\\tis\\tthe\\tcost\\tfunction\\tused\\tto\\ttrain\\tthe\\tcritic\\tDQN.\\tAs\\tyou\\tcan\\tsee,\\tit\\tis\\tjust\\tthe\\tMean\\tSquared', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 561}), Document(page_content=\"Error\\tbetween\\tthe\\ttarget\\tQ-Values\\t\\ny\\n(\\ni\\n)\\n\\tas\\testimated\\tby\\tthe\\tactor\\tDQN,\\tand\\tthe\\tcritic\\tDQN’s\\npredictions\\tof\\tthese\\tQ-Values.\\nNOTE\\nThe\\treplay\\tmemory\\tis\\toptional,\\tbut\\thighly\\trecommended.\\tWithout\\tit,\\tyou\\twould\\ttrain\\tthe\\tcritic\\tDQN\\tusing\\tconsecutive\\nexperiences\\tthat\\tmay\\tbe\\tvery\\tcorrelated.\\tThis\\twould\\tintroduce\\ta\\tlot\\tof\\tbias\\tand\\tslow\\tdown\\tthe\\ttraining\\talgorithm’s\\tconvergence.\\nBy\\tusing\\ta\\treplay\\tmemory,\\twe\\tensure\\tthat\\tthe\\tmemories\\tfed\\tto\\tthe\\ttraining\\talgorithm\\tcan\\tbe\\tfairly\\tuncorrelated.\\nLet’s\\tadd\\tthe\\tcritic\\tDQN’s\\ttraining\\toperations.\\tFirst,\\t\\nwe\\tneed\\tto\\tbe\\table\\tto\\tcompute\\tits\\tpredicted\\tQ-\\nValues\\tfor\\teach\\tstate-action\\tin\\tthe\\tmemory\\tbatch.\\tSince\\tthe\\tDQN\\toutputs\\tone\\tQ-Value\\tfor\\tevery\\tpossible\\naction,\\twe\\tneed\\tto\\tkeep\\tonly\\tthe\\tQ-Value\\tthat\\tcorresponds\\tto\\tthe\\taction\\tthat\\twas\\tactually\\tchosen\\tin\\tthis\\nmemory.\\tFor\\tthis,\\twe\\twill\\tconvert\\tthe\\taction\\tto\\ta\\tone-hot\\tvector\\t(recall\\tthat\\tthis\\tis\\ta\\tvector\\tfull\\tof\\t0s\\nexcept\\tfor\\ta\\t1\\tat\\tthe\\ti\\nth\\n\\tindex),\\tand\\tmultiply\\tit\\tby\\tthe\\tQ-Values:\\tthis\\twill\\tzero\\tout\\tall\\tQ-Values\\texcept\\tfor\\nthe\\tone\\tcorresponding\\tto\\tthe\\tmemorized\\taction.\\tThen\\tjust\\tsum\\tover\\tthe\\tfirst\\taxis\\tto\\tobtain\\tonly\\tthe\\tdesired\\nQ-Value\\tprediction\\tfor\\teach\\t\\nmemory.\\nX_action\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n])\\nq_value\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\ncritic_q_values\\n\\t\\n*\\n\\t\\ntf\\n.\\none_hot\\n(\\nX_action\\n,\\n\\t\\nn_outputs\\n),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\naxis\\n=\\n1\\n,\\n\\t\\nkeep_dims\\n=\\nTrue\\n)\\nNext\\tlet’s\\tadd\\tthe\\ttraining\\toperations,\\tassuming\\tthe\\ttarget\\tQ-Values\\twill\\tbe\\tfed\\tthrough\\ta\\tplaceholder.\\tWe\\nalso\\tcreate\\ta\\tnontrainable\\tvariable\\tcalled\\t\\nglobal_step\\n.\\t\\nThe\\toptimizer’s\\t\\nminimize()\\n\\toperation\\t\\nwill\\ntake\\tcare\\tof\\tincrementing\\tit.\\tPlus\\twe\\tcreate\\tthe\\tusual\\t\\ninit\\n\\toperation\\t\\nand\\ta\\t\\nSaver\\n.\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\n1\\n])\\ncost\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\ny\\n\\t\\n-\\n\\t\\nq_value\\n))\\nglobal_step\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0\\n,\\n\\t\\ntrainable\\n=\\nFalse\\n,\\n\\t\\nname\\n=\\n'global_step'\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\ncost\\n,\\n\\t\\nglobal_step\\n=\\nglobal_step\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\nThat’s\\tit\\tfor\\tthe\\tconstruction\\tphase.\\tBefore\\twe\\tlook\\tat\\tthe\\texecution\\tphase,\\twe\\twill\\tneed\\ta\\tcouple\\tof\\ntools.\\tFirst,\\tlet’s\\tstart\\tby\\timplementing\\tthe\\treplay\\tmemory.\\tWe\\twill\\tuse\\ta\\t\\ndeque\\n\\tlist\\tsince\\tit\\tis\\tvery\\nefficient\\tat\\tpushing\\titems\\tto\\tthe\\tqueue\\tand\\tpopping\\tthem\\tout\\tfrom\\tthe\\tend\\tof\\tthe\\tlist\\twhen\\tthe\\tmaximum\\nmemory\\tsize\\tis\\treached.\\tWe\\twill\\talso\\twrite\\ta\\tsmall\\tfunction\\tto\\trandomly\\tsample\\ta\\tbatch\\tof\\texperiences\\nfrom\\tthe\\treplay\\tmemory:\\nfrom\\n\\t\\ncollections\\n\\t\\nimport\\n\\t\\ndeque\\nreplay_memory_size\\n\\t\\n=\\n\\t\\n10000\\nreplay_memory\\n\\t\\n=\\n\\t\\ndeque\\n([],\\n\\t\\nmaxlen\\n=\\nreplay_memory_size\\n)\\ndef\\n\\t\\nsample_memories\\n(\\nbatch_size\\n):\\n\\t\\t\\t\\t\\nindices\\n\\t\\n=\\n\\t\\nrnd\\n.\\npermutation\\n(\\nlen\\n(\\nreplay_memory\\n))[:\\nbatch_size\\n]\\n\\t\\t\\t\\t\\ncols\\n\\t\\n=\\n\\t\\n[[],\\n\\t\\n[],\\n\\t\\n[],\\n\\t\\n[],\\n\\t\\n[]]\\n\\t\\n#\\tstate,\\taction,\\treward,\\tnext_state,\\tcontinue\\n\\t\\t\\t\\t\\nfor\\n\\t\\nidx\\n\\t\\nin\\n\\t\\nindices\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nmemory\\n\\t\\n=\\n\\t\\nreplay_memory\\n[\\nidx\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ncol\\n,\\n\\t\\nvalue\\n\\t\\nin\\n\\t\\nzip\\n(\\ncols\\n,\\n\\t\\nmemory\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncol\\n.\\nappend\\n(\\nvalue\\n)\\n\\t\\t\\t\\t\\ncols\\n\\t\\n=\\n\\t\\n[\\nnp\\n.\\narray\\n(\\ncol\\n)\\n\\t\\nfor\\n\\t\\ncol\\n\\t\\nin\\n\\t\\ncols\\n]\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 562}), Document(page_content='\\t\\t\\t\\t\\nreturn\\n\\t\\n(\\ncols\\n[\\n0\\n],\\n\\t\\ncols\\n[\\n1\\n],\\n\\t\\ncols\\n[\\n2\\n]\\n.\\nreshape\\n(\\n-\\n1\\n,\\n\\t\\n1\\n),\\n\\t\\ncols\\n[\\n3\\n],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncols\\n[\\n4\\n]\\n.\\nreshape\\n(\\n-\\n1\\n,\\n\\t\\n1\\n))\\nNext,\\twe\\twill\\tneed\\tthe\\tactor\\tto\\texplore\\tthe\\tgame.\\tWe\\twill\\tuse\\tthe\\tε-greedy\\tpolicy,\\tand\\tgradually\\tdecrease\\nε\\tfrom\\t1.0\\tto\\t0.05,\\tin\\t50,000\\ttraining\\tsteps:\\neps_min\\n\\t\\n=\\n\\t\\n0.05\\neps_max\\n\\t\\n=\\n\\t\\n1.0\\neps_decay_steps\\n\\t\\n=\\n\\t\\n50000\\ndef\\n\\t\\nepsilon_greedy\\n(\\nq_values\\n,\\n\\t\\nstep\\n):\\n\\t\\t\\t\\t\\nepsilon\\n\\t\\n=\\n\\t\\nmax\\n(\\neps_min\\n,\\n\\t\\neps_max\\n\\t\\n-\\n\\t\\n(\\neps_max\\n-\\neps_min\\n)\\n\\t\\n*\\n\\t\\nstep\\n/\\neps_decay_steps\\n)\\n\\t\\t\\t\\t\\nif\\n\\t\\nrnd\\n.\\nrand\\n()\\n\\t\\n<\\n\\t\\nepsilon\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nrnd\\n.\\nrandint\\n(\\nn_outputs\\n)\\n\\t\\n#\\trandom\\taction\\n\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nnp\\n.\\nargmax\\n(\\nq_values\\n)\\n\\t\\n#\\toptimal\\taction\\nThat’s\\tit!\\tWe\\thave\\tall\\twe\\tneed\\tto\\tstart\\ttraining.\\tThe\\texecution\\tphase\\tdoes\\tnot\\tcontain\\tanything\\ttoo\\ncomplex,\\tbut\\tit\\tis\\ta\\tbit\\tlong,\\tso\\ttake\\ta\\tdeep\\tbreath.\\tReady?\\tLet’s\\tgo!\\tFirst,\\tlet’s\\tinitialize\\ta\\tfew\\tvariables:\\nn_steps\\n\\t\\n=\\n\\t\\n100000\\n\\t\\t\\n#\\ttotal\\tnumber\\tof\\ttraining\\tsteps\\ntraining_start\\n\\t\\n=\\n\\t\\n1000\\n\\t\\t\\n#\\tstart\\ttraining\\tafter\\t1,000\\tgame\\titerations\\ntraining_interval\\n\\t\\n=\\n\\t\\n3\\n\\t\\t\\n#\\trun\\ta\\ttraining\\tstep\\tevery\\t3\\tgame\\titerations\\nsave_steps\\n\\t\\n=\\n\\t\\n50\\n\\t\\t\\n#\\tsave\\tthe\\tmodel\\tevery\\t50\\ttraining\\tsteps\\ncopy_steps\\n\\t\\n=\\n\\t\\n25\\n\\t\\t\\n#\\tcopy\\tthe\\tcritic\\tto\\tthe\\tactor\\tevery\\t25\\ttraining\\tsteps\\ndiscount_rate\\n\\t\\n=\\n\\t\\n0.95\\nskip_start\\n\\t\\n=\\n\\t\\n90\\n\\t\\t\\n#\\tskip\\tthe\\tstart\\tof\\tevery\\tgame\\t(it\\'s\\tjust\\twaiting\\ttime)\\nbatch_size\\n\\t\\n=\\n\\t\\n50\\niteration\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\n#\\tgame\\titerations\\ncheckpoint_path\\n\\t\\n=\\n\\t\\n\"./my_dqn.ckpt\"\\ndone\\n\\t\\n=\\n\\t\\nTrue\\n\\t\\n#\\tenv\\tneeds\\tto\\tbe\\treset\\nNext,\\tlet’s\\topen\\tthe\\tsession\\tand\\trun\\tthe\\tmain\\ttraining\\tloop:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nif\\n\\t\\nos\\n.\\npath\\n.\\nisfile\\n(\\ncheckpoint_path\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\ncheckpoint_path\\n)\\n\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nwhile\\n\\t\\nTrue\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nstep\\n\\t\\n=\\n\\t\\nglobal_step\\n.\\neval\\n()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nstep\\n\\t\\n>=\\n\\t\\nn_steps\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbreak\\n\\t\\t\\t\\t\\t\\t\\t\\t\\niteration\\n\\t\\n+=\\n\\t\\n1\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\ndone\\n:\\n\\t\\n#\\tgame\\tover,\\tstart\\tagain\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n\\t\\n=\\n\\t\\nenv\\n.\\nreset\\n()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nskip\\n\\t\\nin\\n\\t\\nrange\\n(\\nskip_start\\n):\\n\\t\\n#\\tskip\\tthe\\tstart\\tof\\teach\\tgame\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n,\\n\\t\\nreward\\n,\\n\\t\\ndone\\n,\\n\\t\\ninfo\\n\\t\\n=\\n\\t\\nenv\\n.\\nstep\\n(\\n0\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nstate\\n\\t\\n=\\n\\t\\npreprocess_observation\\n(\\nobs\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tActor\\tevaluates\\twhat\\tto\\tdo\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nq_values\\n\\t\\n=\\n\\t\\nactor_q_values\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX_state\\n:\\n\\t\\n[\\nstate\\n]})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\naction\\n\\t\\n=\\n\\t\\nepsilon_greedy\\n(\\nq_values\\n,\\n\\t\\nstep\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tActor\\tplays\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n,\\n\\t\\nreward\\n,\\n\\t\\ndone\\n,\\n\\t\\ninfo\\n\\t\\n=\\n\\t\\nenv\\n.\\nstep\\n(\\naction\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nnext_state\\n\\t\\n=\\n\\t\\npreprocess_observation\\n(\\nobs\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tLet\\'s\\tmemorize\\twhat\\tjust\\thappened\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreplay_memory\\n.\\nappend\\n((\\nstate\\n,\\n\\t\\naction\\n,\\n\\t\\nreward\\n,\\n\\t\\nnext_state\\n,\\n\\t\\n1.0\\n\\t\\n-\\n\\t\\ndone\\n))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nstate\\n\\t\\n=\\n\\t\\nnext_state\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\niteration\\n\\t\\n<\\n\\t\\ntraining_start\\n\\t\\nor\\n\\t\\niteration\\n\\t\\n%\\n\\t\\ntraining_interval\\n\\t\\n!=\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncontinue\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tCritic\\tlearns\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nX_state_val\\n,\\n\\t\\nX_action_val\\n,\\n\\t\\nrewards\\n,\\n\\t\\nX_next_state_val\\n,\\n\\t\\ncontinues\\n\\t\\n=\\n\\t\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsample_memories\\n(\\nbatch_size\\n))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nnext_q_values\\n\\t\\n=\\n\\t\\nactor_q_values\\n.\\neval\\n(', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 563}), Document(page_content='\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n=\\n{\\nX_state\\n:\\n\\t\\nX_next_state_val\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nmax_next_q_values\\n\\t\\n=\\n\\t\\nnp\\n.\\nmax\\n(\\nnext_q_values\\n,\\n\\t\\naxis\\n=\\n1\\n,\\n\\t\\nkeepdims\\n=\\nTrue\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ny_val\\n\\t\\n=\\n\\t\\nrewards\\n\\t\\n+\\n\\t\\ncontinues\\n\\t\\n*\\n\\t\\ndiscount_rate\\n\\t\\n*\\n\\t\\nmax_next_q_values\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntraining_op\\n.\\nrun\\n(\\nfeed_dict\\n=\\n{\\nX_state\\n:\\n\\t\\nX_state_val\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_action\\n:\\n\\t\\nX_action_val\\n,\\n\\t\\ny\\n:\\n\\t\\ny_val\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tRegularly\\tcopy\\tcritic\\tto\\tactor\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nstep\\n\\t\\n%\\n\\t\\ncopy_steps\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncopy_critic_to_actor\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tAnd\\tsave\\tregularly\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nstep\\n\\t\\n%\\n\\t\\nsave_steps\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\ncheckpoint_path\\n)\\nWe\\tstart\\tby\\trestoring\\tthe\\tmodels\\tif\\ta\\tcheckpoint\\tfile\\texists,\\tor\\telse\\twe\\tjust\\tinitialize\\tthe\\tvariables\\nnormally.\\tThen\\tthe\\tmain\\tloop\\tstarts,\\twhere\\t\\niteration\\n\\tcounts\\tthe\\ttotal\\tnumber\\tof\\tgame\\tsteps\\twe\\thave\\ngone\\tthrough\\tsince\\tthe\\tprogram\\tstarted,\\tand\\t\\nstep\\n\\tcounts\\tthe\\ttotal\\tnumber\\tof\\ttraining\\tsteps\\tsince\\ttraining\\nstarted\\t(if\\ta\\tcheckpoint\\tis\\trestored,\\tthe\\tglobal\\tstep\\tis\\trestored\\tas\\twell).\\tThen\\tthe\\tcode\\tresets\\tthe\\tgame\\n(and\\tskips\\tthe\\tfirst\\tboring\\tgame\\tsteps,\\twhere\\tnothing\\thappens).\\tNext,\\tthe\\tactor\\tevaluates\\twhat\\tto\\tdo,\\tand\\nplays\\tthe\\tgame,\\tand\\tits\\texperience\\tis\\tmemorized\\tin\\treplay\\tmemory.\\tThen,\\tat\\tregular\\tintervals\\t(after\\ta\\nwarmup\\tperiod),\\tthe\\tcritic\\tgoes\\tthrough\\ta\\ttraining\\tstep.\\tIt\\tsamples\\ta\\tbatch\\tof\\tmemories\\tand\\tasks\\tthe\\tactor\\nto\\testimate\\tthe\\tQ-Values\\tof\\tall\\tactions\\tfor\\tthe\\tnext\\tstate,\\tand\\tit\\tapplies\\t\\nEquation\\t16-7\\n\\tto\\tcompute\\tthe\\ttarget\\nQ-Value\\t\\ny_val\\n.\\tThe\\tonly\\ttricky\\tpart\\there\\tis\\tthat\\twe\\tmust\\tmultiply\\tthe\\tnext\\tstate’s\\tQ-Values\\tby\\tthe\\ncontinues\\n\\tvector\\tto\\tzero\\tout\\tthe\\tQ-Values\\tcorresponding\\tto\\tmemories\\twhere\\tthe\\tgame\\twas\\tover.\\tNext\\nwe\\trun\\ta\\ttraining\\toperation\\tto\\timprove\\tthe\\tcritic’s\\tability\\tto\\tpredict\\tQ-Values.\\tFinally,\\tat\\tregular\\tintervals\\nwe\\tcopy\\tthe\\tcritic\\tto\\tthe\\tactor,\\tand\\twe\\tsave\\tthe\\tmodel.\\nTIP\\nUnfortunately,\\ttraining\\tis\\tvery\\tslow:\\tif\\tyou\\tuse\\tyour\\tlaptop\\tfor\\ttraining,\\tit\\twill\\ttake\\tdays\\tbefore\\tMs.\\tPac-Man\\tgets\\tany\\tgood,\\tand\\tif\\nyou\\tlook\\tat\\tthe\\tlearning\\tcurve,\\tmeasuring\\tthe\\t\\naverage\\trewards\\tper\\tepisode,\\tyou\\twill\\tnotice\\tthat\\tit\\tis\\textremely\\tnoisy.\\tAt\\tsome\\npoints\\tthere\\tmay\\tbe\\tno\\tapparent\\tprogress\\tfor\\ta\\tvery\\tlong\\ttime\\tuntil\\tsuddenly\\tthe\\tagent\\tlearns\\tto\\tsurvive\\ta\\treasonable\\tamount\\tof\\ntime.\\tAs\\tmentioned\\tearlier,\\tone\\tsolution\\tis\\tto\\tinject\\tas\\tmuch\\tprior\\tknowledge\\tas\\tpossible\\tinto\\tthe\\tmodel\\t(e.g.,\\tthrough\\npreprocessing,\\trewards,\\tand\\tso\\ton),\\tand\\tyou\\tcan\\talso\\ttry\\tto\\tbootstrap\\t\\nthe\\tmodel\\tby\\tfirst\\ttraining\\tit\\tto\\timitate\\ta\\tbasic\\tstrategy.\\tIn\\nany\\tcase,\\tRL\\tstill\\trequires\\tquite\\ta\\tlot\\tof\\tpatience\\tand\\ttweaking,\\tbut\\tthe\\tend\\tresult\\tis\\tvery\\t\\nexciting.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 564}), Document(page_content='Exercises\\n1\\n.\\t\\nHow\\twould\\tyou\\tdefine\\tReinforcement\\tLearning?\\tHow\\tis\\tit\\tdifferent\\tfrom\\tregular\\tsupervised\\tor\\nunsupervised\\tlearning?\\n2\\n.\\t\\nCan\\tyou\\tthink\\tof\\tthree\\tpossible\\tapplications\\tof\\tRL\\tthat\\twere\\tnot\\tmentioned\\tin\\tthis\\tchapter?\\tFor\\teach\\nof\\tthem,\\twhat\\tis\\tthe\\tenvironment?\\tWhat\\tis\\tthe\\tagent?\\tWhat\\tare\\tpossible\\tactions?\\tWhat\\tare\\tthe\\nrewards?\\n3\\n.\\t\\nWhat\\tis\\tthe\\tdiscount\\trate?\\tCan\\tthe\\toptimal\\tpolicy\\tchange\\tif\\tyou\\tmodify\\tthe\\tdiscount\\trate?\\n4\\n.\\t\\nHow\\tdo\\tyou\\tmeasure\\tthe\\tperformance\\tof\\ta\\tReinforcement\\tLearning\\tagent?\\n5\\n.\\t\\nWhat\\tis\\tthe\\tcredit\\tassignment\\tproblem?\\tWhen\\tdoes\\tit\\toccur?\\tHow\\tcan\\tyou\\talleviate\\tit?\\n6\\n.\\t\\nWhat\\tis\\tthe\\tpoint\\tof\\tusing\\ta\\treplay\\tmemory?\\n7\\n.\\t\\nWhat\\tis\\tan\\toff-policy\\tRL\\talgorithm?\\n8\\n.\\t\\nUse\\tDeep\\tQ-Learning\\tto\\ttackle\\tOpenAI\\tgym’s\\t“BypedalWalker-v2.”\\tThe\\tQ-networks\\tdo\\tnot\\tneed\\tto\\nbe\\tvery\\tdeep\\tfor\\tthis\\ttask.\\n9\\n.\\t\\nUse\\tpolicy\\tgradients\\tto\\ttrain\\tan\\tagent\\tto\\tplay\\t\\nPong\\n,\\tthe\\tfamous\\tAtari\\tgame\\t(\\nPong-v0\\n\\tin\\tthe\\tOpenAI\\ngym).\\tBeware:\\tan\\tindividual\\tobservation\\tis\\tinsufficient\\tto\\ttell\\tthe\\tdirection\\tand\\tspeed\\tof\\tthe\\tball.\\nOne\\tsolution\\tis\\tto\\tpass\\ttwo\\tobservations\\tat\\ta\\ttime\\tto\\tthe\\tneural\\tnetwork\\tpolicy.\\tTo\\treduce\\ndimensionality\\tand\\tspeed\\tup\\ttraining,\\tyou\\tshould\\tdefinitely\\tpreprocess\\tthese\\timages\\t(crop,\\tresize,\\nand\\tconvert\\tthem\\tto\\tblack\\tand\\twhite),\\tand\\tpossibly\\tmerge\\tthem\\tinto\\ta\\tsingle\\timage\\t(e.g.,\\tby\\noverlaying\\tthem).\\n10\\n.\\t\\nIf\\tyou\\thave\\tabout\\t$100\\tto\\tspare,\\tyou\\tcan\\tpurchase\\ta\\tRaspberry\\tPi\\t3\\tplus\\tsome\\tcheap\\trobotics\\ncomponents,\\tinstall\\tTensorFlow\\ton\\tthe\\tPi,\\tand\\tgo\\twild!\\tFor\\tan\\texample,\\tcheck\\tout\\tthis\\t\\nfun\\tpost\\n\\tby\\nLukas\\tBiewald,\\tor\\ttake\\ta\\tlook\\tat\\tGoPiGo\\tor\\tBrickPi.\\tWhy\\tnot\\ttry\\tto\\tbuild\\ta\\treal-life\\tcartpole\\tby\\ntraining\\tthe\\trobot\\tusing\\tpolicy\\tgradients?\\tOr\\tbuild\\ta\\trobotic\\tspider\\tthat\\tlearns\\tto\\twalk;\\tgive\\tit\\nrewards\\tany\\ttime\\tit\\tgets\\tcloser\\tto\\tsome\\tobjective\\t(you\\twill\\tneed\\tsensors\\tto\\tmeasure\\tthe\\tdistance\\tto\\nthe\\tobjective).\\tThe\\tonly\\tlimit\\tis\\tyour\\timagination.\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 565}), Document(page_content='Thank\\tYou!\\nBefore\\twe\\tclose\\tthe\\tlast\\tchapter\\tof\\tthis\\tbook,\\tI\\twould\\tlike\\tto\\tthank\\tyou\\tfor\\treading\\tit\\tup\\tto\\tthe\\tlast\\nparagraph.\\tI\\ttruly\\thope\\tthat\\tyou\\thad\\tas\\tmuch\\tpleasure\\treading\\tthis\\tbook\\tas\\tI\\thad\\twriting\\tit,\\tand\\tthat\\tit\\twill\\nbe\\tuseful\\tfor\\tyour\\tprojects,\\tbig\\tor\\tsmall.\\nIf\\tyou\\tfind\\terrors,\\tplease\\tsend\\tfeedback.\\tMore\\tgenerally,\\tI\\twould\\tlove\\tto\\tknow\\twhat\\tyou\\tthink,\\tso\\tplease\\ndon’t\\thesitate\\tto\\tcontact\\tme\\tvia\\tO’Reilly,\\tor\\tthrough\\tthe\\t\\nageron/handson-ml\\n\\tGitHub\\tproject.\\nGoing\\tforward,\\tmy\\tbest\\tadvice\\tto\\tyou\\tis\\tto\\tpractice\\tand\\tpractice:\\ttry\\tgoing\\tthrough\\tall\\tthe\\texercises\\tif\\tyou\\nhave\\tnot\\tdone\\tso\\talready,\\tplay\\twith\\tthe\\tJupyter\\tnotebooks,\\tjoin\\tKaggle.com\\tor\\tsome\\tother\\tML\\tcommunity,\\nwatch\\tML\\tcourses,\\tread\\tpapers,\\tattend\\tconferences,\\tmeet\\texperts.\\tYou\\tmay\\talso\\twant\\tto\\tstudy\\tsome\\ttopics\\nthat\\twe\\tdid\\tnot\\tcover\\tin\\tthis\\tbook,\\tincluding\\trecommender\\tsystems,\\tclustering\\talgorithms,\\tanomaly\\ndetection\\talgorithms,\\tand\\tgenetic\\talgorithms.\\nMy\\tgreatest\\thope\\tis\\tthat\\tthis\\tbook\\twill\\tinspire\\tyou\\tto\\tbuild\\ta\\twonderful\\tML\\tapplication\\tthat\\twill\\tbenefit\\nall\\tof\\tus!\\t\\nWhat\\twill\\tit\\tbe?\\nAurélien\\tGéron,\\tNovember\\t26th,\\t2016\\nFor\\tmore\\tdetails,\\tbe\\tsure\\tto\\tcheck\\tout\\tRichard\\tSutton\\tand\\tAndrew\\tBarto’s\\t\\nbook\\ton\\tRL\\n,\\t\\nReinforcement\\tLearning:\\tAn\\tIntroduction\\n\\t(MIT\\nPress),\\tor\\tDavid\\tSilver’s\\tfree\\t\\nonline\\tRL\\tcourse\\n\\tat\\tUniversity\\tCollege\\tLondon.\\n“Playing\\tAtari\\twith\\tDeep\\tReinforcement\\tLearning,”\\tV.\\tMnih\\tet\\tal.\\t(2013).\\n“Human-level\\tcontrol\\tthrough\\tdeep\\treinforcement\\tlearning,”\\tV.\\tMnih\\tet\\tal.\\t(2015).\\nCheck\\tout\\tthe\\tvideos\\tof\\tDeepMind’s\\tsystem\\tlearning\\tto\\tplay\\t\\nSpace\\tInvaders\\n,\\t\\nBreakout\\n,\\tand\\tmore\\tat\\t\\nhttps://goo.gl/yTsH6X\\n.\\nImages\\t(a),\\t(c),\\tand\\t(d)\\tare\\treproduced\\tfrom\\tWikipedia.\\t(a)\\tand\\t(d)\\tare\\tin\\tthe\\tpublic\\tdomain.\\t(c)\\twas\\tcreated\\tby\\tuser\\tStevertigo\\tand\\nreleased\\tunder\\t\\nCreative\\tCommons\\tBY-SA\\t2.0\\n.\\t(b)\\tis\\ta\\tscreenshot\\tfrom\\tthe\\tMs.\\tPac-Man\\tgame,\\tcopyright\\tAtari\\t(the\\tauthor\\tbelieves\\tit\\tto\\nbe\\tfair\\tuse\\tin\\tthis\\tchapter).\\t(e)\\twas\\treproduced\\tfrom\\tPixabay,\\treleased\\tunder\\t\\nCreative\\tCommons\\tCC0\\n.\\nIt\\tis\\toften\\tbetter\\tto\\tgive\\tthe\\tpoor\\tperformers\\ta\\tslight\\tchance\\tof\\tsurvival,\\tto\\tpreserve\\tsome\\tdiversity\\tin\\tthe\\t“gene\\tpool.”\\nIf\\tthere\\tis\\ta\\tsingle\\tparent,\\tthis\\tis\\tcalled\\t\\nasexual\\treproduction\\n.\\tWith\\ttwo\\t(or\\tmore)\\tparents,\\tit\\tis\\tcalled\\t\\nsexual\\treproduction\\n.\\tAn\\toffspring’s\\ngenome\\t(in\\tthis\\tcase\\ta\\tset\\tof\\tpolicy\\tparameters)\\tis\\trandomly\\tcomposed\\tof\\tparts\\tof\\tits\\tparents’\\tgenomes.\\nOpenAI\\tis\\ta\\tnonprofit\\tartificial\\tintelligence\\tresearch\\tcompany,\\tfunded\\tin\\tpart\\tby\\tElon\\tMusk.\\tIts\\tstated\\tgoal\\tis\\tto\\tpromote\\tand\\tdevelop\\nfriendly\\tAIs\\tthat\\twill\\tbenefit\\thumanity\\t(rather\\tthan\\texterminate\\tit).\\n“Simple\\tStatistical\\tGradient-Following\\tAlgorithms\\tfor\\tConnectionist\\tReinforcement\\tLearning,”\\tR.\\tWilliams\\t(1992).\\nWe\\talready\\tdid\\tsomething\\tsimilar\\tin\\t\\nChapter\\t11\\n\\twhen\\twe\\tdiscussed\\tGradient\\tClipping:\\twe\\tfirst\\tcomputed\\tthe\\tgradients,\\tthen\\twe\\tclipped\\nthem,\\tand\\tfinally\\twe\\tapplied\\tthe\\tclipped\\tgradients.\\n“A\\tMarkovian\\tDecision\\tProcess,”\\tR.\\tBellman\\t(1957).\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 566}), Document(page_content='Appendix\\tA.\\t\\nExercise\\tSolutions\\nNOTE\\nSolutions\\tto\\tthe\\tcoding\\texercises\\tare\\tavailable\\tin\\tthe\\tonline\\tJupyter\\tnotebooks\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 567}), Document(page_content='Chapter\\t1\\n:\\tThe\\tMachine\\tLearning\\tLandscape\\n1\\n.\\t\\nMachine\\tLearning\\tis\\tabout\\tbuilding\\tsystems\\tthat\\tcan\\tlearn\\tfrom\\tdata.\\tLearning\\tmeans\\tgetting\\tbetter\\tat\\nsome\\ttask,\\tgiven\\tsome\\tperformance\\tmeasure.\\n2\\n.\\t\\nMachine\\tLearning\\tis\\tgreat\\tfor\\tcomplex\\tproblems\\tfor\\twhich\\twe\\thave\\tno\\talgorithmic\\tsolution,\\tto\\nreplace\\tlong\\tlists\\tof\\thand-tuned\\trules,\\tto\\tbuild\\tsystems\\tthat\\tadapt\\tto\\tfluctuating\\tenvironments,\\tand\\nfinally\\tto\\thelp\\thumans\\tlearn\\t(e.g.,\\tdata\\tmining).\\n3\\n.\\t\\nA\\tlabeled\\ttraining\\tset\\tis\\ta\\ttraining\\tset\\tthat\\tcontains\\tthe\\tdesired\\tsolution\\t(a.k.a.\\ta\\tlabel)\\tfor\\teach\\ninstance.\\n4\\n.\\t\\nThe\\ttwo\\tmost\\tcommon\\tsupervised\\ttasks\\tare\\tregression\\tand\\tclassification.\\n5\\n.\\t\\nCommon\\tunsupervised\\ttasks\\tinclude\\tclustering,\\tvisualization,\\tdimensionality\\treduction,\\tand\\nassociation\\trule\\tlearning.\\n6\\n.\\t\\nReinforcement\\tLearning\\tis\\tlikely\\tto\\tperform\\tbest\\tif\\twe\\twant\\ta\\trobot\\tto\\tlearn\\tto\\twalk\\tin\\tvarious\\nunknown\\tterrains\\tsince\\tthis\\tis\\ttypically\\tthe\\ttype\\tof\\tproblem\\tthat\\tReinforcement\\tLearning\\ttackles.\\tIt\\nmight\\tbe\\tpossible\\tto\\texpress\\tthe\\tproblem\\tas\\ta\\tsupervised\\tor\\tsemisupervised\\tlearning\\tproblem,\\tbut\\tit\\nwould\\tbe\\tless\\tnatural.\\n7\\n.\\t\\nIf\\tyou\\tdon’t\\tknow\\thow\\tto\\tdefine\\tthe\\tgroups,\\tthen\\tyou\\tcan\\tuse\\ta\\tclustering\\talgorithm\\t(unsupervised\\nlearning)\\tto\\tsegment\\tyour\\tcustomers\\tinto\\tclusters\\tof\\tsimilar\\tcustomers.\\tHowever,\\tif\\tyou\\tknow\\twhat\\ngroups\\tyou\\twould\\tlike\\tto\\thave,\\tthen\\tyou\\tcan\\tfeed\\tmany\\texamples\\tof\\teach\\tgroup\\tto\\ta\\tclassification\\nalgorithm\\t(supervised\\tlearning),\\tand\\tit\\twill\\tclassify\\tall\\tyour\\tcustomers\\tinto\\tthese\\tgroups.\\n8\\n.\\t\\nSpam\\tdetection\\tis\\ta\\ttypical\\tsupervised\\tlearning\\tproblem:\\tthe\\talgorithm\\tis\\tfed\\tmany\\temails\\talong\\nwith\\ttheir\\tlabel\\t(spam\\tor\\tnot\\tspam).\\n9\\n.\\t\\nAn\\tonline\\tlearning\\tsystem\\tcan\\tlearn\\tincrementally,\\tas\\topposed\\tto\\ta\\tbatch\\tlearning\\tsystem.\\tThis\\nmakes\\tit\\tcapable\\tof\\tadapting\\trapidly\\tto\\tboth\\tchanging\\tdata\\tand\\tautonomous\\tsystems,\\tand\\tof\\ttraining\\non\\tvery\\tlarge\\tquantities\\tof\\tdata.\\n10\\n.\\t\\nOut-of-core\\talgorithms\\tcan\\thandle\\tvast\\tquantities\\tof\\tdata\\tthat\\tcannot\\tfit\\tin\\ta\\tcomputer’s\\tmain\\nmemory.\\tAn\\tout-of-core\\tlearning\\talgorithm\\tchops\\tthe\\tdata\\tinto\\tmini-batches\\tand\\tuses\\tonline\\tlearning\\ntechniques\\tto\\tlearn\\tfrom\\tthese\\tmini-batches.\\n11\\n.\\t\\nAn\\tinstance-based\\tlearning\\tsystem\\tlearns\\tthe\\ttraining\\tdata\\tby\\theart;\\tthen,\\twhen\\tgiven\\ta\\tnew\\tinstance,\\nit\\tuses\\ta\\tsimilarity\\tmeasure\\tto\\tfind\\tthe\\tmost\\tsimilar\\tlearned\\tinstances\\tand\\tuses\\tthem\\tto\\tmake\\npredictions.\\n12\\n.\\t\\nA\\tmodel\\thas\\tone\\tor\\tmore\\tmodel\\tparameters\\tthat\\tdetermine\\twhat\\tit\\twill\\tpredict\\tgiven\\ta\\tnew\\tinstance\\n(e.g.,\\tthe\\tslope\\tof\\ta\\tlinear\\tmodel).\\tA\\tlearning\\talgorithm\\ttries\\tto\\tfind\\toptimal\\tvalues\\tfor\\tthese\\nparameters\\tsuch\\tthat\\tthe\\tmodel\\tgeneralizes\\twell\\tto\\tnew\\tinstances.\\tA\\thyperparameter\\tis\\ta\\tparameter\\nof\\tthe\\tlearning\\talgorithm\\titself,\\tnot\\tof\\tthe\\tmodel\\t(e.g.,\\tthe\\tamount\\tof\\tregularization\\tto\\tapply).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 568}), Document(page_content='13\\n.\\t\\nModel-based\\tlearning\\talgorithms\\tsearch\\tfor\\tan\\toptimal\\tvalue\\tfor\\tthe\\tmodel\\tparameters\\tsuch\\tthat\\tthe\\nmodel\\twill\\tgeneralize\\twell\\tto\\tnew\\tinstances.\\tWe\\tusually\\ttrain\\tsuch\\tsystems\\tby\\tminimizing\\ta\\tcost\\nfunction\\tthat\\tmeasures\\thow\\tbad\\tthe\\tsystem\\tis\\tat\\tmaking\\tpredictions\\ton\\tthe\\ttraining\\tdata,\\tplus\\ta\\npenalty\\tfor\\tmodel\\tcomplexity\\tif\\tthe\\tmodel\\tis\\tregularized.\\tTo\\tmake\\tpredictions,\\twe\\tfeed\\tthe\\tnew\\ninstance’s\\tfeatures\\tinto\\tthe\\tmodel’s\\tprediction\\tfunction,\\tusing\\tthe\\tparameter\\tvalues\\tfound\\tby\\tthe\\nlearning\\talgorithm.\\n14\\n.\\t\\nSome\\tof\\tthe\\tmain\\tchallenges\\tin\\tMachine\\tLearning\\tare\\tthe\\tlack\\tof\\tdata,\\tpoor\\tdata\\tquality,\\nnonrepresentative\\tdata,\\tuninformative\\tfeatures,\\texcessively\\tsimple\\tmodels\\tthat\\tunderfit\\tthe\\ttraining\\ndata,\\tand\\texcessively\\tcomplex\\tmodels\\tthat\\toverfit\\tthe\\tdata.\\n15\\n.\\t\\nIf\\ta\\tmodel\\tperforms\\tgreat\\ton\\tthe\\ttraining\\tdata\\tbut\\tgeneralizes\\tpoorly\\tto\\tnew\\tinstances,\\tthe\\tmodel\\tis\\nlikely\\toverfitting\\tthe\\ttraining\\tdata\\t(or\\twe\\tgot\\textremely\\tlucky\\ton\\tthe\\ttraining\\tdata).\\tPossible\\tsolutions\\nto\\toverfitting\\tare\\tgetting\\tmore\\tdata,\\tsimplifying\\tthe\\tmodel\\t(selecting\\ta\\tsimpler\\talgorithm,\\treducing\\nthe\\tnumber\\tof\\tparameters\\tor\\tfeatures\\tused,\\tor\\tregularizing\\tthe\\tmodel),\\tor\\treducing\\tthe\\tnoise\\tin\\tthe\\ntraining\\tdata.\\n16\\n.\\t\\nA\\ttest\\tset\\tis\\tused\\tto\\testimate\\tthe\\tgeneralization\\terror\\tthat\\ta\\tmodel\\twill\\tmake\\ton\\tnew\\tinstances,\\tbefore\\nthe\\tmodel\\tis\\tlaunched\\tin\\tproduction.\\n17\\n.\\t\\nA\\tvalidation\\tset\\tis\\tused\\tto\\tcompare\\tmodels.\\tIt\\tmakes\\tit\\tpossible\\tto\\tselect\\tthe\\tbest\\tmodel\\tand\\ttune\\tthe\\nhyperparameters.\\n18\\n.\\t\\nIf\\tyou\\ttune\\thyperparameters\\tusing\\tthe\\ttest\\tset,\\tyou\\trisk\\toverfitting\\tthe\\ttest\\tset,\\tand\\tthe\\tgeneralization\\nerror\\tyou\\tmeasure\\twill\\tbe\\toptimistic\\t(you\\tmay\\tlaunch\\ta\\tmodel\\tthat\\tperforms\\tworse\\tthan\\tyou\\texpect).\\n19\\n.\\t\\nCross-validation\\tis\\ta\\ttechnique\\tthat\\tmakes\\tit\\tpossible\\tto\\tcompare\\tmodels\\t(for\\tmodel\\tselection\\tand\\nhyperparameter\\ttuning)\\twithout\\tthe\\tneed\\tfor\\ta\\tseparate\\tvalidation\\tset.\\tThis\\tsaves\\tprecious\\ttraining\\ndata.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 569}), Document(page_content='Chapter\\t2\\n:\\tEnd-to-End\\tMachine\\tLearning\\tProject\\nSee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 570}), Document(page_content='Chapter\\t3\\n:\\tClassification\\nSee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 571}), Document(page_content='Chapter\\t4\\n:\\tTraining\\tModels\\n1\\n.\\t\\nIf\\tyou\\thave\\ta\\ttraining\\tset\\twith\\tmillions\\tof\\tfeatures\\tyou\\tcan\\tuse\\tStochastic\\tGradient\\tDescent\\tor\\tMini-\\nbatch\\tGradient\\tDescent,\\tand\\tperhaps\\tBatch\\tGradient\\tDescent\\tif\\tthe\\ttraining\\tset\\tfits\\tin\\tmemory.\\tBut\\nyou\\tcannot\\tuse\\tthe\\tNormal\\tEquation\\tbecause\\tthe\\tcomputational\\tcomplexity\\tgrows\\tquickly\\t(more\\tthan\\nquadratically)\\twith\\tthe\\tnumber\\tof\\tfeatures.\\n2\\n.\\t\\nIf\\tthe\\tfeatures\\tin\\tyour\\ttraining\\tset\\thave\\tvery\\tdifferent\\tscales,\\tthe\\tcost\\tfunction\\twill\\thave\\tthe\\tshape\\tof\\nan\\telongated\\tbowl,\\tso\\tthe\\tGradient\\tDescent\\talgorithms\\twill\\ttake\\ta\\tlong\\ttime\\tto\\tconverge.\\tTo\\tsolve\\nthis\\tyou\\tshould\\tscale\\tthe\\tdata\\tbefore\\ttraining\\tthe\\tmodel.\\tNote\\tthat\\tthe\\tNormal\\tEquation\\twill\\twork\\njust\\tfine\\twithout\\tscaling.\\tMoreover,\\tregularized\\tmodels\\tmay\\tconverge\\tto\\ta\\tsuboptimal\\tsolution\\tif\\tthe\\nfeatures\\tare\\tnot\\tscaled:\\tindeed,\\tsince\\tregularization\\tpenalizes\\tlarge\\tweights,\\tfeatures\\twith\\tsmaller\\nvalues\\twill\\ttend\\tto\\tbe\\tignored\\tcompared\\tto\\tfeatures\\twith\\tlarger\\tvalues.\\n3\\n.\\t\\nGradient\\tDescent\\tcannot\\tget\\tstuck\\tin\\ta\\tlocal\\tminimum\\twhen\\ttraining\\ta\\tLogistic\\tRegression\\tmodel\\nbecause\\tthe\\tcost\\tfunction\\tis\\tconvex.\\n1\\n4\\n.\\t\\nIf\\tthe\\toptimization\\tproblem\\tis\\tconvex\\t(such\\tas\\tLinear\\tRegression\\tor\\tLogistic\\tRegression),\\tand\\nassuming\\tthe\\tlearning\\trate\\tis\\tnot\\ttoo\\thigh,\\tthen\\tall\\tGradient\\tDescent\\talgorithms\\twill\\tapproach\\tthe\\nglobal\\toptimum\\tand\\tend\\tup\\tproducing\\tfairly\\tsimilar\\tmodels.\\tHowever,\\tunless\\tyou\\tgradually\\treduce\\nthe\\tlearning\\trate,\\tStochastic\\tGD\\tand\\tMini-batch\\tGD\\twill\\tnever\\ttruly\\tconverge;\\tinstead,\\tthey\\twill\\nkeep\\tjumping\\tback\\tand\\tforth\\taround\\tthe\\tglobal\\toptimum.\\tThis\\tmeans\\tthat\\teven\\tif\\tyou\\tlet\\tthem\\trun\\tfor\\na\\tvery\\tlong\\ttime,\\tthese\\tGradient\\tDescent\\talgorithms\\twill\\tproduce\\tslightly\\tdifferent\\tmodels.\\n5\\n.\\t\\nIf\\tthe\\tvalidation\\terror\\tconsistently\\tgoes\\tup\\tafter\\tevery\\tepoch,\\tthen\\tone\\tpossibility\\tis\\tthat\\tthe\\tlearning\\nrate\\tis\\ttoo\\thigh\\tand\\tthe\\talgorithm\\tis\\tdiverging.\\tIf\\tthe\\ttraining\\terror\\talso\\tgoes\\tup,\\tthen\\tthis\\tis\\tclearly\\nthe\\tproblem\\tand\\tyou\\tshould\\treduce\\tthe\\tlearning\\trate.\\tHowever,\\tif\\tthe\\ttraining\\terror\\tis\\tnot\\tgoing\\tup,\\nthen\\tyour\\tmodel\\tis\\toverfitting\\tthe\\ttraining\\tset\\tand\\tyou\\tshould\\tstop\\ttraining.\\n6\\n.\\t\\nDue\\tto\\ttheir\\trandom\\tnature,\\tneither\\tStochastic\\tGradient\\tDescent\\tnor\\tMini-batch\\tGradient\\tDescent\\tis\\nguaranteed\\tto\\tmake\\tprogress\\tat\\tevery\\tsingle\\ttraining\\titeration.\\tSo\\tif\\tyou\\timmediately\\tstop\\ttraining\\nwhen\\tthe\\tvalidation\\terror\\tgoes\\tup,\\tyou\\tmay\\tstop\\tmuch\\ttoo\\tearly,\\tbefore\\tthe\\toptimum\\tis\\treached.\\tA\\nbetter\\toption\\tis\\tto\\tsave\\tthe\\tmodel\\tat\\tregular\\tintervals,\\tand\\twhen\\tit\\thas\\tnot\\timproved\\tfor\\ta\\tlong\\ttime\\n(meaning\\tit\\twill\\tprobably\\tnever\\tbeat\\tthe\\trecord),\\tyou\\tcan\\trevert\\tto\\tthe\\tbest\\tsaved\\tmodel.\\n7\\n.\\t\\nStochastic\\tGradient\\tDescent\\thas\\tthe\\tfastest\\ttraining\\titeration\\tsince\\tit\\tconsiders\\tonly\\tone\\ttraining\\ninstance\\tat\\ta\\ttime,\\tso\\tit\\tis\\tgenerally\\tthe\\tfirst\\tto\\treach\\tthe\\tvicinity\\tof\\tthe\\tglobal\\toptimum\\t(or\\tMini-\\nbatch\\tGD\\twith\\ta\\tvery\\tsmall\\tmini-batch\\tsize).\\tHowever,\\tonly\\tBatch\\tGradient\\tDescent\\twill\\tactually\\nconverge,\\tgiven\\tenough\\ttraining\\ttime.\\tAs\\tmentioned,\\tStochastic\\tGD\\tand\\tMini-batch\\tGD\\twill\\tbounce\\naround\\tthe\\toptimum,\\tunless\\tyou\\tgradually\\treduce\\tthe\\tlearning\\trate.\\n8\\n.\\t\\nIf\\tthe\\tvalidation\\terror\\tis\\tmuch\\thigher\\tthan\\tthe\\ttraining\\terror,\\tthis\\tis\\tlikely\\tbecause\\tyour\\tmodel\\tis\\noverfitting\\tthe\\ttraining\\tset.\\tOne\\tway\\tto\\ttry\\tto\\tfix\\tthis\\tis\\tto\\treduce\\tthe\\tpolynomial\\tdegree:\\ta\\tmodel\\nwith\\tfewer\\tdegrees\\tof\\tfreedom\\tis\\tless\\tlikely\\tto\\toverfit.\\tAnother\\tthing\\tyou\\tcan\\ttry\\tis\\tto\\tregularize\\tthe\\nmodel\\t—\\tfor\\texample,\\tby\\tadding\\tan\\tℓ\\n2\\n\\tpenalty\\t(Ridge)\\tor\\tan\\tℓ\\n1\\n\\tpenalty\\t(Lasso)\\tto\\tthe\\tcost\\tfunction.\\nThis\\twill\\talso\\treduce\\tthe\\tdegrees\\tof\\tfreedom\\tof\\tthe\\tmodel.\\tLastly,\\tyou\\tcan\\ttry\\tto\\tincrease\\tthe\\tsize\\tof', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 572}), Document(page_content='the\\ttraining\\tset.\\n9\\n.\\t\\nIf\\tboth\\tthe\\ttraining\\terror\\tand\\tthe\\tvalidation\\terror\\tare\\talmost\\tequal\\tand\\tfairly\\thigh,\\tthe\\tmodel\\tis\\tlikely\\nunderfitting\\tthe\\ttraining\\tset,\\twhich\\tmeans\\tit\\thas\\ta\\thigh\\tbias.\\tYou\\tshould\\ttry\\treducing\\tthe\\nregularization\\thyperparameter\\t\\nα\\n.\\n10\\n.\\t\\nLet’s\\tsee:\\nA\\tmodel\\twith\\tsome\\tregularization\\ttypically\\tperforms\\tbetter\\tthan\\ta\\tmodel\\twithout\\tany\\nregularization,\\tso\\tyou\\tshould\\tgenerally\\tprefer\\tRidge\\tRegression\\tover\\tplain\\tLinear\\tRegression.\\n2\\nLasso\\tRegression\\tuses\\tan\\tℓ\\n1\\n\\tpenalty,\\twhich\\ttends\\tto\\tpush\\tthe\\tweights\\tdown\\tto\\texactly\\tzero.\\nThis\\tleads\\tto\\tsparse\\tmodels,\\twhere\\tall\\tweights\\tare\\tzero\\texcept\\tfor\\tthe\\tmost\\timportant\\tweights.\\nThis\\tis\\ta\\tway\\tto\\tperform\\tfeature\\tselection\\tautomatically,\\twhich\\tis\\tgood\\tif\\tyou\\tsuspect\\tthat\\tonly\\na\\tfew\\tfeatures\\tactually\\tmatter.\\tWhen\\tyou\\tare\\tnot\\tsure,\\tyou\\tshould\\tprefer\\tRidge\\tRegression.\\nElastic\\tNet\\tis\\tgenerally\\tpreferred\\tover\\tLasso\\tsince\\tLasso\\tmay\\tbehave\\terratically\\tin\\tsome\\tcases\\n(when\\tseveral\\tfeatures\\tare\\tstrongly\\tcorrelated\\tor\\twhen\\tthere\\tare\\tmore\\tfeatures\\tthan\\ttraining\\ninstances).\\tHowever,\\tit\\tdoes\\tadd\\tan\\textra\\thyperparameter\\tto\\ttune.\\tIf\\tyou\\tjust\\twant\\tLasso\\nwithout\\tthe\\terratic\\tbehavior,\\tyou\\tcan\\tjust\\tuse\\tElastic\\tNet\\twith\\tan\\t\\nl1_ratio\\n\\tclose\\tto\\t1.\\n11\\n.\\t\\nIf\\tyou\\twant\\tto\\tclassify\\tpictures\\tas\\toutdoor/indoor\\tand\\tdaytime/nighttime,\\tsince\\tthese\\tare\\tnot\\nexclusive\\tclasses\\t(i.e.,\\tall\\tfour\\tcombinations\\tare\\tpossible)\\tyou\\tshould\\ttrain\\ttwo\\tLogistic\\tRegression\\nclassifiers.\\n12\\n.\\t\\nSee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 573}), Document(page_content='Chapter\\t5\\n:\\tSupport\\tVector\\tMachines\\n1\\n.\\t\\nThe\\tfundamental\\tidea\\tbehind\\tSupport\\tVector\\tMachines\\tis\\tto\\tfit\\tthe\\twidest\\tpossible\\t“street”\\tbetween\\nthe\\tclasses.\\tIn\\tother\\twords,\\tthe\\tgoal\\tis\\tto\\thave\\tthe\\tlargest\\tpossible\\tmargin\\tbetween\\tthe\\tdecision\\nboundary\\tthat\\tseparates\\tthe\\ttwo\\tclasses\\tand\\tthe\\ttraining\\tinstances.\\tWhen\\tperforming\\tsoft\\tmargin\\nclassification,\\tthe\\tSVM\\tsearches\\tfor\\ta\\tcompromise\\tbetween\\tperfectly\\tseparating\\tthe\\ttwo\\tclasses\\tand\\nhaving\\tthe\\twidest\\tpossible\\tstreet\\t(i.e.,\\ta\\tfew\\tinstances\\tmay\\tend\\tup\\ton\\tthe\\tstreet).\\tAnother\\tkey\\tidea\\tis\\nto\\tuse\\tkernels\\twhen\\ttraining\\ton\\tnonlinear\\tdatasets.\\n2\\n.\\t\\nAfter\\ttraining\\tan\\tSVM,\\ta\\t\\nsupport\\tvector\\n\\tis\\tany\\tinstance\\tlocated\\ton\\tthe\\t“street”\\t(see\\tthe\\tprevious\\nanswer),\\tincluding\\tits\\tborder.\\tThe\\tdecision\\tboundary\\tis\\tentirely\\tdetermined\\tby\\tthe\\tsupport\\tvectors.\\nAny\\tinstance\\tthat\\tis\\t\\nnot\\n\\ta\\tsupport\\tvector\\t(i.e.,\\toff\\tthe\\tstreet)\\thas\\tno\\tinfluence\\twhatsoever;\\tyou\\tcould\\nremove\\tthem,\\tadd\\tmore\\tinstances,\\tor\\tmove\\tthem\\taround,\\tand\\tas\\tlong\\tas\\tthey\\tstay\\toff\\tthe\\tstreet\\tthey\\nwon’t\\taffect\\tthe\\tdecision\\tboundary.\\tComputing\\tthe\\tpredictions\\tonly\\tinvolves\\tthe\\tsupport\\tvectors,\\tnot\\nthe\\twhole\\ttraining\\tset.\\n3\\n.\\t\\nSVMs\\ttry\\tto\\tfit\\tthe\\tlargest\\tpossible\\t“street”\\tbetween\\tthe\\tclasses\\t(see\\tthe\\tfirst\\tanswer),\\tso\\tif\\tthe\\ntraining\\tset\\tis\\tnot\\tscaled,\\tthe\\tSVM\\twill\\ttend\\tto\\tneglect\\tsmall\\tfeatures\\t(see\\t\\nFigure\\t5-2\\n).\\n4\\n.\\t\\nAn\\tSVM\\tclassifier\\tcan\\toutput\\tthe\\tdistance\\tbetween\\tthe\\ttest\\tinstance\\tand\\tthe\\tdecision\\tboundary,\\tand\\nyou\\tcan\\tuse\\tthis\\tas\\ta\\tconfidence\\tscore.\\tHowever,\\tthis\\tscore\\tcannot\\tbe\\tdirectly\\tconverted\\tinto\\tan\\nestimation\\tof\\tthe\\tclass\\tprobability.\\tIf\\tyou\\tset\\t\\nprobability=True\\n\\twhen\\tcreating\\tan\\tSVM\\tin\\tScikit-\\nLearn,\\tthen\\tafter\\ttraining\\tit\\twill\\tcalibrate\\tthe\\tprobabilities\\tusing\\tLogistic\\tRegression\\ton\\tthe\\tSVM’s\\nscores\\t(trained\\tby\\tan\\tadditional\\tfive-fold\\tcross-validation\\ton\\tthe\\ttraining\\tdata).\\tThis\\twill\\tadd\\tthe\\npredict_proba()\\n\\tand\\t\\npredict_log_proba()\\n\\tmethods\\tto\\tthe\\tSVM.\\n5\\n.\\t\\nThis\\tquestion\\tapplies\\tonly\\tto\\tlinear\\tSVMs\\tsince\\tkernelized\\tcan\\tonly\\tuse\\tthe\\tdual\\tform.\\tThe\\ncomputational\\tcomplexity\\tof\\tthe\\tprimal\\tform\\tof\\tthe\\tSVM\\tproblem\\tis\\tproportional\\tto\\tthe\\tnumber\\tof\\ntraining\\tinstances\\t\\nm\\n,\\twhile\\tthe\\tcomputational\\tcomplexity\\tof\\tthe\\tdual\\tform\\tis\\tproportional\\tto\\ta\\nnumber\\tbetween\\t\\nm\\n2\\n\\tand\\t\\nm\\n3\\n.\\tSo\\tif\\tthere\\tare\\tmillions\\tof\\tinstances,\\tyou\\tshould\\tdefinitely\\tuse\\tthe\\tprimal\\nform,\\tbecause\\tthe\\tdual\\tform\\twill\\tbe\\tmuch\\ttoo\\tslow.\\n6\\n.\\t\\nIf\\tan\\tSVM\\tclassifier\\ttrained\\twith\\tan\\tRBF\\tkernel\\tunderfits\\tthe\\ttraining\\tset,\\tthere\\tmight\\tbe\\ttoo\\tmuch\\nregularization.\\tTo\\tdecrease\\tit,\\tyou\\tneed\\tto\\tincrease\\t\\ngamma\\n\\tor\\t\\nC\\n\\t(or\\tboth).\\n7\\n.\\t\\nLet’s\\tcall\\tthe\\tQP\\tparameters\\tfor\\tthe\\thard-margin\\tproblem\\t\\nH\\n′,\\t\\nf\\n′,\\t\\nA\\n′\\tand\\t\\nb\\n′\\t(see\\t\\n“Quadratic\\nProgramming”\\n).\\tThe\\tQP\\tparameters\\tfor\\tthe\\tsoft-margin\\tproblem\\thave\\t\\nm\\n\\tadditional\\tparameters\\t(\\nn\\np\\n\\t=\\nn\\n\\t+\\t1\\t+\\t\\nm\\n)\\tand\\t\\nm\\n\\tadditional\\tconstraints\\t(\\nn\\nc\\n\\t=\\t2\\nm\\n).\\tThey\\tcan\\tbe\\tdefined\\tlike\\tso:\\nH\\n\\tis\\tequal\\tto\\t\\nH\\n′,\\tplus\\t\\nm\\n\\tcolumns\\tof\\t0s\\ton\\tthe\\tright\\tand\\t\\nm\\n\\trows\\tof\\t0s\\tat\\tthe\\tbottom:\\t\\nf\\n\\tis\\tequal\\tto\\t\\nf\\n′\\twith\\t\\nm\\n\\tadditional\\telements,\\tall\\tequal\\tto\\tthe\\tvalue\\tof\\tthe\\thyperparameter\\t\\nC\\n.\\nb\\n\\tis\\tequal\\tto\\t\\nb\\n′\\twith\\t\\nm\\n\\tadditional\\telements,\\tall\\tequal\\tto\\t0.\\nA\\n\\tis\\tequal\\tto\\t\\nA\\n′,\\twith\\tan\\textra\\t\\nm\\n\\t×\\t\\nm\\n\\tidentity\\tmatrix\\t\\nI\\nm\\n\\tappended\\tto\\tthe\\tright,\\t–\\t\\nI\\nm\\n\\tjust\\tbelow\\tit,', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 574}), Document(page_content='and\\tthe\\trest\\tfilled\\twith\\tzeros:\\t\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 575}), Document(page_content='Chapter\\t6\\n:\\tDecision\\tTrees\\n1\\n.\\t\\nThe\\tdepth\\tof\\ta\\twell-balanced\\tbinary\\ttree\\tcontaining\\t\\nm\\n\\tleaves\\tis\\tequal\\tto\\tlog\\n2\\n(\\nm\\n)\\n3\\n,\\trounded\\tup.\\tA\\nbinary\\tDecision\\tTree\\t(one\\tthat\\tmakes\\tonly\\tbinary\\tdecisions,\\tas\\tis\\tthe\\tcase\\tof\\tall\\ttrees\\tin\\tScikit-\\nLearn)\\twill\\tend\\tup\\tmore\\tor\\tless\\twell\\tbalanced\\tat\\tthe\\tend\\tof\\ttraining,\\twith\\tone\\tleaf\\tper\\ttraining\\ninstance\\tif\\tit\\tis\\ttrained\\twithout\\trestrictions.\\tThus,\\tif\\tthe\\ttraining\\tset\\tcontains\\tone\\tmillion\\tinstances,\\nthe\\tDecision\\tTree\\twill\\thave\\ta\\tdepth\\tof\\tlog\\n2\\n(10\\n6\\n)\\t≈\\t20\\t(actually\\ta\\tbit\\tmore\\tsince\\tthe\\ttree\\twill\\ngenerally\\tnot\\tbe\\tperfectly\\twell\\tbalanced).\\n2\\n.\\t\\nA\\tnode’s\\tGini\\timpurity\\tis\\tgenerally\\tlower\\tthan\\tits\\tparent’s.\\tThis\\tis\\tdue\\tto\\tthe\\tCART\\ttraining\\nalgorithm’s\\tcost\\tfunction,\\twhich\\tsplits\\teach\\tnode\\tin\\ta\\tway\\tthat\\tminimizes\\tthe\\tweighted\\tsum\\tof\\tits\\nchildren’s\\tGini\\timpurities.\\tHowever,\\tit\\tis\\tpossible\\tfor\\ta\\tnode\\tto\\thave\\ta\\thigher\\tGini\\timpurity\\tthan\\tits\\nparent,\\tas\\tlong\\tas\\tthis\\tincrease\\tis\\tmore\\tthan\\tcompensated\\tfor\\tby\\ta\\tdecrease\\tof\\tthe\\tother\\tchild’s\\nimpurity.\\tFor\\texample,\\tconsider\\ta\\tnode\\tcontaining\\tfour\\tinstances\\tof\\tclass\\tA\\tand\\t1\\tof\\tclass\\tB.\\tIts\\tGini\\nimpurity\\tis\\t\\n\\t=\\t0.32.\\tNow\\tsuppose\\tthe\\tdataset\\tis\\tone-dimensional\\tand\\tthe\\tinstances\\tare\\nlined\\tup\\tin\\tthe\\tfollowing\\torder:\\tA,\\tB,\\tA,\\tA,\\tA.\\tYou\\tcan\\tverify\\tthat\\tthe\\talgorithm\\twill\\tsplit\\tthis\\tnode\\nafter\\tthe\\tsecond\\tinstance,\\tproducing\\tone\\tchild\\tnode\\twith\\tinstances\\tA,\\tB,\\tand\\tthe\\tother\\tchild\\tnode\\nwith\\tinstances\\tA,\\tA,\\tA.\\tThe\\tfirst\\tchild\\tnode’s\\tGini\\timpurity\\tis\\t\\n\\t=\\t0.5,\\twhich\\tis\\thigher\\nthan\\tits\\tparent.\\tThis\\tis\\tcompensated\\tfor\\tby\\tthe\\tfact\\tthat\\tthe\\tother\\tnode\\tis\\tpure,\\tso\\tthe\\toverall\\nweighted\\tGini\\timpurity\\tis\\t\\n\\t0.5\\t+\\t\\n\\t=\\t0.2\\t,\\twhich\\tis\\tlower\\tthan\\tthe\\tparent’s\\tGini\\timpurity.\\n3\\n.\\t\\nIf\\ta\\tDecision\\tTree\\tis\\toverfitting\\tthe\\ttraining\\tset,\\tit\\tmay\\tbe\\ta\\tgood\\tidea\\tto\\tdecrease\\t\\nmax_depth\\n,\\tsince\\nthis\\twill\\tconstrain\\tthe\\tmodel,\\tregularizing\\tit.\\n4\\n.\\t\\nDecision\\tTrees\\tdon’t\\tcare\\twhether\\tor\\tnot\\tthe\\ttraining\\tdata\\tis\\tscaled\\tor\\tcentered;\\tthat’s\\tone\\tof\\tthe\\tnice\\nthings\\tabout\\tthem.\\tSo\\tif\\ta\\tDecision\\tTree\\tunderfits\\tthe\\ttraining\\tset,\\tscaling\\tthe\\tinput\\tfeatures\\twill\\tjust\\nbe\\ta\\twaste\\tof\\ttime.\\n5\\n.\\t\\nThe\\tcomputational\\tcomplexity\\tof\\ttraining\\ta\\tDecision\\tTree\\tis\\t\\nO\\n(\\nn\\n\\t×\\t\\nm\\n\\tlog(\\nm\\n)).\\tSo\\tif\\tyou\\tmultiply\\tthe\\ntraining\\tset\\tsize\\tby\\t10,\\tthe\\ttraining\\ttime\\twill\\tbe\\tmultiplied\\tby\\t\\nK\\n\\t=\\t(\\nn\\n\\t×\\t10\\nm\\n\\t×\\tlog(10\\nm\\n))\\t/\\t(\\nn\\n\\t×\\t\\nm\\n\\t×\\nlog(\\nm\\n))\\t=\\t10\\t×\\tlog(10\\nm\\n)\\t/\\tlog(\\nm\\n).\\tIf\\t\\nm\\n\\t=\\t10\\n6\\n,\\tthen\\t\\nK\\n\\t≈\\t11.7,\\tso\\tyou\\tcan\\texpect\\tthe\\ttraining\\ttime\\tto\\tbe\\nroughly\\t11.7\\thours.\\n6\\n.\\t\\nPresorting\\tthe\\ttraining\\tset\\tspeeds\\tup\\ttraining\\tonly\\tif\\tthe\\tdataset\\tis\\tsmaller\\tthan\\ta\\tfew\\tthousand\\ninstances.\\tIf\\tit\\tcontains\\t100,000\\tinstances,\\tsetting\\t\\npresort=True\\n\\twill\\tconsiderably\\tslow\\tdown\\ntraining.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t7\\tand\\t8,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 576}), Document(page_content='Chapter\\t7\\n:\\tEnsemble\\tLearning\\tand\\tRandom\\tForests\\n1\\n.\\t\\nIf\\tyou\\thave\\ttrained\\tfive\\tdifferent\\tmodels\\tand\\tthey\\tall\\tachieve\\t95%\\tprecision,\\tyou\\tcan\\ttry\\tcombining\\nthem\\tinto\\ta\\tvoting\\tensemble,\\twhich\\twill\\toften\\tgive\\tyou\\teven\\tbetter\\tresults.\\tIt\\tworks\\tbetter\\tif\\tthe\\nmodels\\tare\\tvery\\tdifferent\\t(e.g.,\\tan\\tSVM\\tclassifier,\\ta\\tDecision\\tTree\\tclassifier,\\ta\\tLogistic\\tRegression\\nclassifier,\\tand\\tso\\ton).\\tIt\\tis\\teven\\tbetter\\tif\\tthey\\tare\\ttrained\\ton\\tdifferent\\ttraining\\tinstances\\t(that’s\\tthe\\nwhole\\tpoint\\tof\\tbagging\\tand\\tpasting\\tensembles),\\tbut\\tif\\tnot\\tit\\twill\\tstill\\twork\\tas\\tlong\\tas\\tthe\\tmodels\\tare\\nvery\\tdifferent.\\n2\\n.\\t\\nA\\thard\\tvoting\\tclassifier\\tjust\\tcounts\\tthe\\tvotes\\tof\\teach\\tclassifier\\tin\\tthe\\tensemble\\tand\\tpicks\\tthe\\tclass\\nthat\\tgets\\tthe\\tmost\\tvotes.\\tA\\tsoft\\tvoting\\tclassifier\\tcomputes\\tthe\\taverage\\testimated\\tclass\\tprobability\\tfor\\neach\\tclass\\tand\\tpicks\\tthe\\tclass\\twith\\tthe\\thighest\\tprobability.\\tThis\\tgives\\thigh-confidence\\tvotes\\tmore\\nweight\\tand\\toften\\tperforms\\tbetter,\\tbut\\tit\\tworks\\tonly\\tif\\tevery\\tclassifier\\tis\\table\\tto\\testimate\\tclass\\nprobabilities\\t(e.g.,\\tfor\\tthe\\tSVM\\tclassifiers\\tin\\tScikit-Learn\\tyou\\tmust\\tset\\t\\nprobability=True\\n).\\n3\\n.\\t\\nIt\\tis\\tquite\\tpossible\\tto\\tspeed\\tup\\ttraining\\tof\\ta\\tbagging\\tensemble\\tby\\tdistributing\\tit\\tacross\\tmultiple\\nservers,\\tsince\\teach\\tpredictor\\tin\\tthe\\tensemble\\tis\\tindependent\\tof\\tthe\\tothers.\\tThe\\tsame\\tgoes\\tfor\\tpasting\\nensembles\\tand\\tRandom\\tForests,\\tfor\\tthe\\tsame\\treason.\\tHowever,\\teach\\tpredictor\\tin\\ta\\tboosting\\nensemble\\tis\\tbuilt\\tbased\\ton\\tthe\\tprevious\\tpredictor,\\tso\\ttraining\\tis\\tnecessarily\\tsequential,\\tand\\tyou\\twill\\nnot\\tgain\\tanything\\tby\\tdistributing\\ttraining\\tacross\\tmultiple\\tservers.\\tRegarding\\tstacking\\tensembles,\\tall\\nthe\\tpredictors\\tin\\ta\\tgiven\\tlayer\\tare\\tindependent\\tof\\teach\\tother,\\tso\\tthey\\tcan\\tbe\\ttrained\\tin\\tparallel\\ton\\nmultiple\\tservers.\\tHowever,\\tthe\\tpredictors\\tin\\tone\\tlayer\\tcan\\tonly\\tbe\\ttrained\\tafter\\tthe\\tpredictors\\tin\\tthe\\nprevious\\tlayer\\thave\\tall\\tbeen\\ttrained.\\n4\\n.\\t\\nWith\\tout-of-bag\\tevaluation,\\teach\\tpredictor\\tin\\ta\\tbagging\\tensemble\\tis\\tevaluated\\tusing\\tinstances\\tthat\\tit\\nwas\\tnot\\ttrained\\ton\\t(they\\twere\\theld\\tout).\\tThis\\tmakes\\tit\\tpossible\\tto\\thave\\ta\\tfairly\\tunbiased\\tevaluation\\nof\\tthe\\tensemble\\twithout\\tthe\\tneed\\tfor\\tan\\tadditional\\tvalidation\\tset.\\tThus,\\tyou\\thave\\tmore\\tinstances\\navailable\\tfor\\ttraining,\\tand\\tyour\\tensemble\\tcan\\tperform\\tslightly\\tbetter.\\n5\\n.\\t\\nWhen\\tyou\\tare\\tgrowing\\ta\\ttree\\tin\\ta\\tRandom\\tForest,\\tonly\\ta\\trandom\\tsubset\\tof\\tthe\\tfeatures\\tis\\tconsidered\\nfor\\tsplitting\\tat\\teach\\tnode.\\tThis\\tis\\ttrue\\tas\\twell\\tfor\\tExtra-Trees,\\tbut\\tthey\\tgo\\tone\\tstep\\tfurther:\\trather\\nthan\\tsearching\\tfor\\tthe\\tbest\\tpossible\\tthresholds,\\tlike\\tregular\\tDecision\\tTrees\\tdo,\\tthey\\tuse\\trandom\\nthresholds\\tfor\\teach\\tfeature.\\tThis\\textra\\trandomness\\tacts\\tlike\\ta\\tform\\tof\\tregularization:\\tif\\ta\\tRandom\\nForest\\toverfits\\tthe\\ttraining\\tdata,\\tExtra-Trees\\tmight\\tperform\\tbetter.\\tMoreover,\\tsince\\tExtra-Trees\\ndon’t\\tsearch\\tfor\\tthe\\tbest\\tpossible\\tthresholds,\\tthey\\tare\\tmuch\\tfaster\\tto\\ttrain\\tthan\\tRandom\\tForests.\\nHowever,\\tthey\\tare\\tneither\\tfaster\\tnor\\tslower\\tthan\\tRandom\\tForests\\twhen\\tmaking\\tpredictions.\\n6\\n.\\t\\nIf\\tyour\\tAdaBoost\\tensemble\\tunderfits\\tthe\\ttraining\\tdata,\\tyou\\tcan\\ttry\\tincreasing\\tthe\\tnumber\\tof\\nestimators\\tor\\treducing\\tthe\\tregularization\\thyperparameters\\tof\\tthe\\tbase\\testimator.\\tYou\\tmay\\talso\\ttry\\nslightly\\tincreasing\\tthe\\tlearning\\trate.\\n7\\n.\\t\\nIf\\tyour\\tGradient\\tBoosting\\tensemble\\toverfits\\tthe\\ttraining\\tset,\\tyou\\tshould\\ttry\\tdecreasing\\tthe\\tlearning\\nrate.\\tYou\\tcould\\talso\\tuse\\tearly\\tstopping\\tto\\tfind\\tthe\\tright\\tnumber\\tof\\tpredictors\\t(you\\tprobably\\thave\\ttoo\\nmany).\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8\\tand\\t9,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 577}), Document(page_content='https://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 578}), Document(page_content='Chapter\\t8\\n:\\tDimensionality\\tReduction\\n1\\n.\\t\\nMotivations\\tand\\tdrawbacks:\\nThe\\tmain\\tmotivations\\tfor\\tdimensionality\\treduction\\tare:\\nTo\\tspeed\\tup\\ta\\tsubsequent\\ttraining\\talgorithm\\t(in\\tsome\\tcases\\tit\\tmay\\teven\\tremove\\tnoise\\tand\\nredundant\\tfeatures,\\tmaking\\tthe\\ttraining\\talgorithm\\tperform\\tbetter).\\nTo\\tvisualize\\tthe\\tdata\\tand\\tgain\\tinsights\\ton\\tthe\\tmost\\timportant\\tfeatures.\\nSimply\\tto\\tsave\\tspace\\t(compression).\\nThe\\tmain\\tdrawbacks\\tare:\\nSome\\tinformation\\tis\\tlost,\\tpossibly\\tdegrading\\tthe\\tperformance\\tof\\tsubsequent\\ttraining\\nalgorithms.\\nIt\\tcan\\tbe\\tcomputationally\\tintensive.\\nIt\\tadds\\tsome\\tcomplexity\\tto\\tyour\\tMachine\\tLearning\\tpipelines.\\nTransformed\\tfeatures\\tare\\toften\\thard\\tto\\tinterpret.\\n2\\n.\\t\\nThe\\tcurse\\tof\\tdimensionality\\trefers\\tto\\tthe\\tfact\\tthat\\tmany\\tproblems\\tthat\\tdo\\tnot\\texist\\tin\\tlow-\\ndimensional\\tspace\\tarise\\tin\\thigh-dimensional\\tspace.\\tIn\\tMachine\\tLearning,\\tone\\tcommon\\tmanifestation\\nis\\tthe\\tfact\\tthat\\trandomly\\tsampled\\thigh-dimensional\\tvectors\\tare\\tgenerally\\tvery\\tsparse,\\tincreasing\\tthe\\nrisk\\tof\\toverfitting\\tand\\tmaking\\tit\\tvery\\tdifficult\\tto\\tidentify\\tpatterns\\tin\\tthe\\tdata\\twithout\\thaving\\tplenty\\tof\\ntraining\\tdata.\\n3\\n.\\t\\nOnce\\ta\\tdataset’s\\tdimensionality\\thas\\tbeen\\treduced\\tusing\\tone\\tof\\tthe\\talgorithms\\twe\\tdiscussed,\\tit\\tis\\nalmost\\talways\\timpossible\\tto\\tperfectly\\treverse\\tthe\\toperation,\\tbecause\\tsome\\tinformation\\tgets\\tlost\\nduring\\tdimensionality\\treduction.\\tMoreover,\\twhile\\tsome\\talgorithms\\t(such\\tas\\tPCA)\\thave\\ta\\tsimple\\nreverse\\ttransformation\\tprocedure\\tthat\\tcan\\treconstruct\\ta\\tdataset\\trelatively\\tsimilar\\tto\\tthe\\toriginal,\\nother\\talgorithms\\t(such\\tas\\tT-SNE)\\tdo\\tnot.\\n4\\n.\\t\\nPCA\\tcan\\tbe\\tused\\tto\\tsignificantly\\treduce\\tthe\\tdimensionality\\tof\\tmost\\tdatasets,\\teven\\tif\\tthey\\tare\\thighly\\nnonlinear,\\tbecause\\tit\\tcan\\tat\\tleast\\tget\\trid\\tof\\tuseless\\tdimensions.\\tHowever,\\tif\\tthere\\tare\\tno\\tuseless\\ndimensions\\t—\\tfor\\texample,\\tthe\\tSwiss\\troll\\t—\\tthen\\treducing\\tdimensionality\\twith\\tPCA\\twill\\tlose\\ttoo\\nmuch\\tinformation.\\tYou\\twant\\tto\\tunroll\\tthe\\tSwiss\\troll,\\tnot\\tsquash\\tit.\\n5\\n.\\t\\nThat’s\\ta\\ttrick\\tquestion:\\tit\\tdepends\\ton\\tthe\\tdataset.\\tLet’s\\tlook\\tat\\ttwo\\textreme\\texamples.\\tFirst,\\tsuppose\\nthe\\tdataset\\tis\\tcomposed\\tof\\tpoints\\tthat\\tare\\talmost\\tperfectly\\taligned.\\tIn\\tthis\\tcase,\\tPCA\\tcan\\treduce\\tthe\\ndataset\\tdown\\tto\\tjust\\tone\\tdimension\\twhile\\tstill\\tpreserving\\t95%\\tof\\tthe\\tvariance.\\tNow\\timagine\\tthat\\tthe\\ndataset\\tis\\tcomposed\\tof\\tperfectly\\trandom\\tpoints,\\tscattered\\tall\\taround\\tthe\\t1,000\\tdimensions.\\tIn\\tthis\\ncase\\troughly\\t950\\tdimensions\\tare\\trequired\\tto\\tpreserve\\t95%\\tof\\tthe\\tvariance.\\tSo\\tthe\\tanswer\\tis,\\tit\\ndepends\\ton\\tthe\\tdataset,\\tand\\tit\\tcould\\tbe\\tany\\tnumber\\tbetween\\t1\\tand\\t950.\\tPlotting\\tthe\\texplained\\nvariance\\tas\\ta\\tfunction\\tof\\tthe\\tnumber\\tof\\tdimensions\\tis\\tone\\tway\\tto\\tget\\ta\\trough\\tidea\\tof\\tthe\\tdataset’s\\nintrinsic\\tdimensionality.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 579}), Document(page_content='6\\n.\\t\\nRegular\\tPCA\\tis\\tthe\\tdefault,\\tbut\\tit\\tworks\\tonly\\tif\\tthe\\tdataset\\tfits\\tin\\tmemory.\\tIncremental\\tPCA\\tis\\tuseful\\nfor\\tlarge\\tdatasets\\tthat\\tdon’t\\tfit\\tin\\tmemory,\\tbut\\tit\\tis\\tslower\\tthan\\tregular\\tPCA,\\tso\\tif\\tthe\\tdataset\\tfits\\tin\\nmemory\\tyou\\tshould\\tprefer\\tregular\\tPCA.\\tIncremental\\tPCA\\tis\\talso\\tuseful\\tfor\\tonline\\ttasks,\\twhen\\tyou\\nneed\\tto\\tapply\\tPCA\\ton\\tthe\\tfly,\\tevery\\ttime\\ta\\tnew\\tinstance\\tarrives.\\tRandomized\\tPCA\\tis\\tuseful\\twhen\\nyou\\twant\\tto\\tconsiderably\\treduce\\tdimensionality\\tand\\tthe\\tdataset\\tfits\\tin\\tmemory;\\tin\\tthis\\tcase,\\tit\\tis\\nmuch\\tfaster\\tthan\\tregular\\tPCA.\\tFinally,\\tKernel\\tPCA\\tis\\tuseful\\tfor\\tnonlinear\\tdatasets.\\n7\\n.\\t\\nIntuitively,\\ta\\tdimensionality\\treduction\\talgorithm\\tperforms\\twell\\tif\\tit\\teliminates\\ta\\tlot\\tof\\tdimensions\\nfrom\\tthe\\tdataset\\twithout\\tlosing\\ttoo\\tmuch\\tinformation.\\tOne\\tway\\tto\\tmeasure\\tthis\\tis\\tto\\tapply\\tthe\\nreverse\\ttransformation\\tand\\tmeasure\\tthe\\treconstruction\\terror.\\tHowever,\\tnot\\tall\\tdimensionality\\nreduction\\talgorithms\\tprovide\\ta\\treverse\\ttransformation.\\tAlternatively,\\tif\\tyou\\tare\\tusing\\tdimensionality\\nreduction\\tas\\ta\\tpreprocessing\\tstep\\tbefore\\tanother\\tMachine\\tLearning\\talgorithm\\t(e.g.,\\ta\\tRandom\\tForest\\nclassifier),\\tthen\\tyou\\tcan\\tsimply\\tmeasure\\tthe\\tperformance\\tof\\tthat\\tsecond\\talgorithm;\\tif\\tdimensionality\\nreduction\\tdid\\tnot\\tlose\\ttoo\\tmuch\\tinformation,\\tthen\\tthe\\talgorithm\\tshould\\tperform\\tjust\\tas\\twell\\tas\\twhen\\nusing\\tthe\\toriginal\\tdataset.\\n8\\n.\\t\\nIt\\tcan\\tabsolutely\\tmake\\tsense\\tto\\tchain\\ttwo\\tdifferent\\tdimensionality\\treduction\\talgorithms.\\tA\\tcommon\\nexample\\tis\\tusing\\tPCA\\tto\\tquickly\\tget\\trid\\tof\\ta\\tlarge\\tnumber\\tof\\tuseless\\tdimensions,\\tthen\\tapplying\\nanother\\tmuch\\tslower\\tdimensionality\\treduction\\talgorithm,\\tsuch\\tas\\tLLE.\\tThis\\ttwo-step\\tapproach\\twill\\nlikely\\tyield\\tthe\\tsame\\tperformance\\tas\\tusing\\tLLE\\tonly,\\tbut\\tin\\ta\\tfraction\\tof\\tthe\\ttime.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t9\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 580}), Document(page_content='Chapter\\t9\\n:\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\n1\\n.\\t\\nMain\\tbenefits\\tand\\tdrawbacks\\tof\\tcreating\\ta\\tcomputation\\tgraph\\trather\\tthan\\tdirectly\\texecuting\\tthe\\ncomputations:\\nMain\\tbenefits:\\nTensorFlow\\tcan\\tautomatically\\tcompute\\tthe\\tgradients\\tfor\\tyou\\t(using\\treverse-mode\\nautodiff).\\nTensorFlow\\tcan\\ttake\\tcare\\tof\\trunning\\tthe\\toperations\\tin\\tparallel\\tin\\tdifferent\\tthreads.\\nIt\\tmakes\\tit\\teasier\\tto\\trun\\tthe\\tsame\\tmodel\\tacross\\tdifferent\\tdevices.\\nIt\\tsimplifies\\tintrospection\\t—\\tfor\\texample,\\tto\\tview\\tthe\\tmodel\\tin\\tTensorBoard.\\nMain\\tdrawbacks:\\nIt\\tmakes\\tthe\\tlearning\\tcurve\\tsteeper.\\nIt\\tmakes\\tstep-by-step\\tdebugging\\tharder.\\n2\\n.\\t\\nYes,\\tthe\\tstatement\\t\\na_val\\n\\t\\n=\\n\\t\\na.eval(session=sess)\\n\\tis\\tindeed\\tequivalent\\tto\\t\\na_val\\n\\t\\n=\\n\\t\\nsess.run(a)\\n.\\n3\\n.\\t\\nNo,\\tthe\\tstatement\\t\\na_val,\\tb_val\\n\\t\\n=\\n\\t\\na.eval(session=sess),\\tb.eval(session=sess)\\n\\tis\\tnot\\nequivalent\\tto\\t\\na_val,\\tb_val\\n\\t\\n=\\n\\t\\nsess.run([a,\\tb])\\n.\\tIndeed,\\tthe\\tfirst\\tstatement\\truns\\tthe\\tgraph\\ttwice\\n(once\\tto\\tcompute\\t\\na\\n,\\tonce\\tto\\tcompute\\t\\nb\\n),\\twhile\\tthe\\tsecond\\tstatement\\truns\\tthe\\tgraph\\tonly\\tonce.\\tIf\\tany\\nof\\tthese\\toperations\\t(or\\tthe\\tops\\tthey\\tdepend\\ton)\\thave\\tside\\teffects\\t(e.g.,\\ta\\tvariable\\tis\\tmodified,\\tan\\nitem\\tis\\tinserted\\tin\\ta\\tqueue,\\tor\\ta\\treader\\treads\\ta\\tfile),\\tthen\\tthe\\teffects\\twill\\tbe\\tdifferent.\\tIf\\tthey\\tdon’t\\nhave\\tside\\teffects,\\tboth\\tstatements\\twill\\treturn\\tthe\\tsame\\tresult,\\tbut\\tthe\\tsecond\\tstatement\\twill\\tbe\\tfaster\\nthan\\tthe\\tfirst.\\n4\\n.\\t\\nNo,\\tyou\\tcannot\\trun\\ttwo\\tgraphs\\tin\\tthe\\tsame\\tsession.\\tYou\\twould\\thave\\tto\\tmerge\\tthe\\tgraphs\\tinto\\ta\\tsingle\\ngraph\\tfirst.\\n5\\n.\\t\\nIn\\tlocal\\tTensorFlow,\\tsessions\\tmanage\\tvariable\\tvalues,\\tso\\tif\\tyou\\tcreate\\ta\\tgraph\\t\\ng\\n\\tcontaining\\ta\\nvariable\\t\\nw\\n,\\tthen\\tstart\\ttwo\\tthreads\\tand\\topen\\ta\\tlocal\\tsession\\tin\\teach\\tthread,\\tboth\\tusing\\tthe\\tsame\\tgraph\\ng\\n,\\tthen\\teach\\tsession\\twill\\thave\\tits\\town\\tcopy\\tof\\tthe\\tvariable\\t\\nw\\n.\\tHowever,\\tin\\tdistributed\\tTensorFlow,\\nvariable\\tvalues\\tare\\tstored\\tin\\tcontainers\\tmanaged\\tby\\tthe\\tcluster,\\tso\\tif\\tboth\\tsessions\\tconnect\\tto\\tthe\\nsame\\tcluster\\tand\\tuse\\tthe\\tsame\\t\\ncontainer,\\tthen\\tthey\\twill\\tshare\\tthe\\tsame\\tvariable\\tvalue\\tfor\\t\\nw\\n.\\n6\\n.\\t\\nA\\tvariable\\tis\\tinitialized\\twhen\\tyou\\tcall\\tits\\tinitializer,\\tand\\tit\\tis\\tdestroyed\\twhen\\tthe\\tsession\\tends.\\tIn\\ndistributed\\tTensorFlow,\\tvariables\\tlive\\tin\\tcontainers\\ton\\tthe\\tcluster,\\tso\\tclosing\\ta\\tsession\\twill\\tnot\\ndestroy\\tthe\\tvariable.\\tTo\\tdestroy\\ta\\tvariable,\\tyou\\tneed\\tto\\tclear\\tits\\tcontainer.\\n7\\n.\\t\\nVariables\\tand\\tplaceholders\\tare\\textremely\\tdifferent,\\tbut\\tbeginners\\toften\\tconfuse\\tthem:\\nA\\tvariable\\tis\\tan\\toperation\\tthat\\tholds\\ta\\tvalue.\\tIf\\tyou\\trun\\tthe\\tvariable,\\tit\\treturns\\tthat\\tvalue.\\nBefore\\tyou\\tcan\\trun\\tit,\\tyou\\tneed\\tto\\tinitialize\\tit.\\tYou\\tcan\\tchange\\tthe\\tvariable’s\\tvalue\\t(for', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 581}), Document(page_content='example,\\tby\\tusing\\tan\\tassignment\\toperation).\\tIt\\tis\\tstateful:\\tthe\\tvariable\\tkeeps\\tthe\\tsame\\tvalue\\nupon\\tsuccessive\\truns\\tof\\tthe\\tgraph.\\tIt\\tis\\ttypically\\tused\\tto\\thold\\tmodel\\tparameters\\tbut\\talso\\tfor\\nother\\tpurposes\\t(e.g.,\\tto\\tcount\\tthe\\tglobal\\ttraining\\tstep).\\nPlaceholders\\ttechnically\\tdon’t\\tdo\\tmuch:\\tthey\\tjust\\thold\\tinformation\\tabout\\tthe\\ttype\\tand\\tshape\\tof\\nthe\\ttensor\\tthey\\trepresent,\\tbut\\tthey\\thave\\tno\\tvalue.\\tIn\\tfact,\\tif\\tyou\\ttry\\tto\\tevaluate\\tan\\toperation\\tthat\\ndepends\\ton\\ta\\tplaceholder,\\tyou\\tmust\\tfeed\\tTensorFlow\\tthe\\tvalue\\tof\\tthe\\tplaceholder\\t(using\\tthe\\nfeed_dict\\n\\targument)\\tor\\telse\\tyou\\twill\\tget\\tan\\texception.\\tPlaceholders\\tare\\ttypically\\tused\\tto\\tfeed\\ntraining\\tor\\ttest\\tdata\\tto\\tTensorFlow\\tduring\\tthe\\texecution\\tphase.\\tThey\\tare\\talso\\tuseful\\tto\\tpass\\ta\\nvalue\\tto\\tan\\tassignment\\tnode,\\tto\\tchange\\tthe\\tvalue\\tof\\ta\\tvariable\\t(e.g.,\\tmodel\\tweights).\\n8\\n.\\t\\nIf\\tyou\\trun\\tthe\\tgraph\\tto\\tevaluate\\tan\\toperation\\tthat\\tdepends\\ton\\ta\\tplaceholder\\tbut\\tyou\\tdon’t\\tfeed\\tits\\nvalue,\\tyou\\tget\\tan\\texception.\\tIf\\tthe\\toperation\\tdoes\\tnot\\tdepend\\ton\\tthe\\tplaceholder,\\tthen\\tno\\texception\\tis\\nraised.\\n9\\n.\\t\\nWhen\\tyou\\trun\\ta\\tgraph,\\tyou\\tcan\\tfeed\\tthe\\toutput\\tvalue\\tof\\tany\\toperation,\\tnot\\tjust\\tthe\\tvalue\\tof\\nplaceholders.\\tIn\\tpractice,\\thowever,\\tthis\\tis\\trather\\trare\\t(it\\tcan\\tbe\\tuseful,\\tfor\\texample,\\twhen\\tyou\\tare\\ncaching\\tthe\\toutput\\tof\\tfrozen\\tlayers;\\tsee\\t\\nChapter\\t11\\n).\\n10\\n.\\t\\nYou\\tcan\\tspecify\\ta\\tvariable’s\\tinitial\\tvalue\\twhen\\tconstructing\\tthe\\tgraph,\\tand\\tit\\twill\\tbe\\tinitialized\\tlater\\nwhen\\tyou\\trun\\tthe\\tvariable’s\\tinitializer\\tduring\\tthe\\texecution\\tphase.\\tIf\\tyou\\twant\\tto\\tchange\\tthat\\nvariable’s\\tvalue\\tto\\tanything\\tyou\\twant\\tduring\\tthe\\texecution\\tphase,\\tthen\\tthe\\tsimplest\\toption\\tis\\tto\\tcreate\\nan\\tassignment\\tnode\\t(during\\tthe\\tgraph\\tconstruction\\tphase)\\tusing\\tthe\\t\\ntf.assign()\\n\\tfunction,\\t\\npassing\\nthe\\tvariable\\tand\\ta\\tplaceholder\\tas\\tparameters.\\tDuring\\tthe\\texecution\\tphase,\\tyou\\tcan\\trun\\tthe\\tassignment\\noperation\\tand\\tfeed\\tthe\\tvariable’s\\tnew\\tvalue\\tusing\\tthe\\t\\nplaceholder.\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nx\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_uniform\\n(\\nshape\\n=\\n(),\\n\\t\\nminval\\n=\\n0.0\\n,\\n\\t\\nmaxval\\n=\\n1.0\\n))\\nx_new_val\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\nshape\\n=\\n(),\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nx_assign\\n\\t\\n=\\n\\t\\ntf\\n.\\nassign\\n(\\nx\\n,\\n\\t\\nx_new_val\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n():\\n\\t\\t\\t\\t\\nx\\n.\\ninitializer\\n.\\nrun\\n()\\n\\t\\n#\\trandom\\tnumber\\tis\\tsampled\\t*now*\\n\\t\\t\\t\\t\\nprint\\n(\\nx\\n.\\neval\\n())\\n\\t\\n#\\t0.646157\\t(some\\trandom\\tnumber)\\n\\t\\t\\t\\t\\nx_assign\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nx_new_val\\n:\\n\\t\\n5.0\\n})\\n\\t\\t\\t\\t\\nprint\\n(\\nx\\n.\\neval\\n())\\n\\t\\n#\\t5.0\\n11\\n.\\t\\nReverse-mode\\tautodiff\\t(implemented\\tby\\tTensorFlow)\\tneeds\\tto\\ttraverse\\tthe\\tgraph\\tonly\\ttwice\\tin\\norder\\tto\\tcompute\\tthe\\tgradients\\tof\\tthe\\tcost\\tfunction\\twith\\tregards\\tto\\tany\\tnumber\\tof\\tvariables.\\tOn\\tthe\\nother\\thand,\\tforward-mode\\tautodiff\\twould\\tneed\\tto\\trun\\tonce\\tfor\\teach\\tvariable\\t(so\\t10\\ttimes\\tif\\twe\\twant\\nthe\\tgradients\\twith\\tregards\\tto\\t10\\tdifferent\\tvariables).\\tAs\\tfor\\tsymbolic\\tdifferentiation,\\tit\\twould\\tbuild\\na\\tdifferent\\tgraph\\tto\\tcompute\\tthe\\tgradients,\\tso\\tit\\twould\\tnot\\ttraverse\\tthe\\toriginal\\tgraph\\tat\\tall\\t(except\\nwhen\\tbuilding\\tthe\\tnew\\tgradients\\tgraph).\\tA\\thighly\\toptimized\\tsymbolic\\tdifferentiation\\tsystem\\tcould\\npotentially\\trun\\tthe\\tnew\\tgradients\\tgraph\\tonly\\tonce\\tto\\tcompute\\tthe\\tgradients\\twith\\tregards\\tto\\tall\\nvariables,\\tbut\\tthat\\tnew\\tgraph\\tmay\\tbe\\thorribly\\tcomplex\\tand\\tinefficient\\tcompared\\tto\\tthe\\toriginal\\ngraph.\\n12\\n.\\t\\nSee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 582}), Document(page_content='Chapter\\t10\\n:\\tIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n1\\n.\\t\\nHere\\tis\\ta\\tneural\\tnetwork\\tbased\\ton\\tthe\\toriginal\\tartificial\\tneurons\\tthat\\tcomputes\\t\\nA\\n\\t\\n\\t\\nB\\n\\t(where\\t\\nrepresents\\tthe\\texclusive\\tOR),\\tusing\\tthe\\tfact\\tthat\\t\\nA\\n\\t\\n\\t\\nB\\n\\t=\\t(\\nA\\n\\t\\n\\t¬\\t\\nB\\n)\\t\\n\\t(¬\\t\\nA\\n\\t\\n\\t\\nB\\n).\\tThere\\tare\\tother\\nsolutions\\t—\\tfor\\texample,\\tusing\\tthe\\tfact\\tthat\\t\\nA\\n\\t\\n\\t\\nB\\n\\t=\\t(\\nA\\n\\t\\n\\t\\nB\\n)\\t\\n\\t¬(\\nA\\n\\t\\n\\t\\nB\\n),\\tor\\tthe\\tfact\\tthat\\t\\nA\\n\\t\\n\\t\\nB\\n\\t=\\t(\\nA\\n\\t\\nB\\n)\\t\\n\\t(¬\\t\\nA\\n\\t\\n\\t\\n\\t\\nB\\n),\\tand\\tso\\ton.\\n2\\n.\\t\\nA\\tclassical\\tPerceptron\\twill\\tconverge\\tonly\\tif\\tthe\\tdataset\\tis\\tlinearly\\tseparable,\\tand\\tit\\twon’t\\tbe\\table\\tto\\nestimate\\tclass\\tprobabilities.\\tIn\\tcontrast,\\ta\\tLogistic\\tRegression\\tclassifier\\twill\\tconverge\\tto\\ta\\tgood\\nsolution\\teven\\tif\\tthe\\tdataset\\tis\\tnot\\tlinearly\\tseparable,\\tand\\tit\\twill\\toutput\\tclass\\tprobabilities.\\tIf\\tyou\\nchange\\tthe\\tPerceptron’s\\tactivation\\tfunction\\tto\\tthe\\tlogistic\\tactivation\\tfunction\\t(or\\tthe\\tsoftmax\\nactivation\\tfunction\\tif\\tthere\\tare\\tmultiple\\tneurons),\\tand\\tif\\tyou\\ttrain\\tit\\tusing\\tGradient\\tDescent\\t(or\\tsome\\nother\\toptimization\\talgorithm\\tminimizing\\tthe\\tcost\\tfunction,\\ttypically\\tcross\\tentropy),\\tthen\\tit\\tbecomes\\nequivalent\\tto\\ta\\tLogistic\\tRegression\\tclassifier.\\n3\\n.\\t\\nThe\\tlogistic\\tactivation\\tfunction\\twas\\ta\\tkey\\tingredient\\tin\\ttraining\\tthe\\tfirst\\tMLPs\\tbecause\\tits\\tderivative\\nis\\talways\\tnonzero,\\tso\\tGradient\\tDescent\\tcan\\talways\\troll\\tdown\\tthe\\tslope.\\tWhen\\tthe\\tactivation\\nfunction\\tis\\ta\\tstep\\tfunction,\\tGradient\\tDescent\\tcannot\\tmove,\\tas\\tthere\\tis\\tno\\tslope\\tat\\tall.\\n4\\n.\\t\\nThe\\tstep\\tfunction,\\tthe\\tlogistic\\tfunction,\\tthe\\thyperbolic\\ttangent,\\tthe\\trectified\\tlinear\\tunit\\t(see\\nFigure\\t10-8\\n).\\tSee\\t\\nChapter\\t11\\n\\tfor\\tother\\texamples,\\tsuch\\tas\\tELU\\tand\\tvariants\\tof\\tthe\\tReLU.\\n5\\n.\\t\\nConsidering\\tthe\\tMLP\\tdescribed\\tin\\tthe\\tquestion:\\tsuppose\\tyou\\thave\\tan\\tMLP\\tcomposed\\tof\\tone\\tinput\\nlayer\\twith\\t10\\tpassthrough\\tneurons,\\tfollowed\\tby\\tone\\thidden\\tlayer\\twith\\t50\\tartificial\\tneurons,\\tand\\nfinally\\tone\\toutput\\tlayer\\twith\\t3\\tartificial\\tneurons.\\tAll\\tartificial\\tneurons\\tuse\\tthe\\tReLU\\tactivation\\nfunction.\\nThe\\tshape\\tof\\tthe\\tinput\\tmatrix\\t\\nX\\n\\tis\\t\\nm\\n\\t×\\t10,\\twhere\\t\\nm\\n\\trepresents\\tthe\\ttraining\\tbatch\\tsize.\\nThe\\tshape\\tof\\tthe\\thidden\\tlayer’s\\tweight\\tvector\\t\\nW\\nh\\n\\tis\\t10\\t×\\t50\\tand\\tthe\\tlength\\tof\\tits\\tbias\\tvector\\t\\nb\\nh\\nis\\t50.\\nThe\\tshape\\tof\\tthe\\toutput\\tlayer’s\\tweight\\tvector\\t\\nW\\no\\n\\tis\\t50\\t×\\t3,\\tand\\tthe\\tlength\\tof\\tits\\tbias\\tvector\\t\\nb\\no', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 583}), Document(page_content='is\\t3.\\nThe\\tshape\\tof\\tthe\\tnetwork’s\\toutput\\tmatrix\\t\\nY\\n\\tis\\t\\nm\\n\\t×\\t3.\\nY\\n\\t=\\tReLU(ReLU(\\nX\\n\\t·\\t\\nW\\nh\\n\\t+\\t\\nb\\nh\\n)\\t·\\t\\nW\\no\\n\\t+\\t\\nb\\no\\n).\\tRecall\\tthat\\tthe\\tReLU\\tfunction\\tjust\\tsets\\tevery\\nnegative\\tnumber\\tin\\tthe\\tmatrix\\tto\\tzero.\\tAlso\\tnote\\tthat\\twhen\\tyou\\tare\\tadding\\ta\\tbias\\tvector\\tto\\ta\\nmatrix,\\tit\\tis\\tadded\\tto\\tevery\\tsingle\\trow\\tin\\tthe\\tmatrix,\\twhich\\tis\\tcalled\\t\\nbroadcasting\\n.\\n6\\n.\\t\\nTo\\tclassify\\temail\\tinto\\tspam\\tor\\tham,\\tyou\\tjust\\tneed\\tone\\tneuron\\tin\\tthe\\toutput\\tlayer\\tof\\ta\\tneural\\tnetwork\\n—\\tfor\\texample,\\tindicating\\tthe\\tprobability\\tthat\\tthe\\temail\\tis\\tspam.\\tYou\\twould\\ttypically\\tuse\\tthe\\tlogistic\\nactivation\\tfunction\\tin\\tthe\\toutput\\tlayer\\twhen\\testimating\\ta\\tprobability.\\tIf\\tinstead\\tyou\\twant\\tto\\ttackle\\nMNIST,\\tyou\\tneed\\t10\\tneurons\\tin\\tthe\\toutput\\tlayer,\\tand\\tyou\\tmust\\treplace\\tthe\\tlogistic\\tfunction\\twith\\tthe\\nsoftmax\\tactivation\\tfunction,\\twhich\\tcan\\thandle\\tmultiple\\tclasses,\\toutputting\\tone\\tprobability\\tper\\tclass.\\nNow,\\tif\\tyou\\twant\\tyour\\tneural\\tnetwork\\tto\\tpredict\\thousing\\tprices\\tlike\\tin\\t\\nChapter\\t2\\n,\\tthen\\tyou\\tneed\\tone\\noutput\\tneuron,\\tusing\\tno\\tactivation\\tfunction\\tat\\tall\\tin\\tthe\\toutput\\tlayer.\\n4\\n7\\n.\\t\\nBackpropagation\\tis\\ta\\ttechnique\\tused\\tto\\ttrain\\tartificial\\tneural\\tnetworks.\\tIt\\tfirst\\tcomputes\\tthe\\tgradients\\nof\\tthe\\tcost\\tfunction\\twith\\tregards\\tto\\tevery\\tmodel\\tparameter\\t(all\\tthe\\tweights\\tand\\tbiases),\\tand\\tthen\\tit\\nperforms\\ta\\tGradient\\tDescent\\tstep\\tusing\\tthese\\tgradients.\\tThis\\tbackpropagation\\tstep\\tis\\ttypically\\nperformed\\tthousands\\tor\\tmillions\\tof\\ttimes,\\tusing\\tmany\\ttraining\\tbatches,\\tuntil\\tthe\\tmodel\\tparameters\\nconverge\\tto\\tvalues\\tthat\\t(hopefully)\\tminimize\\tthe\\tcost\\tfunction.\\tTo\\tcompute\\tthe\\tgradients,\\nbackpropagation\\tuses\\treverse-mode\\tautodiff\\t(although\\tit\\twasn’t\\tcalled\\tthat\\twhen\\tbackpropagation\\nwas\\tinvented,\\tand\\tit\\thas\\tbeen\\treinvented\\tseveral\\ttimes).\\tReverse-mode\\tautodiff\\tperforms\\ta\\tforward\\npass\\tthrough\\ta\\tcomputation\\tgraph,\\tcomputing\\tevery\\tnode’s\\tvalue\\tfor\\tthe\\tcurrent\\ttraining\\tbatch,\\tand\\nthen\\tit\\tperforms\\ta\\treverse\\tpass,\\tcomputing\\tall\\tthe\\tgradients\\tat\\tonce\\t(see\\t\\nAppendix\\tD\\n\\tfor\\tmore\\ndetails).\\tSo\\twhat’s\\tthe\\tdifference?\\tWell,\\tbackpropagation\\trefers\\tto\\tthe\\twhole\\tprocess\\tof\\ttraining\\tan\\nartificial\\tneural\\tnetwork\\tusing\\tmultiple\\tbackpropagation\\tsteps,\\teach\\tof\\twhich\\tcomputes\\tgradients\\nand\\tuses\\tthem\\tto\\tperform\\ta\\tGradient\\tDescent\\tstep.\\tIn\\tcontrast,\\treverse-mode\\tautodiff\\tis\\ta\\tsimply\\ta\\ntechnique\\tto\\tcompute\\tgradients\\tefficiently,\\tand\\tit\\thappens\\tto\\tbe\\tused\\tby\\tbackpropagation.\\n8\\n.\\t\\nHere\\tis\\ta\\tlist\\tof\\tall\\tthe\\thyperparameters\\tyou\\tcan\\ttweak\\tin\\ta\\tbasic\\tMLP:\\tthe\\tnumber\\tof\\thidden\\tlayers,\\nthe\\tnumber\\tof\\tneurons\\tin\\teach\\thidden\\tlayer,\\tand\\tthe\\tactivation\\tfunction\\tused\\tin\\teach\\thidden\\tlayer\\tand\\nin\\tthe\\toutput\\tlayer.\\n5\\n\\tIn\\tgeneral,\\tthe\\tReLU\\tactivation\\tfunction\\t(or\\tone\\tof\\tits\\tvariants;\\tsee\\t\\nChapter\\t11\\n)\\nis\\ta\\tgood\\tdefault\\tfor\\tthe\\thidden\\tlayers.\\tFor\\tthe\\toutput\\tlayer,\\tin\\tgeneral\\tyou\\twill\\twant\\tthe\\tlogistic\\nactivation\\tfunction\\tfor\\tbinary\\tclassification,\\tthe\\tsoftmax\\tactivation\\tfunction\\tfor\\tmulticlass\\nclassification,\\tor\\tno\\tactivation\\tfunction\\tfor\\tregression.\\t\\nIf\\tthe\\tMLP\\toverfits\\tthe\\ttraining\\tdata,\\tyou\\tcan\\ttry\\treducing\\tthe\\tnumber\\tof\\thidden\\tlayers\\tand\\treducing\\nthe\\tnumber\\tof\\tneurons\\tper\\thidden\\tlayer.\\n9\\n.\\t\\nSee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 584}), Document(page_content='Chapter\\t11\\n:\\tTraining\\tDeep\\tNeural\\tNets\\n1\\n.\\t\\nNo,\\tall\\tweights\\tshould\\tbe\\tsampled\\tindependently;\\tthey\\tshould\\tnot\\tall\\thave\\tthe\\tsame\\tinitial\\tvalue.\\nOne\\timportant\\tgoal\\tof\\tsampling\\tweights\\trandomly\\tis\\tto\\tbreak\\tsymmetries:\\tif\\tall\\tthe\\tweights\\thave\\tthe\\nsame\\tinitial\\tvalue,\\teven\\tif\\tthat\\tvalue\\tis\\tnot\\tzero,\\tthen\\tsymmetry\\tis\\tnot\\tbroken\\t(i.e.,\\tall\\tneurons\\tin\\ta\\ngiven\\tlayer\\tare\\tequivalent),\\tand\\tbackpropagation\\twill\\tbe\\tunable\\tto\\tbreak\\tit.\\tConcretely,\\tthis\\tmeans\\nthat\\tall\\tthe\\tneurons\\tin\\tany\\tgiven\\tlayer\\twill\\talways\\thave\\tthe\\tsame\\tweights.\\tIt’s\\tlike\\thaving\\tjust\\tone\\nneuron\\tper\\tlayer,\\tand\\tmuch\\tslower.\\tIt\\tis\\tvirtually\\timpossible\\tfor\\tsuch\\ta\\tconfiguration\\tto\\tconverge\\tto\\na\\tgood\\tsolution.\\n2\\n.\\t\\nIt\\tis\\tperfectly\\tfine\\tto\\tinitialize\\tthe\\tbias\\tterms\\tto\\tzero.\\tSome\\tpeople\\tlike\\tto\\tinitialize\\tthem\\tjust\\tlike\\nweights,\\tand\\tthat’s\\tokay\\ttoo;\\tit\\tdoes\\tnot\\tmake\\tmuch\\tdifference.\\n3\\n.\\t\\nA\\tfew\\tadvantages\\tof\\tthe\\tELU\\tfunction\\tover\\tthe\\tReLU\\tfunction\\tare:\\nIt\\tcan\\ttake\\ton\\tnegative\\tvalues,\\tso\\tthe\\taverage\\toutput\\tof\\tthe\\tneurons\\tin\\tany\\tgiven\\tlayer\\tis\\ntypically\\tcloser\\tto\\t0\\tthan\\twhen\\tusing\\tthe\\tReLU\\tactivation\\tfunction\\t(which\\tnever\\toutputs\\nnegative\\tvalues).\\tThis\\thelps\\talleviate\\tthe\\tvanishing\\tgradients\\tproblem.\\nIt\\talways\\thas\\ta\\tnonzero\\tderivative,\\twhich\\tavoids\\tthe\\tdying\\tunits\\tissue\\tthat\\tcan\\taffect\\tReLU\\nunits.\\nIt\\tis\\tsmooth\\teverywhere,\\twhereas\\tthe\\tReLU’s\\tslope\\tabruptly\\tjumps\\tfrom\\t0\\tto\\t1\\tat\\t\\nz\\n\\t=\\t0.\\tSuch\\tan\\nabrupt\\tchange\\tcan\\tslow\\tdown\\tGradient\\tDescent\\tbecause\\tit\\twill\\tbounce\\taround\\t\\nz\\n\\t=\\t0.\\n4\\n.\\t\\nThe\\tELU\\tactivation\\tfunction\\tis\\ta\\tgood\\tdefault.\\tIf\\tyou\\tneed\\tthe\\tneural\\tnetwork\\tto\\tbe\\tas\\tfast\\tas\\npossible,\\tyou\\tcan\\tuse\\tone\\tof\\tthe\\tleaky\\tReLU\\tvariants\\tinstead\\t(e.g.,\\ta\\tsimple\\tleaky\\tReLU\\tusing\\tthe\\ndefault\\thyperparameter\\tvalue).\\tThe\\tsimplicity\\tof\\tthe\\tReLU\\tactivation\\tfunction\\tmakes\\tit\\tmany\\npeople’s\\tpreferred\\toption,\\tdespite\\tthe\\tfact\\tthat\\tthey\\tare\\tgenerally\\toutperformed\\tby\\tthe\\tELU\\tand\\tleaky\\nReLU.\\tHowever,\\tthe\\tReLU\\tactivation\\tfunction’s\\tcapability\\tof\\toutputting\\tprecisely\\tzero\\tcan\\tbe\\tuseful\\nin\\tsome\\tcases\\t(e.g.,\\tsee\\t\\nChapter\\t15\\n).\\tThe\\thyperbolic\\ttangent\\t(tanh)\\tcan\\tbe\\tuseful\\tin\\tthe\\toutput\\tlayer\\tif\\nyou\\tneed\\tto\\toutput\\ta\\tnumber\\tbetween\\t–1\\tand\\t1,\\tbut\\tnowadays\\tit\\tis\\tnot\\tused\\tmuch\\tin\\thidden\\tlayers.\\nThe\\tlogistic\\tactivation\\tfunction\\tis\\talso\\tuseful\\tin\\tthe\\toutput\\tlayer\\twhen\\tyou\\tneed\\tto\\testimate\\ta\\nprobability\\t(e.g.,\\tfor\\tbinary\\tclassification),\\tbut\\tit\\tis\\talso\\trarely\\tused\\tin\\thidden\\tlayers\\t(there\\tare\\nexceptions\\t—\\tfor\\texample,\\tfor\\tthe\\tcoding\\tlayer\\tof\\tvariational\\tautoencoders;\\tsee\\t\\nChapter\\t15\\n).\\nFinally,\\tthe\\tsoftmax\\tactivation\\tfunction\\tis\\tuseful\\tin\\tthe\\toutput\\tlayer\\tto\\toutput\\tprobabilities\\tfor\\nmutually\\texclusive\\tclasses,\\tbut\\tother\\tthan\\tthat\\tit\\tis\\trarely\\t(if\\tever)\\tused\\tin\\thidden\\tlayers.\\n5\\n.\\t\\nIf\\tyou\\tset\\tthe\\t\\nmomentum\\n\\thyperparameter\\ttoo\\tclose\\tto\\t1\\t(e.g.,\\t0.99999)\\twhen\\tusing\\ta\\nMomentumOptimizer\\n,\\tthen\\tthe\\talgorithm\\twill\\tlikely\\tpick\\tup\\ta\\tlot\\tof\\tspeed,\\thopefully\\troughly\\ttoward\\nthe\\tglobal\\tminimum,\\tbut\\tthen\\tit\\twill\\tshoot\\tright\\tpast\\tthe\\tminimum,\\tdue\\tto\\tits\\t\\nmomentum.\\tThen\\tit\\twill\\nslow\\tdown\\tand\\tcome\\tback,\\taccelerate\\tagain,\\tovershoot\\tagain,\\tand\\tso\\ton.\\tIt\\tmay\\toscillate\\tthis\\tway\\nmany\\ttimes\\tbefore\\tconverging,\\tso\\toverall\\tit\\twill\\ttake\\tmuch\\tlonger\\tto\\tconverge\\tthan\\twith\\ta\\tsmaller\\nmomentum\\n\\tvalue.\\n6\\n.\\t\\nOne\\tway\\tto\\tproduce\\ta\\tsparse\\tmodel\\t(i.e.,\\twith\\tmost\\tweights\\tequal\\tto\\tzero)\\tis\\tto\\ttrain\\tthe\\tmodel\\nnormally,\\tthen\\tzero\\tout\\ttiny\\tweights.\\tFor\\tmore\\tsparsity,\\tyou\\tcan\\tapply\\tℓ\\n1\\n\\tregularization\\tduring', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 585}), Document(page_content='training,\\twhich\\tpushes\\tthe\\toptimizer\\ttoward\\tsparsity.\\tA\\tthird\\toption\\tis\\tto\\tcombine\\tℓ\\n1\\n\\tregularization\\nwith\\t\\ndual\\taveraging\\n,\\tusing\\tTensorFlow’s\\t\\nFTRLOptimizer\\n\\tclass.\\n7\\n.\\t\\nYes,\\tdropout\\tdoes\\tslow\\tdown\\ttraining,\\tin\\tgeneral\\troughly\\tby\\ta\\tfactor\\tof\\ttwo.\\tHowever,\\tit\\thas\\tno\\nimpact\\ton\\tinference\\tsince\\tit\\tis\\tonly\\tturned\\ton\\tduring\\ttraining.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 586}), Document(page_content='Chapter\\t12\\n:\\tDistributing\\tTensorFlow\\tAcross\\tDevices\\tand\\tServers\\n1\\n.\\t\\nWhen\\ta\\tTensorFlow\\tprocess\\tstarts,\\tit\\tgrabs\\tall\\tthe\\tavailable\\tmemory\\ton\\tall\\tGPU\\tdevices\\tthat\\tare\\nvisible\\tto\\tit,\\tso\\tif\\tyou\\tget\\ta\\t\\nCUDA_ERROR_OUT_OF_MEMORY\\n\\twhen\\tstarting\\tyour\\tTensorFlow\\tprogram,\\nit\\tprobably\\tmeans\\tthat\\tother\\tprocesses\\tare\\trunning\\tthat\\thave\\talready\\tgrabbed\\tall\\tthe\\tmemory\\ton\\tat\\nleast\\tone\\tvisible\\tGPU\\tdevice\\t(most\\tlikely\\tit\\tis\\tanother\\tTensorFlow\\tprocess).\\tTo\\tfix\\tthis\\tproblem,\\ta\\ntrivial\\tsolution\\tis\\tto\\tstop\\tthe\\tother\\tprocesses\\tand\\ttry\\tagain.\\tHowever,\\tif\\tyou\\tneed\\tall\\tprocesses\\tto\\trun\\nsimultaneously,\\ta\\tsimple\\toption\\tis\\tto\\tdedicate\\tdifferent\\tdevices\\tto\\teach\\tprocess,\\tby\\tsetting\\tthe\\nCUDA_VISIBLE_DEVICES\\n\\tenvironment\\tvariable\\tappropriately\\tfor\\teach\\tdevice.\\tAnother\\toption\\tis\\tto\\nconfigure\\t\\nTensorFlow\\tto\\tgrab\\tonly\\tpart\\tof\\tthe\\tGPU\\tmemory,\\tinstead\\tof\\tall\\tof\\tit,\\tby\\tcreating\\ta\\nConfigProto\\n,\\tsetting\\tits\\t\\ngpu_options.per_process_gpu_memory_fraction\\n\\tto\\tthe\\tproportion\\tof\\nthe\\ttotal\\tmemory\\tthat\\tit\\tshould\\tgrab\\t(e.g.,\\t0.4),\\tand\\tusing\\tthis\\t\\nConfigProto\\n\\twhen\\topening\\ta\\tsession.\\nThe\\tlast\\toption\\tis\\tto\\ttell\\tTensorFlow\\tto\\tgrab\\tmemory\\tonly\\twhen\\tit\\tneeds\\tit\\tby\\tsetting\\tthe\\ngpu_options.allow_growth\\n\\tto\\t\\nTrue\\n.\\tHowever,\\tthis\\tlast\\toption\\tis\\tusually\\tnot\\trecommended\\nbecause\\tany\\tmemory\\tthat\\tTensorFlow\\tgrabs\\tis\\tnever\\treleased,\\tand\\tit\\tis\\tharder\\tto\\tguarantee\\ta\\nrepeatable\\tbehavior\\t(there\\tmay\\tbe\\trace\\tconditions\\tdepending\\ton\\twhich\\tprocesses\\tstart\\tfirst,\\thow\\nmuch\\tmemory\\tthey\\tneed\\tduring\\ttraining,\\tand\\tso\\ton).\\n2\\n.\\t\\nBy\\tpinning\\tan\\toperation\\ton\\ta\\tdevice,\\tyou\\tare\\ttelling\\tTensorFlow\\tthat\\tthis\\tis\\twhere\\tyou\\twould\\tlike\\nthis\\toperation\\tto\\tbe\\tplaced.\\tHowever,\\tsome\\tconstraints\\tmay\\tprevent\\tTensorFlow\\tfrom\\thonoring\\tyour\\nrequest.\\tFor\\texample,\\tthe\\toperation\\tmay\\thave\\tno\\timplementation\\t(called\\ta\\t\\nkernel\\n)\\tfor\\tthat\\tparticular\\ntype\\tof\\tdevice.\\tIn\\tthis\\tcase,\\tTensorFlow\\twill\\traise\\tan\\texception\\tby\\tdefault,\\tbut\\tyou\\tcan\\tconfigure\\tit\\nto\\tfall\\tback\\tto\\tthe\\tCPU\\tinstead\\t(this\\tis\\tcalled\\t\\nsoft\\tplacement\\n).\\tAnother\\texample\\tis\\tan\\toperation\\tthat\\ncan\\tmodify\\ta\\tvariable;\\tthis\\toperation\\tand\\tthe\\tvariable\\tneed\\tto\\tbe\\tcollocated.\\tSo\\tthe\\tdifference\\nbetween\\tpinning\\tan\\toperation\\tand\\tplacing\\tan\\toperation\\tis\\tthat\\tpinning\\tis\\twhat\\tyou\\task\\tTensorFlow\\n(“Please\\tplace\\tthis\\toperation\\ton\\tGPU\\t#1”)\\twhile\\tplacement\\tis\\twhat\\tTensorFlow\\tactually\\tends\\tup\\ndoing\\t(“Sorry,\\tfalling\\tback\\tto\\tthe\\tCPU”).\\n3\\n.\\t\\nIf\\tyou\\tare\\trunning\\ton\\ta\\tGPU-enabled\\tTensorFlow\\tinstallation,\\tand\\tyou\\tjust\\tuse\\tthe\\tdefault\\tplacement,\\nthen\\tif\\tall\\toperations\\thave\\ta\\tGPU\\tkernel\\t(i.e.,\\ta\\tGPU\\timplementation),\\tyes,\\tthey\\twill\\tall\\tbe\\tplaced\\ton\\nthe\\tfirst\\tGPU.\\tHowever,\\tif\\tone\\tor\\tmore\\toperations\\tdo\\tnot\\thave\\ta\\tGPU\\tkernel,\\tthen\\tby\\tdefault\\nTensorFlow\\twill\\traise\\tan\\texception.\\tIf\\tyou\\tconfigure\\tTensorFlow\\tto\\tfall\\tback\\tto\\tthe\\tCPU\\tinstead\\n(soft\\tplacement),\\tthen\\tall\\toperations\\twill\\tbe\\tplaced\\ton\\tthe\\tfirst\\tGPU\\texcept\\tthe\\tones\\twithout\\ta\\tGPU\\nkernel\\tand\\tall\\tthe\\toperations\\tthat\\tmust\\tbe\\tcollocated\\twith\\tthem\\t(see\\tthe\\tanswer\\tto\\tthe\\tprevious\\nexercise).\\n4\\n.\\t\\nYes,\\tif\\tyou\\tpin\\ta\\tvariable\\tto\\t\\n\"/gpu:0\"\\n,\\tit\\tcan\\tbe\\tused\\tby\\toperations\\tplaced\\ton\\t\\n/gpu:1\\n.\\tTensorFlow\\nwill\\tautomatically\\ttake\\tcare\\tof\\tadding\\tthe\\tappropriate\\toperations\\tto\\ttransfer\\tthe\\tvariable’s\\tvalue\\nacross\\tdevices.\\tThe\\tsame\\tgoes\\tfor\\tdevices\\tlocated\\ton\\tdifferent\\tservers\\t(as\\tlong\\tas\\tthey\\tare\\tpart\\tof\\nthe\\tsame\\tcluster).\\n5\\n.\\t\\nYes,\\ttwo\\toperations\\tplaced\\ton\\tthe\\tsame\\tdevice\\tcan\\trun\\tin\\tparallel:\\tTensorFlow\\tautomatically\\ttakes\\ncare\\tof\\trunning\\toperations\\tin\\tparallel\\t(on\\tdifferent\\tCPU\\tcores\\tor\\tdifferent\\tGPU\\tthreads),\\tas\\tlong\\tas\\nno\\toperation\\tdepends\\ton\\tanother\\toperation’s\\toutput.\\tMoreover,\\tyou\\tcan\\tstart\\tmultiple\\tsessions\\tin\\nparallel\\tthreads\\t(or\\tprocesses),\\tand\\tevaluate\\toperations\\tin\\teach\\tthread.\\tSince\\tsessions\\tare\\nindependent,\\tTensorFlow\\twill\\tbe\\table\\tto\\tevaluate\\tany\\toperation\\tfrom\\tone\\tsession\\tin\\tparallel\\twith', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 587}), Document(page_content='any\\toperation\\tfrom\\tanother\\tsession.\\n6\\n.\\t\\nControl\\tdependencies\\tare\\tused\\twhen\\tyou\\twant\\tto\\tpostpone\\tthe\\tevaluation\\tof\\tan\\toperation\\tX\\tuntil\\nafter\\tsome\\tother\\toperations\\tare\\trun,\\teven\\tthough\\tthese\\toperations\\tare\\tnot\\trequired\\tto\\tcompute\\tX.\\nThis\\tis\\tuseful\\tin\\tparticular\\twhen\\tX\\twould\\toccupy\\ta\\tlot\\tof\\tmemory\\tand\\tyou\\tonly\\tneed\\tit\\tlater\\tin\\tthe\\ncomputation\\tgraph,\\tor\\tif\\tX\\tuses\\tup\\ta\\tlot\\tof\\tI/O\\t(for\\texample,\\tit\\trequires\\ta\\tlarge\\tvariable\\tvalue\\nlocated\\ton\\ta\\tdifferent\\tdevice\\tor\\tserver)\\tand\\tyou\\tdon’t\\twant\\tit\\tto\\trun\\tat\\tthe\\tsame\\ttime\\tas\\tother\\tI/O-\\nhungry\\toperations,\\tto\\tavoid\\tsaturating\\tthe\\tbandwidth.\\n7\\n.\\t\\nYou’re\\tin\\tluck!\\tIn\\tdistributed\\tTensorFlow,\\tthe\\tvariable\\tvalues\\tlive\\tin\\tcontainers\\tmanaged\\tby\\tthe\\ncluster,\\tso\\teven\\tif\\tyou\\tclose\\tthe\\tsession\\tand\\texit\\tthe\\tclient\\tprogram,\\tthe\\tmodel\\tparameters\\tare\\tstill\\nalive\\tand\\twell\\ton\\tthe\\tcluster.\\tYou\\tsimply\\tneed\\tto\\topen\\ta\\tnew\\tsession\\tto\\tthe\\tcluster\\tand\\tsave\\tthe\\nmodel\\t(make\\tsure\\tyou\\tdon’t\\tcall\\tthe\\tvariable\\tinitializers\\tor\\trestore\\ta\\tprevious\\tmodel,\\tas\\tthis\\twould\\ndestroy\\tyour\\tprecious\\tnew\\tmodel!).\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 588}), Document(page_content='Chapter\\t13\\n:\\tConvolutional\\tNeural\\tNetworks\\n1\\n.\\t\\nThese\\tare\\tthe\\tmain\\tadvantages\\tof\\ta\\tCNN\\tover\\ta\\tfully\\tconnected\\tDNN\\tfor\\timage\\tclassification:\\nBecause\\tconsecutive\\tlayers\\tare\\tonly\\tpartially\\tconnected\\tand\\tbecause\\tit\\theavily\\treuses\\tits\\nweights,\\ta\\tCNN\\thas\\tmany\\tfewer\\tparameters\\tthan\\ta\\tfully\\tconnected\\tDNN,\\twhich\\tmakes\\tit\\tmuch\\nfaster\\tto\\ttrain,\\treduces\\tthe\\trisk\\tof\\toverfitting,\\tand\\trequires\\tmuch\\tless\\ttraining\\tdata.\\nWhen\\ta\\tCNN\\thas\\tlearned\\ta\\tkernel\\tthat\\tcan\\tdetect\\ta\\tparticular\\tfeature,\\tit\\tcan\\tdetect\\tthat\\tfeature\\nanywhere\\ton\\tthe\\timage.\\tIn\\tcontrast,\\twhen\\ta\\tDNN\\tlearns\\ta\\tfeature\\tin\\tone\\tlocation,\\tit\\tcan\\tdetect\\tit\\nonly\\tin\\tthat\\tparticular\\tlocation.\\tSince\\timages\\ttypically\\thave\\tvery\\trepetitive\\tfeatures,\\tCNNs\\tare\\nable\\tto\\tgeneralize\\tmuch\\tbetter\\tthan\\tDNNs\\tfor\\timage\\tprocessing\\ttasks\\tsuch\\tas\\tclassification,\\nusing\\tfewer\\ttraining\\texamples.\\nFinally,\\ta\\tDNN\\thas\\tno\\tprior\\tknowledge\\tof\\thow\\tpixels\\tare\\torganized;\\tit\\tdoes\\tnot\\tknow\\tthat\\nnearby\\tpixels\\tare\\tclose.\\tA\\tCNN’s\\tarchitecture\\tembeds\\tthis\\tprior\\tknowledge.\\tLower\\tlayers\\ntypically\\tidentify\\tfeatures\\tin\\tsmall\\tareas\\tof\\tthe\\timages,\\twhile\\thigher\\tlayers\\tcombine\\tthe\\tlower-\\nlevel\\tfeatures\\tinto\\tlarger\\tfeatures.\\tThis\\tworks\\twell\\twith\\tmost\\tnatural\\timages,\\tgiving\\tCNNs\\ta\\ndecisive\\thead\\tstart\\tcompared\\tto\\tDNNs.\\n2\\n.\\t\\nLet’s\\tcompute\\thow\\tmany\\tparameters\\tthe\\tCNN\\thas.\\tSince\\tits\\tfirst\\tconvolutional\\tlayer\\thas\\t3\\t×\\t3\\nkernels,\\tand\\tthe\\tinput\\thas\\tthree\\tchannels\\t(red,\\tgreen,\\tand\\tblue),\\tthen\\teach\\tfeature\\tmap\\thas\\t3\\t×\\t3\\t×\\t3\\nweights,\\tplus\\ta\\tbias\\tterm.\\tThat’s\\t28\\tparameters\\tper\\tfeature\\tmap.\\tSince\\tthis\\tfirst\\tconvolutional\\tlayer\\nhas\\t100\\tfeature\\tmaps,\\tit\\thas\\ta\\ttotal\\tof\\t2,800\\tparameters.\\tThe\\tsecond\\tconvolutional\\tlayer\\thas\\t3\\t×\\t3\\nkernels,\\tand\\tits\\tinput\\tis\\tthe\\tset\\tof\\t100\\tfeature\\tmaps\\tof\\tthe\\tprevious\\tlayer,\\tso\\teach\\tfeature\\tmap\\thas\\t3\\t×\\n3\\t×\\t100\\t=\\t900\\tweights,\\tplus\\ta\\tbias\\tterm.\\tSince\\tit\\thas\\t200\\tfeature\\tmaps,\\tthis\\tlayer\\thas\\t901\\t×\\t200\\t=\\n180,200\\tparameters.\\tFinally,\\tthe\\tthird\\tand\\tlast\\tconvolutional\\tlayer\\talso\\thas\\t3\\t×\\t3\\tkernels,\\tand\\tits\\ninput\\tis\\tthe\\tset\\tof\\t200\\tfeature\\tmaps\\tof\\tthe\\tprevious\\tlayers,\\tso\\teach\\tfeature\\tmap\\thas\\t3\\t×\\t3\\t×\\t200\\t=\\n1,800\\tweights,\\tplus\\ta\\tbias\\tterm.\\tSince\\tit\\thas\\t400\\tfeature\\tmaps,\\tthis\\tlayer\\thas\\ta\\ttotal\\tof\\t1,801\\t×\\t400\\t=\\n720,400\\tparameters.\\tAll\\tin\\tall,\\tthe\\tCNN\\thas\\t2,800\\t+\\t180,200\\t+\\t720,400\\t=\\t\\n903,400\\tparameters.\\t\\nNow\\tlet’s\\tcompute\\thow\\tmuch\\tRAM\\tthis\\tneural\\tnetwork\\twill\\trequire\\t(at\\tleast)\\twhen\\tmaking\\ta\\nprediction\\tfor\\ta\\tsingle\\tinstance.\\tFirst\\tlet’s\\tcompute\\tthe\\tfeature\\tmap\\tsize\\tfor\\teach\\tlayer.\\tSince\\twe\\tare\\nusing\\ta\\tstride\\tof\\t2\\tand\\tSAME\\tpadding,\\tthe\\thorizontal\\tand\\tvertical\\tsize\\tof\\tthe\\tfeature\\tmaps\\tare\\ndivided\\tby\\t2\\tat\\teach\\tlayer\\t(rounding\\tup\\tif\\tnecessary),\\tso\\tas\\tthe\\tinput\\tchannels\\tare\\t200\\t×\\t300\\tpixels,\\nthe\\tfirst\\tlayer’s\\tfeature\\tmaps\\tare\\t100\\t×\\t150,\\tthe\\tsecond\\tlayer’s\\tfeature\\tmaps\\tare\\t50\\t×\\t75,\\tand\\tthe\\nthird\\tlayer’s\\tfeature\\tmaps\\tare\\t25\\t×\\t38.\\tSince\\t32\\tbits\\tis\\t4\\tbytes\\tand\\tthe\\tfirst\\tconvolutional\\tlayer\\thas\\n100\\tfeature\\tmaps,\\tthis\\tfirst\\tlayer\\ttakes\\tup\\t4\\tx\\t100\\t×\\t150\\t×\\t100\\t=\\t6\\tmillion\\tbytes\\t(about\\t5.7\\tMB,\\nconsidering\\tthat\\t1\\tMB\\t=\\t1,024\\tKB\\tand\\t1\\tKB\\t=\\t1,024\\tbytes).\\tThe\\tsecond\\tlayer\\ttakes\\tup\\t4\\t×\\t50\\t×\\t75\\n×\\t200\\t=\\t3\\tmillion\\tbytes\\t(about\\t2.9\\tMB).\\tFinally,\\tthe\\tthird\\tlayer\\ttakes\\tup\\t4\\t×\\t25\\t×\\t38\\t×\\t400\\t=\\n1,520,000\\tbytes\\t(about\\t1.4\\tMB).\\tHowever,\\tonce\\ta\\tlayer\\thas\\tbeen\\tcomputed,\\tthe\\tmemory\\toccupied\\nby\\tthe\\tprevious\\tlayer\\tcan\\tbe\\treleased,\\tso\\tif\\teverything\\tis\\twell\\toptimized,\\tonly\\t6\\t+\\t9\\t=\\t15\\tmillion\\nbytes\\t(about\\t14.3\\tMB)\\tof\\tRAM\\twill\\tbe\\trequired\\t(when\\tthe\\tsecond\\tlayer\\thas\\tjust\\tbeen\\tcomputed,\\tbut\\nthe\\tmemory\\toccupied\\tby\\tthe\\tfirst\\tlayer\\tis\\tnot\\treleased\\tyet).\\tBut\\twait,\\tyou\\talso\\tneed\\tto\\tadd\\tthe\\nmemory\\toccupied\\tby\\tthe\\tCNN’s\\tparameters.\\tWe\\tcomputed\\tearlier\\tthat\\tit\\thas\\t903,400\\tparameters,\\neach\\tusing\\tup\\t4\\tbytes,\\tso\\tthis\\tadds\\t3,613,600\\tbytes\\t(about\\t3.4\\tMB).\\tThe\\ttotal\\tRAM\\trequired\\tis\\t(at\\nleast)\\t18,613,600\\tbytes\\t(about\\t17.8\\tMB).\\t', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 589}), Document(page_content='Lastly,\\tlet’s\\tcompute\\tthe\\tminimum\\tamount\\tof\\tRAM\\trequired\\twhen\\ttraining\\tthe\\tCNN\\ton\\ta\\tmini-batch\\nof\\t50\\timages.\\tDuring\\ttraining\\tTensorFlow\\tuses\\tbackpropagation,\\twhich\\trequires\\tkeeping\\tall\\tvalues\\ncomputed\\tduring\\tthe\\tforward\\tpass\\tuntil\\tthe\\treverse\\tpass\\tbegins.\\tSo\\twe\\tmust\\tcompute\\tthe\\ttotal\\tRAM\\nrequired\\tby\\tall\\tlayers\\tfor\\ta\\tsingle\\tinstance\\tand\\tmultiply\\tthat\\tby\\t50!\\tAt\\tthat\\tpoint\\tlet’s\\tstart\\tcounting\\tin\\nmegabytes\\trather\\tthan\\tbytes.\\tWe\\tcomputed\\tbefore\\tthat\\tthe\\tthree\\tlayers\\trequire\\trespectively\\t5.7,\\t2.9,\\nand\\t1.4\\tMB\\tfor\\teach\\tinstance.\\tThat’s\\ta\\ttotal\\tof\\t10.0\\tMB\\tper\\tinstance.\\tSo\\tfor\\t50\\tinstances\\tthe\\ttotal\\nRAM\\tis\\t500\\tMB.\\tAdd\\tto\\tthat\\tthe\\tRAM\\trequired\\tby\\tthe\\tinput\\timages,\\twhich\\tis\\t50\\t×\\t4\\t×\\t200\\t×\\t300\\t×\\n3\\t=\\t36\\tmillion\\tbytes\\t(about\\t34.3\\tMB),\\tplus\\tthe\\tRAM\\trequired\\tfor\\tthe\\tmodel\\tparameters,\\twhich\\tis\\nabout\\t3.4\\tMB\\t(computed\\tearlier),\\tplus\\tsome\\tRAM\\tfor\\tthe\\tgradients\\t(we\\twill\\tneglect\\tthem\\tsince\\tthey\\ncan\\tbe\\treleased\\tgradually\\tas\\tbackpropagation\\tgoes\\tdown\\tthe\\tlayers\\tduring\\tthe\\treverse\\tpass).\\tWe\\tare\\nup\\tto\\ta\\ttotal\\tof\\troughly\\t500.0\\t+\\t34.3\\t+\\t3.4\\t=\\t537.7\\tMB.\\tAnd\\tthat’s\\treally\\tan\\toptimistic\\tbare\\nminimum.\\n3\\n.\\t\\nIf\\tyour\\tGPU\\truns\\tout\\tof\\tmemory\\twhile\\ttraining\\ta\\tCNN,\\there\\tare\\tfive\\tthings\\tyou\\tcould\\ttry\\tto\\tsolve\\tthe\\nproblem\\t(other\\tthan\\tpurchasing\\ta\\tGPU\\twith\\tmore\\tRAM):\\nReduce\\tthe\\tmini-batch\\tsize.\\nReduce\\tdimensionality\\tusing\\ta\\tlarger\\tstride\\tin\\tone\\tor\\tmore\\tlayers.\\nRemove\\tone\\tor\\tmore\\tlayers.\\nUse\\t16-bit\\tfloats\\tinstead\\tof\\t32-bit\\tfloats.\\nDistribute\\tthe\\tCNN\\tacross\\tmultiple\\tdevices.\\n4\\n.\\t\\nA\\tmax\\tpooling\\tlayer\\thas\\tno\\tparameters\\tat\\tall,\\twhereas\\ta\\tconvolutional\\tlayer\\thas\\tquite\\ta\\tfew\\t(see\\tthe\\nprevious\\tquestions).\\n5\\n.\\t\\nA\\t\\nlocal\\tresponse\\tnormalization\\n\\tlayer\\tmakes\\tthe\\tneurons\\tthat\\tmost\\tstrongly\\tactivate\\tinhibit\\tneurons\\nat\\tthe\\tsame\\tlocation\\tbut\\tin\\tneighboring\\tfeature\\tmaps,\\twhich\\tencourages\\tdifferent\\tfeature\\tmaps\\tto\\nspecialize\\tand\\tpushes\\tthem\\tapart,\\tforcing\\tthem\\tto\\texplore\\ta\\twider\\trange\\tof\\tfeatures.\\tIt\\tis\\ttypically\\nused\\tin\\tthe\\tlower\\tlayers\\tto\\thave\\ta\\tlarger\\tpool\\tof\\tlow-level\\tfeatures\\tthat\\tthe\\tupper\\tlayers\\tcan\\tbuild\\nupon.\\n6\\n.\\t\\nThe\\tmain\\tinnovations\\tin\\tAlexNet\\tcompared\\tto\\tLeNet-5\\tare\\t(1)\\tit\\tis\\tmuch\\tlarger\\tand\\tdeeper,\\tand\\t(2)\\nit\\tstacks\\tconvolutional\\tlayers\\tdirectly\\ton\\ttop\\tof\\teach\\tother,\\tinstead\\tof\\tstacking\\ta\\tpooling\\tlayer\\ton\\ttop\\nof\\teach\\tconvolutional\\tlayer.\\tThe\\tmain\\tinnovation\\tin\\tGoogLeNet\\tis\\tthe\\tintroduction\\tof\\t\\ninception\\nmodules\\n,\\twhich\\tmake\\tit\\tpossible\\tto\\thave\\ta\\tmuch\\tdeeper\\tnet\\tthan\\tprevious\\tCNN\\tarchitectures,\\twith\\nfewer\\tparameters.\\tFinally,\\tResNet’s\\tmain\\tinnovation\\tis\\tthe\\tintroduction\\tof\\tskip\\tconnections,\\twhich\\nmake\\tit\\tpossible\\tto\\tgo\\twell\\tbeyond\\t100\\tlayers.\\tArguably,\\tits\\tsimplicity\\tand\\tconsistency\\tare\\talso\\nrather\\tinnovative.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t7,\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 590}), Document(page_content='Chapter\\t14\\n:\\tRecurrent\\tNeural\\tNetworks\\n1\\n.\\t\\nHere\\tare\\ta\\tfew\\tRNN\\tapplications:\\nFor\\ta\\tsequence-to-sequence\\tRNN:\\tpredicting\\tthe\\tweather\\t(or\\tany\\tother\\ttime\\tseries),\\tmachine\\ntranslation\\t(using\\tan\\tencoder–decoder\\tarchitecture),\\tvideo\\tcaptioning,\\tspeech\\tto\\ttext,\\tmusic\\ngeneration\\t(or\\tother\\tsequence\\tgeneration),\\tidentifying\\tthe\\tchords\\tof\\ta\\tsong.\\nFor\\ta\\tsequence-to-vector\\tRNN:\\tclassifying\\tmusic\\tsamples\\tby\\tmusic\\tgenre,\\tanalyzing\\tthe\\nsentiment\\tof\\ta\\tbook\\treview,\\tpredicting\\twhat\\tword\\tan\\taphasic\\tpatient\\tis\\tthinking\\tof\\tbased\\ton\\nreadings\\tfrom\\tbrain\\timplants,\\tpredicting\\tthe\\tprobability\\tthat\\ta\\tuser\\twill\\twant\\tto\\twatch\\ta\\tmovie\\nbased\\ton\\ther\\twatch\\thistory\\t(this\\tis\\tone\\tof\\tmany\\tpossible\\timplementations\\tof\\t\\ncollaborative\\nfiltering\\n).\\nFor\\ta\\tvector-to-sequence\\tRNN:\\timage\\tcaptioning,\\tcreating\\ta\\tmusic\\tplaylist\\tbased\\ton\\tan\\nembedding\\tof\\tthe\\tcurrent\\tartist,\\tgenerating\\ta\\tmelody\\tbased\\ton\\ta\\tset\\tof\\tparameters,\\tlocating\\npedestrians\\tin\\ta\\tpicture\\t(e.g.,\\ta\\tvideo\\tframe\\tfrom\\ta\\tself-driving\\tcar’s\\tcamera).\\n2\\n.\\t\\nIn\\tgeneral,\\tif\\tyou\\ttranslate\\ta\\tsentence\\tone\\tword\\tat\\ta\\ttime,\\tthe\\tresult\\twill\\tbe\\tterrible.\\tFor\\texample,\\tthe\\nFrench\\tsentence\\t“Je\\tvous\\ten\\tprie”\\tmeans\\t“You\\tare\\twelcome,”\\tbut\\tif\\tyou\\ttranslate\\tit\\tone\\tword\\tat\\ta\\ntime,\\tyou\\tget\\t“I\\tyou\\tin\\tpray.”\\tHuh?\\tIt\\tis\\tmuch\\tbetter\\tto\\tread\\tthe\\twhole\\tsentence\\tfirst\\tand\\tthen\\ttranslate\\nit.\\tA\\tplain\\tsequence-to-sequence\\tRNN\\twould\\tstart\\ttranslating\\ta\\tsentence\\timmediately\\tafter\\treading\\nthe\\tfirst\\tword,\\twhile\\tan\\tencoder–decoder\\tRNN\\twill\\tfirst\\tread\\tthe\\twhole\\tsentence\\tand\\tthen\\ttranslate\\nit.\\tThat\\tsaid,\\tone\\tcould\\timagine\\ta\\tplain\\tsequence-to-sequence\\tRNN\\tthat\\twould\\toutput\\tsilence\\nwhenever\\tit\\tis\\tunsure\\tabout\\twhat\\tto\\tsay\\tnext\\t(just\\tlike\\thuman\\ttranslators\\tdo\\twhen\\tthey\\tmust\\ttranslate\\na\\tlive\\tbroadcast).\\n3\\n.\\t\\nTo\\tclassify\\tvideos\\tbased\\ton\\tthe\\tvisual\\tcontent,\\tone\\tpossible\\tarchitecture\\tcould\\tbe\\tto\\ttake\\t(say)\\tone\\nframe\\tper\\tsecond,\\tthen\\trun\\teach\\tframe\\tthrough\\ta\\tconvolutional\\tneural\\tnetwork,\\tfeed\\tthe\\toutput\\tof\\tthe\\nCNN\\tto\\ta\\tsequence-to-vector\\tRNN,\\tand\\tfinally\\trun\\tits\\toutput\\tthrough\\ta\\tsoftmax\\tlayer,\\tgiving\\tyou\\tall\\nthe\\tclass\\tprobabilities.\\tFor\\ttraining\\tyou\\twould\\tjust\\tuse\\tcross\\tentropy\\tas\\tthe\\tcost\\tfunction.\\tIf\\tyou\\nwanted\\tto\\tuse\\tthe\\taudio\\tfor\\tclassification\\tas\\twell,\\tyou\\tcould\\tconvert\\tevery\\tsecond\\tof\\taudio\\tto\\ta\\nspectrograph,\\tfeed\\tthis\\tspectrograph\\tto\\ta\\tCNN,\\tand\\tfeed\\tthe\\toutput\\tof\\tthis\\tCNN\\tto\\tthe\\tRNN\\t(along\\nwith\\tthe\\tcorresponding\\toutput\\tof\\tthe\\tother\\tCNN).\\n4\\n.\\t\\nBuilding\\tan\\tRNN\\tusing\\t\\ndynamic_rnn()\\n\\trather\\tthan\\t\\nstatic_rnn()\\n\\toffers\\tseveral\\t\\nadvantages:\\nIt\\tis\\tbased\\ton\\ta\\t\\nwhile_loop()\\n\\toperation\\tthat\\tis\\table\\tto\\tswap\\tthe\\tGPU’s\\tmemory\\tto\\tthe\\tCPU’s\\nmemory\\tduring\\tbackpropagation,\\tavoiding\\tout-of-memory\\terrors.\\nIt\\tis\\targuably\\teasier\\tto\\tuse,\\tas\\tit\\tcan\\tdirectly\\ttake\\ta\\tsingle\\ttensor\\tas\\tinput\\tand\\toutput\\t(covering\\nall\\ttime\\tsteps),\\trather\\tthan\\ta\\tlist\\tof\\ttensors\\t(one\\tper\\ttime\\tstep).\\tNo\\tneed\\tto\\t\\nstack,\\tunstack,\\tor\\ntranspose.\\nIt\\tgenerates\\ta\\tsmaller\\tgraph,\\teasier\\tto\\tvisualize\\tin\\tTensorBoard.\\n5\\n.\\t\\nTo\\thandle\\tvariable\\tlength\\tinput\\tsequences,\\tthe\\tsimplest\\toption\\tis\\tto\\tset\\tthe\\t\\nsequence_length\\nparameter\\twhen\\tcalling\\tthe\\t\\nstatic_rnn()\\n\\tor\\t\\ndynamic_rnn()\\n\\tfunctions.\\tAnother\\toption\\tis\\tto\\tpad', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 591}), Document(page_content='the\\tsmaller\\tinputs\\t(e.g.,\\twith\\tzeros)\\tto\\tmake\\tthem\\tthe\\tsame\\tsize\\tas\\tthe\\tlargest\\tinput\\t(this\\tmay\\tbe\\nfaster\\tthan\\tthe\\tfirst\\toption\\tif\\tthe\\tinput\\tsequences\\tall\\thave\\tvery\\tsimilar\\tlengths).\\tTo\\thandle\\tvariable-\\nlength\\toutput\\tsequences,\\tif\\tyou\\tknow\\tin\\tadvance\\tthe\\tlength\\tof\\teach\\toutput\\tsequence,\\tyou\\tcan\\tuse\\tthe\\nsequence_length\\n\\tparameter\\t(for\\texample,\\tconsider\\ta\\tsequence-to-sequence\\tRNN\\tthat\\tlabels\\tevery\\nframe\\tin\\ta\\tvideo\\twith\\ta\\tviolence\\tscore:\\tthe\\toutput\\tsequence\\twill\\tbe\\texactly\\tthe\\tsame\\tlength\\tas\\tthe\\ninput\\tsequence).\\tIf\\tyou\\tdon’t\\tknow\\tin\\tadvance\\tthe\\tlength\\tof\\tthe\\toutput\\tsequence,\\tyou\\tcan\\tuse\\tthe\\npadding\\ttrick:\\talways\\toutput\\tthe\\tsame\\tsize\\tsequence,\\tbut\\tignore\\tany\\toutputs\\tthat\\tcome\\tafter\\tthe\\tend-\\nof-sequence\\ttoken\\t(by\\tignoring\\tthem\\twhen\\tcomputing\\tthe\\tcost\\tfunction).\\n6\\n.\\t\\nTo\\tdistribute\\ttraining\\tand\\texecution\\tof\\ta\\tdeep\\tRNN\\tacross\\tmultiple\\tGPUs,\\ta\\tcommon\\ttechnique\\tis\\nsimply\\tto\\tplace\\teach\\tlayer\\ton\\ta\\tdifferent\\tGPU\\t(see\\t\\nChapter\\t12\\n).\\nFor\\tthe\\tsolutions\\tto\\texercises\\t7,\\t8,\\tand\\t9,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 592}), Document(page_content='Chapter\\t15\\n:\\tAutoencoders\\n1\\n.\\t\\nHere\\tare\\tsome\\tof\\tthe\\tmain\\ttasks\\tthat\\tautoencoders\\tare\\tused\\tfor:\\nFeature\\textraction\\nUnsupervised\\tpretraining\\nDimensionality\\treduction\\nGenerative\\tmodels\\nAnomaly\\tdetection\\t(an\\tautoencoder\\tis\\tgenerally\\tbad\\tat\\treconstructing\\toutliers)\\n2\\n.\\t\\nIf\\tyou\\twant\\tto\\ttrain\\ta\\tclassifier\\tand\\tyou\\thave\\tplenty\\tof\\tunlabeled\\ttraining\\tdata,\\tbut\\tonly\\ta\\tfew\\nthousand\\tlabeled\\tinstances,\\tthen\\tyou\\tcould\\tfirst\\ttrain\\ta\\tdeep\\tautoencoder\\ton\\tthe\\tfull\\tdataset\\t(labeled\\n+\\tunlabeled),\\tthen\\treuse\\tits\\tlower\\thalf\\tfor\\tthe\\tclassifier\\t(i.e.,\\treuse\\tthe\\tlayers\\tup\\tto\\tthe\\tcodings\\tlayer,\\nincluded)\\tand\\ttrain\\tthe\\tclassifier\\tusing\\tthe\\tlabeled\\tdata.\\tIf\\tyou\\thave\\tlittle\\tlabeled\\tdata,\\tyou\\tprobably\\nwant\\tto\\tfreeze\\tthe\\treused\\tlayers\\twhen\\ttraining\\tthe\\tclassifier.\\n3\\n.\\t\\nThe\\tfact\\tthat\\tan\\tautoencoder\\tperfectly\\treconstructs\\tits\\tinputs\\tdoes\\tnot\\tnecessarily\\tmean\\tthat\\tit\\tis\\ta\\ngood\\tautoencoder;\\tperhaps\\tit\\tis\\tsimply\\tan\\tovercomplete\\tautoencoder\\tthat\\tlearned\\tto\\tcopy\\tits\\tinputs\\nto\\tthe\\tcodings\\tlayer\\tand\\tthen\\tto\\tthe\\toutputs.\\tIn\\tfact,\\teven\\tif\\tthe\\tcodings\\tlayer\\tcontained\\ta\\tsingle\\nneuron,\\tit\\twould\\tbe\\tpossible\\tfor\\ta\\tvery\\tdeep\\tautoencoder\\tto\\tlearn\\tto\\tmap\\teach\\ttraining\\tinstance\\tto\\ta\\ndifferent\\tcoding\\t(e.g.,\\tthe\\tfirst\\tinstance\\tcould\\tbe\\tmapped\\tto\\t0.001,\\tthe\\tsecond\\tto\\t0.002,\\tthe\\tthird\\tto\\n0.003,\\tand\\tso\\ton),\\tand\\tit\\tcould\\tlearn\\t“by\\theart”\\tto\\treconstruct\\tthe\\tright\\ttraining\\tinstance\\tfor\\teach\\ncoding.\\tIt\\twould\\tperfectly\\treconstruct\\tits\\tinputs\\twithout\\treally\\tlearning\\tany\\tuseful\\tpattern\\tin\\tthe\\tdata.\\nIn\\tpractice\\tsuch\\ta\\tmapping\\tis\\tunlikely\\tto\\thappen,\\tbut\\tit\\tillustrates\\tthe\\tfact\\tthat\\tperfect\\treconstructions\\nare\\tnot\\ta\\tguarantee\\tthat\\tthe\\tautoencoder\\tlearned\\tanything\\tuseful.\\tHowever,\\tif\\tit\\tproduces\\tvery\\tbad\\nreconstructions,\\tthen\\tit\\tis\\talmost\\tguaranteed\\tto\\tbe\\ta\\tbad\\tautoencoder.\\tTo\\tevaluate\\tthe\\tperformance\\tof\\nan\\tautoencoder,\\tone\\toption\\tis\\tto\\tmeasure\\tthe\\treconstruction\\tloss\\t(e.g.,\\tcompute\\tthe\\tMSE,\\tthe\\tmean\\nsquare\\tof\\tthe\\toutputs\\tminus\\tthe\\tinputs).\\tAgain,\\ta\\thigh\\treconstruction\\tloss\\tis\\ta\\tgood\\tsign\\tthat\\tthe\\nautoencoder\\tis\\tbad,\\tbut\\ta\\tlow\\treconstruction\\tloss\\tis\\tnot\\ta\\tguarantee\\tthat\\tit\\tis\\tgood.\\tYou\\tshould\\talso\\nevaluate\\tthe\\tautoencoder\\taccording\\tto\\twhat\\tit\\twill\\tbe\\tused\\tfor.\\tFor\\texample,\\tif\\tyou\\tare\\tusing\\tit\\tfor\\nunsupervised\\tpretraining\\tof\\ta\\tclassifier,\\tthen\\tyou\\tshould\\talso\\tevaluate\\tthe\\tclassifier’s\\tperformance.\\n4\\n.\\t\\nAn\\tundercomplete\\tautoencoder\\tis\\tone\\twhose\\tcodings\\tlayer\\tis\\tsmaller\\tthan\\tthe\\tinput\\tand\\toutput\\nlayers.\\tIf\\tit\\tis\\tlarger,\\tthen\\tit\\tis\\tan\\tovercomplete\\tautoencoder.\\tThe\\tmain\\trisk\\tof\\tan\\texcessively\\nundercomplete\\tautoencoder\\tis\\tthat\\tit\\tmay\\tfail\\tto\\treconstruct\\tthe\\tinputs.\\tThe\\tmain\\trisk\\tof\\tan\\novercomplete\\tautoencoder\\tis\\tthat\\tit\\tmay\\tjust\\tcopy\\tthe\\tinputs\\tto\\tthe\\toutputs,\\twithout\\tlearning\\tany\\nuseful\\tfeature.\\n5\\n.\\t\\nTo\\ttie\\tthe\\tweights\\tof\\tan\\tencoder\\tlayer\\tand\\tits\\tcorresponding\\tdecoder\\tlayer,\\tyou\\tsimply\\tmake\\tthe\\ndecoder\\tweights\\tequal\\tto\\tthe\\ttranspose\\tof\\tthe\\tencoder\\tweights.\\tThis\\treduces\\tthe\\tnumber\\tof\\nparameters\\tin\\tthe\\tmodel\\tby\\thalf,\\toften\\tmaking\\ttraining\\tconverge\\tfaster\\twith\\tless\\ttraining\\tdata,\\tand\\nreducing\\tthe\\trisk\\tof\\toverfitting\\tthe\\ttraining\\tset.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 593}), Document(page_content='6\\n.\\t\\nTo\\tvisualize\\tthe\\tfeatures\\tlearned\\tby\\tthe\\tlower\\tlayer\\tof\\ta\\tstacked\\tautoencoder,\\ta\\tcommon\\ttechnique\\tis\\nsimply\\tto\\tplot\\tthe\\tweights\\tof\\teach\\tneuron,\\tby\\treshaping\\teach\\tweight\\tvector\\tto\\tthe\\tsize\\tof\\tan\\tinput\\nimage\\t(e.g.,\\tfor\\tMNIST,\\treshaping\\ta\\tweight\\tvector\\tof\\tshape\\t\\n[784]\\n\\tto\\t\\n[28,\\t28]\\n).\\tTo\\tvisualize\\tthe\\nfeatures\\tlearned\\tby\\thigher\\tlayers,\\tone\\ttechnique\\tis\\tto\\tdisplay\\tthe\\ttraining\\tinstances\\tthat\\tmost\\tactivate\\neach\\tneuron.\\n7\\n.\\t\\nA\\tgenerative\\tmodel\\tis\\ta\\tmodel\\tcapable\\tof\\trandomly\\tgenerating\\toutputs\\tthat\\tresemble\\tthe\\ttraining\\ninstances.\\tFor\\texample,\\tonce\\ttrained\\tsuccessfully\\ton\\tthe\\tMNIST\\tdataset,\\ta\\tgenerative\\tmodel\\tcan\\tbe\\nused\\tto\\trandomly\\tgenerate\\trealistic\\timages\\tof\\tdigits.\\tThe\\toutput\\tdistribution\\tis\\ttypically\\tsimilar\\tto\\nthe\\ttraining\\tdata.\\tFor\\texample,\\tsince\\tMNIST\\tcontains\\tmany\\timages\\tof\\teach\\tdigit,\\tthe\\tgenerative\\nmodel\\twould\\toutput\\troughly\\tthe\\tsame\\tnumber\\tof\\timages\\tof\\teach\\tdigit.\\tSome\\tgenerative\\tmodels\\tcan\\nbe\\tparametrized\\t—\\tfor\\texample,\\tto\\tgenerate\\tonly\\tsome\\tkinds\\tof\\toutputs.\\tAn\\texample\\tof\\ta\\tgenerative\\nautoencoder\\tis\\tthe\\tvariational\\tautoencoder.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 594}), Document(page_content='Chapter\\t16\\n:\\tReinforcement\\tLearning\\n1\\n.\\t\\nReinforcement\\tLearning\\tis\\tan\\tarea\\tof\\tMachine\\tLearning\\taimed\\tat\\tcreating\\tagents\\tcapable\\tof\\ttaking\\nactions\\tin\\tan\\tenvironment\\tin\\ta\\tway\\tthat\\tmaximizes\\trewards\\tover\\ttime.\\tThere\\tare\\tmany\\tdifferences\\nbetween\\tRL\\tand\\tregular\\tsupervised\\tand\\tunsupervised\\tlearning.\\tHere\\tare\\ta\\tfew:\\nIn\\tsupervised\\tand\\tunsupervised\\tlearning,\\tthe\\tgoal\\tis\\tgenerally\\tto\\tfind\\tpatterns\\tin\\tthe\\tdata.\\tIn\\nReinforcement\\tLearning,\\tthe\\tgoal\\tis\\tto\\tfind\\ta\\tgood\\tpolicy.\\nUnlike\\tin\\tsupervised\\tlearning,\\tthe\\tagent\\tis\\tnot\\texplicitly\\tgiven\\tthe\\t“right”\\tanswer.\\tIt\\tmust\\tlearn\\nby\\ttrial\\tand\\terror.\\nUnlike\\tin\\tunsupervised\\tlearning,\\tthere\\tis\\ta\\tform\\tof\\tsupervision,\\tthrough\\trewards.\\tWe\\tdo\\tnot\\ttell\\nthe\\tagent\\thow\\tto\\tperform\\tthe\\ttask,\\tbut\\twe\\tdo\\ttell\\tit\\twhen\\tit\\tis\\tmaking\\tprogress\\tor\\twhen\\tit\\tis\\nfailing.\\nA\\tReinforcement\\tLearning\\tagent\\tneeds\\tto\\tfind\\tthe\\tright\\tbalance\\tbetween\\texploring\\tthe\\nenvironment,\\tlooking\\tfor\\tnew\\tways\\tof\\tgetting\\trewards,\\tand\\texploiting\\tsources\\tof\\trewards\\tthat\\tit\\nalready\\tknows.\\tIn\\tcontrast,\\tsupervised\\tand\\tunsupervised\\tlearning\\tsystems\\tgenerally\\tdon’t\\tneed\\nto\\tworry\\tabout\\texploration;\\tthey\\tjust\\tfeed\\ton\\tthe\\ttraining\\tdata\\tthey\\tare\\tgiven.\\nIn\\tsupervised\\tand\\tunsupervised\\tlearning,\\ttraining\\tinstances\\tare\\ttypically\\tindependent\\t(in\\tfact,\\nthey\\tare\\tgenerally\\tshuffled).\\tIn\\tReinforcement\\tLearning,\\tconsecutive\\tobservations\\tare\\tgenerally\\nnot\\n\\tindependent.\\tAn\\tagent\\tmay\\tremain\\tin\\tthe\\tsame\\tregion\\tof\\tthe\\tenvironment\\tfor\\ta\\twhile\\tbefore\\nit\\tmoves\\ton,\\tso\\tconsecutive\\tobservations\\twill\\tbe\\tvery\\tcorrelated.\\tIn\\tsome\\tcases\\ta\\treplay\\nmemory\\tis\\tused\\tto\\tensure\\tthat\\tthe\\ttraining\\talgorithm\\tgets\\tfairly\\tindependent\\tobservations.\\n2\\n.\\t\\nHere\\tare\\ta\\tfew\\tpossible\\tapplications\\tof\\tReinforcement\\tLearning,\\tother\\tthan\\tthose\\tmentioned\\tin\\nChapter\\t16\\n:\\nMusic\\tpersonalization\\nThe\\tenvironment\\tis\\ta\\tuser’s\\tpersonalized\\tweb\\tradio.\\tThe\\tagent\\tis\\tthe\\tsoftware\\tdeciding\\twhat\\nsong\\tto\\tplay\\tnext\\tfor\\tthat\\tuser.\\tIts\\tpossible\\tactions\\tare\\tto\\tplay\\tany\\tsong\\tin\\tthe\\tcatalog\\t(it\\tmust\\ttry\\nto\\tchoose\\ta\\tsong\\tthe\\tuser\\twill\\tenjoy)\\tor\\tto\\tplay\\tan\\tadvertisement\\t(it\\tmust\\ttry\\tto\\tchoose\\tan\\tad\\nthat\\tthe\\tuser\\twill\\tbe\\tinterested\\tin).\\tIt\\tgets\\ta\\tsmall\\treward\\tevery\\ttime\\tthe\\tuser\\tlistens\\tto\\ta\\tsong,\\ta\\nlarger\\treward\\tevery\\ttime\\tthe\\tuser\\tlistens\\tto\\tan\\tad,\\ta\\tnegative\\treward\\twhen\\tthe\\tuser\\tskips\\ta\\tsong\\nor\\tan\\tad,\\tand\\ta\\tvery\\tnegative\\treward\\tif\\tthe\\tuser\\tleaves.\\nMarketing\\nThe\\tenvironment\\tis\\tyour\\tcompany’s\\tmarketing\\tdepartment.\\tThe\\tagent\\tis\\tthe\\tsoftware\\tthat\\tdefines\\nwhich\\tcustomers\\ta\\tmailing\\tcampaign\\tshould\\tbe\\tsent\\tto,\\tgiven\\ttheir\\tprofile\\tand\\tpurchase\\thistory\\n(for\\teach\\tcustomer\\tit\\thas\\ttwo\\tpossible\\tactions:\\tsend\\tor\\tdon’t\\tsend).\\tIt\\tgets\\ta\\tnegative\\treward\\nfor\\tthe\\tcost\\tof\\tthe\\tmailing\\tcampaign,\\tand\\ta\\tpositive\\treward\\tfor\\testimated\\trevenue\\tgenerated\\nfrom\\tthis\\tcampaign.\\nProduct\\tdelivery', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 595}), Document(page_content='Let\\tthe\\tagent\\tcontrol\\ta\\tfleet\\tof\\tdelivery\\ttrucks,\\tdeciding\\twhat\\tthey\\tshould\\tpick\\tup\\tat\\tthe\\tdepots,\\nwhere\\tthey\\tshould\\tgo,\\twhat\\tthey\\tshould\\tdrop\\toff,\\tand\\tso\\ton.\\tThey\\twould\\tget\\tpositive\\trewards\\nfor\\teach\\tproduct\\tdelivered\\ton\\ttime,\\tand\\tnegative\\trewards\\tfor\\tlate\\tdeliveries.\\n3\\n.\\t\\nWhen\\testimating\\tthe\\tvalue\\tof\\tan\\taction,\\tReinforcement\\tLearning\\talgorithms\\ttypically\\tsum\\tall\\tthe\\nrewards\\tthat\\tthis\\taction\\tled\\tto,\\tgiving\\tmore\\tweight\\tto\\timmediate\\trewards,\\tand\\tless\\tweight\\tto\\tlater\\nrewards\\t(considering\\tthat\\tan\\taction\\thas\\tmore\\tinfluence\\ton\\tthe\\tnear\\tfuture\\tthan\\ton\\tthe\\tdistant\\tfuture).\\nTo\\tmodel\\tthis,\\ta\\tdiscount\\trate\\tis\\ttypically\\tapplied\\tat\\teach\\ttime\\tstep.\\tFor\\texample,\\twith\\ta\\tdiscount\\nrate\\tof\\t0.9,\\ta\\treward\\tof\\t100\\tthat\\tis\\treceived\\ttwo\\ttime\\tsteps\\tlater\\tis\\tcounted\\tas\\tonly\\t0.9\\n2\\n\\t×\\t100\\t=\\t81\\nwhen\\tyou\\tare\\testimating\\tthe\\tvalue\\tof\\tthe\\taction.\\tYou\\tcan\\tthink\\tof\\tthe\\tdiscount\\trate\\tas\\ta\\tmeasure\\tof\\nhow\\tmuch\\tthe\\tfuture\\tis\\tvalued\\trelative\\tto\\tthe\\tpresent:\\tif\\tit\\tis\\tvery\\tclose\\tto\\t1,\\tthen\\tthe\\tfuture\\tis\\tvalued\\nalmost\\tas\\tmuch\\tas\\tthe\\tpresent.\\tIf\\tit\\tis\\tclose\\tto\\t0,\\tthen\\tonly\\timmediate\\trewards\\tmatter.\\tOf\\tcourse,\\tthis\\nimpacts\\tthe\\toptimal\\tpolicy\\ttremendously:\\tif\\tyou\\tvalue\\tthe\\tfuture,\\tyou\\tmay\\tbe\\twilling\\tto\\tput\\tup\\twith\\ta\\nlot\\tof\\timmediate\\tpain\\tfor\\tthe\\tprospect\\tof\\teventual\\trewards,\\twhile\\tif\\tyou\\tdon’t\\tvalue\\tthe\\tfuture,\\tyou\\nwill\\tjust\\tgrab\\tany\\timmediate\\treward\\tyou\\tcan\\tfind,\\tnever\\tinvesting\\tin\\tthe\\tfuture.\\n4\\n.\\t\\nTo\\tmeasure\\tthe\\tperformance\\tof\\ta\\tReinforcement\\tLearning\\tagent,\\tyou\\tcan\\tsimply\\tsum\\tup\\tthe\\trewards\\nit\\tgets.\\tIn\\ta\\tsimulated\\tenvironment,\\tyou\\tcan\\trun\\tmany\\tepisodes\\tand\\tlook\\tat\\tthe\\ttotal\\trewards\\tit\\tgets\\ton\\naverage\\t(and\\tpossibly\\tlook\\tat\\tthe\\tmin,\\tmax,\\tstandard\\tdeviation,\\tand\\tso\\ton).\\n5\\n.\\t\\nThe\\tcredit\\tassignment\\tproblem\\tis\\tthe\\tfact\\tthat\\twhen\\ta\\tReinforcement\\tLearning\\tagent\\treceives\\ta\\nreward,\\tit\\thas\\tno\\tdirect\\tway\\tof\\tknowing\\twhich\\tof\\tits\\tprevious\\tactions\\tcontributed\\tto\\tthis\\treward.\\tIt\\ntypically\\toccurs\\twhen\\tthere\\tis\\ta\\tlarge\\tdelay\\tbetween\\tan\\taction\\tand\\tthe\\tresulting\\trewards\\t(e.g.,\\tduring\\na\\tgame\\tof\\tAtari’s\\t\\nPong\\n,\\tthere\\tmay\\tbe\\ta\\tfew\\tdozen\\ttime\\tsteps\\tbetween\\tthe\\tmoment\\tthe\\tagent\\thits\\tthe\\nball\\tand\\tthe\\tmoment\\tit\\twins\\tthe\\tpoint).\\tOne\\tway\\tto\\talleviate\\tit\\tis\\tto\\tprovide\\tthe\\tagent\\twith\\tshorter-\\nterm\\trewards,\\twhen\\tpossible.\\tThis\\tusually\\trequires\\tprior\\tknowledge\\tabout\\tthe\\ttask.\\tFor\\texample,\\tif\\nwe\\twant\\tto\\tbuild\\tan\\tagent\\tthat\\twill\\tlearn\\tto\\tplay\\tchess,\\tinstead\\tof\\tgiving\\tit\\ta\\treward\\tonly\\twhen\\tit\\nwins\\tthe\\tgame,\\twe\\tcould\\tgive\\tit\\ta\\treward\\tevery\\ttime\\tit\\tcaptures\\tone\\tof\\tthe\\topponent’s\\tpieces.\\n6\\n.\\t\\nAn\\tagent\\tcan\\toften\\tremain\\tin\\tthe\\tsame\\tregion\\tof\\tits\\tenvironment\\tfor\\ta\\twhile,\\tso\\tall\\tof\\tits\\texperiences\\nwill\\tbe\\tvery\\tsimilar\\tfor\\tthat\\tperiod\\tof\\ttime.\\tThis\\tcan\\tintroduce\\tsome\\tbias\\tin\\tthe\\tlearning\\talgorithm.\\tIt\\nmay\\ttune\\tits\\tpolicy\\tfor\\tthis\\tregion\\tof\\tthe\\tenvironment,\\tbut\\tit\\twill\\tnot\\tperform\\twell\\tas\\tsoon\\tas\\tit\\nmoves\\tout\\tof\\tthis\\tregion.\\tTo\\tsolve\\tthis\\tproblem,\\tyou\\tcan\\tuse\\ta\\treplay\\tmemory;\\tinstead\\tof\\tusing\\tonly\\nthe\\tmost\\timmediate\\texperiences\\tfor\\tlearning,\\tthe\\tagent\\twill\\tlearn\\tbased\\ton\\ta\\tbuffer\\tof\\tits\\tpast\\nexperiences,\\trecent\\tand\\tnot\\tso\\trecent\\t(perhaps\\tthis\\tis\\twhy\\twe\\tdream\\tat\\tnight:\\tto\\treplay\\tour\\nexperiences\\tof\\tthe\\tday\\tand\\tbetter\\tlearn\\tfrom\\tthem?).\\n7\\n.\\t\\nAn\\toff-policy\\tRL\\talgorithm\\tlearns\\tthe\\tvalue\\tof\\tthe\\toptimal\\tpolicy\\t(i.e.,\\tthe\\tsum\\tof\\tdiscounted\\nrewards\\tthat\\tcan\\tbe\\texpected\\tfor\\teach\\tstate\\tif\\tthe\\tagent\\tacts\\toptimally),\\tindependently\\tof\\thow\\tthe\\nagent\\tactually\\tacts.\\tQ-Learning\\tis\\ta\\tgood\\texample\\tof\\tsuch\\tan\\talgorithm.\\tIn\\tcontrast,\\tan\\ton-policy\\nalgorithm\\tlearns\\tthe\\tvalue\\tof\\tthe\\tpolicy\\tthat\\tthe\\tagent\\tactually\\texecutes,\\tincluding\\tboth\\texploration\\nand\\texploitation.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.\\nIf\\tyou\\tdraw\\ta\\tstraight\\tline\\tbetween\\tany\\ttwo\\tpoints\\ton\\tthe\\tcurve,\\tthe\\tline\\tnever\\tcrosses\\tthe\\tcurve.\\n1', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 596}), Document(page_content='Moreover,\\tthe\\tNormal\\tEquation\\trequires\\tcomputing\\tthe\\tinverse\\tof\\ta\\tmatrix,\\tbut\\tthat\\tmatrix\\tis\\tnot\\talways\\tinvertible.\\tIn\\tcontrast,\\tthe\\tmatrix\\nfor\\tRidge\\tRegression\\tis\\talways\\tinvertible.\\nlog\\n2\\n\\tis\\tthe\\tbinary\\tlog,\\tlog\\n2\\n(\\nm\\n)\\t=\\tlog(\\nm\\n)\\t/\\tlog(2).\\nWhen\\tthe\\tvalues\\tto\\tpredict\\tcan\\tvary\\tby\\tmany\\torders\\tof\\tmagnitude,\\tthen\\tyou\\tmay\\twant\\tto\\tpredict\\tthe\\tlogarithm\\tof\\tthe\\ttarget\\tvalue\\trather\\nthan\\tthe\\ttarget\\tvalue\\tdirectly.\\tSimply\\tcomputing\\tthe\\texponential\\tof\\tthe\\tneural\\tnetwork’s\\toutput\\twill\\tgive\\tyou\\tthe\\testimated\\tvalue\\t(since\\nexp(log\\t\\nv\\n)\\t=\\t\\nv\\n).\\nIn\\t\\nChapter\\t11\\n\\twe\\tdiscuss\\tmany\\ttechniques\\tthat\\tintroduce\\tadditional\\thyperparameters:\\ttype\\tof\\tweight\\tinitialization,\\tactivation\\tfunction\\nhyperparameters\\t(e.g.,\\tamount\\tof\\tleak\\tin\\tleaky\\tReLU),\\tGradient\\tClipping\\tthreshold,\\ttype\\tof\\toptimizer\\tand\\tits\\thyperparameters\\t\\n(e.g.,\\tthe\\nmomentum\\thyperparameter\\twhen\\tusing\\ta\\t\\nMomentumOptimizer\\n),\\ttype\\tof\\tregularization\\tfor\\teach\\tlayer,\\tand\\tthe\\tregularization\\nhyperparameters\\t(e.g.,\\tdropout\\trate\\twhen\\tusing\\tdropout)\\tand\\tso\\ton.\\n2\\n3\\n4\\n5', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 597}), Document(page_content='Appendix\\tB.\\t\\nMachine\\tLearning\\tProject\\nChecklist\\nThis\\t\\nchecklist\\tcan\\tguide\\tyou\\tthrough\\tyour\\tMachine\\tLearning\\tprojects.\\tThere\\tare\\teight\\tmain\\tsteps:\\n1\\n.\\t\\nFrame\\tthe\\tproblem\\tand\\tlook\\tat\\tthe\\tbig\\tpicture.\\n2\\n.\\t\\nGet\\tthe\\tdata.\\n3\\n.\\t\\nExplore\\tthe\\tdata\\tto\\tgain\\tinsights.\\n4\\n.\\t\\nPrepare\\tthe\\tdata\\tto\\tbetter\\texpose\\tthe\\tunderlying\\tdata\\tpatterns\\tto\\tMachine\\tLearning\\talgorithms.\\n5\\n.\\t\\nExplore\\tmany\\tdifferent\\tmodels\\tand\\tshort-list\\tthe\\tbest\\tones.\\n6\\n.\\t\\nFine-tune\\tyour\\tmodels\\tand\\tcombine\\tthem\\tinto\\ta\\tgreat\\tsolution.\\n7\\n.\\t\\nPresent\\tyour\\tsolution.\\n8\\n.\\t\\nLaunch,\\tmonitor,\\tand\\tmaintain\\tyour\\tsystem.\\nObviously,\\tyou\\tshould\\tfeel\\tfree\\tto\\tadapt\\tthis\\tchecklist\\tto\\tyour\\tneeds.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 598}), Document(page_content='Frame\\tthe\\tProblem\\tand\\tLook\\tat\\tthe\\tBig\\tPicture\\n1\\n.\\t\\nDefine\\tthe\\tobjective\\tin\\tbusiness\\tterms.\\n2\\n.\\t\\nHow\\twill\\tyour\\tsolution\\tbe\\tused?\\n3\\n.\\t\\nWhat\\tare\\tthe\\tcurrent\\tsolutions/workarounds\\t(if\\tany)?\\n4\\n.\\t\\nHow\\tshould\\tyou\\tframe\\tthis\\tproblem\\t(supervised/unsupervised,\\tonline/offline,\\tetc.)?\\n5\\n.\\t\\nHow\\tshould\\tperformance\\tbe\\tmeasured?\\n6\\n.\\t\\nIs\\tthe\\tperformance\\tmeasure\\taligned\\twith\\tthe\\tbusiness\\tobjective?\\n7\\n.\\t\\nWhat\\twould\\tbe\\tthe\\tminimum\\tperformance\\tneeded\\tto\\treach\\tthe\\tbusiness\\tobjective?\\n8\\n.\\t\\nWhat\\tare\\tcomparable\\tproblems?\\tCan\\tyou\\treuse\\texperience\\tor\\ttools?\\n9\\n.\\t\\nIs\\thuman\\texpertise\\tavailable?\\n10\\n.\\t\\nHow\\twould\\tyou\\tsolve\\tthe\\tproblem\\tmanually?\\n11\\n.\\t\\nList\\tthe\\tassumptions\\tyou\\t(or\\tothers)\\thave\\tmade\\tso\\tfar.\\n12\\n.\\t\\nVerify\\tassumptions\\tif\\tpossible.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 599}), Document(page_content='Get\\tthe\\tData\\nNote:\\tautomate\\tas\\tmuch\\tas\\tpossible\\tso\\tyou\\tcan\\teasily\\tget\\tfresh\\tdata.\\n1\\n.\\t\\nList\\tthe\\tdata\\tyou\\tneed\\tand\\thow\\tmuch\\tyou\\tneed.\\n2\\n.\\t\\nFind\\tand\\tdocument\\twhere\\tyou\\tcan\\tget\\tthat\\tdata.\\n3\\n.\\t\\nCheck\\thow\\tmuch\\tspace\\tit\\twill\\ttake.\\n4\\n.\\t\\nCheck\\tlegal\\tobligations,\\tand\\tget\\tauthorization\\tif\\tnecessary.\\n5\\n.\\t\\nGet\\taccess\\tauthorizations.\\n6\\n.\\t\\nCreate\\ta\\tworkspace\\t(with\\tenough\\tstorage\\tspace).\\n7\\n.\\t\\nGet\\tthe\\tdata.\\n8\\n.\\t\\nConvert\\tthe\\tdata\\tto\\ta\\tformat\\tyou\\tcan\\teasily\\tmanipulate\\t(without\\tchanging\\tthe\\tdata\\titself).\\n9\\n.\\t\\nEnsure\\tsensitive\\tinformation\\tis\\tdeleted\\tor\\tprotected\\t(e.g.,\\tanonymized).\\n10\\n.\\t\\nCheck\\tthe\\tsize\\tand\\ttype\\tof\\tdata\\t(time\\tseries,\\tsample,\\tgeographical,\\tetc.).\\n11\\n.\\t\\nSample\\ta\\ttest\\tset,\\tput\\tit\\taside,\\tand\\tnever\\tlook\\tat\\tit\\t(no\\tdata\\tsnooping!).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 600}), Document(page_content='Explore\\tthe\\tData\\nNote:\\ttry\\tto\\tget\\tinsights\\tfrom\\ta\\tfield\\texpert\\tfor\\tthese\\tsteps.\\n1\\n.\\t\\nCreate\\ta\\tcopy\\tof\\tthe\\tdata\\tfor\\texploration\\t(sampling\\tit\\tdown\\tto\\ta\\tmanageable\\tsize\\tif\\tnecessary).\\n2\\n.\\t\\nCreate\\ta\\tJupyter\\tnotebook\\tto\\tkeep\\ta\\trecord\\tof\\tyour\\tdata\\texploration.\\n3\\n.\\t\\nStudy\\teach\\tattribute\\tand\\tits\\tcharacteristics:\\nName\\nType\\t(categorical,\\tint/float,\\tbounded/unbounded,\\ttext,\\tstructured,\\tetc.)\\n%\\tof\\tmissing\\tvalues\\nNoisiness\\tand\\ttype\\tof\\tnoise\\t(stochastic,\\toutliers,\\trounding\\terrors,\\tetc.)\\nPossibly\\tuseful\\tfor\\tthe\\ttask?\\nType\\tof\\tdistribution\\t(Gaussian,\\tuniform,\\tlogarithmic,\\tetc.)\\n4\\n.\\t\\nFor\\tsupervised\\tlearning\\ttasks,\\tidentify\\tthe\\ttarget\\tattribute(s).\\n5\\n.\\t\\nVisualize\\tthe\\tdata.\\n6\\n.\\t\\nStudy\\tthe\\tcorrelations\\tbetween\\tattributes.\\n7\\n.\\t\\nStudy\\thow\\tyou\\twould\\tsolve\\tthe\\tproblem\\tmanually.\\n8\\n.\\t\\nIdentify\\tthe\\tpromising\\ttransformations\\tyou\\tmay\\twant\\tto\\tapply.\\n9\\n.\\t\\nIdentify\\textra\\tdata\\tthat\\twould\\tbe\\tuseful\\t(go\\tback\\tto\\t\\n“Get\\tthe\\tData”\\n).\\n10\\n.\\t\\nDocument\\twhat\\tyou\\thave\\tlearned.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 601}), Document(page_content='Prepare\\tthe\\tData\\nNotes:\\nWork\\ton\\tcopies\\tof\\tthe\\tdata\\t(keep\\tthe\\toriginal\\tdataset\\tintact).\\nWrite\\tfunctions\\tfor\\tall\\tdata\\ttransformations\\tyou\\tapply,\\tfor\\tfive\\treasons:\\nSo\\tyou\\tcan\\teasily\\tprepare\\tthe\\tdata\\tthe\\tnext\\ttime\\tyou\\tget\\ta\\tfresh\\tdataset\\nSo\\tyou\\tcan\\tapply\\tthese\\ttransformations\\tin\\tfuture\\tprojects\\nTo\\tclean\\tand\\tprepare\\tthe\\ttest\\tset\\nTo\\tclean\\tand\\tprepare\\tnew\\tdata\\tinstances\\tonce\\tyour\\tsolution\\tis\\tlive\\nTo\\tmake\\tit\\teasy\\tto\\ttreat\\tyour\\tpreparation\\tchoices\\tas\\thyperparameters\\n1\\n.\\t\\nData\\tcleaning:\\nFix\\tor\\tremove\\toutliers\\t(optional).\\nFill\\tin\\tmissing\\tvalues\\t(e.g.,\\twith\\tzero,\\tmean,\\tmedian…)\\tor\\tdrop\\ttheir\\trows\\t(or\\tcolumns).\\n2\\n.\\t\\nFeature\\tselection\\n\\t(optional):\\nDrop\\tthe\\tattributes\\tthat\\tprovide\\tno\\tuseful\\tinformation\\tfor\\tthe\\ttask.\\n3\\n.\\t\\nFeature\\tengineering,\\twhere\\tappropriate:\\nDiscretize\\tcontinuous\\tfeatures.\\nDecompose\\tfeatures\\t(e.g.,\\tcategorical,\\tdate/time,\\tetc.).\\nAdd\\tpromising\\ttransformations\\tof\\tfeatures\\t(e.g.,\\tlog(x),\\tsqrt(x),\\tx^2,\\tetc.).\\nAggregate\\tfeatures\\tinto\\tpromising\\tnew\\tfeatures.\\n4\\n.\\t\\nFeature\\tscaling:\\tstandardize\\tor\\tnormalize\\tfeatures.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 602}), Document(page_content='Short-List\\tPromising\\tModels\\nNotes:\\nIf\\tthe\\tdata\\tis\\thuge,\\tyou\\tmay\\twant\\tto\\tsample\\tsmaller\\ttraining\\tsets\\tso\\tyou\\tcan\\ttrain\\tmany\\tdifferent\\nmodels\\tin\\ta\\treasonable\\ttime\\t(be\\taware\\tthat\\tthis\\tpenalizes\\tcomplex\\tmodels\\tsuch\\tas\\tlarge\\tneural\\tnets\\nor\\tRandom\\tForests).\\nOnce\\tagain,\\ttry\\tto\\tautomate\\tthese\\tsteps\\tas\\tmuch\\tas\\tpossible.\\n1\\n.\\t\\nTrain\\tmany\\tquick\\tand\\tdirty\\tmodels\\tfrom\\tdifferent\\tcategories\\t(e.g.,\\tlinear,\\tnaive\\tBayes,\\tSVM,\\nRandom\\tForests,\\tneural\\tnet,\\tetc.)\\tusing\\tstandard\\tparameters.\\n2\\n.\\t\\nMeasure\\tand\\tcompare\\ttheir\\tperformance.\\nFor\\teach\\tmodel,\\tuse\\t\\nN\\n-fold\\tcross-validation\\tand\\tcompute\\tthe\\tmean\\tand\\tstandard\\tdeviation\\tof\\nthe\\tperformance\\tmeasure\\ton\\tthe\\t\\nN\\n\\tfolds.\\n3\\n.\\t\\nAnalyze\\tthe\\tmost\\tsignificant\\tvariables\\tfor\\teach\\talgorithm.\\n4\\n.\\t\\nAnalyze\\tthe\\ttypes\\tof\\terrors\\tthe\\tmodels\\tmake.\\nWhat\\tdata\\twould\\ta\\thuman\\thave\\tused\\tto\\tavoid\\tthese\\terrors?\\n5\\n.\\t\\nHave\\ta\\tquick\\tround\\tof\\tfeature\\tselection\\tand\\tengineering.\\n6\\n.\\t\\nHave\\tone\\tor\\ttwo\\tmore\\tquick\\titerations\\tof\\tthe\\tfive\\tprevious\\tsteps.\\n7\\n.\\t\\nShort-list\\tthe\\ttop\\tthree\\tto\\tfive\\tmost\\tpromising\\tmodels,\\tpreferring\\tmodels\\tthat\\tmake\\tdifferent\\ttypes\\tof\\nerrors.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 603}), Document(page_content='Fine-Tune\\tthe\\tSystem\\nNotes:\\nYou\\twill\\twant\\tto\\tuse\\tas\\tmuch\\tdata\\tas\\tpossible\\tfor\\tthis\\tstep,\\tespecially\\tas\\tyou\\tmove\\ttoward\\tthe\\tend\\nof\\tfine-tuning.\\nAs\\talways\\tautomate\\twhat\\tyou\\tcan.\\n1\\n.\\t\\nFine-tune\\tthe\\thyperparameters\\tusing\\tcross-validation.\\nTreat\\tyour\\tdata\\ttransformation\\tchoices\\tas\\thyperparameters,\\tespecially\\twhen\\tyou\\tare\\tnot\\tsure\\nabout\\tthem\\t(e.g.,\\tshould\\tI\\treplace\\tmissing\\tvalues\\twith\\tzero\\tor\\twith\\tthe\\tmedian\\tvalue?\\tOr\\tjust\\ndrop\\tthe\\trows?).\\nUnless\\tthere\\tare\\tvery\\tfew\\thyperparameter\\tvalues\\tto\\texplore,\\tprefer\\trandom\\tsearch\\tover\\tgrid\\nsearch.\\tIf\\ttraining\\tis\\tvery\\tlong,\\tyou\\tmay\\tprefer\\ta\\tBayesian\\toptimization\\tapproach\\t(e.g.,\\tusing\\nGaussian\\tprocess\\tpriors,\\t\\nas\\tdescribed\\tby\\tJasper\\tSnoek,\\tHugo\\tLarochelle,\\tand\\tRyan\\tAdams\\n).\\n1\\n2\\n.\\t\\nTry\\tEnsemble\\tmethods.\\tCombining\\tyour\\tbest\\tmodels\\twill\\toften\\tperform\\tbetter\\tthan\\trunning\\tthem\\nindividually.\\n3\\n.\\t\\nOnce\\tyou\\tare\\tconfident\\tabout\\tyour\\tfinal\\tmodel,\\tmeasure\\tits\\tperformance\\ton\\tthe\\ttest\\tset\\tto\\testimate\\nthe\\tgeneralization\\terror.\\nWARNING\\nDon’t\\ttweak\\tyour\\tmodel\\tafter\\tmeasuring\\tthe\\tgeneralization\\terror:\\tyou\\twould\\tjust\\tstart\\toverfitting\\tthe\\ttest\\tset.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 604}), Document(page_content='Present\\tYour\\tSolution\\n1\\n.\\t\\nDocument\\twhat\\tyou\\thave\\tdone.\\n2\\n.\\t\\nCreate\\ta\\tnice\\tpresentation.\\nMake\\tsure\\tyou\\thighlight\\tthe\\tbig\\tpicture\\tfirst.\\n3\\n.\\t\\nExplain\\twhy\\tyour\\tsolution\\tachieves\\tthe\\tbusiness\\tobjective.\\n4\\n.\\t\\nDon’t\\tforget\\tto\\tpresent\\tinteresting\\tpoints\\tyou\\tnoticed\\talong\\tthe\\tway.\\nDescribe\\twhat\\tworked\\tand\\twhat\\tdid\\tnot.\\nList\\tyour\\tassumptions\\tand\\tyour\\tsystem’s\\tlimitations.\\n5\\n.\\t\\nEnsure\\tyour\\tkey\\tfindings\\tare\\tcommunicated\\tthrough\\tbeautiful\\tvisualizations\\tor\\teasy-to-remember\\nstatements\\t(e.g.,\\t“the\\tmedian\\tincome\\tis\\tthe\\tnumber-one\\tpredictor\\tof\\thousing\\tprices”).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 605}), Document(page_content='Launch!\\n1\\n.\\t\\nGet\\tyour\\tsolution\\tready\\tfor\\tproduction\\t(plug\\tinto\\tproduction\\tdata\\tinputs,\\twrite\\tunit\\ttests,\\tetc.).\\n2\\n.\\t\\nWrite\\tmonitoring\\tcode\\tto\\tcheck\\tyour\\tsystem’s\\tlive\\tperformance\\tat\\tregular\\tintervals\\tand\\ttrigger\\talerts\\nwhen\\tit\\tdrops.\\nBeware\\tof\\tslow\\tdegradation\\ttoo:\\tmodels\\ttend\\tto\\t“rot”\\tas\\tdata\\tevolves.\\nMeasuring\\tperformance\\tmay\\trequire\\ta\\thuman\\tpipeline\\t(e.g.,\\tvia\\ta\\tcrowdsourcing\\tservice).\\nAlso\\tmonitor\\tyour\\tinputs’\\tquality\\t(e.g.,\\ta\\tmalfunctioning\\tsensor\\tsending\\trandom\\tvalues,\\tor\\nanother\\tteam’s\\toutput\\tbecoming\\tstale).\\tThis\\tis\\tparticularly\\timportant\\tfor\\tonline\\tlearning\\nsystems.\\n3\\n.\\t\\nRetrain\\tyour\\tmodels\\ton\\ta\\tregular\\tbasis\\ton\\tfresh\\tdata\\t\\n(automate\\tas\\tmuch\\tas\\tpossible).\\n“Practical\\tBayesian\\tOptimization\\tof\\tMachine\\tLearning\\tAlgorithms,”\\tJ.\\tSnoek,\\tH.\\tLarochelle,\\tR.\\tAdams\\t(2012).\\n1', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 606}), Document(page_content='Appendix\\tC.\\t\\nSVM\\tDual\\tProblem\\nTo\\tunderstand\\t\\nduality\\n,\\t\\nyou\\tfirst\\tneed\\tto\\tunderstand\\tthe\\t\\nLagrange\\tmultipliers\\n\\tmethod.\\tThe\\tgeneral\\tidea\\tis\\nto\\ttransform\\ta\\tconstrained\\toptimization\\t\\nobjective\\tinto\\tan\\tunconstrained\\tone,\\tby\\tmoving\\tthe\\tconstraints\\ninto\\tthe\\tobjective\\tfunction.\\tLet’s\\tlook\\tat\\ta\\tsimple\\texample.\\tSuppose\\tyou\\twant\\tto\\tfind\\tthe\\tvalues\\tof\\t\\nx\\n\\tand\\t\\ny\\nthat\\tminimize\\tthe\\tfunction\\t\\nf\\n(\\nx\\n,\\ny\\n)\\t=\\t\\nx\\n2\\n\\t+\\t2\\ny\\n,\\tsubject\\tto\\tan\\t\\nequality\\tconstraint\\n:\\t3\\nx\\n\\t+\\t2\\ny\\n\\t+\\t1\\t=\\t0.\\tUsing\\tthe\\nLagrange\\tmultipliers\\tmethod,\\twe\\tstart\\tby\\tdefining\\ta\\tnew\\tfunction\\tcalled\\tthe\\t\\nLagrangian\\n\\t(or\\t\\nLagrange\\nfunction\\n):\\t\\ng\\n(\\nx\\n,\\t\\ny\\n,\\t\\nα\\n)\\t=\\t\\nf\\n(\\nx\\n,\\t\\ny\\n)\\t–\\t\\nα\\n(3\\nx\\n\\t+\\t2\\ny\\n\\t+\\t1).\\tEach\\tconstraint\\t(in\\tthis\\tcase\\tjust\\tone)\\tis\\tsubtracted\\tfrom\\nthe\\toriginal\\tobjective,\\tmultiplied\\tby\\ta\\tnew\\tvariable\\tcalled\\ta\\t\\nLagrange\\tmultiplier.\\nJoseph-Louis\\tLagrange\\tshowed\\tthat\\tif\\t\\n\\tis\\ta\\tsolution\\tto\\tthe\\tconstrained\\toptimization\\tproblem,\\tthen\\nthere\\tmust\\texist\\tan\\t\\n\\tsuch\\tthat\\t\\n\\tis\\t\\na\\t\\nstationary\\tpoint\\n\\tof\\tthe\\tLagrangian\\t(a\\tstationary\\tpoint\\tis\\ta\\npoint\\twhere\\tall\\tpartial\\tderivatives\\tare\\tequal\\tto\\tzero).\\tIn\\tother\\twords,\\twe\\tcan\\tcompute\\tthe\\tpartial\\nderivatives\\tof\\t\\ng\\n(\\nx\\n,\\t\\ny\\n,\\t\\nα\\n)\\twith\\tregards\\tto\\t\\nx\\n,\\t\\ny\\n,\\tand\\t\\nα\\n;\\twe\\tcan\\tfind\\tthe\\tpoints\\twhere\\tthese\\tderivatives\\tare\\tall\\nequal\\tto\\tzero;\\tand\\tthe\\tsolutions\\tto\\tthe\\tconstrained\\toptimization\\tproblem\\t(if\\tthey\\texist)\\tmust\\tbe\\tamong\\nthese\\tstationary\\tpoints.\\nIn\\tthis\\texample\\tthe\\tpartial\\tderivatives\\tare:\\t\\nWhen\\tall\\tthese\\tpartial\\tderivatives\\tare\\tequal\\tto\\t0,\\twe\\tfind\\tthat\\t\\n,\\tfrom\\twhich\\twe\\tcan\\teasily\\tfind\\tthat\\t\\n,\\t\\n,\\tand\\t\\n.\\tThis\\tis\\tthe\\tonly\\tstationary\\tpoint,\\tand\\tas\\tit\\trespects\\tthe\\tconstraint,\\tit\\tmust\\tbe\\tthe\\nsolution\\tto\\tthe\\tconstrained\\toptimization\\tproblem.\\nHowever,\\tthis\\tmethod\\tapplies\\tonly\\tto\\t\\nequality\\tconstraints.\\tFortunately,\\tunder\\tsome\\tregularity\\tconditions\\n(which\\tare\\trespected\\tby\\tthe\\tSVM\\tobjectives),\\tthis\\tmethod\\tcan\\tbe\\tgeneralized\\tto\\t\\ninequality\\tconstraints\\n\\t\\nas\\nwell\\t(e.g.,\\t3\\nx\\n\\t+\\t2\\ny\\n\\t+\\t1\\t≥\\t0).\\t\\nThe\\t\\ngeneralized\\tLagrangian\\n\\tfor\\tthe\\thard\\tmargin\\tproblem\\tis\\tgiven\\tby\\nEquation\\tC-1\\n,\\twhere\\tthe\\t\\nα\\n(\\ni\\n)\\n\\tvariables\\tare\\tcalled\\t\\nthe\\t\\nKarush–Kuhn–Tucker\\n\\t(KKT)\\tmultipliers,\\tand\\tthey\\nmust\\tbe\\tgreater\\tor\\tequal\\tto\\tzero.\\nEquation\\tC-1.\\t\\nGeneralized\\tLagrangian\\tfor\\tthe\\thard\\tmargin\\tproblem\\nJust\\tlike\\twith\\tthe\\tLagrange\\tmultipliers\\tmethod,\\tyou\\tcan\\tcompute\\tthe\\tpartial\\tderivatives\\tand\\tlocate\\tthe\\nstationary\\tpoints.\\tIf\\tthere\\tis\\ta\\tsolution,\\tit\\twill\\tnecessarily\\tbe\\tamong\\tthe\\tstationary\\tpoints\\t\\n\\tthat\\nrespect\\tthe\\t\\nKKT\\tconditions\\n:\\nRespect\\tthe\\tproblem’s\\tconstraints:\\t\\n,', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 607}), Document(page_content='Verify\\t\\n,\\nEither\\t\\n\\tor\\tthe\\ti\\nth\\n\\tconstraint\\tmust\\tbe\\tan\\t\\nactive\\tconstraint\\n,\\t\\nmeaning\\tit\\tmust\\thold\\tby\\tequality:\\t\\n.\\tThis\\tcondition\\tis\\tcalled\\tthe\\t\\ncomplementary\\tslackness\\n\\t\\ncondition.\\tIt\\timplies\\nthat\\teither\\t\\n\\tor\\tthe\\ti\\nth\\n\\tinstance\\tlies\\ton\\tthe\\tboundary\\t(it\\tis\\ta\\tsupport\\tvector).\\nNote\\tthat\\tthe\\tKKT\\tconditions\\tare\\tnecessary\\tconditions\\tfor\\ta\\tstationary\\tpoint\\tto\\tbe\\ta\\tsolution\\tof\\tthe\\nconstrained\\toptimization\\tproblem.\\tUnder\\tsome\\tconditions,\\tthey\\tare\\talso\\tsufficient\\tconditions.\\tLuckily,\\tthe\\nSVM\\toptimization\\tproblem\\thappens\\tto\\tmeet\\tthese\\tconditions,\\tso\\tany\\tstationary\\tpoint\\tthat\\tmeets\\tthe\\tKKT\\nconditions\\tis\\tguaranteed\\tto\\tbe\\ta\\tsolution\\tto\\tthe\\tconstrained\\toptimization\\tproblem.\\nWe\\tcan\\tcompute\\tthe\\tpartial\\tderivatives\\tof\\tthe\\tgeneralized\\tLagrangian\\twith\\tregards\\tto\\t\\nw\\n\\tand\\t\\nb\\n\\twith\\nEquation\\tC-2\\n.\\nEquation\\tC-2.\\t\\nPartial\\tderivatives\\tof\\tthe\\tgeneralized\\tLagrangian\\nWhen\\tthese\\tpartial\\tderivatives\\tare\\tequal\\tto\\t0,\\twe\\thave\\t\\nEquation\\tC-3\\n.\\nEquation\\tC-3.\\t\\nProperties\\tof\\tthe\\tstationary\\tpoints\\nIf\\t\\nwe\\tplug\\tthese\\tresults\\tinto\\tthe\\tdefinition\\tof\\tthe\\t\\ngeneralized\\tLagrangian,\\tsome\\tterms\\tdisappear\\tand\\twe', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 608}), Document(page_content='find\\t\\nEquation\\tC-4\\n.\\nEquation\\tC-4.\\t\\nDual\\tform\\tof\\tthe\\tSVM\\tproblem\\nThe\\tgoal\\tis\\tnow\\tto\\tfind\\tthe\\tvector\\t\\n\\tthat\\tminimizes\\tthis\\tfunction,\\twith\\t\\n\\tfor\\tall\\tinstances.\\tThis\\nconstrained\\toptimization\\tproblem\\tis\\tthe\\tdual\\tproblem\\twe\\twere\\tlooking\\tfor.\\nOnce\\tyou\\tfind\\tthe\\toptimal\\t\\n,\\tyou\\tcan\\tcompute\\t\\n\\tusing\\tthe\\tfirst\\tline\\tof\\t\\nEquation\\tC-3\\n.\\tTo\\tcompute\\t\\n,\\tyou\\ncan\\tuse\\tthe\\tfact\\tthat\\ta\\tsupport\\tvector\\tverifies\\t\\nt\\n(\\ni\\n)\\n(\\nw\\nT\\n\\t·\\t\\nx\\n(\\ni\\n)\\n\\t+\\t\\nb\\n)\\t=\\t1,\\tso\\tif\\tthe\\tk\\nth\\n\\tinstance\\tis\\ta\\tsupport\\tvector\\n(i.e.,\\t\\nα\\nk\\n\\t>\\t0),\\tyou\\tcan\\tuse\\tit\\tto\\tcompute\\t\\n.\\tHowever,\\tit\\tis\\toften\\tprefered\\tto\\tcompute\\nthe\\taverage\\tover\\tall\\tsupport\\tvectors\\tto\\tget\\ta\\tmore\\tstable\\tand\\tprecise\\tvalue,\\t\\nas\\tin\\t\\nEquation\\tC-5\\n.\\nEquation\\tC-5.\\t\\nBias\\tterm\\testimation\\tusing\\tthe\\tdual\\tform\\n', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 609}), Document(page_content='Appendix\\tD.\\t\\nAutodiff\\nThis\\t\\nappendix\\texplains\\thow\\tTensorFlow’s\\tautodiff\\tfeature\\tworks,\\tand\\thow\\tit\\tcompares\\tto\\tother\\tsolutions.\\nSuppose\\tyou\\tdefine\\ta\\tfunction\\t\\nf\\n(\\nx\\n,\\ny\\n)\\t=\\t\\nx\\n2\\ny\\n\\t+\\t\\ny\\n\\t+\\t2,\\tand\\tyou\\tneed\\tits\\tpartial\\tderivatives\\t\\n\\tand\\t\\n,\\ntypically\\tto\\tperform\\tGradient\\tDescent\\t(or\\tsome\\tother\\toptimization\\talgorithm).\\tYour\\tmain\\toptions\\tare\\nmanual\\tdifferentiation,\\tsymbolic\\tdifferentiation,\\tnumerical\\tdifferentiation,\\tforward-mode\\tautodiff,\\tand\\nfinally\\treverse-mode\\tautodiff.\\tTensorFlow\\timplements\\tthis\\tlast\\toption.\\tLet’s\\tgo\\tthrough\\teach\\tof\\tthese\\noptions.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 610}), Document(page_content='Manual\\tDifferentiation\\nThe\\t\\nfirst\\tapproach\\tis\\tto\\tpick\\tup\\ta\\tpencil\\tand\\ta\\tpiece\\tof\\tpaper\\tand\\tuse\\tyour\\tcalculus\\tknowledge\\tto\\tderive\\nthe\\tpartial\\tderivatives\\tmanually.\\tFor\\tthe\\tfunction\\t\\nf\\n(\\nx\\n,\\ny\\n)\\tjust\\tdefined,\\tit\\tis\\tnot\\ttoo\\thard;\\tyou\\tjust\\tneed\\tto\\tuse\\nfive\\trules:\\nThe\\tderivative\\tof\\ta\\tconstant\\tis\\t0.\\nThe\\tderivative\\tof\\t\\nλx\\n\\tis\\t\\nλ\\n\\t(where\\t\\nλ\\n\\tis\\ta\\tconstant).\\nThe\\tderivative\\tof\\t\\nx\\nλ\\n\\tis\\t\\nλx\\nλ\\n\\t–\\t1\\n,\\tso\\tthe\\tderivative\\tof\\t\\nx\\n2\\n\\tis\\t2\\nx\\n.\\nThe\\tderivative\\tof\\ta\\tsum\\tof\\tfunctions\\tis\\tthe\\tsum\\tof\\tthese\\tfunctions’\\tderivatives.\\nThe\\tderivative\\tof\\t\\nλ\\n\\ttimes\\ta\\tfunction\\tis\\t\\nλ\\n\\ttimes\\tits\\tderivative.\\nFrom\\tthese\\trules,\\tyou\\tcan\\tderive\\t\\nEquation\\tD-1\\n:\\nEquation\\tD-1.\\t\\nPartial\\tderivatives\\tof\\t\\nf\\n(\\nx\\n,\\ny\\n)\\nThis\\tapproach\\tcan\\tbecome\\tvery\\ttedious\\tfor\\tmore\\tcomplex\\tfunctions,\\tand\\tyou\\trun\\tthe\\trisk\\tof\\tmaking\\nmistakes.\\tThe\\tgood\\tnews\\tis\\tthat\\tderiving\\tthe\\tmathematical\\tequations\\tfor\\tthe\\tpartial\\tderivatives\\tlike\\twe\\njust\\tdid\\tcan\\tbe\\tautomated,\\tthrough\\ta\\tprocess\\tcalled\\t\\nsymbolic\\tdifferentiation\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 611}), Document(page_content='Symbolic\\tDifferentiation\\nFigure\\tD-1\\n\\tshows\\t\\nhow\\tsymbolic\\tdifferentiation\\tworks\\ton\\tan\\teven\\tsimpler\\tfunction,\\t\\ng\\n(\\nx\\n,\\ny\\n)\\t=\\t5\\t+\\t\\nxy\\n.\\tThe\\ngraph\\tfor\\tthat\\tfunction\\tis\\trepresented\\ton\\tthe\\tleft.\\tAfter\\tsymbolic\\tdifferentiation,\\twe\\tget\\tthe\\tgraph\\ton\\tthe\\nright,\\twhich\\trepresents\\tthe\\tpartial\\tderivative\\t\\n\\t(we\\tcould\\tsimilarly\\tobtain\\nthe\\tpartial\\tderivative\\twith\\tregards\\tto\\t\\ny\\n).\\nFigure\\tD-1.\\t\\nSymbolic\\tdifferentiation\\nThe\\talgorithm\\tstarts\\tby\\tgetting\\tthe\\tpartial\\tderivative\\tof\\tthe\\tleaf\\tnodes.\\tThe\\tconstant\\tnode\\t(5)\\treturns\\tthe\\nconstant\\t0,\\tsince\\tthe\\tderivative\\tof\\ta\\tconstant\\tis\\talways\\t0.\\tThe\\tvariable\\t\\nx\\n\\treturns\\tthe\\tconstant\\t1\\tsince\\t\\n,\\tand\\tthe\\tvariable\\t\\ny\\n\\treturns\\tthe\\tconstant\\t0\\tsince\\t\\n\\t(if\\twe\\twere\\tlooking\\tfor\\tthe\\tpartial\\nderivative\\twith\\tregards\\tto\\t\\ny\\n,\\tit\\twould\\tbe\\tthe\\treverse).\\nNow\\twe\\thave\\tall\\twe\\tneed\\tto\\tmove\\tup\\tthe\\tgraph\\tto\\tthe\\tmultiplication\\tnode\\tin\\tfunction\\t\\ng\\n.\\tCalculus\\ttells\\tus\\nthat\\tthe\\tderivative\\tof\\tthe\\tproduct\\tof\\ttwo\\tfunctions\\t\\nu\\n\\tand\\t\\nv\\n\\tis\\t\\n.\\tWe\\tcan\\ntherefore\\tconstruct\\ta\\tlarge\\tpart\\tof\\tthe\\tgraph\\ton\\tthe\\tright,\\trepresenting\\t0\\t×\\t\\nx\\n\\t+\\t\\ny\\n\\t×\\t1.\\nFinally,\\twe\\tcan\\tgo\\tup\\tto\\tthe\\taddition\\tnode\\tin\\tfunction\\t\\ng\\n.\\tAs\\tmentioned,\\tthe\\tderivative\\tof\\ta\\tsum\\tof\\nfunctions\\tis\\tthe\\tsum\\tof\\tthese\\tfunctions’\\tderivatives.\\tSo\\twe\\tjust\\tneed\\tto\\tcreate\\tan\\taddition\\tnode\\tand\\nconnect\\tit\\tto\\tthe\\tparts\\tof\\tthe\\tgraph\\twe\\thave\\talready\\tcomputed.\\tWe\\tget\\tthe\\tcorrect\\tpartial\\tderivative:\\t\\n.\\nHowever,\\tit\\tcan\\tbe\\tsimplified\\t(a\\tlot).\\tA\\tfew\\ttrivial\\t\\npruning\\tsteps\\tcan\\tbe\\tapplied\\tto\\tthis\\tgraph\\tto\\tget\\trid\\tof\\nall\\tunnecessary\\toperations,\\tand\\twe\\tget\\ta\\tmuch\\tsmaller\\tgraph\\twith\\tjust\\tone\\tnode:\\t\\n.\\nIn\\tthis\\tcase,\\tsimplification\\tis\\tfairly\\teasy,\\tbut\\tfor\\ta\\tmore\\tcomplex\\tfunction,\\tsymbolic\\tdifferentiation\\tcan\\nproduce\\ta\\thuge\\tgraph\\tthat\\tmay\\tbe\\ttough\\tto\\tsimplify\\tand\\tlead\\tto\\tsuboptimal\\tperformance.\\tMost\\timportantly,\\nsymbolic\\tdifferentiation\\tcannot\\tdeal\\twith\\tfunctions\\tdefined\\twith\\tarbitrary\\tcode\\t—\\tfor\\texample,\\tthe', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 612}), Document(page_content='following\\tfunction\\t\\ndiscussed\\tin\\t\\nChapter\\t9\\n:\\ndef\\n\\t\\nmy_func\\n(\\na\\n,\\n\\t\\nb\\n):\\n\\t\\t\\t\\t\\nz\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\t\\t\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n100\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nz\\n\\t\\n=\\n\\t\\na\\n\\t\\n*\\n\\t\\nnp\\n.\\ncos\\n(\\nz\\n\\t\\n+\\n\\t\\ni\\n)\\n\\t\\n+\\n\\t\\nz\\n\\t\\n*\\n\\t\\nnp\\n.\\nsin\\n(\\nb\\n\\t\\n-\\n\\t\\ni\\n)\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nz', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 613}), Document(page_content='Numerical\\tDifferentiation\\nThe\\t\\nsimplest\\tsolution\\tis\\tto\\tcompute\\tan\\tapproximation\\tof\\tthe\\tderivatives,\\tnumerically.\\tRecall\\tthat\\tthe\\nderivative\\t\\nh\\n′(\\nx\\n0\\n)\\tof\\ta\\tfunction\\t\\nh\\n(\\nx\\n)\\tat\\ta\\tpoint\\t\\nx\\n0\\n\\tis\\tthe\\tslope\\tof\\tthe\\tfunction\\tat\\tthat\\tpoint,\\tor\\tmore\\tprecisely\\nEquation\\tD-2\\n.\\nEquation\\tD-2.\\t\\nDerivative\\tof\\ta\\tfunction\\t\\nh\\n(\\nx\\n)\\tat\\tpoint\\t\\nx\\n0\\nSo\\tif\\twe\\twant\\tto\\tcalculate\\tthe\\tpartial\\tderivative\\tof\\t\\nf\\n(\\nx\\n,\\ny\\n)\\twith\\tregards\\tto\\t\\nx\\n,\\tat\\t\\nx\\n\\t=\\t3\\tand\\t\\ny\\n\\t=\\t4,\\twe\\tcan\\nsimply\\tcompute\\t\\nf\\n(3\\t+\\t\\nϵ\\n,\\t4)\\t–\\t\\nf\\n(3,\\t4)\\tand\\tdivide\\tthe\\tresult\\tby\\t\\nϵ\\n,\\tusing\\ta\\tvery\\tsmall\\tvalue\\tfor\\t\\nϵ\\n.\\tThat’s\\nexactly\\twhat\\tthe\\tfollowing\\tcode\\tdoes:\\ndef\\n\\t\\nf\\n(\\nx\\n,\\n\\t\\ny\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nx\\n**\\n2\\n*\\ny\\n\\t\\n+\\n\\t\\ny\\n\\t\\n+\\n\\t\\n2\\ndef\\n\\t\\nderivative\\n(\\nf\\n,\\n\\t\\nx\\n,\\n\\t\\ny\\n,\\n\\t\\nx_eps\\n,\\n\\t\\ny_eps\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\n(\\nf\\n(\\nx\\n\\t\\n+\\n\\t\\nx_eps\\n,\\n\\t\\ny\\n\\t\\n+\\n\\t\\ny_eps\\n)\\n\\t\\n-\\n\\t\\nf\\n(\\nx\\n,\\n\\t\\ny\\n))\\n\\t\\n/\\n\\t\\n(\\nx_eps\\n\\t\\n+\\n\\t\\ny_eps\\n)\\ndf_dx\\n\\t\\n=\\n\\t\\nderivative\\n(\\nf\\n,\\n\\t\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n0.00001\\n,\\n\\t\\n0\\n)\\ndf_dy\\n\\t\\n=\\n\\t\\nderivative\\n(\\nf\\n,\\n\\t\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n0\\n,\\n\\t\\n0.00001\\n)\\nUnfortunately,\\tthe\\tresult\\tis\\timprecise\\t(and\\tit\\tgets\\tworse\\tfor\\tmore\\tcomplex\\tfunctions).\\tThe\\tcorrect\\tresults\\nare\\trespectively\\t24\\tand\\t10,\\tbut\\tinstead\\twe\\tget:\\n>>>\\t\\nprint\\n(\\ndf_dx\\n)\\n24.000039999805264\\n>>>\\t\\nprint\\n(\\ndf_dy\\n)\\n10.000000000331966\\nNotice\\tthat\\tto\\tcompute\\tboth\\tpartial\\tderivatives,\\twe\\thave\\tto\\tcall\\t\\nf()\\n\\tat\\tleast\\tthree\\ttimes\\t(we\\tcalled\\tit\\tfour\\ntimes\\tin\\tthe\\tpreceding\\tcode,\\tbut\\tit\\tcould\\tbe\\toptimized).\\tIf\\tthere\\twere\\t1,000\\tparameters,\\twe\\twould\\tneed\\tto\\ncall\\t\\nf()\\n\\tat\\tleast\\t1,001\\ttimes.\\tWhen\\tyou\\tare\\tdealing\\twith\\tlarge\\tneural\\tnetworks,\\tthis\\tmakes\\tnumerical\\ndifferentiation\\tway\\ttoo\\tinefficient.\\nHowever,\\tnumerical\\tdifferentiation\\tis\\tso\\tsimple\\tto\\timplement\\tthat\\tit\\tis\\ta\\tgreat\\ttool\\tto\\tcheck\\tthat\\tthe\\tother\\nmethods\\tare\\timplemented\\tcorrectly.\\tFor\\texample,\\tif\\tit\\tdisagrees\\twith\\tyour\\tmanually\\tderived\\tfunction,\\tthen\\nyour\\tfunction\\tprobably\\tcontains\\ta\\tmistake.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 614}), Document(page_content='Forward-Mode\\tAutodiff\\nForward-mode\\tautodiff\\n\\tis\\t\\nneither\\tnumerical\\tdifferentiation\\tnor\\tsymbolic\\tdifferentiation,\\tbut\\tin\\tsome\\nways\\tit\\tis\\ttheir\\tlove\\tchild.\\tIt\\trelies\\ton\\t\\ndual\\tnumbers\\n,\\t\\nwhich\\tare\\t(weird\\tbut\\tfascinating)\\tnumbers\\tof\\tthe\\nform\\t\\na\\n\\t+\\t\\nbϵ\\n\\twhere\\t\\na\\n\\tand\\t\\nb\\n\\tare\\treal\\tnumbers\\tand\\t\\nϵ\\n\\tis\\tan\\tinfinitesimal\\tnumber\\tsuch\\tthat\\t\\nϵ\\n2\\n\\t=\\t0\\t(but\\t\\nϵ\\n\\t≠\\t0).\\nYou\\tcan\\tthink\\tof\\tthe\\tdual\\tnumber\\t42\\t+\\t24\\nϵ\\n\\tas\\tsomething\\takin\\tto\\t42.0000000024\\twith\\tan\\tinfinite\\tnumber\\nof\\t0s\\t(but\\tof\\tcourse\\tthis\\tis\\tsimplified\\tjust\\tto\\tgive\\tyou\\tsome\\tidea\\tof\\twhat\\tdual\\tnumbers\\tare).\\tA\\tdual\\nnumber\\tis\\trepresented\\tin\\tmemory\\tas\\ta\\tpair\\tof\\tfloats.\\tFor\\texample,\\t42\\t+\\t24\\nϵ\\n\\tis\\trepresented\\tby\\tthe\\tpair\\n(42.0,\\t24.0).\\nDual\\tnumbers\\tcan\\tbe\\tadded,\\tmultiplied,\\tand\\tso\\ton,\\tas\\tshown\\tin\\t\\nEquation\\tD-3\\n.\\nEquation\\tD-3.\\t\\nA\\tfew\\toperations\\twith\\tdual\\tnumbers\\nMost\\timportantly,\\tit\\tcan\\tbe\\tshown\\tthat\\t\\nh\\n(\\na\\n\\t+\\t\\nbϵ\\n)\\t=\\t\\nh\\n(\\na\\n)\\t+\\t\\nb\\n\\t×\\t\\nh\\n′(\\na\\n)\\nϵ\\n,\\tso\\tcomputing\\t\\nh\\n(\\na\\n\\t+\\t\\nϵ\\n)\\tgives\\tyou\\tboth\\nh\\n(\\na\\n)\\tand\\tthe\\tderivative\\t\\nh\\n′(\\na\\n)\\tin\\tjust\\tone\\tshot.\\t\\nFigure\\tD-2\\n\\tshows\\thow\\tforward-mode\\tautodiff\\tcomputes\\tthe\\npartial\\tderivative\\tof\\t\\nf\\n(\\nx\\n,\\ny\\n)\\twith\\tregards\\tto\\t\\nx\\n\\tat\\t\\nx\\n\\t=\\t3\\tand\\t\\ny\\n\\t=\\t4.\\tAll\\twe\\tneed\\tto\\tdo\\tis\\tcompute\\t\\nf\\n(3\\t+\\t\\nϵ\\n,\\t4);\\nthis\\twill\\toutput\\ta\\tdual\\tnumber\\twhose\\tfirst\\tcomponent\\tis\\tequal\\tto\\t\\nf\\n(3,\\t4)\\tand\\twhose\\tsecond\\tcomponent\\tis\\nequal\\tto\\t\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 615}), Document(page_content='Figure\\tD-2.\\t\\nForward-mode\\tautodiff\\nTo\\tcompute\\t\\n\\twe\\twould\\thave\\tto\\tgo\\tthrough\\tthe\\tgraph\\tagain,\\tbut\\tthis\\ttime\\twith\\t\\nx\\n\\t=\\t3\\tand\\t\\ny\\n\\t=\\t4\\t+\\t\\nϵ\\n.\\nSo\\tforward-mode\\tautodiff\\tis\\tmuch\\tmore\\taccurate\\tthan\\tnumerical\\tdifferentiation,\\tbut\\tit\\tsuffers\\tfrom\\tthe\\nsame\\tmajor\\tflaw:\\tif\\tthere\\twere\\t1,000\\tparameters,\\tit\\twould\\trequire\\t1,000\\tpasses\\tthrough\\tthe\\tgraph\\tto\\ncompute\\tall\\tthe\\tpartial\\tderivatives.\\tThis\\tis\\twhere\\treverse-mode\\tautodiff\\tshines:\\tit\\tcan\\tcompute\\tall\\tof\\nthem\\tin\\tjust\\ttwo\\tpasses\\t\\nthrough\\tthe\\tgraph.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 616}), Document(page_content='Reverse-Mode\\tAutodiff\\nReverse-mode\\tautodiff\\t\\nis\\tthe\\tsolution\\timplemented\\tby\\tTensorFlow.\\tIt\\tfirst\\tgoes\\tthrough\\tthe\\tgraph\\tin\\tthe\\nforward\\tdirection\\t(i.e.,\\tfrom\\tthe\\tinputs\\tto\\tthe\\toutput)\\tto\\tcompute\\tthe\\tvalue\\tof\\teach\\tnode.\\tThen\\tit\\tdoes\\ta\\nsecond\\tpass,\\tthis\\ttime\\tin\\tthe\\treverse\\tdirection\\t(i.e.,\\tfrom\\tthe\\toutput\\tto\\tthe\\tinputs)\\tto\\tcompute\\tall\\tthe\\tpartial\\nderivatives.\\t\\nFigure\\tD-3\\n\\trepresents\\tthe\\tsecond\\tpass.\\tDuring\\tthe\\tfirst\\tpass,\\tall\\tthe\\tnode\\tvalues\\twere\\ncomputed,\\tstarting\\tfrom\\t\\nx\\n\\t=\\t3\\tand\\t\\ny\\n\\t=\\t4.\\tYou\\tcan\\tsee\\tthose\\tvalues\\tat\\tthe\\tbottom\\tright\\tof\\teach\\tnode\\t(e.g.,\\t\\nx\\n×\\t\\nx\\n\\t=\\t9).\\tThe\\tnodes\\tare\\tlabeled\\t\\nn\\n1\\n\\tto\\t\\nn\\n7\\n\\tfor\\tclarity.\\tThe\\toutput\\tnode\\tis\\t\\nn\\n7\\n:\\t\\nf\\n(3,4)\\t=\\t\\nn\\n7\\n\\t=\\t42.\\nFigure\\tD-3.\\t\\nReverse-mode\\tautodiff\\nThe\\tidea\\tis\\tto\\tgradually\\tgo\\tdown\\tthe\\tgraph,\\tcomputing\\tthe\\tpartial\\tderivative\\tof\\t\\nf\\n(\\nx\\n,\\ny\\n)\\twith\\tregards\\tto\\teach\\nconsecutive\\tnode,\\tuntil\\twe\\treach\\tthe\\tvariable\\tnodes.\\tFor\\tthis,\\treverse-mode\\tautodiff\\trelies\\theavily\\ton\\tthe\\nchain\\trule\\n,\\tshown\\tin\\t\\nEquation\\tD-4\\n.\\nEquation\\tD-4.\\t\\nChain\\trule\\nSince\\t\\nn\\n7\\n\\tis\\tthe\\toutput\\tnode,\\t\\nf\\n\\t=\\t\\nn\\n7\\n\\tso\\ttrivially\\t\\n.\\nLet’s\\tcontinue\\tdown\\tthe\\tgraph\\tto\\t\\nn\\n5\\n:\\thow\\tmuch\\tdoes\\t\\nf\\n\\tvary\\twhen\\t\\nn\\n5\\n\\tvaries?\\tThe\\tanswer\\tis\\t\\n.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 617}), Document(page_content='We\\talready\\tknow\\tthat\\t\\n,\\tso\\tall\\twe\\tneed\\tis\\t\\n.\\tSince\\t\\nn\\n7\\n\\tsimply\\tperforms\\tthe\\tsum\\t\\nn\\n5\\n\\t+\\t\\nn\\n6\\n,\\twe\\tfind\\nthat\\t\\n,\\tso\\t\\n.\\nNow\\twe\\tcan\\tproceed\\tto\\tnode\\t\\nn\\n4\\n:\\thow\\tmuch\\tdoes\\t\\nf\\n\\tvary\\twhen\\t\\nn\\n4\\n\\tvaries?\\tThe\\tanswer\\tis\\t\\n.\\nSince\\t\\nn\\n5\\n\\t=\\t\\nn\\n4\\n\\t×\\t\\nn\\n2\\n,\\twe\\tfind\\tthat\\t\\n,\\tso\\t\\n.\\nThe\\tprocess\\tcontinues\\tuntil\\twe\\treach\\tthe\\tbottom\\tof\\tthe\\tgraph.\\tAt\\tthat\\tpoint\\twe\\twill\\thave\\tcalculated\\tall\\tthe\\npartial\\tderivatives\\tof\\t\\nf\\n(\\nx\\n,\\ny\\n)\\tat\\tthe\\tpoint\\t\\nx\\n\\t=\\t3\\tand\\t\\ny\\n\\t=\\t4.\\tIn\\tthis\\texample,\\twe\\tfind\\t\\n\\tand\\t\\n.\\nSounds\\tabout\\tright!\\nReverse-mode\\tautodiff\\tis\\ta\\tvery\\tpowerful\\tand\\taccurate\\ttechnique,\\tespecially\\twhen\\tthere\\tare\\tmany\\tinputs\\nand\\tfew\\toutputs,\\tsince\\tit\\trequires\\tonly\\tone\\tforward\\tpass\\tplus\\tone\\treverse\\tpass\\tper\\toutput\\tto\\tcompute\\tall\\nthe\\tpartial\\tderivatives\\tfor\\tall\\toutputs\\twith\\tregards\\tto\\tall\\tthe\\tinputs.\\tMost\\timportantly,\\tit\\tcan\\tdeal\\twith\\nfunctions\\tdefined\\tby\\tarbitrary\\tcode.\\tIt\\tcan\\talso\\thandle\\tfunctions\\tthat\\tare\\tnot\\tentirely\\tdifferentiable,\\tas\\tlong\\nas\\tyou\\task\\tit\\tto\\tcompute\\tthe\\tpartial\\tderivatives\\tat\\tpoints\\tthat\\t\\nare\\tdifferentiable.\\nTIP\\nIf\\tyou\\timplement\\ta\\tnew\\ttype\\tof\\toperation\\tin\\tTensorFlow\\tand\\tyou\\twant\\tto\\tmake\\tit\\tcompatible\\twith\\tautodiff,\\tthen\\tyou\\tneed\\tto\\nprovide\\ta\\tfunction\\tthat\\tbuilds\\ta\\tsubgraph\\tto\\tcompute\\tits\\tpartial\\tderivatives\\twith\\tregards\\tto\\tits\\tinputs.\\tFor\\texample,\\tsuppose\\tyou\\nimplement\\ta\\tfunction\\tthat\\tcomputes\\tthe\\tsquare\\tof\\tits\\tinput\\t\\nf\\n(\\nx\\n)\\t=\\t\\nx\\n2\\n.\\tIn\\tthat\\tcase\\tyou\\twould\\tneed\\tto\\tprovide\\tthe\\tcorresponding\\nderivative\\tfunction\\t\\nf\\n′(\\nx\\n)\\t=\\t2\\nx\\n.\\tNote\\tthat\\tthis\\tfunction\\tdoes\\tnot\\tcompute\\ta\\tnumerical\\tresult,\\tbut\\tinstead\\tbuilds\\ta\\tsubgraph\\tthat\\twill\\n(later)\\tcompute\\tthe\\tresult.\\tThis\\tis\\tvery\\tuseful\\tbecause\\tit\\tmeans\\tthat\\tyou\\tcan\\tcompute\\tgradients\\tof\\tgradients\\t(to\\tcompute\\tsecond-\\norder\\tderivatives,\\tor\\teven\\thigher-order\\t\\nderivatives).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 618}), Document(page_content='Appendix\\tE.\\t\\nOther\\tPopular\\tANN\\tArchitectures\\nIn\\tthis\\tappendix\\twe\\twill\\tgive\\ta\\tquick\\toverview\\tof\\ta\\tfew\\thistorically\\timportant\\tneural\\tnetwork\\narchitectures\\tthat\\tare\\tmuch\\tless\\tused\\ttoday\\tthan\\tdeep\\tMulti-Layer\\tPerceptrons\\t(\\nChapter\\t10\\n),\\nconvolutional\\tneural\\tnetworks\\t(\\nChapter\\t13\\n),\\trecurrent\\tneural\\tnetworks\\t(\\nChapter\\t14\\n),\\tor\\tautoencoders\\n(\\nChapter\\t15\\n).\\tThey\\tare\\toften\\tmentioned\\tin\\tthe\\tliterature,\\tand\\tsome\\tare\\tstill\\tused\\tin\\tmany\\tapplications,\\tso\\nit\\tis\\tworth\\tknowing\\tabout\\tthem.\\tMoreover,\\twe\\twill\\tdiscuss\\t\\ndeep\\tbelief\\tnets\\n\\t(DBNs),\\twhich\\twere\\tthe\\nstate\\tof\\tthe\\tart\\tin\\tDeep\\tLearning\\tuntil\\tthe\\tearly\\t2010s.\\tThey\\tare\\tstill\\tthe\\tsubject\\tof\\tvery\\tactive\\tresearch,\\tso\\nthey\\tmay\\twell\\tcome\\tback\\twith\\ta\\tvengeance\\tin\\tthe\\tnear\\tfuture.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 619}), Document(page_content='Hopfield\\tNetworks\\nHopfield\\tnetworks\\n\\twere\\t\\nfirst\\tintroduced\\tby\\tW.\\tA.\\tLittle\\tin\\t1974,\\tthen\\tpopularized\\tby\\tJ.\\tHopfield\\tin\\t1982.\\nThey\\t\\nare\\t\\nassociative\\tmemory\\n\\tnetworks:\\tyou\\tfirst\\tteach\\tthem\\tsome\\tpatterns,\\tand\\tthen\\twhen\\tthey\\tsee\\ta\\tnew\\npattern\\tthey\\t(hopefully)\\toutput\\tthe\\tclosest\\tlearned\\tpattern.\\tThis\\thas\\tmade\\tthem\\tuseful\\tin\\tparticular\\tfor\\ncharacter\\trecognition\\tbefore\\tthey\\twere\\toutperformed\\tby\\tother\\tapproaches.\\tYou\\tfirst\\ttrain\\tthe\\tnetwork\\tby\\nshowing\\tit\\texamples\\tof\\tcharacter\\timages\\t(each\\tbinary\\tpixel\\tmaps\\tto\\tone\\tneuron),\\tand\\tthen\\twhen\\tyou\\tshow\\nit\\ta\\tnew\\tcharacter\\timage,\\tafter\\ta\\tfew\\titerations\\tit\\toutputs\\tthe\\tclosest\\tlearned\\tcharacter.\\nThey\\tare\\tfully\\tconnected\\tgraphs\\t(see\\t\\nFigure\\tE-1\\n);\\tthat\\tis,\\tevery\\tneuron\\tis\\tconnected\\tto\\tevery\\tother\\tneuron.\\nNote\\tthat\\ton\\tthe\\tdiagram\\tthe\\timages\\tare\\t6\\t×\\t6\\tpixels,\\tso\\tthe\\tneural\\tnetwork\\ton\\tthe\\tleft\\tshould\\tcontain\\t36\\nneurons\\t(and\\t648\\tconnections),\\tbut\\tfor\\tvisual\\tclarity\\ta\\tmuch\\tsmaller\\tnetwork\\tis\\trepresented.\\nFigure\\tE-1.\\t\\nHopfield\\tnetwork\\nThe\\ttraining\\talgorithm\\tworks\\tby\\tusing\\t\\nHebb’s\\trule:\\tfor\\teach\\ttraining\\timage,\\tthe\\tweight\\tbetween\\ttwo\\nneurons\\tis\\tincreased\\tif\\tthe\\tcorresponding\\tpixels\\tare\\tboth\\ton\\tor\\tboth\\toff,\\tbut\\tdecreased\\tif\\tone\\tpixel\\tis\\ton\\nand\\tthe\\tother\\tis\\toff.\\nTo\\tshow\\ta\\tnew\\timage\\tto\\tthe\\tnetwork,\\tyou\\tjust\\tactivate\\tthe\\tneurons\\tthat\\tcorrespond\\tto\\tactive\\tpixels.\\tThe\\nnetwork\\tthen\\tcomputes\\tthe\\toutput\\tof\\tevery\\tneuron,\\tand\\tthis\\tgives\\tyou\\ta\\tnew\\timage.\\tYou\\tcan\\tthen\\ttake\\tthis\\nnew\\timage\\tand\\trepeat\\tthe\\twhole\\tprocess.\\tAfter\\ta\\twhile,\\tthe\\tnetwork\\treaches\\ta\\tstable\\tstate.\\tGenerally,\\tthis\\ncorresponds\\tto\\tthe\\ttraining\\timage\\tthat\\tmost\\tresembles\\tthe\\tinput\\timage.\\nA\\tso-called\\t\\nenergy\\tfunction\\n\\t\\nis\\tassociated\\twith\\tHopfield\\tnets.\\tAt\\teach\\titeration,\\tthe\\tenergy\\tdecreases,\\tso\\nthe\\tnetwork\\tis\\tguaranteed\\tto\\teventually\\tstabilize\\tto\\ta\\tlow-energy\\tstate.\\tThe\\ttraining\\talgorithm\\ttweaks\\tthe\\nweights\\tin\\ta\\tway\\tthat\\tdecreases\\tthe\\tenergy\\tlevel\\tof\\tthe\\ttraining\\tpatterns,\\tso\\tthe\\tnetwork\\tis\\tlikely\\tto\\nstabilize\\tin\\tone\\tof\\tthese\\tlow-energy\\tconfigurations.\\tUnfortunately,\\tsome\\tpatterns\\tthat\\twere\\tnot\\tin\\tthe\\ntraining\\tset\\talso\\tend\\tup\\twith\\tlow\\tenergy,\\tso\\tthe\\tnetwork\\tsometimes\\tstabilizes\\tin\\ta\\tconfiguration\\tthat\\twas', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 620}), Document(page_content='not\\tlearned.\\tThese\\tare\\tc\\nalled\\t\\nspurious\\tpatterns\\n.\\nAnother\\tmajor\\tflaw\\twith\\tHopfield\\tnets\\tis\\tthat\\tthey\\tdon’t\\tscale\\tvery\\twell\\t—\\ttheir\\tmemory\\tcapacity\\tis\\nroughly\\tequal\\tto\\t14%\\tof\\tthe\\tnumber\\tof\\tneurons.\\tFor\\texample,\\tto\\tclassify\\t28\\t×\\t28\\timages,\\tyou\\twould\\tneed\\na\\tHopfield\\tnet\\twith\\t784\\tfully\\tconnected\\tneurons\\tand\\t306,936\\tweights.\\tSuch\\ta\\tnetwork\\twould\\tonly\\tbe\\table\\nto\\tlearn\\tabout\\t110\\tdifferent\\tcharacters\\t(14%\\tof\\t784).\\tThat’s\\ta\\tlot\\tof\\tparameters\\tfor\\tsuch\\ta\\t\\nsmall\\tmemory.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 621}), Document(page_content='Boltzmann\\tMachines\\nBoltzmann\\tmachines\\n\\t\\nwere\\tinvented\\tin\\t1985\\tby\\tGeoffrey\\tHinton\\tand\\tTerrence\\tSejnowski.\\tJust\\tlike\\nHopfield\\tnets,\\tthey\\tare\\tfully\\tconnected\\tANNs,\\tbut\\tthey\\tare\\tbased\\t\\non\\t\\nstochastic\\tneurons\\n:\\tinstead\\tof\\tusing\\na\\tdeterministic\\tstep\\tfunction\\tto\\tdecide\\twhat\\tvalue\\tto\\toutput,\\tthese\\tneurons\\toutput\\t1\\twith\\tsome\\tprobability,\\nand\\t0\\totherwise.\\tThe\\tprobability\\tfunction\\tthat\\tthese\\tANNs\\tuse\\tis\\tbased\\ton\\tthe\\tBoltzmann\\tdistribution\\n(used\\tin\\tstatistical\\tmechanics)\\thence\\ttheir\\tname.\\t\\nEquation\\tE-1\\n\\tgives\\tthe\\tprobability\\tthat\\ta\\tparticular\\nneuron\\twill\\toutput\\ta\\t1.\\nEquation\\tE-1.\\t\\nProbability\\tthat\\tthe\\ti\\nth\\n\\tneuron\\twill\\toutput\\t1\\ns\\nj\\n\\tis\\tthe\\tj\\nth\\n\\tneuron’s\\tstate\\t(0\\tor\\t1).\\nw\\ni\\n,\\nj\\n\\tis\\tthe\\tconnection\\tweight\\tbetween\\tthe\\ti\\nth\\n\\tand\\tj\\nth\\n\\tneurons.\\tNote\\tthat\\t\\nw\\ni\\n,\\ni\\n\\t=\\t0.\\nb\\ni\\n\\tis\\tthe\\ti\\nth\\n\\tneuron’s\\tbias\\tterm.\\tWe\\tcan\\timplement\\tthis\\tterm\\tby\\tadding\\ta\\tbias\\tneuron\\tto\\tthe\\tnetwork.\\nN\\n\\tis\\tthe\\tnumber\\tof\\tneurons\\tin\\tthe\\tnetwork.\\nT\\n\\tis\\ta\\tnumber\\tcalled\\tthe\\tnetwork’s\\t\\ntemperature\\n;\\tthe\\thigher\\tthe\\ttemperature,\\tthe\\tmore\\trandom\\tthe\\noutput\\tis\\t(i.e.,\\tthe\\tmore\\tthe\\tprobability\\tapproaches\\t50%).\\nσ\\n\\tis\\tthe\\tlogistic\\tfunction.\\nNeurons\\tin\\tBoltzmann\\tmachines\\tare\\tseparated\\tinto\\ttwo\\tgroups:\\t\\nvisible\\tunits\\n\\tand\\t\\nhidden\\tunits\\n\\t(see\\nFigure\\tE-2\\n).\\tAll\\tneurons\\twork\\tin\\tthe\\tsame\\tstochastic\\tway,\\tbut\\tthe\\tvisible\\tunits\\tare\\tthe\\tones\\tthat\\treceive\\nthe\\tinputs\\tand\\tfrom\\twhich\\toutputs\\tare\\tread.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 622}), Document(page_content='Figure\\tE-2.\\t\\nBoltzmann\\tmachine\\nBecause\\tof\\tits\\tstochastic\\tnature,\\ta\\tBoltzmann\\tmachine\\twill\\tnever\\tstabilize\\tinto\\ta\\tfixed\\tconfiguration,\\tbut\\ninstead\\tit\\twill\\tkeep\\tswitching\\tbetween\\tmany\\tconfigurations.\\tIf\\tit\\tis\\tleft\\trunning\\tfor\\ta\\tsufficiently\\tlong\\ttime,\\nthe\\tprobability\\tof\\tobserving\\ta\\tparticular\\tconfiguration\\twill\\tonly\\tbe\\ta\\tfunction\\tof\\tthe\\tconnection\\tweights\\nand\\tbias\\tterms,\\tnot\\tof\\tthe\\toriginal\\tconfiguration\\t(similarly,\\tafter\\tyou\\tshuffle\\ta\\tdeck\\tof\\tcards\\tfor\\tlong\\nenough,\\tthe\\tconfiguration\\tof\\tthe\\tdeck\\tdoes\\tnot\\tdepend\\ton\\tthe\\tinitial\\tstate).\\tWhen\\tthe\\tnetwork\\treaches\\tthis\\nstate\\twhere\\tthe\\toriginal\\tconfiguration\\tis\\t“forgotten,”\\tit\\tis\\tsaid\\tto\\tbe\\t\\nin\\t\\nthermal\\tequilibrium\\n\\t(although\\tits\\nconfiguration\\tkeeps\\tchanging\\tall\\tthe\\ttime).\\tBy\\tsetting\\tthe\\tnetwork\\tparameters\\tappropriately,\\tletting\\tthe\\nnetwork\\treach\\tthermal\\tequilibrium,\\tand\\tthen\\tobserving\\tits\\tstate,\\twe\\tcan\\tsimulate\\ta\\twide\\trange\\tof\\nprobability\\tdistributions.\\tThis\\tis\\tcalled\\t\\na\\t\\ngenerative\\tmodel\\n.\\nTraining\\ta\\tBoltzmann\\tmachine\\tmeans\\tfinding\\tthe\\tparameters\\tthat\\twill\\tmake\\tthe\\tnetwork\\tapproximate\\tthe\\ntraining\\tset’s\\tprobability\\tdistribution.\\tFor\\texample,\\tif\\tthere\\tare\\tthree\\tvisible\\tneurons\\tand\\tthe\\ttraining\\tset\\ncontains\\t75%\\t(0,\\t1,\\t1)\\ttriplets,\\t10%\\t(0,\\t0,\\t1)\\ttriplets,\\tand\\t15%\\t(1,\\t1,\\t1)\\ttriplets,\\tthen\\tafter\\ttraining\\ta\\nBoltzmann\\tmachine,\\tyou\\tcould\\tuse\\tit\\tto\\tgenerate\\trandom\\tbinary\\ttriplets\\twith\\tabout\\tthe\\tsame\\tprobability\\ndistribution.\\tFor\\texample,\\tabout\\t75%\\tof\\tthe\\ttime\\tit\\twould\\toutput\\tthe\\t(0,\\t1,\\t1)\\ttriplet.\\nSuch\\ta\\tgenerative\\tmodel\\tcan\\tbe\\tused\\tin\\ta\\tvariety\\tof\\tways.\\tFor\\texample,\\tif\\tit\\tis\\ttrained\\ton\\timages,\\tand\\tyou\\nprovide\\tan\\tincomplete\\tor\\tnoisy\\timage\\tto\\tthe\\tnetwork,\\tit\\twill\\tautomatically\\t“repair”\\tthe\\timage\\tin\\ta\\nreasonable\\tway.\\tYou\\tcan\\talso\\tuse\\ta\\tgenerative\\tmodel\\tfor\\tclassification.\\tJust\\tadd\\ta\\tfew\\tvisible\\tneurons\\tto\\nencode\\tthe\\ttraining\\timage’s\\tclass\\t(e.g.,\\tadd\\t10\\tvisible\\tneurons\\tand\\tturn\\ton\\tonly\\tthe\\tfifth\\tneuron\\twhen\\tthe\\ntraining\\timage\\trepresents\\ta\\t5).\\tThen,\\twhen\\tgiven\\ta\\tnew\\timage,\\tthe\\tnetwork\\twill\\tautomatically\\tturn\\ton\\tthe', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 623}), Document(page_content='appropriate\\tvisible\\tneurons,\\tindicating\\tthe\\timage’s\\tclass\\t(e.g.,\\tit\\twill\\tturn\\ton\\tthe\\tfifth\\tvisible\\tneuron\\tif\\tthe\\nimage\\trepresents\\ta\\t5).\\nUnfortunately,\\tthere\\tis\\tno\\tefficient\\ttechnique\\tto\\ttrain\\tBoltzmann\\tmachines.\\tHowever,\\tfairly\\tefficient\\nalgorithms\\thave\\tbeen\\t\\ndeveloped\\tto\\t\\ntrain\\t\\nrestricted\\tBoltzmann\\tmachines\\n\\t(RBM).', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 624}), Document(page_content='Restricted\\tBoltzmann\\tMachines\\nAn\\tRBM\\tis\\tsimply\\ta\\tBoltzmann\\tmachine\\tin\\twhich\\tthere\\tare\\tno\\tconnections\\tbetween\\tvisible\\tunits\\tor\\nbetween\\thidden\\tunits,\\tonly\\tbetween\\tvisible\\tand\\thidden\\tunits.\\tFor\\texample,\\t\\nFigure\\tE-3\\n\\trepresents\\tan\\tRBM\\nwith\\tthree\\tvisible\\tunits\\tand\\tfour\\thidden\\tunits.\\nFigure\\tE-3.\\t\\nRestricted\\tBoltzmann\\tmachine\\nA\\tvery\\tefficient\\ttraining\\talgorithm,\\tcalled\\t\\nContrastive\\tDivergence\\n,\\t\\nwas\\t\\nintroduced\\tin\\t2005\\tby\\tMiguel\\tÁ.\\nCarreira-Perpiñán\\tand\\tGeoffrey\\tHinton\\n.\\n1\\n\\tHere\\tis\\thow\\tit\\tworks:\\tfor\\teach\\ttraining\\tinstance\\t\\nx\\n,\\tthe\\talgorithm\\nstarts\\tby\\tfeeding\\tit\\tto\\tthe\\tnetwork\\tby\\tsetting\\tthe\\tstate\\tof\\tthe\\tvisible\\tunits\\tto\\t\\nx\\n1\\n,\\t\\nx\\n2\\n,\\t,\\t\\nx\\nn\\n.\\tThen\\tyou\\tcompute\\nthe\\tstate\\tof\\tthe\\thidden\\tunits\\tby\\tapplying\\tthe\\tstochastic\\tequation\\tdescribed\\tbefore\\t(\\nEquation\\tE-1\\n).\\tThis\\ngives\\tyou\\ta\\thidden\\tvector\\t\\nh\\n\\t(where\\t\\nh\\ni\\n\\tis\\tequal\\tto\\tthe\\tstate\\tof\\tthe\\ti\\nth\\n\\tunit).\\tNext\\tyou\\tcompute\\tthe\\tstate\\tof\\tthe\\nvisible\\tunits,\\tby\\tapplying\\tthe\\tsame\\tstochastic\\tequation.\\tThis\\tgives\\tyou\\ta\\tvector\\t\\n.\\tThen\\tonce\\tagain\\tyou\\ncompute\\tthe\\tstate\\tof\\tthe\\thidden\\tunits,\\twhich\\tgives\\tyou\\ta\\tvector\\t\\n.\\tNow\\tyou\\tcan\\tupdate\\teach\\tconnection\\nweight\\tby\\tapplying\\tthe\\trule\\tin\\t\\nEquation\\tE-2\\n.\\nEquation\\tE-2.\\t\\nContrastive\\tdivergence\\tweight\\tupdate\\nThe\\tgreat\\tbenefit\\tof\\tthis\\talgorithm\\tit\\tthat\\tit\\tdoes\\tnot\\trequire\\twaiting\\tfor\\tthe\\tnetwork\\tto\\treach\\tthermal\\nequilibrium:\\tit\\tjust\\tgoes\\tforward,\\tbackward,\\tand\\tforward\\tagain,\\tand\\tthat’s\\tit.\\tThis\\tmakes\\tit\\tincomparably\\nmore\\tefficient\\tthan\\tprevious\\talgorithms,\\tand\\tit\\twas\\ta\\tkey\\tingredient\\tto\\tthe\\tfirst\\tsuccess\\tof\\tDeep\\tLearning\\nbased\\ton\\tmultiple\\tstacked\\tRBMs.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 625}), Document(page_content='Deep\\tBelief\\tNets\\nSeveral\\t\\nlayers\\tof\\tRBMs\\tcan\\tbe\\tstacked;\\tthe\\thidden\\tunits\\tof\\tthe\\tfirst-level\\tRBM\\tserves\\tas\\tthe\\tvisible\\tunits\\nfor\\tthe\\tsecond-layer\\tRBM,\\tand\\tso\\ton.\\tSuch\\tan\\tRBM\\tstack\\tis\\tcalled\\ta\\t\\ndeep\\tbelief\\tnet\\n\\t(DBN).\\nYee-Whye\\tTeh,\\tone\\tof\\tGeoffrey\\tHinton’s\\tstudents,\\tobserved\\tthat\\tit\\twas\\tpossible\\tto\\ttrain\\tDBNs\\tone\\tlayer\\nat\\ta\\ttime\\tusing\\tContrastive\\tDivergence,\\tstarting\\twith\\tthe\\tlower\\tlayers\\tand\\tthen\\tgradually\\tmoving\\tup\\tto\\tthe\\ntop\\tlayers.\\tThis\\tled\\tto\\tthe\\t\\ngroundbreaking\\tarticle\\tthat\\tkickstarted\\tthe\\tDeep\\tLearning\\ttsunami\\tin\\t2006\\n.\\n2\\nJust\\tlike\\tRBMs,\\tDBNs\\tlearn\\tto\\treproduce\\tthe\\tprobability\\tdistribution\\tof\\ttheir\\tinputs,\\twithout\\tany\\nsupervision.\\tHowever,\\tthey\\tare\\tmuch\\tbetter\\tat\\tit,\\tfor\\tthe\\tsame\\treason\\tthat\\tdeep\\tneural\\tnetworks\\tare\\tmore\\npowerful\\tthan\\tshallow\\tones:\\treal-world\\tdata\\tis\\toften\\torganized\\tin\\thierarchical\\tpatterns,\\tand\\tDBNs\\ttake\\nadvantage\\tof\\tthat.\\tTheir\\tlower\\tlayers\\tlearn\\tlow-level\\tfeatures\\tin\\tthe\\tinput\\tdata,\\twhile\\thigher\\tlayers\\tlearn\\nhigh-level\\tfeatures.\\nJust\\tlike\\tRBMs,\\tDBNs\\tare\\tfundamentally\\tunsupervised,\\tbut\\tyou\\tcan\\talso\\ttrain\\tthem\\tin\\ta\\tsupervised\\nmanner\\tby\\tadding\\tsome\\tvisible\\tunits\\tto\\trepresent\\tthe\\tlabels.\\tMoreover,\\tone\\tgreat\\tfeature\\tof\\tDBNs\\tis\\tthat\\nthey\\tcan\\tbe\\ttrained\\tin\\ta\\tsemisupervised\\tfashion.\\t\\nFigure\\tE-4\\n\\trepresents\\tsuch\\ta\\tDBN\\tconfigured\\tfor\\nsemisupervised\\tlearning.\\nFigure\\tE-4.\\t\\nA\\tdeep\\tbelief\\tnetwork\\tconfigured\\tfor\\tsemisupervised\\tlearning\\nFirst,\\tthe\\tRBM\\t1\\tis\\ttrained\\twithout\\tsupervision.\\tIt\\tlearns\\tlow-level\\tfeatures\\tin\\tthe\\ttraining\\tdata.\\tThen\\nRBM\\t2\\tis\\ttrained\\twith\\tRBM\\t1’s\\thidden\\tunits\\tas\\tinputs,\\tagain\\twithout\\tsupervision:\\tit\\tlearns\\thigher-level\\nfeatures\\t(note\\tthat\\tRBM\\t2’s\\thidden\\tunits\\tinclude\\tonly\\tthe\\tthree\\trightmost\\tunits,\\tnot\\tthe\\tlabel\\tunits).\\nSeveral\\tmore\\tRBMs\\tcould\\tbe\\tstacked\\tthis\\tway,\\tbut\\tyou\\tget\\tthe\\tidea.\\tSo\\tfar,\\ttraining\\twas\\t100%\\nunsupervised.\\tLastly,\\tRBM\\t3\\tis\\ttrained\\tusing\\tboth\\tRBM\\t2’s\\thidden\\tunits\\tas\\tinputs,\\tas\\twell\\tas\\textra\\nvisible\\tunits\\tused\\tto\\trepresent\\tthe\\ttarget\\tlabels\\t(e.g.,\\ta\\tone-hot\\tvector\\trepresenting\\tthe\\tinstance\\tclass).\\tIt\\nlearns\\tto\\tassociate\\thigh-level\\tfeatures\\twith\\ttraining\\tlabels.\\tThis\\tis\\tthe\\tsupervised\\tstep.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 626}), Document(page_content='At\\tthe\\tend\\tof\\ttraining,\\tif\\tyou\\tfeed\\tRBM\\t1\\ta\\tnew\\tinstance,\\tthe\\tsignal\\twill\\tpropagate\\tup\\tto\\tRBM\\t2,\\tthen\\tup\\nto\\tthe\\ttop\\tof\\tRBM\\t3,\\tand\\tthen\\tback\\tdown\\tto\\tthe\\tlabel\\tunits;\\thopefully,\\tthe\\tappropriate\\tlabel\\twill\\tlight\\tup.\\nThis\\tis\\thow\\ta\\tDBN\\tcan\\tbe\\tused\\tfor\\tclassification.\\nOne\\tgreat\\tbenefit\\tof\\tthis\\tsemisupervised\\tapproach\\tis\\tthat\\tyou\\tdon’t\\tneed\\tmuch\\tlabeled\\ttraining\\tdata.\\tIf\\tthe\\nunsupervised\\tRBMs\\tdo\\ta\\tgood\\tenough\\tjob,\\tthen\\tonly\\ta\\tsmall\\tamount\\tof\\tlabeled\\ttraining\\tinstances\\tper\\nclass\\twill\\tbe\\tnecessary.\\tSimilarly,\\ta\\tbaby\\tlearns\\tto\\trecognize\\tobjects\\twithout\\tsupervision,\\tso\\twhen\\tyou\\npoint\\tto\\ta\\tchair\\tand\\tsay\\t“chair,”\\tthe\\tbaby\\tcan\\tassociate\\tthe\\tword\\t“chair”\\twith\\tthe\\tclass\\tof\\tobjects\\tit\\thas\\nalready\\tlearned\\tto\\trecognize\\ton\\tits\\town.\\tYou\\tdon’t\\tneed\\tto\\tpoint\\tto\\tevery\\tsingle\\tchair\\tand\\tsay\\t“chair”;\\nonly\\ta\\tfew\\texamples\\twill\\tsuffice\\t(just\\tenough\\tso\\tthe\\tbaby\\tcan\\tbe\\tsure\\tthat\\tyou\\tare\\tindeed\\treferring\\tto\\tthe\\nchair,\\tnot\\tto\\tits\\tcolor\\tor\\tone\\tof\\tthe\\tchair’s\\tparts).\\nQuite\\tamazingly,\\tDBNs\\tcan\\talso\\twork\\tin\\treverse.\\tIf\\tyou\\tactivate\\tone\\tof\\tthe\\tlabel\\tunits,\\tthe\\tsignal\\twill\\npropagate\\tup\\tto\\tthe\\thidden\\tunits\\tof\\tRBM\\t3,\\tthen\\tdown\\tto\\tRBM\\t2,\\tand\\tthen\\tRBM\\t1,\\tand\\ta\\tnew\\tinstance\\nwill\\tbe\\toutput\\tby\\tthe\\tvisible\\tunits\\tof\\tRBM\\t1.\\tThis\\tnew\\tinstance\\twill\\tusually\\tlook\\tlike\\t\\na\\tregular\\tinstance\\nof\\tthe\\tclass\\twhose\\tlabel\\tunit\\tyou\\tactivated.\\tThis\\tgenerative\\tcapability\\tof\\tDBNs\\tis\\tquite\\tpowerful.\\tFor\\nexample,\\tit\\thas\\tbeen\\tused\\tto\\tautomatically\\tgenerate\\tcaptions\\tfor\\timages,\\tand\\tvice\\tversa:\\tfirst\\ta\\tDBN\\tis\\ntrained\\t(without\\tsupervision)\\tto\\tlearn\\tfeatures\\tin\\timages,\\tand\\tanother\\tDBN\\tis\\ttrained\\t(again\\twithout\\nsupervision)\\tto\\tlearn\\tfeatures\\tin\\tsets\\tof\\tcaptions\\t(e.g.,\\t“car”\\toften\\tcomes\\twith\\t“automobile”).\\tThen\\tan\\nRBM\\tis\\tstacked\\ton\\ttop\\tof\\tboth\\tDBNs\\tand\\ttrained\\twith\\ta\\tset\\tof\\timages\\talong\\twith\\ttheir\\tcaptions;\\tit\\tlearns\\nto\\tassociate\\thigh-level\\tfeatures\\tin\\timages\\twith\\thigh-level\\tfeatures\\tin\\tcaptions.\\tNext,\\tif\\tyou\\tfeed\\tthe\\timage\\nDBN\\tan\\timage\\tof\\ta\\tcar,\\tthe\\tsignal\\twill\\tpropagate\\tthrough\\tthe\\tnetwork,\\tup\\tto\\tthe\\ttop-level\\tRBM,\\tand\\tback\\ndown\\tto\\tthe\\tbottom\\tof\\tthe\\tcaption\\tDBN,\\tproducing\\ta\\tcaption.\\tDue\\tto\\tthe\\tstochastic\\tnature\\tof\\tRBMs\\tand\\nDBNs,\\tthe\\tcaption\\twill\\tkeep\\tchanging\\trandomly,\\tbut\\tit\\twill\\tgenerally\\tbe\\tappropriate\\tfor\\tthe\\timage.\\tIf\\tyou\\ngenerate\\ta\\tfew\\thundred\\tcaptions,\\tthe\\tmost\\tfrequently\\tgenerated\\tones\\twill\\tlikely\\tbe\\ta\\tgood\\t\\ndescription\\tof\\nthe\\timage.\\n3', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 627}), Document(page_content='Self-Organizing\\tMaps\\nSelf-organizing\\tmaps\\n\\t(SOM)\\t\\nare\\tquite\\tdifferent\\tfrom\\tall\\tthe\\tother\\ttypes\\tof\\tneural\\tnetworks\\twe\\thave\\ndiscussed\\tso\\tfar.\\tThey\\tare\\tused\\tto\\tproduce\\ta\\tlow-dimensional\\trepresentation\\tof\\ta\\thigh-dimensional\\ndataset,\\tgenerally\\tfor\\tvisualization,\\tclustering,\\tor\\tclassification.\\tThe\\tneurons\\tare\\tspread\\tacross\\ta\\tmap\\n(typically\\t2D\\tfor\\tvisualization,\\tbut\\tit\\tcan\\tbe\\tany\\tnumber\\tof\\tdimensions\\tyou\\twant),\\tas\\tshown\\tin\\t\\nFigure\\tE-5\\n,\\nand\\teach\\tneuron\\thas\\ta\\tweighted\\tconnection\\tto\\tevery\\tinput\\t(note\\tthat\\tthe\\tdiagram\\tshows\\tjust\\ttwo\\tinputs,\\tbut\\nthere\\tare\\ttypically\\ta\\tvery\\tlarge\\tnumber,\\tsince\\tthe\\twhole\\tpoint\\tof\\tSOMs\\tis\\tto\\treduce\\tdimensionality).\\nFigure\\tE-5.\\t\\nSelf-organizing\\tmaps\\nOnce\\tthe\\tnetwork\\tis\\ttrained,\\tyou\\tcan\\tfeed\\tit\\ta\\tnew\\tinstance\\tand\\tthis\\twill\\tactivate\\tonly\\tone\\tneuron\\t(i.e.,\\nhence\\tone\\tpoint\\ton\\tthe\\tmap):\\tthe\\tneuron\\twhose\\tweight\\tvector\\tis\\tclosest\\tto\\tthe\\tinput\\tvector.\\tIn\\tgeneral,\\ninstances\\tthat\\tare\\tnearby\\tin\\tthe\\toriginal\\tinput\\tspace\\twill\\tactivate\\tneurons\\tthat\\tare\\tnearby\\ton\\tthe\\tmap.\\tThis\\nmakes\\tSOMs\\tuseful\\tfor\\tvisualization\\t(in\\tparticular,\\tyou\\tcan\\teasily\\tidentify\\tclusters\\ton\\tthe\\tmap),\\tbut\\talso\\nfor\\tapplications\\tlike\\tspeech\\trecognition.\\tFor\\texample,\\tif\\teach\\tinstance\\trepresents\\tthe\\taudio\\trecording\\tof\\ta\\nperson\\tpronouncing\\ta\\tvowel,\\tthen\\tdifferent\\tpronunciations\\tof\\tthe\\tvowel\\t“a”\\twill\\tactivate\\tneurons\\tin\\tthe\\nsame\\tarea\\tof\\tthe\\tmap,\\twhile\\tinstances\\tof\\tthe\\tvowel\\t“e”\\twill\\tactivate\\tneurons\\tin\\tanother\\tarea,\\tand\\nintermediate\\tsounds\\twill\\tgenerally\\tactivate\\tintermediate\\tneurons\\ton\\tthe\\tmap.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 628}), Document(page_content='NOTE\\nOne\\timportant\\tdifference\\twith\\tthe\\tother\\tdimensionality\\treduction\\ttechniques\\tdiscussed\\tin\\t\\nChapter\\t8\\n\\tis\\tthat\\tall\\tinstances\\tget\\nmapped\\tto\\ta\\tdiscrete\\tnumber\\tof\\tpoints\\tin\\tthe\\tlow-dimensional\\tspace\\t(one\\tpoint\\tper\\tneuron).\\tWhen\\tthere\\tare\\tvery\\tfew\\tneurons,\\nthis\\ttechnique\\tis\\tbetter\\tdescribed\\tas\\tclustering\\trather\\tthan\\tdimensionality\\treduction.\\nThe\\ttraining\\talgorithm\\tis\\tunsupervised.\\tIt\\tworks\\tby\\thaving\\tall\\tthe\\tneurons\\tcompete\\tagainst\\teach\\tother.\\nFirst,\\tall\\tthe\\tweights\\tare\\tinitialized\\trandomly.\\tThen\\ta\\ttraining\\tinstance\\tis\\tpicked\\trandomly\\tand\\tfed\\tto\\tthe\\nnetwork.\\tAll\\tneurons\\tcompute\\tthe\\tdistance\\tbetween\\ttheir\\tweight\\tvector\\tand\\tthe\\tinput\\tvector\\t(this\\tis\\tvery\\ndifferent\\tfrom\\tthe\\tartificial\\tneurons\\twe\\thave\\tseen\\tso\\tfar).\\tThe\\tneuron\\tthat\\tmeasures\\tthe\\tsmallest\\tdistance\\nwins\\tand\\ttweaks\\tits\\tweight\\tvector\\tto\\tbe\\teven\\tslightly\\tcloser\\tto\\tthe\\tinput\\tvector,\\tmaking\\tit\\tmore\\tlikely\\tto\\nwin\\tfuture\\tcompetitions\\tfor\\tother\\tinputs\\tsimilar\\tto\\tthis\\tone.\\tIt\\talso\\trecruits\\tits\\tneighboring\\tneurons,\\tand\\nthey\\ttoo\\tupdate\\ttheir\\tweight\\tvector\\tto\\tbe\\tslightly\\tcloser\\tto\\tthe\\tinput\\tvector\\t(but\\tthey\\tdon’t\\tupdate\\ttheir\\nweights\\tas\\tmuch\\tas\\tthe\\twinner\\tneuron).\\tThen\\tthe\\talgorithm\\tpicks\\tanother\\ttraining\\tinstance\\tand\\trepeats\\tthe\\nprocess,\\tagain\\tand\\tagain.\\tThis\\t\\nalgorithm\\ttends\\tto\\tmake\\tnearby\\tneurons\\tgradually\\tspecialize\\tin\\tsimilar\\ninputs.\\n4\\n“On\\tContrastive\\tDivergence\\tLearning,”\\tM.\\tÁ.\\tCarreira-Perpiñán\\tand\\tG.\\tHinton\\t(2005).\\n“A\\tFast\\tLearning\\tAlgorithm\\tfor\\tDeep\\tBelief\\tNets,”\\tG.\\tHinton,\\tS.\\tOsindero,\\tY.\\tTeh\\t(2006).\\nSee\\tthis\\tvideo\\tby\\tGeoffrey\\tHinton\\tfor\\tmore\\tdetails\\tand\\ta\\tdemo:\\t\\nhttp://goo.gl/7Z5QiS\\n.\\nYou\\tcan\\timagine\\ta\\tclass\\tof\\tyoung\\tchildren\\twith\\troughly\\tsimilar\\tskills.\\tOne\\tchild\\thappens\\tto\\tbe\\tslightly\\tbetter\\tat\\tbasketball.\\tThis\\tmotivates\\nher\\tto\\tpractice\\tmore,\\tespecially\\twith\\ther\\tfriends.\\tAfter\\ta\\twhile,\\tthis\\tgroup\\tof\\tfriends\\tgets\\tso\\tgood\\tat\\tbasketball\\tthat\\tother\\tkids\\tcannot\\ncompete.\\tBut\\tthat’s\\tokay,\\tbecause\\tthe\\tother\\tkids\\tspecialize\\tin\\tother\\ttopics.\\tAfter\\ta\\twhile,\\tthe\\tclass\\tis\\tfull\\tof\\tlittle\\tspecialized\\tgroups.\\n1\\n2\\n3\\n4', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 629}), Document(page_content='Index\\nSymbols\\n__call__()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\nε-greedy\\tpolicy\\n,\\t\\nExploration\\tPolicies\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nε-insensitive\\n,\\t\\nSVM\\tRegression\\nχ\\t2\\ttest\\n\\t(\\nsee\\n\\tchi\\tsquare\\ttest)\\nℓ\\t0\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nℓ\\t1\\tand\\tℓ\\t2\\tregularization\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n-\\nℓ1\\tand\\tℓ2\\tRegularization\\nℓ\\t1\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n,\\t\\nLasso\\tRegression\\n,\\t\\nDecision\\tBoundaries\\n,\\t\\nAdam\\nOptimization\\n,\\t\\nAvoiding\\tOverfitting\\tThrough\\tRegularization\\nℓ\\t2\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n,\\t\\nRidge\\tRegression\\n-\\nLasso\\tRegression\\n,\\t\\nDecision\\nBoundaries\\n,\\t\\nSoftmax\\tRegression\\n,\\t\\nAvoiding\\tOverfitting\\tThrough\\tRegularization\\n,\\t\\nMax-Norm\\nRegularization\\nℓ\\tk\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nℓ\\t∞\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nA\\naccuracy\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\n-\\nMeasuring\\nAccuracy\\tUsing\\tCross-Validation\\nactions,\\tevaluating\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\n-\\nEvaluating\\tActions:\\nThe\\tCredit\\tAssignment\\tProblem\\nactivation\\tfunctions\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n-\\nMulti-Layer\\tPerceptron\\nand\\tBackpropagation\\nactive\\tconstraints\\n,\\t\\nSVM\\tDual\\tProblem\\nactors\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nactual\\tclass\\n,\\t\\nConfusion\\tMatrix', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 630}), Document(page_content='AdaBoost\\n,\\t\\nAdaBoost\\n-\\nAdaBoost\\nAdagrad\\n,\\t\\nAdaGrad\\n-\\nAdaGrad\\nAdam\\toptimization\\n,\\t\\nAdam\\tOptimization\\n-\\nAdam\\tOptimization\\n,\\t\\nAdam\\tOptimization\\nadaptive\\tlearning\\trate\\n,\\t\\nAdaGrad\\nadaptive\\tmoment\\toptimization\\n,\\t\\nAdam\\tOptimization\\nagents\\n,\\t\\nLearning\\tto\\tOptimize\\tRewards\\nAlexNet\\tarchitecture\\n,\\t\\nAlexNet\\n-\\nAlexNet\\nalgorithms\\npreparing\\tdata\\tfor\\n,\\t\\nPrepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\tAlgorithms\\n-\\nSelect\\tand\\nTrain\\ta\\tModel\\nAlphaGo\\n,\\t\\nReinforcement\\tLearning\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n,\\t\\nReinforcement\\nLearning\\n,\\t\\nPolicy\\tGradients\\nAnaconda\\n,\\t\\nCreate\\tthe\\tWorkspace\\nanomaly\\tdetection\\n,\\t\\nUnsupervised\\tlearning\\nApple’s\\tSiri\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\napply_gradients()\\n,\\t\\nGradient\\tClipping\\n,\\t\\nPolicy\\tGradients\\narea\\tunder\\tthe\\tcurve\\t(AUC)\\n,\\t\\nThe\\tROC\\tCurve\\narray_split()\\n,\\t\\nIncremental\\tPCA\\nartificial\\tneural\\tnetworks\\t(ANNs)\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n-\\nExercises\\nBoltzmann\\tMachines\\n,\\t\\nBoltzmann\\tMachines\\n-\\nBoltzmann\\tMachines\\ndeep\\tbelief\\tnetworks\\t(DBNs)\\n,\\t\\nDeep\\tBelief\\tNets\\n-\\nDeep\\tBelief\\tNets\\nevolution\\tof\\n,\\t\\nFrom\\tBiological\\tto\\tArtificial\\tNeurons\\nHopfield\\tNetworks\\n,\\t\\nHopfield\\tNetworks\\n-\\nHopfield\\tNetworks\\nhyperparameter\\tfine-tuning\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\n-\\nActivation', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 631}), Document(page_content='Functions\\noverview\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n-\\nFrom\\tBiological\\tto\\tArtificial\\nNeurons\\nPerceptrons\\n,\\t\\nThe\\tPerceptron\\n-\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nself-organizing\\tmaps\\n,\\t\\nSelf-Organizing\\tMaps\\n-\\nSelf-Organizing\\tMaps\\ntraining\\ta\\tDNN\\twith\\tTensorFlow\\n,\\t\\nTraining\\ta\\tDNN\\tUsing\\tPlain\\tTensorFlow\\n-\\nUsing\\tthe\\nNeural\\tNetwork\\nartificial\\tneuron\\n,\\t\\nLogical\\tComputations\\twith\\tNeurons\\n(\\nsee\\talso\\n\\tartificial\\tneural\\tnetwork\\t(ANN))\\nassign()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\nassociation\\trule\\tlearning\\n,\\t\\nUnsupervised\\tlearning\\nassociative\\tmemory\\tnetworks\\n,\\t\\nHopfield\\tNetworks\\nassumptions,\\tchecking\\n,\\t\\nCheck\\tthe\\tAssumptions\\nasynchronous\\tupdates\\n,\\t\\nAsynchronous\\tupdates\\n-\\nAsynchronous\\tupdates\\nasynchrous\\tcommunication\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\n-\\nPaddingFifoQueue\\natrous_conv2d()\\n,\\t\\nResNet\\nattention\\tmechanism\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nattributes\\n,\\t\\nSupervised\\tlearning\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\n-\\nTake\\ta\\tQuick\\tLook\\nat\\tthe\\tData\\tStructure\\n(\\nsee\\talso\\n\\tdata\\tstructure)\\ncombinations\\tof\\n,\\t\\nExperimenting\\twith\\tAttribute\\tCombinations\\n-\\nExperimenting\\twith\\nAttribute\\tCombinations\\npreprocessed\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\ntarget\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 632}), Document(page_content='autodiff\\n,\\t\\nUsing\\tautodiff\\n-\\nUsing\\tautodiff\\n,\\t\\nAutodiff\\n-\\nReverse-Mode\\tAutodiff\\nforward-mode\\n,\\t\\nForward-Mode\\tAutodiff\\n-\\nForward-Mode\\tAutodiff\\nmanual\\tdifferentiation\\n,\\t\\nManual\\tDifferentiation\\nnumerical\\tdifferentiation\\n,\\t\\nNumerical\\tDifferentiation\\nreverse-mode\\n,\\t\\nReverse-Mode\\tAutodiff\\n-\\nReverse-Mode\\tAutodiff\\nsymbolic\\tdifferentiation\\n,\\t\\nSymbolic\\tDifferentiation\\n-\\nNumerical\\tDifferentiation\\nautoencoders\\n,\\t\\nAutoencoders\\n-\\nExercises\\nadversarial\\n,\\t\\nOther\\tAutoencoders\\ncontractive\\n,\\t\\nOther\\tAutoencoders\\ndenoising\\n,\\t\\nDenoising\\tAutoencoders\\n-\\nTensorFlow\\tImplementation\\nefficient\\tdata\\trepresentations\\n,\\t\\nEfficient\\tData\\tRepresentations\\ngenerative\\tstochastic\\tnetwork\\t(GSN)\\n,\\t\\nOther\\tAutoencoders\\novercomplete\\n,\\t\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\nPCA\\twith\\tundercomplete\\tlinear\\tautoencoder\\n,\\t\\nPerforming\\tPCA\\twith\\tan\\tUndercomplete\\nLinear\\tAutoencoder\\nreconstructions\\n,\\t\\nEfficient\\tData\\tRepresentations\\nsparse\\n,\\t\\nSparse\\tAutoencoders\\n-\\nTensorFlow\\tImplementation\\nstacked\\n,\\t\\nStacked\\tAutoencoders\\n-\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\nstacked\\tconvolutional\\n,\\t\\nOther\\tAutoencoders\\nundercomplete\\n,\\t\\nEfficient\\tData\\tRepresentations\\nvariational\\n,\\t\\nVariational\\tAutoencoders\\n-\\nGenerating\\tDigits\\nvisualizing\\tfeatures\\n,\\t\\nVisualizing\\tFeatures\\n-\\nVisualizing\\tFeatures\\nwinner-take-all\\t(WTA)\\n,\\t\\nOther\\tAutoencoders\\nautomatic\\tdifferentiating\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 633}), Document(page_content='autonomous\\tdriving\\tsystems\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\nAverage\\tAbsolute\\tDeviation\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\naverage\\tpooling\\tlayer\\n,\\t\\nPooling\\tLayer\\navg_pool()\\n,\\t\\nPooling\\tLayer\\nB\\nbackpropagation\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n-\\nMulti-Layer\\tPerceptron\\tand\\nBackpropagation\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n,\\t\\nUnsupervised\\tPretraining\\n,\\nVisualizing\\tFeatures\\nbackpropagation\\tthrough\\ttime\\t(BPTT)\\n,\\t\\nTraining\\tRNNs\\nbagging\\tand\\tpasting\\n,\\t\\nBagging\\tand\\tPasting\\n-\\nOut-of-Bag\\tEvaluation\\nout-of-bag\\tevaluation\\n,\\t\\nOut-of-Bag\\tEvaluation\\n-\\nOut-of-Bag\\tEvaluation\\nin\\tScikit-Learn\\n,\\t\\nBagging\\tand\\tPasting\\tin\\tScikit-Learn\\n-\\nBagging\\tand\\tPasting\\tin\\tScikit-\\nLearn\\nbandwidth\\tsaturation\\n,\\t\\nBandwidth\\tsaturation\\n-\\nBandwidth\\tsaturation\\nBasicLSTMCell\\n,\\t\\nLSTM\\tCell\\nBasicRNNCell\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\n-\\nDistributing\\ta\\tDeep\\tRNN\\nAcross\\tMultiple\\tGPUs\\nBatch\\tGradient\\tDescent\\n,\\t\\nBatch\\tGradient\\tDescent\\n-\\nBatch\\tGradient\\tDescent\\n,\\t\\nLasso\\tRegression\\nbatch\\tlearning\\n,\\t\\nBatch\\tlearning\\n-\\nBatch\\tlearning\\nBatch\\tNormalization\\n,\\t\\nBatch\\tNormalization\\n-\\nImplementing\\tBatch\\tNormalization\\twith\\nTensorFlow\\n,\\t\\nResNet\\noperation\\tsummary\\n,\\t\\nBatch\\tNormalization\\nwith\\tTensorFlow\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n-\\nImplementing\\nBatch\\tNormalization\\twith\\tTensorFlow\\nbatch()\\n,\\t\\nOther\\tconvenience\\tfunctions\\nbatch_join()\\n,\\t\\nOther\\tconvenience\\tfunctions', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 634}), Document(page_content='batch_normalization()\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n-\\nImplementing\\nBatch\\tNormalization\\twith\\tTensorFlow\\nBellman\\tOptimality\\tEquation\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nbetween-graph\\treplication\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\nbias\\tneurons\\n,\\t\\nThe\\tPerceptron\\nbias\\tterm\\n,\\t\\nLinear\\tRegression\\nbias/variance\\ttradeoff\\n,\\t\\nLearning\\tCurves\\nbiases\\n,\\t\\nConstruction\\tPhase\\nbinary\\tclassifiers\\n,\\t\\nTraining\\ta\\tBinary\\tClassifier\\n,\\t\\nLogistic\\tRegression\\nbiological\\tneurons\\n,\\t\\nFrom\\tBiological\\tto\\tArtificial\\tNeurons\\n-\\nBiological\\tNeurons\\nblack\\tbox\\tmodels\\n,\\t\\nMaking\\tPredictions\\nblending\\n,\\t\\nStacking\\n-\\nExercises\\nBoltzmann\\tMachines\\n,\\t\\nBoltzmann\\tMachines\\n-\\nBoltzmann\\tMachines\\n(\\nsee\\talso\\n\\trestricted\\tBoltzman\\tmachines\\t(RBMs))\\nboosting\\n,\\t\\nBoosting\\n-\\nGradient\\tBoosting\\nAdaBoost\\n,\\t\\nAdaBoost\\n-\\nAdaBoost\\nGradient\\tBoosting\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\nbootstrap\\taggregation\\n\\t(\\nsee\\n\\tbagging)\\nbootstrapping\\n,\\t\\nGrid\\tSearch\\n,\\t\\nBagging\\tand\\tPasting\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\n,\\t\\nLearning\\tto\\nPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nbottleneck\\tlayers\\n,\\t\\nGoogLeNet\\nbrew\\n,\\t\\nStacking\\nC\\nCaffe\\tmodel\\tzoo\\n,\\t\\nModel\\tZoos', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 635}), Document(page_content='CART\\t(Classification\\tand\\tRegression\\tTree)\\talgorithm\\n,\\t\\nMaking\\tPredictions\\n-\\nThe\\tCART\\tTraining\\nAlgorithm\\n,\\t\\nRegression\\ncategorical\\tattributes\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\n-\\nHandling\\tText\\tand\\nCategorical\\tAttributes\\ncell\\twrapper\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\nchi\\tsquare\\ttest\\n,\\t\\nRegularization\\tHyperparameters\\nclassification\\tversus\\tregression\\n,\\t\\nSupervised\\tlearning\\n,\\t\\nMultioutput\\tClassification\\nclassifiers\\nbinary\\n,\\t\\nTraining\\ta\\tBinary\\tClassifier\\nerror\\tanalysis\\n,\\t\\nError\\tAnalysis\\n-\\nError\\tAnalysis\\nevaluating\\n,\\t\\nMulticlass\\tClassification\\nMNIST\\tdataset\\n,\\t\\nMNIST\\n-\\nMNIST\\nmulticlass\\n,\\t\\nMulticlass\\tClassification\\n-\\nMulticlass\\tClassification\\nmultilabel\\n,\\t\\nMultilabel\\tClassification\\n-\\nMultilabel\\tClassification\\nmultioutput\\n,\\t\\nMultioutput\\tClassification\\n-\\nMultioutput\\tClassification\\nperformance\\tmeasures\\n,\\t\\nPerformance\\tMeasures\\n-\\nThe\\tROC\\tCurve\\nprecision\\tof\\n,\\t\\nConfusion\\tMatrix\\nvoting\\n,\\t\\nVoting\\tClassifiers\\n-\\nVoting\\tClassifiers\\nclip_by_value()\\n,\\t\\nGradient\\tClipping\\nclosed-form\\tequation\\n,\\t\\nTraining\\tModels\\n,\\t\\nRidge\\tRegression\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\ncluster\\tspecification\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\nclustering\\talgorithms\\n,\\t\\nUnsupervised\\tlearning\\nclusters\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\ncoding\\tspace\\n,\\t\\nVariational\\tAutoencoders', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 636}), Document(page_content='codings\\n,\\t\\nAutoencoders\\ncomplementary\\tslackness\\tcondition\\n,\\t\\nSVM\\tDual\\tProblem\\ncomponents_\\n,\\t\\nUsing\\tScikit-Learn\\ncomputational\\tcomplexity\\n,\\t\\nComputational\\tComplexity\\n,\\t\\nComputational\\tComplexity\\n,\\nComputational\\tComplexity\\ncompute_gradients()\\n,\\t\\nGradient\\tClipping\\n,\\t\\nPolicy\\tGradients\\nconcat()\\n,\\t\\nGoogLeNet\\nconfig.gpu_options\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM\\nConfigProto\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM\\nconfusion\\tmatrix\\n,\\t\\nConfusion\\tMatrix\\n-\\nConfusion\\tMatrix\\n,\\t\\nError\\tAnalysis\\n-\\nError\\tAnalysis\\nconnectionism\\n,\\t\\nThe\\tPerceptron\\nconstrained\\toptimization\\n,\\t\\nTraining\\tObjective\\n,\\t\\nSVM\\tDual\\tProblem\\nContrastive\\tDivergence\\n,\\t\\nRestricted\\tBoltzmann\\tMachines\\ncontrol\\tdependencies\\n,\\t\\nControl\\tDependencies\\nconv1d()\\n,\\t\\nResNet\\nconv2d_transpose()\\n,\\t\\nResNet\\nconv3d()\\n,\\t\\nResNet\\nconvergence\\trate\\n,\\t\\nBatch\\tGradient\\tDescent\\nconvex\\tfunction\\n,\\t\\nGradient\\tDescent\\nconvolution\\tkernels\\n,\\t\\nFilters\\n,\\t\\nCNN\\tArchitectures\\n,\\t\\nGoogLeNet\\nconvolutional\\tneural\\tnetworks\\t(CNNs)\\n,\\t\\nConvolutional\\tNeural\\tNetworks\\n-\\nExercises\\narchitectures\\n,\\t\\nCNN\\tArchitectures\\n-\\nResNet\\nAlexNet\\n,\\t\\nAlexNet\\n-\\nAlexNet\\nGoogleNet\\n,\\t\\nGoogLeNet\\n-\\nGoogLeNet', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 637}), Document(page_content='LeNet5\\n,\\t\\nLeNet-5\\n-\\nLeNet-5\\nResNet\\n,\\t\\nResNet\\n-\\nResNet\\nconvolutional\\tlayer\\n,\\t\\nConvolutional\\tLayer\\n-\\nMemory\\tRequirements\\n,\\t\\nGoogLeNet\\n,\\t\\nResNet\\nfeature\\tmaps\\n,\\t\\nStacking\\tMultiple\\tFeature\\tMaps\\n-\\nTensorFlow\\tImplementation\\nfilters\\n,\\t\\nFilters\\nmemory\\trequirement\\n,\\t\\nMemory\\tRequirements\\n-\\nMemory\\tRequirements\\nevolution\\tof\\n,\\t\\nThe\\tArchitecture\\tof\\tthe\\tVisual\\tCortex\\npooling\\tlayer\\n,\\t\\nPooling\\tLayer\\n-\\nPooling\\tLayer\\nTensorFlow\\timplementation\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\tImplementation\\nCoordinator\\tclass\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\ncorrelation\\tcoefficient\\n,\\t\\nLooking\\tfor\\tCorrelations\\n-\\nLooking\\tfor\\tCorrelations\\ncorrelations,\\tfinding\\n,\\t\\nLooking\\tfor\\tCorrelations\\n-\\nLooking\\tfor\\tCorrelations\\ncost\\tfunction\\n,\\t\\nModel-based\\tlearning\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nin\\tAdaBoost\\n,\\t\\nAdaBoost\\nin\\tadagrad\\n,\\t\\nAdaGrad\\nin\\tartificial\\tneural\\tnetworks\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\n,\\nConstruction\\tPhase\\n-\\nConstruction\\tPhase\\nin\\tautodiff\\n,\\t\\nUsing\\tautodiff\\nin\\tbatch\\tnormalization\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\ncross\\tentropy\\n,\\t\\nLeNet-5\\ndeep\\tQ-Learning\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nin\\tElastic\\tNet\\n,\\t\\nElastic\\tNet\\nin\\tGradient\\tDescent\\n,\\t\\nTraining\\tModels\\n,\\t\\nGradient\\tDescent\\n-\\nGradient\\tDescent\\n,\\t\\nBatch', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 638}), Document(page_content='Gradient\\tDescent\\n,\\t\\nBatch\\tGradient\\tDescent\\n-\\nStochastic\\tGradient\\tDescent\\n,\\t\\nGradient\\nBoosting\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\nin\\tLogistic\\tRegression\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\n-\\nTraining\\tand\\tCost\\tFunction\\nin\\tPG\\talgorithms\\n,\\t\\nPolicy\\tGradients\\nin\\tvariational\\tautoencoders\\n,\\t\\nVariational\\tAutoencoders\\nin\\tLasso\\t\\nRegression\\n,\\t\\nLasso\\tRegression\\n-\\nLasso\\tRegression\\nin\\tLinear\\tRegression\\n,\\t\\nThe\\tNormal\\tEquation\\n,\\t\\nGradient\\tDescent\\nin\\tMomentum\\toptimization\\n,\\t\\nMomentum\\tOptimization\\n-\\nNesterov\\tAccelerated\\tGradient\\nin\\tpretrained\\tlayers\\treuse\\n,\\t\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\nin\\tridge\\tregression\\n,\\t\\nRidge\\tRegression\\n-\\nRidge\\tRegression\\nin\\tRNNs\\n,\\t\\nTraining\\tRNNs\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\nstale\\tgradients\\tand\\n,\\t\\nAsynchronous\\tupdates\\ncreative\\tsequences\\n,\\t\\nCreative\\tRNN\\ncredit\\tassignment\\tproblem\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\n-\\nEvaluating\\nActions:\\tThe\\tCredit\\tAssignment\\tProblem\\ncritics\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ncross\\tentropy\\n,\\t\\nSoftmax\\tRegression\\n-\\nSoftmax\\tRegression\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\nHigh-Level\\tAPI\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nPolicy\\tGradients\\ncross-validation\\n,\\t\\nTesting\\tand\\tValidating\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n-\\nBetter\\nEvaluation\\tUsing\\tCross-Validation\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\n-\\nMeasuring\\nAccuracy\\tUsing\\tCross-Validation\\nCUDA\\tlibrary\\n,\\t\\nInstallation\\ncuDNN\\tlibrary\\n,\\t\\nInstallation\\ncurse\\tof\\tdimensionality\\n,\\t\\nDimensionality\\tReduction\\n-\\nThe\\tCurse\\tof\\tDimensionality\\n(\\nsee\\talso\\n\\tdimensionality\\treduction)', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 639}), Document(page_content='custom\\ttransformers\\n,\\t\\nCustom\\tTransformers\\n-\\nCustom\\tTransformers\\nD\\ndata\\n,\\t\\nTesting\\tand\\tValidating\\n(\\nsee\\talso\\n\\ttest\\tdata;\\ttraining\\tdata)\\ncreating\\tworkspace\\tfor\\n,\\t\\nGet\\tthe\\tData\\n-\\nDownload\\tthe\\tData\\ndownloading\\n,\\t\\nDownload\\tthe\\tData\\n-\\nDownload\\tthe\\tData\\nfinding\\tcorrelations\\tin\\n,\\t\\nLooking\\tfor\\tCorrelations\\n-\\nLooking\\tfor\\tCorrelations\\nmaking\\tassumptions\\tabout\\n,\\t\\nTesting\\tand\\tValidating\\npreparing\\tfor\\tMachine\\tLearning\\talgorithms\\n,\\t\\nPrepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\nAlgorithms\\n-\\nSelect\\tand\\tTrain\\ta\\tModel\\ntest-set\\tcreation\\n,\\t\\nCreate\\ta\\tTest\\tSet\\n-\\nCreate\\ta\\tTest\\tSet\\nworking\\twith\\treal\\tdata\\n,\\t\\nWorking\\twith\\tReal\\tData\\ndata\\taugmentation\\n,\\t\\nData\\tAugmentation\\n-\\nData\\tAugmentation\\ndata\\tcleaning\\n,\\t\\nData\\tCleaning\\n-\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\ndata\\tmining\\n,\\t\\nWhy\\tUse\\tMachine\\tLearning?\\ndata\\tparallelism\\n,\\t\\nData\\tParallelism\\n-\\nTensorFlow\\timplementation\\nasynchronous\\tupdates\\n,\\t\\nAsynchronous\\tupdates\\n-\\nAsynchronous\\tupdates\\nbandwidth\\tsaturation\\n,\\t\\nBandwidth\\tsaturation\\n-\\nBandwidth\\tsaturation\\nsynchronous\\tupdates\\n,\\t\\nSynchronous\\tupdates\\nTensorFlow\\timplementation\\n,\\t\\nTensorFlow\\timplementation\\ndata\\tpipeline\\n,\\t\\nFrame\\tthe\\tProblem\\ndata\\tsnooping\\tbias\\n,\\t\\nCreate\\ta\\tTest\\tSet\\ndata\\tstructure\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\n-\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\nStructure', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 640}), Document(page_content='data\\tvisualization\\n,\\t\\nVisualizing\\tGeographical\\tData\\n-\\nVisualizing\\tGeographical\\tData\\nDataFrame\\n,\\t\\nData\\tCleaning\\ndataquest\\n,\\t\\nOther\\tResources\\ndecision\\tboundaries\\n,\\t\\nDecision\\tBoundaries\\n-\\nDecision\\tBoundaries\\n,\\t\\nSoftmax\\tRegression\\n,\\t\\nMaking\\nPredictions\\ndecision\\tfunction\\n,\\t\\nPrecision/Recall\\tTradeoff\\n,\\t\\nDecision\\tFunction\\tand\\tPredictions\\n-\\nDecision\\nFunction\\tand\\tPredictions\\nDecision\\tStumps\\n,\\t\\nAdaBoost\\ndecision\\tthreshold\\n,\\t\\nPrecision/Recall\\tTradeoff\\nDecision\\tTrees\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n-\\nBetter\\tEvaluation\\tUsing\\tCross-\\nValidation\\n,\\t\\nDecision\\tTrees\\n-\\nExercises\\n,\\t\\nEnsemble\\tLearning\\tand\\tRandom\\tForests\\nbinary\\ttrees\\n,\\t\\nMaking\\tPredictions\\nclass\\tprobability\\testimates\\n,\\t\\nEstimating\\tClass\\tProbabilities\\ncomputational\\tcomplexity\\n,\\t\\nComputational\\tComplexity\\ndecision\\tboundaries\\n,\\t\\nMaking\\tPredictions\\nGINI\\timpurity\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\ninstability\\twith\\n,\\t\\nInstability\\n-\\nInstability\\nnumbers\\tof\\tchildren\\n,\\t\\nMaking\\tPredictions\\npredictions\\n,\\t\\nMaking\\tPredictions\\n-\\nEstimating\\tClass\\tProbabilities\\nRandom\\tForests\\n\\t(\\nsee\\n\\tRandom\\tForests)\\nregression\\ttasks\\n,\\t\\nRegression\\n-\\nRegression\\nregularization\\thyperparameters\\n,\\t\\nRegularization\\tHyperparameters\\n-\\nRegularization\\nHyperparameters\\ntraining\\tand\\tvisualizing\\n,\\t\\nTraining\\tand\\tVisualizing\\ta\\tDecision\\tTree\\n-\\nMaking\\tPredictions\\ndecoder\\n,\\t\\nEfficient\\tData\\tRepresentations', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 641}), Document(page_content='deconvolutional\\tlayer\\n,\\t\\nResNet\\ndeep\\tautoencoders\\n\\t(\\nsee\\n\\tstacked\\tautoencoders)\\ndeep\\tbelief\\tnetworks\\t(DBNs)\\n,\\t\\nSemisupervised\\tlearning\\n,\\t\\nDeep\\tBelief\\tNets\\n-\\nDeep\\tBelief\\tNets\\nDeep\\tLearning\\n,\\t\\nReinforcement\\tLearning\\n(\\nsee\\talso\\n\\tReinforcement\\tLearning;\\tTensorFlow)\\nabout\\n,\\t\\nThe\\tMachine\\tLearning\\tTsunami\\n,\\t\\nRoadmap\\nlibraries\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\n-\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\ndeep\\tneural\\tnetworks\\t(DNNs)\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n,\\t\\nTraining\\tDeep\\nNeural\\tNets\\n-\\nExercises\\n(\\nsee\\talso\\n\\tMulti-Layer\\tPerceptrons\\t(MLP))\\nfaster\\toptimizers\\tfor\\n,\\t\\nFaster\\tOptimizers\\n-\\nLearning\\tRate\\tScheduling\\nregularization\\n,\\t\\nAvoiding\\tOverfitting\\tThrough\\tRegularization\\n-\\nData\\tAugmentation\\nreusing\\tpretrained\\tlayers\\n,\\t\\nReusing\\tPretrained\\tLayers\\n-\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\ntraining\\tguidelines\\toverview\\n,\\t\\nPractical\\tGuidelines\\ntraining\\twith\\tTensorFlow\\n,\\t\\nTraining\\ta\\tDNN\\tUsing\\tPlain\\tTensorFlow\\n-\\nUsing\\tthe\\tNeural\\nNetwork\\ntraining\\twith\\tTF.Learn\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nunstable\\tgradients\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\nvanishing\\tand\\texploding\\tgradients\\n,\\t\\nTraining\\tDeep\\tNeural\\tNets\\n-\\nGradient\\tClipping\\nDeep\\tQ-Learning\\n,\\t\\nApproximate\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-\\nLearning\\nMs.\\tPac\\tMan\\texample\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ndeep\\tQ-network\\n,\\t\\nApproximate\\tQ-Learning\\ndeep\\tRNNs\\n,\\t\\nDeep\\tRNNs\\n-\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 642}), Document(page_content='applying\\tdropout\\n,\\t\\nApplying\\tDropout\\ndistributing\\tacross\\tmultiple\\tGPUs\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\nlong\\tsequence\\tdifficulties\\n,\\t\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps\\ntruncated\\tbackpropagation\\tthrough\\ttime\\n,\\t\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\nSteps\\nDeepMind\\n,\\t\\nReinforcement\\tLearning\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n,\\nReinforcement\\tLearning\\n,\\t\\nApproximate\\tQ-Learning\\ndegrees\\tof\\tfreedom\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n,\\t\\nLearning\\tCurves\\ndenoising\\tautoencoders\\n,\\t\\nDenoising\\tAutoencoders\\n-\\nTensorFlow\\tImplementation\\ndense()\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nTying\\tWeights\\ndepth\\tconcat\\tlayer\\n,\\t\\nGoogLeNet\\ndepth\\tradius\\n,\\t\\nAlexNet\\ndepthwise_conv2d()\\n,\\t\\nResNet\\ndequeue()\\n,\\t\\nQueues\\tof\\ttuples\\ndequeue_many()\\n,\\t\\nQueues\\tof\\ttuples\\n,\\t\\nPaddingFifoQueue\\ndequeue_up_to()\\n,\\t\\nClosing\\ta\\tqueue\\n-\\nPaddingFifoQueue\\ndequeuing\\tdata\\n,\\t\\nDequeuing\\tdata\\ndescribe()\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\ndevice\\tblocks\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\ndevice()\\n,\\t\\nSimple\\tplacement\\ndimensionality\\treduction\\n,\\t\\nUnsupervised\\tlearning\\n,\\t\\nDimensionality\\tReduction\\n-\\nExercises\\n,\\nAutoencoders\\napproaches\\tto\\nManifold\\tLearning\\n,\\t\\nManifold\\tLearning', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 643}), Document(page_content='projection\\n,\\t\\nProjection\\n-\\nProjection\\nchoosing\\tthe\\tright\\tnumber\\tof\\tdimensions\\n,\\t\\nChoosing\\tthe\\tRight\\tNumber\\tof\\tDimensions\\ncurse\\tof\\tdimensionality\\n,\\t\\nDimensionality\\tReduction\\n-\\nThe\\tCurse\\tof\\tDimensionality\\nand\\tdata\\tvisualization\\n,\\t\\nDimensionality\\tReduction\\nIsomap\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\nLLE\\t(Locally\\tLinear\\tEmbedding)\\n,\\t\\nLLE\\n-\\nLLE\\nMultidimensional\\tScaling\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\n-\\nOther\\nDimensionality\\tReduction\\tTechniques\\nPCA\\t(Principal\\tComponent\\tAnalysis)\\n,\\t\\nPCA\\n-\\nRandomized\\tPCA\\nt-Distributed\\tStochastic\\tNeighbor\\tEmbedding\\t(t-SNE)\\n,\\t\\nOther\\tDimensionality\\tReduction\\nTechniques\\ndiscount\\trate\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\ndistributed\\tcomputing\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\ndistributed\\tsessions\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\n-\\nSharing\\tState\\nAcross\\tSessions\\tUsing\\tResource\\tContainers\\nDNNClassifier\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\ndrop()\\n,\\t\\nPrepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\tAlgorithms\\ndropconnect\\n,\\t\\nDropout\\ndropna()\\n,\\t\\nData\\tCleaning\\ndropout\\n,\\t\\nNumber\\tof\\tNeurons\\tper\\tHidden\\tLayer\\n,\\t\\nApplying\\tDropout\\ndropout\\trate\\n,\\t\\nDropout\\ndropout()\\n,\\t\\nDropout\\nDropoutWrapper\\n,\\t\\nApplying\\tDropout\\nDRY\\t(Don’t\\tRepeat\\tYourself)\\n,\\t\\nModularity', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 644}), Document(page_content='Dual\\tAveraging\\n,\\t\\nAdam\\tOptimization\\ndual\\tnumbers\\n,\\t\\nForward-Mode\\tAutodiff\\ndual\\tproblem\\n,\\t\\nThe\\tDual\\tProblem\\nduality\\n,\\t\\nSVM\\tDual\\tProblem\\ndying\\tReLUs\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\ndynamic\\tplacements\\n,\\t\\nDynamic\\tplacement\\tfunction\\ndynamic\\tplacer\\n,\\t\\nPlacing\\tOperations\\ton\\tDevices\\nDynamic\\tProgramming\\n,\\t\\nMarkov\\tDecision\\tProcesses\\ndynamic\\tunrolling\\tthrough\\ttime\\n,\\t\\nDynamic\\tUnrolling\\tThrough\\tTime\\ndynamic_rnn()\\n,\\t\\nDynamic\\tUnrolling\\tThrough\\tTime\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\nGPUs\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nE\\nearly\\tstopping\\n,\\t\\nEarly\\tStopping\\n-\\nEarly\\tStopping\\n,\\t\\nGradient\\tBoosting\\n,\\t\\nNumber\\tof\\tNeurons\\tper\\nHidden\\tLayer\\n,\\t\\nEarly\\tStopping\\nElastic\\tNet\\n,\\t\\nElastic\\tNet\\nembedded\\tdevice\\tblocks\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nEmbedded\\tReber\\tgrammars\\n,\\t\\nExercises\\nembeddings\\n,\\t\\nWord\\tEmbeddings\\n-\\nWord\\tEmbeddings\\nembedding_lookup()\\n,\\t\\nWord\\tEmbeddings\\nencoder\\n,\\t\\nEfficient\\tData\\tRepresentations\\nEncoder–Decoder\\n,\\t\\nInput\\tand\\tOutput\\tSequences\\nend-of-sequence\\t(EOS)\\ttoken\\n,\\t\\nHandling\\tVariable-Length\\tOutput\\tSequences\\nenergy\\tfunctions\\n,\\t\\nHopfield\\tNetworks\\nenqueuing\\tdata\\n,\\t\\nEnqueuing\\tdata', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 645}), Document(page_content='Ensemble\\tLearning\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n,\\t\\nEnsemble\\tMethods\\n,\\t\\nEnsemble\\nLearning\\tand\\tRandom\\tForests\\n-\\nExercises\\nbagging\\tand\\tpasting\\n,\\t\\nBagging\\tand\\tPasting\\n-\\nOut-of-Bag\\tEvaluation\\nboosting\\n,\\t\\nBoosting\\n-\\nGradient\\tBoosting\\nin-graph\\tversus\\tbetween-graph\\treplication\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\nReplication\\n-\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\nRandom\\tForests\\n,\\t\\nRandom\\tForests\\n-\\nFeature\\tImportance\\n(\\nsee\\talso\\n\\tRandom\\tForests)\\nrandom\\tpatches\\tand\\trandom\\tsubspaces\\n,\\t\\nRandom\\tPatches\\tand\\tRandom\\tSubspaces\\nstacking\\n,\\t\\nStacking\\n-\\nStacking\\nentropy\\timpurity\\tmeasure\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\nenvironments,\\tin\\treinforcement\\tlearning\\n,\\t\\nLearning\\tto\\tOptimize\\tRewards\\n-\\nEvaluating\\tActions:\\nThe\\tCredit\\tAssignment\\tProblem\\n,\\t\\nExploration\\tPolicies\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\nDeep\\tQ-Learning\\nepisodes\\t(in\\tRL)\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\nProblem\\n-\\nPolicy\\tGradients\\n,\\t\\nPolicy\\tGradients\\n-\\nPolicy\\tGradients\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\nUsing\\tDeep\\tQ-Learning\\nepochs\\n,\\t\\nStochastic\\tGradient\\tDescent\\nε-insensitive\\n,\\t\\nSVM\\tRegression\\nequality\\tcontraints\\n,\\t\\nSVM\\tDual\\tProblem\\nerror\\tanalysis\\n,\\t\\nError\\tAnalysis\\n-\\nError\\tAnalysis\\nestimators\\n,\\t\\nData\\tCleaning\\nEuclidian\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\neval()\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\nevaluating\\tmodels\\n,\\t\\nTesting\\tand\\tValidating\\n-\\nTesting\\tand\\tValidating\\nexplained\\tvariance\\n,\\t\\nChoosing\\tthe\\tRight\\tNumber\\tof\\tDimensions', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 646}), Document(page_content='explained\\tvariance\\tratio\\n,\\t\\nExplained\\tVariance\\tRatio\\nexploding\\tgradients\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n(\\nsee\\talso\\n\\tgradients,\\tvanishing\\tand\\texploding)\\nexploration\\tpolicies\\n,\\t\\nExploration\\tPolicies\\nexponential\\tdecay\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\nexponential\\tlinear\\tunit\\t(ELU)\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\n-\\nNonsaturating\\tActivation\\nFunctions\\nexponential\\tscheduling\\n,\\t\\nLearning\\tRate\\tScheduling\\nExtra-Trees\\n,\\t\\nExtra-Trees\\nF\\nF-1\\tscore\\n,\\t\\nPrecision\\tand\\tRecall\\n-\\nPrecision\\tand\\tRecall\\nface-recognition\\n,\\t\\nMultilabel\\tClassification\\nfake\\tX\\tserver\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\nfalse\\tpositive\\trate\\t(FPR)\\n,\\t\\nThe\\tROC\\tCurve\\n-\\nThe\\tROC\\tCurve\\nfan-in\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\nfan-out\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\nfeature\\tdetection\\n,\\t\\nAutoencoders\\nfeature\\tengineering\\n,\\t\\nIrrelevant\\tFeatures\\nfeature\\textraction\\n,\\t\\nUnsupervised\\tlearning\\nfeature\\timportance\\n,\\t\\nFeature\\tImportance\\n-\\nFeature\\tImportance\\nfeature\\tmaps\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\n,\\t\\nFilters\\n-\\nTensorFlow\\nImplementation\\n,\\t\\nResNet\\nfeature\\tscaling\\n,\\t\\nFeature\\tScaling\\nfeature\\tselection\\n,\\t\\nIrrelevant\\tFeatures\\n,\\t\\nGrid\\tSearch\\n,\\t\\nLasso\\tRegression\\n,\\t\\nFeature\\tImportance\\n,', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 647}), Document(page_content='Prepare\\tthe\\tData\\nfeature\\tspace\\n,\\t\\nKernel\\tPCA\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nfeature\\tvector\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n,\\t\\nLinear\\tRegression\\n,\\t\\nUnder\\tthe\\tHood\\n,\\nImplementing\\tGradient\\tDescent\\nfeatures\\n,\\t\\nSupervised\\tlearning\\nFeatureUnion\\n,\\t\\nTransformation\\tPipelines\\nfeedforward\\tneural\\tnetwork\\t(FNN)\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nfeed_dict\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\nFIFOQueue\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\n,\\t\\nRandomShuffleQueue\\nfillna()\\n,\\t\\nData\\tCleaning\\nfirst-in\\tfirst-out\\t(FIFO)\\tqueues\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\nfirst-order\\tpartial\\tderivatives\\t(Jacobians)\\n,\\t\\nAdam\\tOptimization\\nfit()\\n,\\t\\nData\\tCleaning\\n,\\t\\nTransformation\\tPipelines\\n,\\t\\nIncremental\\tPCA\\nfitness\\tfunction\\n,\\t\\nModel-based\\tlearning\\nfit_inverse_transform=\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nfit_transform()\\n,\\t\\nData\\tCleaning\\n,\\t\\nTransformation\\tPipelines\\nfolds\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n,\\t\\nMNIST\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-\\nValidation\\n-\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nFollow\\tThe\\tRegularized\\tLeader\\t(FTRL)\\n,\\t\\nAdam\\tOptimization\\nforget\\tgate\\n,\\t\\nLSTM\\tCell\\nforward-mode\\tautodiff\\n,\\t\\nForward-Mode\\tAutodiff\\n-\\nForward-Mode\\tAutodiff\\nframing\\ta\\tproblem\\n,\\t\\nFrame\\tthe\\tProblem\\n-\\nFrame\\tthe\\tProblem\\nfrozen\\tlayers\\n,\\t\\nFreezing\\tthe\\tLower\\tLayers\\n-\\nCaching\\tthe\\tFrozen\\tLayers', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 648}), Document(page_content='functools.partial()\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n,\\t\\nTensorFlow\\nImplementation\\n,\\t\\nVariational\\tAutoencoders\\nG\\ngame\\tplay\\n\\t(\\nsee\\n\\treinforcement\\tlearning)\\ngamma\\tvalue\\n,\\t\\nGaussian\\tRBF\\tKernel\\ngate\\tcontrollers\\n,\\t\\nLSTM\\tCell\\nGaussian\\tdistribution\\n,\\t\\nVariational\\tAutoencoders\\n,\\t\\nGenerating\\tDigits\\nGaussian\\tRBF\\n,\\t\\nAdding\\tSimilarity\\tFeatures\\nGaussian\\tRBF\\tkernel\\n,\\t\\nGaussian\\tRBF\\tKernel\\n-\\nGaussian\\tRBF\\tKernel\\n,\\t\\nKernelized\\tSVM\\ngeneralization\\terror\\n,\\t\\nTesting\\tand\\tValidating\\ngeneralized\\tLagrangian\\n,\\t\\nSVM\\tDual\\tProblem\\n-\\nSVM\\tDual\\tProblem\\ngenerative\\tautoencoders\\n,\\t\\nVariational\\tAutoencoders\\ngenerative\\tmodels\\n,\\t\\nAutoencoders\\n,\\t\\nBoltzmann\\tMachines\\ngenetic\\talgorithms\\n,\\t\\nPolicy\\tSearch\\ngeodesic\\tdistance\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\nget_variable()\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\nGINI\\timpurity\\n,\\t\\nMaking\\tPredictions\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\nglobal\\taverage\\tpooling\\n,\\t\\nGoogLeNet\\nglobal_step\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nglobal_variables_initializer()\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\nGlorot\\tinitialization\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n-\\nXavier\\tand\\tHe\\tInitialization\\nGoogle\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nGoogle\\tImages\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 649}), Document(page_content='Google\\tPhotos\\n,\\t\\nSemisupervised\\tlearning\\nGoogleNet\\tarchitecture\\n,\\t\\nGoogLeNet\\n-\\nGoogLeNet\\ngpu_options.per_process_gpu_memory_fraction\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM\\ngradient\\tascent\\n,\\t\\nPolicy\\tSearch\\nGradient\\tBoosted\\tRegression\\tTrees\\t(GBRT)\\n,\\t\\nGradient\\tBoosting\\nGradient\\tBoosting\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\nGradient\\tDescent\\t(GD)\\n,\\t\\nTraining\\tModels\\n,\\t\\nGradient\\tDescent\\n-\\nMini-batch\\tGradient\\tDescent\\n,\\nOnline\\tSVMs\\n,\\t\\nTraining\\tDeep\\tNeural\\tNets\\n,\\t\\nMomentum\\tOptimization\\n,\\t\\nAdaGrad\\nalgorithm\\tcomparisons\\n,\\t\\nMini-batch\\tGradient\\tDescent\\n-\\nMini-batch\\tGradient\\tDescent\\nautomatically\\tcomputing\\tgradients\\n,\\t\\nUsing\\tautodiff\\n-\\nUsing\\tautodiff\\nBatch\\tGD\\n,\\t\\nBatch\\tGradient\\tDescent\\n-\\nBatch\\tGradient\\tDescent\\n,\\t\\nLasso\\tRegression\\ndefining\\n,\\t\\nGradient\\tDescent\\nlocal\\tminimum\\tversus\\tglobal\\tminimum\\n,\\t\\nGradient\\tDescent\\nmanually\\tcomputing\\tgradients\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\nMini-batch\\tGD\\n,\\t\\nMini-batch\\tGradient\\tDescent\\n-\\nMini-batch\\tGradient\\tDescent\\n,\\t\\nFeeding\\nData\\tto\\tthe\\tTraining\\tAlgorithm\\n-\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\noptimizer\\n,\\t\\nUsing\\tan\\tOptimizer\\nStochastic\\tGD\\n,\\t\\nStochastic\\tGradient\\tDescent\\n-\\nStochastic\\tGradient\\tDescent\\n,\\t\\nSoft\\tMargin\\nClassification\\nwith\\tTensorFlow\\n,\\t\\nImplementing\\tGradient\\tDescent\\n-\\nUsing\\tan\\tOptimizer\\nGradient\\tTree\\tBoosting\\n,\\t\\nGradient\\tBoosting\\nGradientDescentOptimizer\\n,\\t\\nConstruction\\tPhase\\ngradients()\\n,\\t\\nUsing\\tautodiff\\ngradients,\\tvanishing\\tand\\texploding\\n,\\t\\nTraining\\tDeep\\tNeural\\tNets\\n-\\nGradient\\tClipping\\n,\\t\\nThe\\nDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 650}), Document(page_content=\"Batch\\tNormalization\\n,\\t\\nBatch\\tNormalization\\n-\\nImplementing\\tBatch\\tNormalization\\twith\\nTensorFlow\\nGlorot\\tand\\tHe\\tinitialization\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n-\\nXavier\\tand\\tHe\\nInitialization\\ngradient\\tclipping\\n,\\t\\nGradient\\tClipping\\nnonsaturating\\tactivation\\tfunctions\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\n-\\nNonsaturating\\nActivation\\tFunctions\\ngraphviz\\n,\\t\\nTraining\\tand\\tVisualizing\\ta\\tDecision\\tTree\\ngreedy\\talgorithm\\n,\\t\\nThe\\tCART\\tTraining\\tAlgorithm\\ngrid\\tsearch\\n,\\t\\nFine-Tune\\tYour\\tModel\\n-\\nGrid\\tSearch\\n,\\t\\nPolynomial\\tKernel\\ngroup()\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nGRU\\t(Gated\\tRecurrent\\tUnit)\\tcell\\n,\\t\\nGRU\\tCell\\n-\\nGRU\\tCell\\nH\\nhailstone\\tsequence\\n,\\t\\nEfficient\\tData\\tRepresentations\\nhard\\tmargin\\tclassification\\n,\\t\\nSoft\\tMargin\\tClassification\\n-\\nSoft\\tMargin\\tClassification\\nhard\\tvoting\\tclassifiers\\n,\\t\\nVoting\\tClassifiers\\n-\\nVoting\\tClassifiers\\nharmonic\\tmean\\n,\\t\\nPrecision\\tand\\tRecall\\nHe\\tinitialization\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n-\\nXavier\\tand\\tHe\\tInitialization\\nHeaviside\\tstep\\tfunction\\n,\\t\\nThe\\tPerceptron\\nHebb's\\trule\\n,\\t\\nThe\\tPerceptron\\n,\\t\\nHopfield\\tNetworks\\nHebbian\\tlearning\\n,\\t\\nThe\\tPerceptron\\nhidden\\tlayers\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nhierarchical\\tclustering\\n,\\t\\nUnsupervised\\tlearning\\nhinge\\tloss\\tfunction\\n,\\t\\nOnline\\tSVMs\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 651}), Document(page_content='histograms\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\n-\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\nStructure\\nhold-out\\tsets\\n,\\t\\nStacking\\n(\\nsee\\talso\\n\\tblenders)\\nHopfield\\tNetworks\\n,\\t\\nHopfield\\tNetworks\\n-\\nHopfield\\tNetworks\\nhyperbolic\\ttangent\\t(htan\\tactivation\\tfunction)\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n,\\nActivation\\tFunctions\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\n,\\nRecurrent\\tNeurons\\nhyperparameters\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n,\\t\\nCustom\\tTransformers\\n,\\t\\nGrid\\tSearch\\n-\\nGrid\\nSearch\\n,\\t\\nEvaluate\\tYour\\tSystem\\ton\\tthe\\tTest\\tSet\\n,\\t\\nGradient\\tDescent\\n,\\t\\nPolynomial\\tKernel\\n,\\nComputational\\tComplexity\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\n(\\nsee\\talso\\n\\tneural\\tnetwork\\thyperparameters)\\nhyperplane\\n,\\t\\nDecision\\tFunction\\tand\\tPredictions\\n,\\t\\nManifold\\tLearning\\n-\\nPCA\\n,\\t\\nProjecting\\tDown\\tto\\td\\nDimensions\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\nhypothesis\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nmanifold\\n,\\t\\nManifold\\tLearning\\nhypothesis\\tboosting\\n\\t(\\nsee\\n\\tboosting)\\nhypothesis\\tfunction\\n,\\t\\nLinear\\tRegression\\nhypothesis,\\tnull\\n,\\t\\nRegularization\\tHyperparameters\\nI\\nidentity\\tmatrix\\n,\\t\\nRidge\\tRegression\\n,\\t\\nQuadratic\\tProgramming\\nILSVRC\\t\\nImageNet\\t\\nchallenge\\n,\\t\\nCNN\\tArchitectures\\nimage\\tclassification\\n,\\t\\nCNN\\tArchitectures\\nimpurity\\tmeasures\\n,\\t\\nMaking\\tPredictions\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\nin-graph\\treplication\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\ninception\\tmodules\\n,\\t\\nGoogLeNet', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 652}), Document(page_content='Inception-v4\\n,\\t\\nResNet\\nincremental\\tlearning\\n,\\t\\nOnline\\tlearning\\n,\\t\\nIncremental\\tPCA\\ninequality\\tconstraints\\n,\\t\\nSVM\\tDual\\tProblem\\ninference\\n,\\t\\nModel-based\\tlearning\\n,\\t\\nExercises\\n,\\t\\nMemory\\tRequirements\\n,\\t\\nAn\\tEncoder–Decoder\\nNetwork\\tfor\\tMachine\\tTranslation\\ninfo()\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\ninformation\\tgain\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\ninformation\\ttheory\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\ninit\\t\\nnode\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\ninput\\tgate\\n,\\t\\nLSTM\\tCell\\ninput\\tneurons\\n,\\t\\nThe\\tPerceptron\\ninput_keep_prob\\n,\\t\\nApplying\\tDropout\\ninstance-based\\tlearning\\n,\\t\\nInstance-based\\tlearning\\n,\\t\\nModel-based\\tlearning\\nInteractiveSession\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\nintercept\\tterm\\n,\\t\\nLinear\\tRegression\\nInternal\\tCovariate\\t\\nShift\\t\\nproblem\\n,\\t\\nBatch\\tNormalization\\ninter_op_parallelism_threads\\n,\\t\\nParallel\\tExecution\\nintra_op_parallelism_threads\\n,\\t\\nParallel\\tExecution\\ninverse_transform()\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nin_top_k()\\n,\\t\\nConstruction\\tPhase\\nirreducible\\terror\\n,\\t\\nLearning\\tCurves\\nisolated\\tenvironment\\n,\\t\\nCreate\\tthe\\tWorkspace\\n-\\nCreate\\tthe\\tWorkspace\\nIsomap\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 653}), Document(page_content='J\\njobs\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\njoin()\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\nand\\ta\\tQueueRunner\\nJupyter\\n,\\t\\nCreate\\tthe\\tWorkspace\\n,\\t\\nCreate\\tthe\\tWorkspace\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\nStructure\\nK\\nK-fold\\tcross-validation\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n-\\nBetter\\tEvaluation\\tUsing\\nCross-Validation\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nk-Nearest\\tNeighbors\\n,\\t\\nModel-based\\tlearning\\n,\\t\\nMultilabel\\tClassification\\nKarush–Kuhn–Tucker\\t(KKT)\\tconditions\\n,\\t\\nSVM\\tDual\\tProblem\\nkeep\\tprobability\\n,\\t\\nDropout\\nKeras\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nKernel\\tPCA\\t(kPCA)\\n,\\t\\nKernel\\tPCA\\n-\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nkernel\\ttrick\\n,\\t\\nPolynomial\\tKernel\\n,\\t\\nGaussian\\tRBF\\tKernel\\n,\\t\\nThe\\tDual\\tProblem\\n-\\nKernelized\\tSVM\\n,\\nKernel\\tPCA\\nkernelized\\tSVM\\n,\\t\\nKernelized\\tSVM\\n-\\nKernelized\\tSVM\\nkernels\\n,\\t\\nPolynomial\\tKernel\\n-\\nGaussian\\tRBF\\tKernel\\n,\\t\\nOperations\\tand\\tkernels\\nKullback–Leibler\\tdivergence\\n,\\t\\nSoftmax\\tRegression\\n,\\t\\nSparse\\tAutoencoders\\nL\\nl1_l2_regularizer()\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\nLabelBinarizer\\n,\\t\\nTransformation\\tPipelines\\nlabels\\n,\\t\\nSupervised\\tlearning\\n,\\t\\nFrame\\tthe\\tProblem\\nLagrange\\tfunction\\n,\\t\\nSVM\\tDual\\tProblem\\n-\\nSVM\\tDual\\tProblem\\nLagrange\\tmultiplier\\n,\\t\\nSVM\\tDual\\tProblem', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 654}), Document(page_content='landmarks\\n,\\t\\nAdding\\tSimilarity\\tFeatures\\n-\\nAdding\\tSimilarity\\tFeatures\\nlarge\\tmargin\\tclassification\\n,\\t\\nLinear\\tSVM\\tClassification\\n-\\nLinear\\tSVM\\tClassification\\nLasso\\tRegression\\n,\\t\\nLasso\\tRegression\\n-\\nLasso\\tRegression\\nlatent\\tloss\\n,\\t\\nVariational\\tAutoencoders\\nlatent\\tspace\\n,\\t\\nVariational\\tAutoencoders\\nlaw\\tof\\tlarge\\tnumbers\\n,\\t\\nVoting\\tClassifiers\\nleaky\\tReLU\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\nlearning\\trate\\n,\\t\\nOnline\\tlearning\\n,\\t\\nGradient\\tDescent\\n,\\t\\nBatch\\tGradient\\tDescent\\n-\\nStochastic\\tGradient\\nDescent\\nlearning\\trate\\tscheduling\\n,\\t\\nStochastic\\tGradient\\tDescent\\n,\\t\\nLearning\\tRate\\tScheduling\\n-\\nLearning\\nRate\\tScheduling\\nLeNet-5\\tarchitecture\\n,\\t\\nThe\\tArchitecture\\tof\\tthe\\tVisual\\tCortex\\n,\\t\\nLeNet-5\\n-\\nLeNet-5\\nLevenshtein\\tdistance\\n,\\t\\nGaussian\\tRBF\\tKernel\\nliblinear\\tlibrary\\n,\\t\\nComputational\\tComplexity\\nlibsvm\\tlibrary\\n,\\t\\nComputational\\tComplexity\\nLinear\\tDiscriminant\\tAnalysis\\t(LDA)\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\nlinear\\tmodels\\nearly\\tstopping\\n,\\t\\nEarly\\tStopping\\n-\\nEarly\\tStopping\\nElastic\\tNet\\n,\\t\\nElastic\\tNet\\nLasso\\tRegression\\n,\\t\\nLasso\\tRegression\\n-\\nLasso\\tRegression\\nLinear\\tRegression\\n\\t(\\nsee\\n\\tLinear\\tRegression)\\nregression\\n\\t(\\nsee\\n\\tLinear\\tRegression)\\nRidge\\tRegression\\n,\\t\\nRidge\\tRegression\\n-\\nRidge\\tRegression\\n,\\t\\nElastic\\tNet\\nSVM\\n,\\t\\nLinear\\tSVM\\tClassification\\n-\\nSoft\\tMargin\\tClassification', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 655}), Document(page_content='Linear\\tRegression\\n,\\t\\nModel-based\\tlearning\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n,\\nTraining\\tModels\\n-\\nMini-batch\\tGradient\\tDescent\\n,\\t\\nElastic\\tNet\\ncomputational\\tcomplexity\\n,\\t\\nComputational\\tComplexity\\nGradient\\tDescent\\tin\\n,\\t\\nGradient\\tDescent\\n-\\nMini-batch\\tGradient\\tDescent\\nlearning\\tcurves\\tin\\n,\\t\\nLearning\\tCurves\\n-\\nLearning\\tCurves\\nNormal\\tEquation\\n,\\t\\nThe\\tNormal\\tEquation\\n-\\nComputational\\tComplexity\\nregularizing\\tmodels\\n\\t(\\nsee\\n\\tregularization)\\nusing\\tStochastic\\tGradient\\tDescent\\t(SGD)\\n,\\t\\nStochastic\\tGradient\\tDescent\\nwith\\tTensorFlow\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n-\\nLinear\\tRegression\\twith\\nTensorFlow\\nlinear\\tSVM\\tclassification\\n,\\t\\nLinear\\tSVM\\tClassification\\n-\\nSoft\\tMargin\\tClassification\\nlinear\\tthreshold\\tunits\\t(LTUs)\\n,\\t\\nThe\\tPerceptron\\nLipschitz\\tcontinuous\\n,\\t\\nGradient\\tDescent\\nLLE\\t(Locally\\tLinear\\tEmbedding)\\n,\\t\\nLLE\\n-\\nLLE\\nload_sample_images()\\n,\\t\\nTensorFlow\\tImplementation\\nlocal\\treceptive\\tfield\\n,\\t\\nThe\\tArchitecture\\tof\\tthe\\tVisual\\tCortex\\nlocal\\tresponse\\tnormalization\\n,\\t\\nAlexNet\\nlocal\\tsessions\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\nlocation\\tinvariance\\n,\\t\\nPooling\\tLayer\\nlog\\tloss\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\nlogging\\tplacements\\n,\\t\\nLogging\\tplacements\\n-\\nLogging\\tplacements\\nlogistic\\tfunction\\n,\\t\\nEstimating\\tProbabilities\\nLogistic\\tRegression\\n,\\t\\nSupervised\\tlearning\\n,\\t\\nLogistic\\tRegression\\n-\\nSoftmax\\tRegression\\ndecision\\tboundaries\\n,\\t\\nDecision\\tBoundaries\\n-\\nDecision\\tBoundaries', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 656}), Document(page_content='estimating\\tprobablities\\n,\\t\\nEstimating\\tProbabilities\\n-\\nEstimating\\tProbabilities\\nSoftmax\\tRegression\\tmodel\\n,\\t\\nSoftmax\\tRegression\\n-\\nSoftmax\\tRegression\\ntraining\\tand\\tcost\\tfunction\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\n-\\nTraining\\tand\\tCost\\tFunction\\nlog_device_placement\\n,\\t\\nLogging\\tplacements\\nLSTM\\t(Long\\tShort-Term\\tMemory)\\tcell\\n,\\t\\nLSTM\\tCell\\n-\\nGRU\\tCell', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 657}), Document(page_content='M\\nmachine\\tcontrol\\n\\t(\\nsee\\n\\treinforcement\\tlearning)\\nMachine\\tLearning\\nlarge-scale\\tprojects\\n\\t(\\nsee\\n\\tTensorFlow)\\nnotations\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n-\\nSelect\\ta\\tPerformance\\tMeasure\\nprocess\\texample\\n,\\t\\nEnd-to-End\\tMachine\\tLearning\\tProject\\n-\\nExercises\\nproject\\tchecklist\\n,\\t\\nLook\\tat\\tthe\\tBig\\tPicture\\n,\\t\\nMachine\\tLearning\\tProject\\tChecklist\\n-\\nLaunch!\\nresources\\ton\\n,\\t\\nOther\\tResources\\n-\\nOther\\tResources\\nuses\\tfor\\n,\\t\\nMachine\\tLearning\\tin\\tYour\\tProjects\\n-\\nMachine\\tLearning\\tin\\tYour\\tProjects\\nMachine\\tLearning\\tbasics\\nattributes\\n,\\t\\nSupervised\\tlearning\\nchallenges\\n,\\t\\nMain\\tChallenges\\tof\\tMachine\\tLearning\\n-\\nStepping\\tBack\\nalgorithm\\tproblems\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n-\\nUnderfitting\\tthe\\tTraining\\nData\\ntraining\\tdata\\tproblems\\n,\\t\\nPoor-Quality\\tData\\ndefinition\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\nfeatures\\n,\\t\\nSupervised\\tlearning\\noverview\\n,\\t\\nThe\\tMachine\\tLearning\\tLandscape\\nreasons\\tfor\\tusing\\n,\\t\\nWhy\\tUse\\tMachine\\tLearning?\\n-\\nWhy\\tUse\\tMachine\\tLearning?\\nspam\\tfilter\\texample\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\n-\\nWhy\\tUse\\tMachine\\tLearning?\\nsummary\\n,\\t\\nStepping\\tBack\\ntesting\\tand\\tvalidating\\n,\\t\\nTesting\\tand\\tValidating\\n-\\nTesting\\tand\\tValidating\\ntypes\\tof\\tsystems\\n,\\t\\nTypes\\tof\\tMachine\\tLearning\\tSystems\\n-\\nModel-based\\tlearning\\nbatch\\tand\\tonline\\tlearning\\n,\\t\\nBatch\\tand\\tOnline\\tLearning\\n-\\nOnline\\tlearning', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 658}), Document(page_content='instance-based\\tversus\\tmodel-based\\tlearning\\n,\\t\\nInstance-Based\\tVersus\\tModel-\\nBased\\tLearning\\n-\\nModel-based\\tlearning\\nsupervised/unsupervised\\tlearning\\n,\\t\\nSupervised/Unsupervised\\tLearning\\n-\\nReinforcement\\tLearning\\nworkflow\\texample\\n,\\t\\nModel-based\\tlearning\\n-\\nModel-based\\tlearning\\nmachine\\ttranslation\\n\\t(\\nsee\\n\\tnatural\\tlanguage\\tprocessing\\t(NLP))\\nmake()\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\nManhattan\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nmanifold\\tassumption/hypothesis\\n,\\t\\nManifold\\tLearning\\nManifold\\tLearning\\n,\\t\\nManifold\\tLearning\\n,\\t\\nLLE\\n(\\nsee\\talso\\n\\tLLE\\t(Locally\\tLinear\\tEmbedding)\\nMapReduce\\n,\\t\\nFrame\\tthe\\tProblem\\nmargin\\tviolations\\n,\\t\\nSoft\\tMargin\\tClassification\\nMarkov\\tchains\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nMarkov\\tdecision\\tprocesses\\n,\\t\\nMarkov\\tDecision\\tProcesses\\n-\\nMarkov\\tDecision\\tProcesses\\nmaster\\tservice\\n,\\t\\nThe\\tMaster\\tand\\tWorker\\tServices\\nMatplotlib\\n,\\t\\nCreate\\tthe\\tWorkspace\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\n,\\t\\nThe\\tROC\\nCurve\\n,\\t\\nError\\tAnalysis\\nmax\\tmargin\\tlearning\\n,\\t\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\nmax\\tpooling\\tlayer\\n,\\t\\nPooling\\tLayer\\nmax-norm\\tregularization\\n,\\t\\nMax-Norm\\tRegularization\\n-\\nMax-Norm\\tRegularization\\nmax_norm()\\n,\\t\\nMax-Norm\\tRegularization\\nmax_norm_regularizer()\\n,\\t\\nMax-Norm\\tRegularization\\nmax_pool()\\n,\\t\\nPooling\\tLayer', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 659}), Document(page_content=\"Mean\\tAbsolute\\tError\\t(MAE)\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n-\\nSelect\\ta\\tPerformance\\tMeasure\\nmean\\tcoding\\n,\\t\\nVariational\\tAutoencoders\\nMean\\tSquare\\tError\\t(MSE)\\n,\\t\\nLinear\\tRegression\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nSparse\\nAutoencoders\\nmeasure\\tof\\tsimilarity\\n,\\t\\nInstance-based\\tlearning\\nmemmap\\n,\\t\\nIncremental\\tPCA\\nmemory\\tcells\\n,\\t\\nModel\\tParallelism\\n,\\t\\nMemory\\tCells\\nMercer's\\ttheorem\\n,\\t\\nKernelized\\tSVM\\nmeta\\tlearner\\n\\t(\\nsee\\n\\tblending)\\nmin-max\\tscaling\\n,\\t\\nFeature\\tScaling\\nMini-batch\\tGradient\\tDescent\\n,\\t\\nMini-batch\\tGradient\\tDescent\\n-\\nMini-batch\\tGradient\\tDescent\\n,\\nTraining\\tand\\tCost\\tFunction\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\n-\\nFeeding\\tData\\tto\\tthe\\nTraining\\tAlgorithm\\nmini-batches\\n,\\t\\nOnline\\tlearning\\nminimize()\\n,\\t\\nGradient\\tClipping\\n,\\t\\nFreezing\\tthe\\tLower\\tLayers\\n,\\t\\nPolicy\\tGradients\\n,\\t\\nLearning\\tto\\tPlay\\nMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nmin_after_dequeue\\n,\\t\\nRandomShuffleQueue\\nMNIST\\tdataset\\n,\\t\\nMNIST\\n-\\nMNIST\\nmodel\\tparallelism\\n,\\t\\nModel\\tParallelism\\n-\\nModel\\tParallelism\\nmodel\\tparameters\\n,\\t\\nGradient\\tDescent\\n,\\t\\nBatch\\tGradient\\tDescent\\n,\\t\\nEarly\\tStopping\\n,\\t\\nUnder\\tthe\\nHood\\n,\\t\\nQuadratic\\tProgramming\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\nConstruction\\tPhase\\n,\\t\\nTraining\\tRNNs\\ndefining\\n,\\t\\nModel-based\\tlearning\\nmodel\\tselection\\n,\\t\\nModel-based\\tlearning\\nmodel\\tzoos\\n,\\t\\nModel\\tZoos\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 660}), Document(page_content='model-based\\tlearning\\n,\\t\\nModel-based\\tlearning\\n-\\nModel-based\\tlearning\\nmodels\\nanalyzing\\n,\\t\\nAnalyze\\tthe\\tBest\\tModels\\tand\\tTheir\\tErrors\\n-\\nAnalyze\\tthe\\tBest\\tModels\\tand\\nTheir\\tErrors\\nevaluating\\ton\\ttest\\tset\\n,\\t\\nEvaluate\\tYour\\tSystem\\ton\\tthe\\tTest\\tSet\\n-\\nEvaluate\\tYour\\tSystem\\ton\\nthe\\tTest\\tSet\\nmoments\\n,\\t\\nAdam\\tOptimization\\nMomentum\\toptimization\\n,\\t\\nMomentum\\tOptimization\\n-\\nMomentum\\tOptimization\\nMonte\\tCarlo\\ttree\\tsearch\\n,\\t\\nPolicy\\tGradients\\nMulti-Layer\\tPerceptrons\\t(MLP)\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n,\\t\\nThe\\tPerceptron\\n-\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n,\\t\\nNeural\\tNetwork\\tPolicies\\ntraining\\twith\\tTF.Learn\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nmulticlass\\tclassifiers\\n,\\t\\nMulticlass\\tClassification\\n-\\nMulticlass\\tClassification\\nMultidimensional\\tScaling\\t(MDS)\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\nmultilabel\\tclassifiers\\n,\\t\\nMultilabel\\tClassification\\n-\\nMultilabel\\tClassification\\nMultinomial\\tLogistic\\tRegression\\n\\t(\\nsee\\n\\tSoftmax\\tRegression)\\nmultinomial()\\n,\\t\\nNeural\\tNetwork\\tPolicies\\nmultioutput\\tclassifiers\\n,\\t\\nMultioutput\\tClassification\\n-\\nMultioutput\\tClassification\\nMultiRNNCell\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\nmultithreaded\\treaders\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\nmultivariate\\tregression\\n,\\t\\nFrame\\tthe\\tProblem\\nN\\nnaive\\tBayes\\tclassifiers\\n,\\t\\nMulticlass\\tClassification\\nname\\tscopes\\n,\\t\\nName\\tScopes', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 661}), Document(page_content='natural\\tlanguage\\tprocessing\\t(NLP)\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\n,\\t\\nNatural\\tLanguage\\nProcessing\\n-\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nencoder-decoder\\tnetwork\\tfor\\tmachine\\ttranslation\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\nMachine\\tTranslation\\n-\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nTensorFlow\\ttutorials\\n,\\t\\nNatural\\tLanguage\\tProcessing\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\nMachine\\tTranslation\\nword\\tembeddings\\n,\\t\\nWord\\tEmbeddings\\n-\\nWord\\tEmbeddings\\nNesterov\\tAccelerated\\tGradient\\t(NAG)\\n,\\t\\nNesterov\\tAccelerated\\tGradient\\n-\\nNesterov\\tAccelerated\\nGradient\\nNesterov\\tmomentum\\toptimization\\n,\\t\\nNesterov\\tAccelerated\\tGradient\\n-\\nNesterov\\tAccelerated\\nGradient\\nnetwork\\ttopology\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\nneural\\tnetwork\\thyperparameters\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\n-\\nActivation\\nFunctions\\nactivation\\tfunctions\\n,\\t\\nActivation\\tFunctions\\nneurons\\tper\\thidden\\tlayer\\n,\\t\\nNumber\\tof\\tNeurons\\tper\\tHidden\\tLayer\\nnumber\\tof\\thidden\\tlayers\\n,\\t\\nNumber\\tof\\tHidden\\tLayers\\n-\\nNumber\\tof\\tHidden\\tLayers\\nneural\\tnetwork\\tpolicies\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n-\\nNeural\\tNetwork\\tPolicies\\nneurons\\nbiological\\n,\\t\\nFrom\\tBiological\\tto\\tArtificial\\tNeurons\\n-\\nBiological\\tNeurons\\nlogical\\tcomputations\\twith\\n,\\t\\nLogical\\tComputations\\twith\\tNeurons\\nneuron_layer()\\n,\\t\\nConstruction\\tPhase\\nnext_batch()\\n,\\t\\nExecution\\tPhase\\nNo\\tFree\\tLunch\\ttheorem\\n,\\t\\nTesting\\tand\\tValidating\\nnode\\tedges\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\nnonlinear\\tdimensionality\\treduction\\t(NLDR)\\n,\\t\\nLLE', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 662}), Document(page_content='(\\nsee\\talso\\n\\tKernel\\tPCA;\\tLLE\\t(Locally\\tLinear\\tEmbedding))\\nnonlinear\\tSVM\\tclassification\\n,\\t\\nNonlinear\\tSVM\\tClassification\\n-\\nComputational\\tComplexity\\ncomputational\\tcomplexity\\n,\\t\\nComputational\\tComplexity\\nGaussian\\tRBF\\tkernel\\n,\\t\\nGaussian\\tRBF\\tKernel\\n-\\nGaussian\\tRBF\\tKernel\\nwith\\tpolynomial\\tfeatures\\n,\\t\\nNonlinear\\tSVM\\tClassification\\n-\\nPolynomial\\tKernel\\npolynomial\\tkernel\\n,\\t\\nPolynomial\\tKernel\\n-\\nPolynomial\\tKernel\\nsimilarity\\tfeatures,\\tadding\\n,\\t\\nAdding\\tSimilarity\\tFeatures\\n-\\nAdding\\tSimilarity\\tFeatures\\nnonparametric\\tmodels\\n,\\t\\nRegularization\\tHyperparameters\\nnonresponse\\tbias\\n,\\t\\nNonrepresentative\\tTraining\\tData\\nnonsaturating\\tactivation\\tfunctions\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\n-\\nNonsaturating\\nActivation\\tFunctions\\nNormal\\tEquation\\n,\\t\\nThe\\tNormal\\tEquation\\n-\\nComputational\\tComplexity\\nnormalization\\n,\\t\\nFeature\\tScaling\\nnormalized\\texponential\\n,\\t\\nSoftmax\\tRegression\\nnorms\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nnotations\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n-\\nSelect\\ta\\tPerformance\\tMeasure\\nNP-Complete\\tproblems\\n,\\t\\nThe\\tCART\\tTraining\\tAlgorithm\\nnull\\thypothesis\\n,\\t\\nRegularization\\tHyperparameters\\nnumerical\\tdifferentiation\\n,\\t\\nNumerical\\tDifferentiation\\nNumPy\\n,\\t\\nCreate\\tthe\\tWorkspace\\nNumPy\\tarrays\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nNVidia\\tCompute\\tCapability\\n,\\t\\nInstallation\\nnvidia-smi\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 663}), Document(page_content='n_components\\n,\\t\\nChoosing\\tthe\\tRight\\tNumber\\tof\\tDimensions\\nO\\nobservation\\tspace\\n,\\t\\nNeural\\tNetwork\\tPolicies\\noff-policy\\talgorithm\\n,\\t\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\noffline\\tlearning\\n,\\t\\nBatch\\tlearning\\none-hot\\tencoding\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\none-versus-all\\t\\n(OvA)\\t\\nstrategy\\n,\\t\\nMulticlass\\tClassification\\n,\\t\\nSoftmax\\tRegression\\n,\\t\\nExercises\\none-versus-one\\t(OvO)\\tstrategy\\n,\\t\\nMulticlass\\tClassification\\nonline\\tlearning\\n,\\t\\nOnline\\tlearning\\n-\\nOnline\\tlearning\\nonline\\tSVMs\\n,\\t\\nOnline\\tSVMs\\n-\\nOnline\\tSVMs\\nOpenAI\\tGym\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\n-\\nIntroduction\\tto\\tOpenAI\\tGym\\noperation_timeout_in_ms\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\nOptical\\tCharacter\\tRecognition\\t(OCR)\\n,\\t\\nThe\\tMachine\\tLearning\\tLandscape\\noptimal\\tstate\\tvalue\\n,\\t\\nMarkov\\tDecision\\tProcesses\\noptimizers\\n,\\t\\nFaster\\tOptimizers\\n-\\nLearning\\tRate\\tScheduling\\nAdaGrad\\n,\\t\\nAdaGrad\\n-\\nAdaGrad\\nAdam\\toptimization\\n,\\t\\nAdam\\tOptimization\\n-\\nAdam\\tOptimization\\n,\\t\\nAdam\\tOptimization\\nGradient\\tDescent\\n\\t(\\nsee\\n\\tGradient\\tDescent\\toptimizer)\\nlearning\\trate\\tscheduling\\n,\\t\\nLearning\\tRate\\tScheduling\\n-\\nLearning\\tRate\\tScheduling\\nMomentum\\toptimization\\n,\\t\\nMomentum\\tOptimization\\n-\\nMomentum\\tOptimization\\nNesterov\\tAccelerated\\tGradient\\t(NAG)\\n,\\t\\nNesterov\\tAccelerated\\tGradient\\n-\\nNesterov\\nAccelerated\\tGradient\\nRMSProp\\n,\\t\\nRMSProp\\nout-of-bag\\tevaluation\\n,\\t\\nOut-of-Bag\\tEvaluation\\n-\\nOut-of-Bag\\tEvaluation', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 664}), Document(page_content='out-of-core\\tlearning\\n,\\t\\nOnline\\tlearning\\nout-of-memory\\t(OOM)\\terrors\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\nout-of-sample\\terror\\n,\\t\\nTesting\\tand\\tValidating\\nOutOfRangeError\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\treaders\\nusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\noutput\\tgate\\n,\\t\\nLSTM\\tCell\\noutput\\tlayer\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nOutputProjectionWrapper\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n-\\nTraining\\tto\\tPredict\\tTime\\tSeries\\noutput_keep_prob\\n,\\t\\nApplying\\tDropout\\novercomplete\\tautoencoder\\n,\\t\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\noverfitting\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n-\\nOverfitting\\tthe\\tTraining\\tData\\n,\\t\\nCreate\\ta\\tTest\\tSet\\n,\\nSoft\\tMargin\\tClassification\\n,\\t\\nGaussian\\tRBF\\tKernel\\n,\\t\\nRegularization\\tHyperparameters\\n,\\nRegression\\n,\\t\\nNumber\\tof\\tNeurons\\tper\\tHidden\\tLayer\\navoiding\\tthrough\\tregularization\\n,\\t\\nAvoiding\\tOverfitting\\tThrough\\tRegularization\\n-\\nData\\nAugmentation\\nP\\np-value\\n,\\t\\nRegularization\\tHyperparameters\\nPaddingFIFOQueue\\n,\\t\\nPaddingFifoQueue\\nPandas\\n,\\t\\nCreate\\tthe\\tWorkspace\\n,\\t\\nDownload\\tthe\\tData\\nscatter_matrix\\n,\\t\\nLooking\\tfor\\tCorrelations\\n-\\nLooking\\tfor\\tCorrelations\\nparallel\\tdistributed\\tcomputing\\n,\\t\\nDistributing\\tTensorFlow\\tAcross\\tDevices\\tand\\tServers\\n-\\nExercises\\ndata\\tparallelism\\n,\\t\\nData\\tParallelism\\n-\\nTensorFlow\\timplementation\\nin-graph\\tversus\\tbetween-graph\\treplication\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\nReplication\\n-\\nModel\\tParallelism\\nmodel\\tparallelism\\n,\\t\\nModel\\tParallelism\\n-\\nModel\\tParallelism\\nmultiple\\tdevices\\tacross\\tmultiple\\tservers\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\n-', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 665}), Document(page_content='Other\\tconvenience\\tfunctions\\nasynchronous\\tcommunication\\tusing\\tqueues\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\nTensorFlow\\tQueues\\n-\\nPaddingFifoQueue\\nloading\\ttraining\\tdata\\n,\\t\\nLoading\\tData\\tDirectly\\tfrom\\tthe\\tGraph\\n-\\nOther\\tconvenience\\nfunctions\\nmaster\\tand\\tworker\\tservices\\n,\\t\\nThe\\tMaster\\tand\\tWorker\\tServices\\nopening\\ta\\tsession\\n,\\t\\nOpening\\ta\\tSession\\npinning\\toperations\\tacross\\ttasks\\n,\\t\\nPinning\\tOperations\\tAcross\\tTasks\\nsharding\\tvariables\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nsharing\\tstate\\tacross\\tsessions\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\nContainers\\n-\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\nmultiple\\tdevices\\ton\\ta\\tsingle\\tmachine\\n,\\t\\nMultiple\\tDevices\\ton\\ta\\tSingle\\tMachine\\n-\\nControl\\nDependencies\\ncontrol\\tdependencies\\n,\\t\\nControl\\tDependencies\\ninstallation\\n,\\t\\nInstallation\\n-\\nInstallation\\nmanaging\\tthe\\tGPU\\tRAM\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM\\n-\\nManaging\\tthe\\tGPU\\tRAM\\nparallel\\texecution\\n,\\t\\nParallel\\tExecution\\n-\\nParallel\\tExecution\\nplacing\\toperations\\ton\\tdevices\\n,\\t\\nPlacing\\tOperations\\ton\\tDevices\\n-\\nSoft\\tplacement\\none\\tneural\\tnetwork\\tper\\tdevice\\n,\\t\\nOne\\tNeural\\tNetwork\\tper\\tDevice\\n-\\nOne\\tNeural\\tNetwork\\nper\\tDevice\\nparameter\\tefficiency\\n,\\t\\nNumber\\tof\\tHidden\\tLayers\\nparameter\\tmatrix\\n,\\t\\nSoftmax\\tRegression\\nparameter\\tserver\\t(ps)\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\nparameter\\tspace\\n,\\t\\nGradient\\tDescent\\nparameter\\tvector\\n,\\t\\nLinear\\tRegression\\n,\\t\\nGradient\\tDescent\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\n,\\t\\nSoftmax\\nRegression', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 666}), Document(page_content=\"parametric\\tmodels\\n,\\t\\nRegularization\\tHyperparameters\\npartial\\tderivative\\n,\\t\\nBatch\\tGradient\\tDescent\\npartial_fit()\\n,\\t\\nIncremental\\tPCA\\nPearson's\\tr\\n,\\t\\nLooking\\tfor\\tCorrelations\\npeephole\\tconnections\\n,\\t\\nPeephole\\tConnections\\npenalties\\n\\t(\\nsee\\n\\trewards,\\tin\\tRL)\\npercentiles\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\nPerceptron\\tconvergence\\ttheorem\\n,\\t\\nThe\\tPerceptron\\nPerceptrons\\n,\\t\\nThe\\tPerceptron\\n-\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nversus\\tLogistic\\tRegression\\n,\\t\\nThe\\tPerceptron\\ntraining\\n,\\t\\nThe\\tPerceptron\\n-\\nThe\\tPerceptron\\nperformance\\tmeasures\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n-\\nSelect\\ta\\tPerformance\\tMeasure\\nconfusion\\tmatrix\\n,\\t\\nConfusion\\tMatrix\\n-\\nConfusion\\tMatrix\\ncross-validation\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\n-\\nMeasuring\\tAccuracy\\nUsing\\tCross-Validation\\nprecision\\tand\\trecall\\n,\\t\\nPrecision\\tand\\tRecall\\n-\\nPrecision/Recall\\tTradeoff\\nROC\\t(receiver\\toperating\\tcharacteristic)\\tcurve\\n,\\t\\nThe\\tROC\\tCurve\\n-\\nThe\\tROC\\tCurve\\nperformance\\tscheduling\\n,\\t\\nLearning\\tRate\\tScheduling\\npermutation()\\n,\\t\\nCreate\\ta\\tTest\\tSet\\nPG\\talgorithms\\n,\\t\\nPolicy\\tGradients\\nphoto-hosting\\tservices\\n,\\t\\nSemisupervised\\tlearning\\npinning\\toperations\\n,\\t\\nPinning\\tOperations\\tAcross\\tTasks\\npip\\n,\\t\\nCreate\\tthe\\tWorkspace\\nPipeline\\tconstructor\\n,\\t\\nTransformation\\tPipelines\\n-\\nSelect\\tand\\tTrain\\ta\\tModel\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 667}), Document(page_content='pipelines\\n,\\t\\nFrame\\tthe\\tProblem\\nplaceholder\\tnodes\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\nplacers\\n\\t(\\nsee\\n\\tsimple\\tplacer;\\tdynamic\\tplacer)\\npolicy\\n,\\t\\nPolicy\\tSearch\\npolicy\\tgradients\\n,\\t\\nPolicy\\tSearch\\n\\t(\\nsee\\n\\tPG\\talgorithms)\\npolicy\\tspace\\n,\\t\\nPolicy\\tSearch\\npolynomial\\tfeatures,\\tadding\\n,\\t\\nNonlinear\\tSVM\\tClassification\\n-\\nPolynomial\\tKernel\\npolynomial\\tkernel\\n,\\t\\nPolynomial\\tKernel\\n-\\nPolynomial\\tKernel\\n,\\t\\nKernelized\\tSVM\\nPolynomial\\tRegression\\n,\\t\\nTraining\\tModels\\n,\\t\\nPolynomial\\tRegression\\n-\\nPolynomial\\tRegression\\nlearning\\tcurves\\tin\\n,\\t\\nLearning\\tCurves\\n-\\nLearning\\tCurves\\npooling\\tkernel\\n,\\t\\nPooling\\tLayer\\npooling\\tlayer\\n,\\t\\nPooling\\tLayer\\n-\\nPooling\\tLayer\\npower\\tscheduling\\n,\\t\\nLearning\\tRate\\tScheduling\\nprecision\\n,\\t\\nConfusion\\tMatrix\\nprecision\\tand\\trecall\\n,\\t\\nPrecision\\tand\\tRecall\\n-\\nPrecision/Recall\\tTradeoff\\nF-1\\tscore\\n,\\t\\nPrecision\\tand\\tRecall\\n-\\nPrecision\\tand\\tRecall\\nprecision/recall\\t(PR)\\tcurve\\n,\\t\\nThe\\tROC\\tCurve\\nprecision/recall\\ttradeoff\\n,\\t\\nPrecision/Recall\\tTradeoff\\n-\\nPrecision/Recall\\tTradeoff\\npredetermined\\tpiecewise\\tconstant\\tlearning\\trate\\n,\\t\\nLearning\\tRate\\tScheduling\\npredict()\\n,\\t\\nData\\tCleaning\\npredicted\\tclass\\n,\\t\\nConfusion\\tMatrix\\npredictions\\n,\\t\\nConfusion\\tMatrix\\n-\\nConfusion\\tMatrix\\n,\\t\\nDecision\\tFunction\\tand\\tPredictions\\n-\\nDecision\\nFunction\\tand\\tPredictions\\n,\\t\\nMaking\\tPredictions\\n-\\nEstimating\\tClass\\tProbabilities', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 668}), Document(page_content='predictors\\n,\\t\\nSupervised\\tlearning\\n,\\t\\nData\\tCleaning\\npreloading\\ttraining\\tdata\\n,\\t\\nPreload\\tthe\\tdata\\tinto\\ta\\tvariable\\nPReLU\\t(parametric\\tleaky\\tReLU)\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\npreprocessed\\tattributes\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\npretrained\\tlayers\\treuse\\n,\\t\\nReusing\\tPretrained\\tLayers\\n-\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\nauxiliary\\ttask\\n,\\t\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\n-\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\ncaching\\tfrozen\\tlayers\\n,\\t\\nCaching\\tthe\\tFrozen\\tLayers\\nfreezing\\tlower\\tlayers\\n,\\t\\nFreezing\\tthe\\tLower\\tLayers\\nmodel\\tzoos\\n,\\t\\nModel\\tZoos\\nother\\tframeworks\\n,\\t\\nReusing\\tModels\\tfrom\\tOther\\tFrameworks\\nTensorFlow\\tmodel\\n,\\t\\nReusing\\ta\\tTensorFlow\\tModel\\n-\\nReusing\\ta\\tTensorFlow\\tModel\\nunsupervised\\tpretraining\\n,\\t\\nUnsupervised\\tPretraining\\n-\\nUnsupervised\\tPretraining\\nupper\\tlayers\\n,\\t\\nTweaking,\\tDropping,\\tor\\tReplacing\\tthe\\tUpper\\tLayers\\nPretty\\tTensor\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nprimal\\tproblem\\n,\\t\\nThe\\tDual\\tProblem\\nprincipal\\tcomponent\\n,\\t\\nPrincipal\\tComponents\\nPrincipal\\tComponent\\tAnalysis\\t(PCA)\\n,\\t\\nPCA\\n-\\nRandomized\\tPCA\\nexplained\\tvariance\\tratios\\n,\\t\\nExplained\\tVariance\\tRatio\\nfinding\\tprincipal\\tcomponents\\n,\\t\\nPrincipal\\tComponents\\n-\\nPrincipal\\tComponents\\nfor\\tcompression\\n,\\t\\nPCA\\tfor\\tCompression\\n-\\nIncremental\\tPCA\\nIncremental\\tPCA\\n,\\t\\nIncremental\\tPCA\\n-\\nRandomized\\tPCA\\nKernel\\tPCA\\t(kPCA)\\n,\\t\\nKernel\\tPCA\\n-\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nprojecting\\tdown\\tto\\td\\tdimensions\\n,\\t\\nProjecting\\tDown\\tto\\td\\tDimensions', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 669}), Document(page_content='Randomized\\tPCA\\n,\\t\\nRandomized\\tPCA\\nScikit\\tLearn\\tfor\\n,\\t\\nUsing\\tScikit-Learn\\nvariance,\\tpreserving\\n,\\t\\nPreserving\\tthe\\tVariance\\n-\\nPreserving\\tthe\\tVariance\\nprobabilistic\\tautoencoders\\n,\\t\\nVariational\\tAutoencoders\\nprobabilities,\\testimating\\n,\\t\\nEstimating\\tProbabilities\\n-\\nEstimating\\tProbabilities\\n,\\t\\nEstimating\\tClass\\nProbabilities\\nproducer\\tfunctions\\n,\\t\\nOther\\tconvenience\\tfunctions\\nprojection\\n,\\t\\nProjection\\n-\\nProjection\\npropositional\\tlogic\\n,\\t\\nFrom\\tBiological\\tto\\tArtificial\\tNeurons\\npruning\\n,\\t\\nRegularization\\tHyperparameters\\n,\\t\\nSymbolic\\tDifferentiation\\nPython\\nisolated\\tenvironment\\tin\\n,\\t\\nCreate\\tthe\\tWorkspace\\n-\\nCreate\\tthe\\tWorkspace\\nnotebooks\\tin\\n,\\t\\nCreate\\tthe\\tWorkspace\\n-\\nDownload\\tthe\\tData\\npickle\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\npip\\n,\\t\\nCreate\\tthe\\tWorkspace\\nQ\\nQ-Learning\\talgorithm\\n,\\t\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\tMs.\\nPac-Man\\tUsing\\tDeep\\tQ-Learning\\napproximate\\tQ-Learning\\n,\\t\\nApproximate\\tQ-Learning\\ndeep\\tQ-Learning\\n,\\t\\nApproximate\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\nQ-Learning\\nQ-Value\\tIteration\\tAlgorithm\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nQ-Values\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nQuadratic\\tProgramming\\t(QP)\\tProblems\\n,\\t\\nQuadratic\\tProgramming\\n-\\nQuadratic\\tProgramming\\nquantizing\\n,\\t\\nBandwidth\\tsaturation', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 670}), Document(page_content='queries\\tper\\tsecond\\t(QPS)\\n,\\t\\nOne\\tNeural\\tNetwork\\tper\\tDevice\\nQueueRunner\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nMultithreaded\\nreaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\nqueues\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\n-\\nPaddingFifoQueue\\nclosing\\n,\\t\\nClosing\\ta\\tqueue\\ndequeuing\\tdata\\n,\\t\\nDequeuing\\tdata\\nenqueuing\\tdata\\n,\\t\\nEnqueuing\\tdata\\nfirst-in\\tfirst-out\\t\\n(FIFO)\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\nof\\ttuples\\n,\\t\\nQueues\\tof\\ttuples\\nPaddingFIFOQueue\\n,\\t\\nPaddingFifoQueue\\nRandomShuffleQueue\\n,\\t\\nRandomShuffleQueue\\nq_network()\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nR\\nRadial\\tBasis\\tFunction\\t(RBF)\\n,\\t\\nAdding\\tSimilarity\\tFeatures\\nRandom\\tForests\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n-\\nGrid\\tSearch\\n,\\t\\nMulticlass\\nClassification\\n,\\t\\nDecision\\tTrees\\n,\\t\\nInstability\\n,\\t\\nEnsemble\\tLearning\\tand\\tRandom\\tForests\\n,\\t\\nRandom\\nForests\\n-\\nFeature\\tImportance\\nExtra-Trees\\n,\\t\\nExtra-Trees\\nfeature\\timportance\\n,\\t\\nFeature\\tImportance\\n-\\nFeature\\tImportance\\nrandom\\tinitialization\\n,\\t\\nGradient\\tDescent\\n,\\t\\nBatch\\tGradient\\tDescent\\n,\\t\\nStochastic\\tGradient\\nDescent\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\nRandom\\tPatches\\tand\\tRandom\\tSubspaces\\n,\\t\\nRandom\\tPatches\\tand\\tRandom\\tSubspaces\\nrandomized\\tleaky\\tReLU\\t\\n(RReLU)\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\nRandomized\\tPCA\\n,\\t\\nRandomized\\tPCA\\nrandomized\\tsearch\\n,\\t\\nRandomized\\tSearch\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 671}), Document(page_content='RandomShuffleQueue\\n,\\t\\nRandomShuffleQueue\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\ngraph\\nrandom_uniform()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\nreader\\toperations\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\nrecall\\n,\\t\\nConfusion\\tMatrix\\nrecognition\\tnetwork\\n,\\t\\nEfficient\\tData\\tRepresentations\\nreconstruction\\terror\\n,\\t\\nPCA\\tfor\\tCompression\\nreconstruction\\tloss\\n,\\t\\nEfficient\\tData\\tRepresentations\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nVariational\\nAutoencoders\\nreconstruction\\tpre-image\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nreconstructions\\n,\\t\\nEfficient\\tData\\tRepresentations\\nrecurrent\\tneural\\tnetworks\\t(RNNs)\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\n-\\nExercises\\ndeep\\tRNNs\\n,\\t\\nDeep\\tRNNs\\n-\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps\\nexploration\\tpolicies\\n,\\t\\nExploration\\tPolicies\\nGRU\\tcell\\n,\\t\\nGRU\\tCell\\n-\\nGRU\\tCell\\ninput\\tand\\toutput\\tsequences\\n,\\t\\nInput\\tand\\tOutput\\tSequences\\n-\\nInput\\tand\\tOutput\\tSequences\\nLSTM\\tcell\\n,\\t\\nLSTM\\tCell\\n-\\nGRU\\tCell\\nnatural\\tlanguage\\tprocessing\\t(NLP)\\n,\\t\\nNatural\\tLanguage\\tProcessing\\n-\\nAn\\tEncoder–Decoder\\nNetwork\\tfor\\tMachine\\tTranslation\\nin\\tTensorFlow\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n-\\nHandling\\tVariable-Length\\tOutput\\tSequences\\ndynamic\\tunrolling\\tthrough\\ttime\\n,\\t\\nDynamic\\tUnrolling\\tThrough\\tTime\\nstatic\\tunrolling\\tthrough\\ttime\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nStatic\\tUnrolling\\nThrough\\tTime\\nvariable\\tlength\\tinput\\tsequences\\n,\\t\\nHandling\\tVariable\\tLength\\tInput\\tSequences\\nvariable\\tlength\\toutput\\tsequences\\n,\\t\\nHandling\\tVariable-Length\\tOutput\\tSequences', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 672}), Document(page_content='training\\n,\\t\\nTraining\\tRNNs\\n-\\nCreative\\tRNN\\nbackpropagation\\tthrough\\ttime\\t(BPTT)\\n,\\t\\nTraining\\tRNNs\\ncreative\\tsequences\\n,\\t\\nCreative\\tRNN\\nsequence\\tclassifiers\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\n-\\nTraining\\ta\\tSequence\\nClassifier\\ntime\\tseries\\tpredictions\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n-\\nTraining\\tto\\tPredict\\tTime\\nSeries\\nrecurrent\\tneurons\\n,\\t\\nRecurrent\\tNeurons\\n-\\nInput\\tand\\tOutput\\tSequences\\nmemory\\tcells\\n,\\t\\nMemory\\tCells\\nreduce_mean()\\n,\\t\\nConstruction\\tPhase\\nreduce_sum()\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\tImplementation\\n,\\t\\nVariational\\nAutoencoders\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nregression\\n,\\t\\nSupervised\\tlearning\\nDecision\\tTrees\\n,\\t\\nRegression\\n-\\nRegression\\nregression\\tmodels\\nlinear\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\nregression\\tversus\\tclassification\\n,\\t\\nMultioutput\\tClassification\\nregularization\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n-\\nOverfitting\\tthe\\tTraining\\tData\\n,\\t\\nTesting\\tand\\nValidating\\n,\\t\\nRegularized\\tLinear\\tModels\\n-\\nEarly\\tStopping\\ndata\\taugmentation\\n,\\t\\nData\\tAugmentation\\n-\\nData\\tAugmentation\\nDecision\\tTrees\\n,\\t\\nRegularization\\tHyperparameters\\n-\\nRegularization\\tHyperparameters\\ndropout\\n,\\t\\nDropout\\n-\\nDropout\\nearly\\tstopping\\n,\\t\\nEarly\\tStopping\\n-\\nEarly\\tStopping\\n,\\t\\nEarly\\tStopping\\nElastic\\tNet\\n,\\t\\nElastic\\tNet\\nLasso\\tRegression\\n,\\t\\nLasso\\tRegression\\n-\\nLasso\\tRegression\\nmax-norm\\n,\\t\\nMax-Norm\\tRegularization\\n-\\nMax-Norm\\tRegularization', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 673}), Document(page_content='Ridge\\tRegression\\n,\\t\\nRidge\\tRegression\\n-\\nRidge\\tRegression\\nshrinkage\\n,\\t\\nGradient\\tBoosting\\nℓ\\t1\\tand\\tℓ\\t2\\tregularization\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n-\\nℓ1\\tand\\tℓ2\\tRegularization\\nREINFORCE\\talgorithms\\n,\\t\\nPolicy\\tGradients\\nReinforcement\\tLearning\\t(RL)\\n,\\t\\nReinforcement\\tLearning\\n-\\nReinforcement\\tLearning\\n,\\nReinforcement\\tLearning\\n-\\nThank\\tYou!\\nactions\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\n-\\nEvaluating\\tActions:\\tThe\\nCredit\\tAssignment\\tProblem\\ncredit\\tassignment\\tproblem\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\n-\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\ndiscount\\trate\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\nexamples\\tof\\n,\\t\\nLearning\\tto\\tOptimize\\tRewards\\nMarkov\\tdecision\\tprocesses\\n,\\t\\nMarkov\\tDecision\\tProcesses\\n-\\nMarkov\\tDecision\\tProcesses\\nneural\\tnetwork\\tpolicies\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n-\\nNeural\\tNetwork\\tPolicies\\nOpenAI\\tgym\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\n-\\nIntroduction\\tto\\tOpenAI\\tGym\\nPG\\talgorithms\\n,\\t\\nPolicy\\tGradients\\n-\\nPolicy\\tGradients\\npolicy\\tsearch\\n,\\t\\nPolicy\\tSearch\\n-\\nPolicy\\tSearch\\nQ-Learning\\talgorithm\\n,\\t\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\nMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nrewards,\\tlearning\\tto\\toptimize\\n,\\t\\nLearning\\tto\\tOptimize\\tRewards\\n-\\nLearning\\tto\\tOptimize\\nRewards\\nTemporal\\tDifference\\t(TD)\\tLearning\\n,\\t\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\n-\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\nReLU\\t(rectified\\t\\nlinear\\t\\nunits)\\n,\\t\\nModularity\\n-\\nModularity\\nReLU\\tactivation\\n,\\t\\nResNet', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 674}), Document(page_content='ReLU\\tfunction\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n,\\t\\nActivation\\tFunctions\\n,\\t\\nXavier\\nand\\tHe\\tInitialization\\n-\\nNonsaturating\\tActivation\\tFunctions\\nrelu(z)\\n,\\t\\nConstruction\\tPhase\\nrender()\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\nreplay\\tmemory\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nreplica_device_setter()\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nrequest_stop()\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\nreset()\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\nreset_default_graph()\\n,\\t\\nManaging\\tGraphs\\nreshape()\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\nresidual\\terrors\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\nresidual\\tlearning\\n,\\t\\nResNet\\nresidual\\tnetwork\\t(ResNet)\\n,\\t\\nModel\\tZoos\\n,\\t\\nResNet\\n-\\nResNet\\nresidual\\tunits\\n,\\t\\nResNet\\nResNet\\n,\\t\\nResNet\\n-\\nResNet\\nresource\\tcontainers\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\n-\\nSharing\\tState\\nAcross\\tSessions\\tUsing\\tResource\\tContainers\\nrestore()\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\nrestricted\\tBoltzmann\\tmachines\\t\\n(RBMs)\\n,\\t\\nSemisupervised\\tlearning\\n,\\t\\nUnsupervised\\tPretraining\\n,\\nBoltzmann\\tMachines\\nreuse_variables()\\n,\\t\\nSharing\\tVariables\\nreverse-mode\\tautodiff\\n,\\t\\nReverse-Mode\\tAutodiff\\n-\\nReverse-Mode\\tAutodiff\\nrewards,\\tin\\tRL\\n,\\t\\nLearning\\tto\\tOptimize\\tRewards\\n-\\nLearning\\tto\\tOptimize\\tRewards\\nrgb_array\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 675}), Document(page_content='Ridge\\tRegression\\n,\\t\\nRidge\\tRegression\\n-\\nRidge\\tRegression\\n,\\t\\nElastic\\tNet\\nRMSProp\\n,\\t\\nRMSProp\\nROC\\t(receiver\\toperating\\tcharacteristic)\\tcurve\\n,\\t\\nThe\\tROC\\tCurve\\n-\\nThe\\tROC\\tCurve\\nRoot\\tMean\\tSquare\\tError\\t(RMSE)\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n-\\nSelect\\ta\\tPerformance\\nMeasure\\n,\\t\\nLinear\\tRegression\\nRReLU\\t(randomized\\tleaky\\tReLU)\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\nrun()\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\nReplication\\nS\\nSampled\\tSoftmax\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nsampling\\tbias\\n,\\t\\nNonrepresentative\\tTraining\\tData\\n-\\nPoor-Quality\\tData\\n,\\t\\nCreate\\ta\\tTest\\tSet\\nsampling\\tnoise\\n,\\t\\nNonrepresentative\\tTraining\\tData\\nsave()\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\nSaver\\t\\nnode\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\nScikit\\tFlow\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nScikit-Learn\\n,\\t\\nCreate\\tthe\\tWorkspace\\nabout\\n,\\t\\nObjective\\tand\\tApproach\\nbagging\\tand\\tpasting\\tin\\n,\\t\\nBagging\\tand\\tPasting\\tin\\tScikit-Learn\\n-\\nBagging\\tand\\tPasting\\tin\\nScikit-Learn\\nCART\\talgorithm\\n,\\t\\nMaking\\tPredictions\\n-\\nThe\\tCART\\tTraining\\tAlgorithm\\n,\\t\\nRegression\\ncross-validation\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n-\\nBetter\\tEvaluation\\tUsing\\nCross-Validation\\ndesign\\tprinciples\\n,\\t\\nData\\tCleaning\\n-\\nData\\tCleaning\\nimputer\\n,\\t\\nData\\tCleaning\\n-\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nLinearSVR\\tclass\\n,\\t\\nSVM\\tRegression', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 676}), Document(page_content='MinMaxScaler\\n,\\t\\nFeature\\tScaling\\nmin_\\tand\\tmax_\\thyperparameters\\n,\\t\\nRegularization\\tHyperparameters\\nPCA\\timplementation\\n,\\t\\nUsing\\tScikit-Learn\\nPerceptron\\tclass\\n,\\t\\nThe\\tPerceptron\\nPipeline\\tconstructor\\n,\\t\\nTransformation\\tPipelines\\n-\\nSelect\\tand\\tTrain\\ta\\tModel\\n,\\t\\nNonlinear\\nSVM\\tClassification\\nRandomized\\tPCA\\n,\\t\\nRandomized\\tPCA\\nRidge\\tRegression\\twith\\n,\\t\\nRidge\\tRegression\\nSAMME\\n,\\t\\nAdaBoost\\nSGDClassifier\\n,\\t\\nTraining\\ta\\tBinary\\tClassifier\\n,\\t\\nPrecision/Recall\\tTradeoff\\n-\\nPrecision/Recall\\nTradeoff\\n,\\t\\nMulticlass\\tClassification\\nSGDRegressor\\n,\\t\\nStochastic\\tGradient\\tDescent\\nsklearn.base.BaseEstimator\\n,\\t\\nCustom\\tTransformers\\n,\\t\\nTransformation\\tPipelines\\n,\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nsklearn.base.clone()\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\n,\\t\\nEarly\\tStopping\\nsklearn.base.TransformerMixin\\n,\\t\\nCustom\\tTransformers\\n,\\t\\nTransformation\\tPipelines\\nsklearn.datasets.fetch_california_housing()\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\nsklearn.datasets.fetch_mldata()\\n,\\t\\nMNIST\\nsklearn.datasets.load_iris()\\n,\\t\\nDecision\\tBoundaries\\n,\\t\\nSoft\\tMargin\\tClassification\\n,\\t\\nTraining\\nand\\tVisualizing\\ta\\tDecision\\tTree\\n,\\t\\nFeature\\tImportance\\n,\\t\\nThe\\tPerceptron\\nsklearn.datasets.load_sample_images()\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\nImplementation\\nsklearn.datasets.make_moons()\\n,\\t\\nNonlinear\\tSVM\\tClassification\\n,\\t\\nExercises\\nsklearn.decomposition.IncrementalPCA\\n,\\t\\nIncremental\\tPCA\\nsklearn.decomposition.KernelPCA\\n,\\t\\nKernel\\tPCA\\n-\\nSelecting\\ta\\tKernel\\tand\\tTuning', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 677}), Document(page_content='Hyperparameters\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nsklearn.decomposition.PCA\\n,\\t\\nUsing\\tScikit-Learn\\nsklearn.ensemble.AdaBoostClassifier\\n,\\t\\nAdaBoost\\nsklearn.ensemble.BaggingClassifier\\n,\\t\\nBagging\\tand\\tPasting\\tin\\tScikit-Learn\\n-\\nRandom\\nForests\\nsklearn.ensemble.GradientBoostingRegressor\\n,\\t\\nGradient\\tBoosting\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\nsklearn.ensemble.RandomForestClassifier\\n,\\t\\nThe\\tROC\\tCurve\\n,\\t\\nMulticlass\\tClassification\\n,\\nVoting\\tClassifiers\\nsklearn.ensemble.RandomForestRegressor\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n,\\nGrid\\tSearch\\n-\\nAnalyze\\tthe\\tBest\\tModels\\tand\\tTheir\\tErrors\\n,\\t\\nRandom\\tForests\\n-\\nExtra-Trees\\n,\\nGradient\\tBoosting\\nsklearn.ensemble.VotingClassifier\\n,\\t\\nVoting\\tClassifiers\\nsklearn.externals.joblib\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\nsklearn.linear_model.ElasticNet\\n,\\t\\nElastic\\tNet\\nsklearn.linear_model.Lasso\\n,\\t\\nLasso\\tRegression\\nsklearn.linear_model.LinearRegression\\n,\\t\\nModel-based\\tlearning\\n-\\nModel-based\\tlearning\\n,\\nData\\tCleaning\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n,\\t\\nThe\\tNormal\\tEquation\\n,\\nMini-batch\\tGradient\\tDescent\\n,\\t\\nPolynomial\\tRegression\\n,\\t\\nLearning\\tCurves\\n-\\nLearning\\nCurves\\nsklearn.linear_model.LogisticRegression\\n,\\t\\nDecision\\tBoundaries\\n,\\t\\nDecision\\tBoundaries\\n,\\nSoftmax\\tRegression\\n,\\t\\nVoting\\tClassifiers\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\nHyperparameters\\nsklearn.linear_model.Perceptron\\n,\\t\\nThe\\tPerceptron\\nsklearn.linear_model.Ridge\\n,\\t\\nRidge\\tRegression\\nsklearn.linear_model.SGDClassifier\\n,\\t\\nTraining\\ta\\tBinary\\tClassifier\\nsklearn.linear_model.SGDRegressor\\n,\\t\\nStochastic\\tGradient\\tDescent\\n-\\nMini-batch\\tGradient', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 678}), Document(page_content='Descent\\n,\\t\\nRidge\\tRegression\\n,\\t\\nLasso\\tRegression\\n-\\nEarly\\tStopping\\nsklearn.manifold.LocallyLinearEmbedding\\n,\\t\\nLLE\\n-\\nLLE\\nsklearn.metrics.accuracy_score()\\n,\\t\\nVoting\\tClassifiers\\n,\\t\\nOut-of-Bag\\tEvaluation\\n,\\t\\nTraining\\nan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nsklearn.metrics.confusion_matrix()\\n,\\t\\nConfusion\\tMatrix\\n,\\t\\nError\\tAnalysis\\nsklearn.metrics.f1_score()\\n,\\t\\nPrecision\\tand\\tRecall\\n,\\t\\nMultilabel\\tClassification\\nsklearn.metrics.mean_squared_error()\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n-\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n,\\t\\nEvaluate\\tYour\\tSystem\\ton\\tthe\\tTest\\tSet\\n,\\nLearning\\tCurves\\n,\\t\\nEarly\\tStopping\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\n,\\t\\nSelecting\\ta\\nKernel\\tand\\tTuning\\tHyperparameters\\nsklearn.metrics.precision_recall_curve()\\n,\\t\\nPrecision/Recall\\tTradeoff\\nsklearn.metrics.precision_score()\\n,\\t\\nPrecision\\tand\\tRecall\\n,\\t\\nPrecision/Recall\\tTradeoff\\nsklearn.metrics.recall_score()\\n,\\t\\nPrecision\\tand\\tRecall\\n,\\t\\nPrecision/Recall\\tTradeoff\\nsklearn.metrics.roc_auc_score()\\n,\\t\\nThe\\tROC\\tCurve\\n-\\nThe\\tROC\\tCurve\\nsklearn.metrics.roc_curve()\\n,\\t\\nThe\\tROC\\tCurve\\n-\\nThe\\tROC\\tCurve\\nsklearn.model_selection.cross_val_predict()\\n,\\t\\nConfusion\\tMatrix\\n,\\t\\nPrecision/Recall\\nTradeoff\\n,\\t\\nThe\\tROC\\tCurve\\n,\\t\\nError\\tAnalysis\\n,\\t\\nMultilabel\\tClassification\\nsklearn.model_selection.cross_val_score()\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n-\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\n-\\nConfusion\\tMatrix\\nsklearn.model_selection.GridSearchCV\\n,\\t\\nGrid\\tSearch\\n-\\nRandomized\\tSearch\\n,\\t\\nExercises\\n,\\nError\\tAnalysis\\n,\\t\\nExercises\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nsklearn.model_selection.StratifiedKFold\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nsklearn.model_selection.StratifiedShuffleSplit\\n,\\t\\nCreate\\ta\\tTest\\tSet\\nsklearn.model_selection.train_test_split()\\n,\\t\\nCreate\\ta\\tTest\\tSet\\n,\\t\\nTraining\\tand\\tEvaluating\\non\\tthe\\tTraining\\tSet\\n,\\t\\nLearning\\tCurves\\n,\\t\\nExercises\\n,\\t\\nGradient\\tBoosting', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 679}), Document(page_content='sklearn.multiclass.OneVsOneClassifier\\n,\\t\\nMulticlass\\tClassification\\nsklearn.neighbors.KNeighborsClassifier\\n,\\t\\nMultilabel\\tClassification\\n,\\t\\nExercises\\nsklearn.neighbors.KNeighborsRegressor\\n,\\t\\nModel-based\\tlearning\\nsklearn.pipeline.FeatureUnion\\n,\\t\\nTransformation\\tPipelines\\nsklearn.pipeline.Pipeline\\n,\\t\\nTransformation\\tPipelines\\n,\\t\\nLearning\\tCurves\\n,\\t\\nSoft\\tMargin\\nClassification\\n-\\nNonlinear\\tSVM\\tClassification\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\nHyperparameters\\nsklearn.preprocessing.Imputer\\n,\\t\\nData\\tCleaning\\n,\\t\\nTransformation\\tPipelines\\nsklearn.preprocessing.LabelBinarizer\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\n,\\nTransformation\\tPipelines\\nsklearn.preprocessing.LabelEncoder\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nsklearn.preprocessing.OneHotEncoder\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nsklearn.preprocessing.PolynomialFeatures\\n,\\t\\nPolynomial\\tRegression\\n-\\nPolynomial\\nRegression\\n,\\t\\nLearning\\tCurves\\n,\\t\\nRidge\\tRegression\\n,\\t\\nNonlinear\\tSVM\\tClassification\\nsklearn.preprocessing.StandardScaler\\n,\\t\\nFeature\\tScaling\\n-\\nTransformation\\tPipelines\\n,\\nMulticlass\\tClassification\\n,\\t\\nGradient\\tDescent\\n,\\t\\nRidge\\tRegression\\n,\\t\\nLinear\\tSVM\\nClassification\\n,\\t\\nSoft\\tMargin\\tClassification\\n-\\nPolynomial\\tKernel\\n,\\t\\nGaussian\\tRBF\\tKernel\\n,\\nImplementing\\tGradient\\tDescent\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nsklearn.svm.LinearSVC\\n,\\t\\nSoft\\tMargin\\tClassification\\n-\\nNonlinear\\tSVM\\tClassification\\n,\\nGaussian\\tRBF\\tKernel\\n-\\nComputational\\tComplexity\\n,\\t\\nSVM\\tRegression\\n,\\t\\nExercises\\nsklearn.svm.LinearSVR\\n,\\t\\nSVM\\tRegression\\n-\\nSVM\\tRegression\\nsklearn.svm.SVC\\n,\\t\\nSoft\\tMargin\\tClassification\\n,\\t\\nPolynomial\\tKernel\\n,\\t\\nGaussian\\tRBF\\tKernel\\n-\\nComputational\\tComplexity\\n,\\t\\nSVM\\tRegression\\n,\\t\\nExercises\\n,\\t\\nVoting\\tClassifiers\\nsklearn.svm.SVR\\n,\\t\\nExercises\\n,\\t\\nSVM\\tRegression\\nsklearn.tree.DecisionTreeClassifier\\n,\\t\\nRegularization\\tHyperparameters\\n,\\t\\nExercises\\n,\\nBagging\\tand\\tPasting\\tin\\tScikit-Learn\\n-\\nOut-of-Bag\\tEvaluation\\n,\\t\\nRandom\\tForests\\n,\\nAdaBoost', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 680}), Document(page_content=\"sklearn.tree.DecisionTreeRegressor\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n,\\nDecision\\tTrees\\n,\\t\\nRegression\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\nsklearn.tree.export_graphviz()\\n,\\t\\nTraining\\tand\\tVisualizing\\ta\\tDecision\\tTree\\nStandardScaler\\n,\\t\\nGradient\\tDescent\\n,\\t\\nImplementing\\tGradient\\tDescent\\n,\\t\\nTraining\\tan\\tMLP\\nwith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nSVM\\tclassification\\tclasses\\n,\\t\\nComputational\\tComplexity\\nTF.Learn\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nuser\\tguide\\n,\\t\\nOther\\tResources\\nscore()\\n,\\t\\nData\\tCleaning\\nsearch\\tspace\\n,\\t\\nRandomized\\tSearch\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\nsecond-order\\tpartial\\tderivatives\\t(Hessians)\\n,\\t\\nAdam\\tOptimization\\nself-organizing\\tmaps\\t(SOMs)\\n,\\t\\nSelf-Organizing\\tMaps\\n-\\nSelf-Organizing\\tMaps\\nsemantic\\thashing\\n,\\t\\nExercises\\nsemisupervised\\tlearning\\n,\\t\\nSemisupervised\\tlearning\\nsensitivity\\n,\\t\\nConfusion\\tMatrix\\n,\\t\\nThe\\tROC\\tCurve\\nsentiment\\tanalysis\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\nseparable_conv2d()\\n,\\t\\nResNet\\nsequences\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\nsequence_length\\n,\\t\\nHandling\\tVariable\\tLength\\tInput\\tSequences\\n-\\nHandling\\tVariable-Length\\tOutput\\nSequences\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nShannon's\\tinformation\\ttheory\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\nshortcut\\tconnections\\n,\\t\\nResNet\\nshow()\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\nshow_graph()\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\", metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 681}), Document(page_content='shrinkage\\n,\\t\\nGradient\\tBoosting\\nshuffle_batch()\\n,\\t\\nOther\\tconvenience\\tfunctions\\nshuffle_batch_join()\\n,\\t\\nOther\\tconvenience\\tfunctions\\nsigmoid\\tfunction\\n,\\t\\nEstimating\\tProbabilities\\nsigmoid_cross_entropy_with_logits()\\n,\\t\\nTensorFlow\\tImplementation\\nsimilarity\\tfunction\\n,\\t\\nAdding\\tSimilarity\\tFeatures\\n-\\nAdding\\tSimilarity\\tFeatures\\nsimulated\\tannealing\\n,\\t\\nStochastic\\tGradient\\tDescent\\nsimulated\\tenvironments\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\n(\\nsee\\talso\\n\\tOpenAI\\tGym)\\nSingular\\tValue\\tDecomposition\\t(SVD)\\n,\\t\\nPrincipal\\tComponents\\nskewed\\tdatasets\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nskip\\tconnections\\n,\\t\\nData\\tAugmentation\\n,\\t\\nResNet\\nslack\\tvariable\\n,\\t\\nTraining\\tObjective\\nsmoothing\\tterms\\n,\\t\\nBatch\\tNormalization\\n,\\t\\nAdaGrad\\n,\\t\\nAdam\\tOptimization\\n,\\t\\nVariational\\nAutoencoders\\nsoft\\tmargin\\tclassification\\n,\\t\\nSoft\\tMargin\\tClassification\\n-\\nSoft\\tMargin\\tClassification\\nsoft\\tplacements\\n,\\t\\nSoft\\tplacement\\nsoft\\tvoting\\n,\\t\\nVoting\\tClassifiers\\nsoftmax\\tfunction\\n,\\t\\nSoftmax\\tRegression\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n,\\t\\nTraining\\nan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nSoftmax\\tRegression\\n,\\t\\nSoftmax\\tRegression\\n-\\nSoftmax\\tRegression\\nsource\\tops\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n,\\t\\nParallel\\tExecution\\nspam\\tfilters\\n,\\t\\nThe\\tMachine\\tLearning\\tLandscape\\n-\\nWhy\\tUse\\tMachine\\tLearning?\\n,\\t\\nSupervised\\nlearning', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 682}), Document(page_content='sparse\\tautoencoders\\n,\\t\\nSparse\\tAutoencoders\\n-\\nTensorFlow\\tImplementation\\nsparse\\tmatrix\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nsparse\\tmodels\\n,\\t\\nLasso\\tRegression\\n,\\t\\nAdam\\tOptimization\\nsparse_softmax_cross_entropy_with_logits()\\n,\\t\\nConstruction\\tPhase\\nsparsity\\tloss\\n,\\t\\nSparse\\tAutoencoders\\nspecificity\\n,\\t\\nThe\\tROC\\tCurve\\nspeech\\trecognition\\n,\\t\\nWhy\\tUse\\tMachine\\tLearning?\\nspurious\\tpatterns\\n,\\t\\nHopfield\\tNetworks\\nstack()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\nstacked\\tautoencoders\\n,\\t\\nStacked\\tAutoencoders\\n-\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\nAutoencoders\\nTensorFlow\\timplementation\\n,\\t\\nTensorFlow\\tImplementation\\ntraining\\tone-at-a-time\\n,\\t\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\n-\\nTraining\\tOne\\tAutoencoder\\nat\\ta\\tTime\\ntying\\tweights\\n,\\t\\nTying\\tWeights\\n-\\nTying\\tWeights\\nunsupervised\\tpretraining\\twith\\n,\\t\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\n-\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\nvisualizing\\tthe\\treconstructions\\n,\\t\\nVisualizing\\tthe\\tReconstructions\\n-\\nVisualizing\\tthe\\nReconstructions\\nstacked\\tdenoising\\tautoencoders\\n,\\t\\nVisualizing\\tFeatures\\n,\\t\\nDenoising\\tAutoencoders\\nstacked\\tdenoising\\tencoders\\n,\\t\\nDenoising\\tAutoencoders\\nstacked\\tgeneralization\\n\\t(\\nsee\\n\\tstacking)\\nstacking\\n,\\t\\nStacking\\n-\\nStacking\\nstale\\tgradients\\n,\\t\\nAsynchronous\\tupdates\\nstandard\\tcorrelation\\tcoefficient\\n,\\t\\nLooking\\tfor\\tCorrelations', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 683}), Document(page_content='standardization\\n,\\t\\nFeature\\tScaling\\nStandardScaler\\n,\\t\\nTransformation\\tPipelines\\n,\\t\\nImplementing\\tGradient\\tDescent\\n,\\t\\nTraining\\tan\\tMLP\\nwith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nstate-action\\tvalues\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nstates\\t\\ntensor\\n,\\t\\nHandling\\tVariable\\tLength\\tInput\\tSequences\\nstate_is_tuple\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\n,\\t\\nLSTM\\tCell\\nstatic\\tunrolling\\tthrough\\ttime\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nStatic\\tUnrolling\\tThrough\\tTime\\nstatic_rnn()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nStatic\\tUnrolling\\tThrough\\tTime\\n,\\t\\nAn\\tEncoder–\\nDecoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nstationary\\tpoint\\n,\\t\\nSVM\\tDual\\tProblem\\n-\\nSVM\\tDual\\tProblem\\nstatistical\\tmode\\n,\\t\\nBagging\\tand\\tPasting\\nstatistical\\tsignificance\\n,\\t\\nRegularization\\tHyperparameters\\nstemming\\n,\\t\\nExercises\\nstep\\tfunctions\\n,\\t\\nThe\\tPerceptron\\nstep()\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\nStochastic\\tGradient\\tBoosting\\n,\\t\\nGradient\\tBoosting\\nStochastic\\tGradient\\tDescent\\t(SGD)\\n,\\t\\nStochastic\\tGradient\\tDescent\\n-\\nStochastic\\tGradient\\nDescent\\n,\\t\\nSoft\\tMargin\\tClassification\\n,\\t\\nThe\\tPerceptron\\ntraining\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\nStochastic\\tGradient\\tDescent\\t(SGD)\\tclassifier\\n,\\t\\nTraining\\ta\\tBinary\\tClassifier\\n,\\t\\nRidge\\tRegression\\nstochastic\\tneurons\\n,\\t\\nBoltzmann\\tMachines\\nstochastic\\tpolicy\\n,\\t\\nPolicy\\tSearch\\nstratified\\tsampling\\n,\\t\\nCreate\\ta\\tTest\\tSet\\n-\\nCreate\\ta\\tTest\\tSet\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-\\nValidation', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 684}), Document(page_content='stride\\n,\\t\\nConvolutional\\tLayer\\nstring\\tkernels\\n,\\t\\nGaussian\\tRBF\\tKernel\\nstring_input_producer()\\n,\\t\\nOther\\tconvenience\\tfunctions\\nstrong\\tlearners\\n,\\t\\nVoting\\tClassifiers\\nsubderivatives\\n,\\t\\nOnline\\tSVMs\\nsubgradient\\tvector\\n,\\t\\nLasso\\tRegression\\nsubsample\\n,\\t\\nGradient\\tBoosting\\n,\\t\\nPooling\\tLayer\\nsupervised\\tlearning\\n,\\t\\nSupervised/Unsupervised\\tLearning\\n-\\nSupervised\\tlearning\\nSupport\\tVector\\tMachines\\t(SVMs)\\n,\\t\\nMulticlass\\tClassification\\n,\\t\\nSupport\\tVector\\tMachines\\n-\\nExercises\\ndecision\\tfunction\\tand\\tpredictions\\n,\\t\\nDecision\\tFunction\\tand\\tPredictions\\n-\\nDecision\\tFunction\\nand\\tPredictions\\ndual\\tproblem\\n,\\t\\nSVM\\tDual\\tProblem\\n-\\nSVM\\tDual\\tProblem\\nkernelized\\tSVM\\n,\\t\\nKernelized\\tSVM\\n-\\nKernelized\\tSVM\\nlinear\\tclassification\\n,\\t\\nLinear\\tSVM\\tClassification\\n-\\nSoft\\tMargin\\tClassification\\nmechanics\\tof\\n,\\t\\nUnder\\tthe\\tHood\\n-\\nOnline\\tSVMs\\nnonlinear\\tclassification\\n,\\t\\nNonlinear\\tSVM\\tClassification\\n-\\nComputational\\tComplexity\\nonline\\tSVMs\\n,\\t\\nOnline\\tSVMs\\n-\\nOnline\\tSVMs\\nQuadratic\\tProgramming\\t(QP)\\tproblems\\n,\\t\\nQuadratic\\tProgramming\\n-\\nQuadratic\\nProgramming\\nSVM\\tregression\\n,\\t\\nSVM\\tRegression\\n-\\nOnline\\tSVMs\\nthe\\tdual\\tproblem\\n,\\t\\nThe\\tDual\\tProblem\\ntraining\\tobjective\\n,\\t\\nTraining\\tObjective\\n-\\nTraining\\tObjective\\nsupport\\tvectors\\n,\\t\\nLinear\\tSVM\\tClassification', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 685}), Document(page_content='svd()\\n,\\t\\nPrincipal\\tComponents\\nsymbolic\\tdifferentiation\\n,\\t\\nUsing\\tautodiff\\n,\\t\\nSymbolic\\tDifferentiation\\n-\\nNumerical\\tDifferentiation\\nsynchronous\\tupdates\\n,\\t\\nSynchronous\\tupdates', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 686}), Document(page_content='T\\nt-Distributed\\tStochastic\\tNeighbor\\tEmbedding\\t(t-SNE)\\n,\\t\\nOther\\tDimensionality\\tReduction\\nTechniques\\ntail\\theavy\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\ntarget\\tattributes\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\ntarget_weights\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\ntasks\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\nTemporal\\tDifference\\t(TD)\\tLearning\\n,\\t\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\n-\\nTemporal\\nDifference\\tLearning\\tand\\tQ-Learning\\ntensor\\tprocessing\\tunits\\t(TPUs)\\n,\\t\\nInstallation\\nTensorBoard\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nTensorFlow\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\n-\\nExercises\\nabout\\n,\\t\\nObjective\\tand\\tApproach\\nautodiff\\n,\\t\\nUsing\\tautodiff\\n-\\nUsing\\tautodiff\\n,\\t\\nAutodiff\\n-\\nReverse-Mode\\tAutodiff\\nBatch\\tNormalization\\twith\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n-\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\nconstruction\\tphase\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\ncontrol\\tdependencies\\n,\\t\\nControl\\tDependencies\\nconvenience\\tfunctions\\n,\\t\\nOther\\tconvenience\\tfunctions\\nconvolutional\\tlayers\\n,\\t\\nResNet\\nconvolutional\\tneural\\tnetworks\\tand\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\nImplementation\\ndata\\tparallelism\\tand\\n,\\t\\nTensorFlow\\timplementation\\ndenoising\\tautoencoders\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\tImplementation\\ndropout\\twith\\n,\\t\\nDropout', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 687}), Document(page_content='dynamic\\tplacer\\n,\\t\\nPlacing\\tOperations\\ton\\tDevices\\nexecution\\tphase\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\nfeeding\\tdata\\tto\\tthe\\ttraining\\talgorithm\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\n-\\nFeeding\\nData\\tto\\tthe\\tTraining\\tAlgorithm\\nGradient\\tDescent\\twith\\n,\\t\\nImplementing\\tGradient\\tDescent\\n-\\nUsing\\tan\\tOptimizer\\ngraphs,\\tmanaging\\n,\\t\\nManaging\\tGraphs\\ninitial\\tgraph\\tcreation\\tand\\tsession\\trun\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\nSession\\n-\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\ninstallation\\n,\\t\\nInstallation\\nl1\\tand\\tl2\\tregularization\\twith\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\nlearning\\tschedules\\tin\\n,\\t\\nLearning\\tRate\\tScheduling\\nLinear\\tRegression\\twith\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n-\\nLinear\\tRegression\\twith\\nTensorFlow\\nmax\\tpooling\\tlayer\\tin\\n,\\t\\nPooling\\tLayer\\nmax-norm\\tregularization\\twith\\n,\\t\\nMax-Norm\\tRegularization\\nmodel\\tzoo\\n,\\t\\nModel\\tZoos\\nmodularity\\n,\\t\\nModularity\\n-\\nModularity\\nMomentum\\toptimization\\tin\\n,\\t\\nMomentum\\tOptimization\\nname\\tscopes\\n,\\t\\nName\\tScopes\\nneural\\tnetwork\\tpolicies\\n,\\t\\nNeural\\tNetwork\\tPolicies\\nNLP\\ttutorials\\n,\\t\\nNatural\\tLanguage\\tProcessing\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\nMachine\\tTranslation\\nnode\\tvalue\\tlifecycle\\n,\\t\\nLifecycle\\tof\\ta\\tNode\\tValue\\noperations\\t(ops)\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 688}), Document(page_content='optimizer\\n,\\t\\nUsing\\tan\\tOptimizer\\noverview\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\n-\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nparallel\\tdistributed\\tcomputing\\n\\t(\\nsee\\n\\tparallel\\tdistributed\\tcomputing\\twith\\tTensorFlow)\\nPython\\tAPI\\nconstruction\\n,\\t\\nConstruction\\tPhase\\n-\\nConstruction\\tPhase\\nexecution\\n,\\t\\nExecution\\tPhase\\nusing\\tthe\\tneural\\tnetwork\\n,\\t\\nUsing\\tthe\\tNeural\\tNetwork\\nqueues\\n\\t(\\nsee\\n\\tqueues)\\nreusing\\tpretrained\\tlayers\\n,\\t\\nReusing\\ta\\tTensorFlow\\tModel\\n-\\nReusing\\ta\\tTensorFlow\\tModel\\nRNNs\\tin\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n-\\nHandling\\tVariable-Length\\tOutput\\tSequences\\n(\\nsee\\talso\\n\\trecurrent\\tneural\\tnetworks\\t(RNNs))\\nsaving\\tand\\trestoring\\tmodels\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\n-\\nSaving\\tand\\tRestoring\\nModels\\nsharing\\tvariables\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\nsimple\\tplacer\\n,\\t\\nPlacing\\tOperations\\ton\\tDevices\\nsparse\\tautoencoders\\twith\\n,\\t\\nTensorFlow\\tImplementation\\nand\\tstacked\\tautoencoders\\n,\\t\\nTensorFlow\\tImplementation\\nTensorBoard\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\n-\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\ntf.abs()\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\ntf.add()\\n,\\t\\nModularity\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\ntf.add_n()\\n,\\t\\nModularity\\n-\\nSharing\\tVariables\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\ntf.add_to_collection()\\n,\\t\\nMax-Norm\\tRegularization\\ntf.assign()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nReusing\\tModels\\tfrom\\tOther', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 689}), Document(page_content='Frameworks\\n,\\t\\nMax-Norm\\tRegularization\\n-\\nMax-Norm\\tRegularization\\n,\\t\\nChapter\\t9:\\tUp\\tand\\nRunning\\twith\\tTensorFlow\\ntf.bfloat16\\n,\\t\\nBandwidth\\tsaturation\\ntf.bool\\n,\\t\\nDropout\\ntf.cast()\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\ntf.clip_by_norm()\\n,\\t\\nMax-Norm\\tRegularization\\n-\\nMax-Norm\\tRegularization\\ntf.clip_by_value()\\n,\\t\\nGradient\\tClipping\\ntf.concat()\\n,\\t\\nExercises\\n,\\t\\nGoogLeNet\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n,\\t\\nPolicy\\tGradients\\ntf.ConfigProto\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM\\n,\\t\\nLogging\\tplacements\\n-\\nSoft\\tplacement\\n,\\t\\nIn-\\nGraph\\tVersus\\tBetween-Graph\\tReplication\\n,\\t\\nChapter\\t12:\\tDistributing\\tTensorFlow\\tAcross\\nDevices\\tand\\tServers\\ntf.constant()\\n,\\t\\nLifecycle\\tof\\ta\\tNode\\tValue\\n-\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nSimple\\nplacement\\n-\\nDynamic\\tplacement\\tfunction\\n,\\t\\nControl\\tDependencies\\n,\\t\\nOpening\\ta\\tSession\\n-\\nPinning\\tOperations\\tAcross\\tTasks\\ntf.constant_initializer()\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\ntf.container()\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\n-\\nAsynchronous\\nCommunication\\tUsing\\tTensorFlow\\tQueues\\n,\\t\\nTensorFlow\\timplementation\\n-\\nExercises\\n,\\nChapter\\t9:\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\ntf.contrib.layers.l1_regularizer()\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n,\\t\\nMax-Norm\\tRegularization\\ntf.contrib.layers.l2_regularizer()\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n,\\t\\nTensorFlow\\nImplementation\\n-\\nTying\\tWeights\\ntf.contrib.layers.variance_scaling_initializer()\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\n-\\nXavier\\tand\\nHe\\tInitialization\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTying\\nWeights\\n,\\t\\nVariational\\tAutoencoders\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n,\\t\\nPolicy\\tGradients\\n,\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.contrib.learn.DNNClassifier\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\ntf.contrib.learn.infer_real_valued_columns_from_input()\\n,\\t\\nTraining\\tan\\tMLP\\twith\\nTensorFlow’s\\tHigh-Level\\tAPI', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 690}), Document(page_content='tf.contrib.rnn.BasicLSTMCell\\n,\\t\\nLSTM\\tCell\\n,\\t\\nPeephole\\tConnections\\ntf.contrib.rnn.BasicRNNCell\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nDynamic\\tUnrolling\\nThrough\\tTime\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n-\\nTraining\\nto\\tPredict\\tTime\\tSeries\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n,\\t\\nDeep\\tRNNs\\n-\\nApplying\\tDropout\\n,\\nLSTM\\tCell\\ntf.contrib.rnn.DropoutWrapper\\n,\\t\\nApplying\\tDropout\\ntf.contrib.rnn.GRUCell\\n,\\t\\nGRU\\tCell\\ntf.contrib.rnn.LSTMCell\\n,\\t\\nPeephole\\tConnections\\ntf.contrib.rnn.MultiRNNCell\\n,\\t\\nDeep\\tRNNs\\n-\\nApplying\\tDropout\\ntf.contrib.rnn.OutputProjectionWrapper\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n-\\nTraining\\tto\\nPredict\\tTime\\tSeries\\ntf.contrib.rnn.RNNCell\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\ntf.contrib.rnn.static_rnn()\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n-\\nHandling\\tVariable\\tLength\\tInput\\nSequences\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\n-\\nExercises\\n,\\nChapter\\t14:\\tRecurrent\\tNeural\\tNetworks\\n-\\nChapter\\t14:\\tRecurrent\\tNeural\\tNetworks\\ntf.contrib.slim\\tmodule\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\n,\\t\\nExercises\\ntf.contrib.slim.nets\\tmodule\\t(nets)\\n,\\t\\nExercises\\ntf.control_dependencies()\\n,\\t\\nControl\\tDependencies\\ntf.decode_csv()\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\nreaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\ntf.device()\\n,\\t\\nSimple\\tplacement\\n-\\nSoft\\tplacement\\n,\\t\\nPinning\\tOperations\\tAcross\\tTasks\\n-\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\nAcross\\tMultiple\\tGPUs\\n-\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\ntf.exp()\\n,\\t\\nVariational\\tAutoencoders\\n-\\nGenerating\\tDigits\\ntf.FIFOQueue\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\n,\\t\\nQueues\\tof\\ntuples\\n-\\nRandomShuffleQueue\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 691}), Document(page_content='tf.float32\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n,\\t\\nChapter\\t9:\\tUp\\tand\\tRunning\\twith\\nTensorFlow\\ntf.get_collection()\\n,\\t\\nReusing\\ta\\tTensorFlow\\tModel\\n-\\nFreezing\\tthe\\tLower\\tLayers\\n,\\t\\nℓ1\\tand\\tℓ2\\nRegularization\\n,\\t\\nMax-Norm\\tRegularization\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nLearning\\tto\\nPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.get_default_graph()\\n,\\t\\nManaging\\tGraphs\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\nUsing\\tTensorBoard\\ntf.get_default_session()\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\ntf.get_variable()\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\n,\\t\\nReusing\\tModels\\tfrom\\tOther\\nFrameworks\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\ntf.global_variables()\\n,\\t\\nReusing\\ta\\tTensorFlow\\tModel\\ntf.global_variables_initializer()\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\nManually\\tComputing\\tthe\\tGradients\\ntf.gradients()\\n,\\t\\nUsing\\tautodiff\\ntf.Graph\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\t\\nManaging\\tGraphs\\n,\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\n,\\t\\nLoading\\tData\\tDirectly\\nfrom\\tthe\\tGraph\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\ntf.GraphKeys.GLOBAL_VARIABLES\\n,\\t\\nReusing\\ta\\tTensorFlow\\tModel\\n-\\nFreezing\\tthe\\nLower\\tLayers\\ntf.GraphKeys.REGULARIZATION_LOSSES\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n,\\t\\nTensorFlow\\nImplementation\\ntf.group()\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.int32\\n,\\t\\nOperations\\tand\\tkernels\\n-\\nQueues\\tof\\ttuples\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\nfrom\\tthe\\tgraph\\n,\\t\\nHandling\\tVariable\\tLength\\tInput\\tSequences\\n,\\t\\nTraining\\ta\\tSequence\\nClassifier\\n,\\t\\nWord\\tEmbeddings\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.int64\\n,\\t\\nConstruction\\tPhase\\ntf.InteractiveSession\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\ntf.layers.batch_normalization()\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n-', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 692}), Document(page_content='Implementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\ntf.layers.dense()\\n,\\t\\nConstruction\\tPhase\\nTF.Learn\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\ntf.log()\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nVariational\\tAutoencoders\\n,\\t\\nNeural\\tNetwork\\nPolicies\\n,\\t\\nPolicy\\tGradients\\ntf.matmul()\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n-\\nManually\\tComputing\\tthe\\tGradients\\n,\\nModularity\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n,\\t\\nTying\\tWeights\\n,\\t\\nTraining\\nOne\\tAutoencoder\\tat\\ta\\tTime\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\tImplementation\\ntf.matrix_inverse()\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\ntf.maximum()\\n,\\t\\nModularity\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\n,\\t\\nNonsaturating\\nActivation\\tFunctions\\ntf.multinomial()\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n,\\t\\nPolicy\\tGradients\\ntf.name_scope()\\n,\\t\\nName\\tScopes\\n,\\t\\nModularity\\n-\\nSharing\\tVariables\\n,\\t\\nConstruction\\tPhase\\n,\\nConstruction\\tPhase\\n-\\nConstruction\\tPhase\\n,\\t\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\n-\\nTraining\\nOne\\tAutoencoder\\tat\\ta\\tTime\\ntf.nn.conv2d()\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\tImplementation\\ntf.nn.dynamic_rnn()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nDynamic\\tUnrolling\\tThrough\\tTime\\n,\\nTraining\\ta\\tSequence\\tClassifier\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n,\\t\\nTraining\\tto\\tPredict\\nTime\\tSeries\\n,\\t\\nDeep\\tRNNs\\n-\\nApplying\\tDropout\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\nMachine\\tTranslation\\n-\\nExercises\\n,\\t\\nChapter\\t14:\\tRecurrent\\tNeural\\tNetworks\\n-\\nChapter\\t14:\\nRecurrent\\tNeural\\tNetworks\\ntf.nn.elu()\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTying\\nWeights\\n,\\t\\nVariational\\tAutoencoders\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n,\\t\\nPolicy\\tGradients\\ntf.nn.embedding_lookup()\\n,\\t\\nWord\\tEmbeddings\\ntf.nn.in_top_k()\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\ntf.nn.max_pool()\\n,\\t\\nPooling\\tLayer\\n-\\nPooling\\tLayer\\ntf.nn.relu()\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n-\\nTraining\\tto\\tPredict', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 693}), Document(page_content='Time\\tSeries\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\nDeep\\tQ-Learning\\ntf.nn.sigmoid_cross_entropy_with_logits()\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nGenerating\\nDigits\\n,\\t\\nPolicy\\tGradients\\n-\\nPolicy\\tGradients\\ntf.nn.sparse_softmax_cross_entropy_with_logits()\\n,\\t\\nConstruction\\tPhase\\n-\\nConstruction\\nPhase\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\ntf.one_hot()\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.PaddingFIFOQueue\\n,\\t\\nPaddingFifoQueue\\ntf.placeholder()\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\n-\\nFeeding\\tData\\tto\\tthe\\tTraining\\nAlgorithm\\n,\\t\\nChapter\\t9:\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\ntf.placeholder_with_default()\\n,\\t\\nTensorFlow\\tImplementation\\ntf.RandomShuffleQueue\\n,\\t\\nRandomShuffleQueue\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\nthe\\tgraph\\n-\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\treaders\\nusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nOther\\tconvenience\\tfunctions\\ntf.random_normal()\\n,\\t\\nModularity\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n,\\t\\nTensorFlow\\nImplementation\\n,\\t\\nVariational\\tAutoencoders\\ntf.random_uniform()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\n,\\nWord\\tEmbeddings\\n,\\t\\nChapter\\t9:\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\ntf.reduce_mean()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nName\\tScopes\\n,\\t\\nConstruction\\nPhase\\n-\\nConstruction\\tPhase\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\n-\\nTraining\\ta\\tSequence\\tClassifier\\n,\\t\\nPerforming\\tPCA\\twith\\tan\\tUndercomplete\\tLinear\\nAutoencoder\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\n,\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nTensorFlow\\nImplementation\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.reduce_sum()\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\nImplementation\\n,\\t\\nVariational\\tAutoencoders\\n-\\nGenerating\\tDigits\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\nPac-Man\\tUsing\\tDeep\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-\\nLearning\\ntf.reset_default_graph()\\n,\\t\\nManaging\\tGraphs', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 694}), Document(page_content='tf.reshape()\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\nDeep\\tQ-Learning\\ntf.RunOptions\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\ntf.Session\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\t\\nChapter\\t9:\\tUp\\tand\\nRunning\\twith\\tTensorFlow\\ntf.shape()\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nVariational\\tAutoencoders\\ntf.square()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nName\\tScopes\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\nSeries\\n,\\t\\nPerforming\\tPCA\\twith\\tan\\tUndercomplete\\tLinear\\tAutoencoder\\n,\\t\\nTensorFlow\\nImplementation\\n,\\t\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\n,\\t\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\nTime\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nVariational\\nAutoencoders\\n-\\nGenerating\\tDigits\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-\\nLearning\\ntf.stack()\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\treaders\\nusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\ntf.string\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\treaders\\tusing\\na\\tCoordinator\\tand\\ta\\tQueueRunner\\ntf.summary.FileWriter\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\n-\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\ntf.summary.scalar()\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\ntf.tanh()\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\ntf.TextLineReader\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\nreaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\ntf.to_float()\\n,\\t\\nPolicy\\tGradients\\n-\\nPolicy\\tGradients\\ntf.train.AdamOptimizer\\n,\\t\\nAdam\\tOptimization\\n,\\t\\nAdam\\tOptimization\\n,\\t\\nTraining\\ta\\tSequence\\nClassifier\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n,\\t\\nPerforming\\tPCA\\twith\\tan\\tUndercomplete\\nLinear\\tAutoencoder\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTying\\tWeights\\n,\\t\\nTraining\\tOne\\nAutoencoder\\tat\\ta\\tTime\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nGenerating\\tDigits\\n,\\t\\nPolicy\\nGradients\\n-\\nPolicy\\tGradients\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.train.ClusterSpec\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 695}), Document(page_content='tf.train.Coordinator\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\ntf.train.exponential_decay()\\n,\\t\\nLearning\\tRate\\tScheduling\\ntf.train.GradientDescentOptimizer\\n,\\t\\nUsing\\tan\\tOptimizer\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nGradient\\nClipping\\n,\\t\\nMomentum\\tOptimization\\n,\\t\\nAdam\\tOptimization\\ntf.train.MomentumOptimizer\\n,\\t\\nUsing\\tan\\tOptimizer\\n,\\t\\nMomentum\\tOptimization\\n-\\nNesterov\\nAccelerated\\tGradient\\n,\\t\\nLearning\\tRate\\tScheduling\\n,\\t\\nExercises\\n,\\t\\nTensorFlow\\nimplementation\\n,\\t\\nChapter\\t10:\\tIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n-\\nChapter\\t11:\\nTraining\\tDeep\\tNeural\\tNets\\ntf.train.QueueRunner\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nOther\\tconvenience\\tfunctions\\ntf.train.replica_device_setter()\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\n-\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\ntf.train.RMSPropOptimizer\\n,\\t\\nRMSProp\\ntf.train.Saver\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\n-\\nSaving\\tand\\tRestoring\\tModels\\n,\\t\\nConstruction\\nPhase\\n,\\t\\nExercises\\n,\\t\\nApplying\\tDropout\\n,\\t\\nPolicy\\tGradients\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\nUsing\\tDeep\\tQ-Learning\\ntf.train.Server\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\ntf.train.start_queue_runners()\\n,\\t\\nOther\\tconvenience\\tfunctions\\ntf.transpose()\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n-\\nManually\\tComputing\\tthe\\tGradients\\n,\\nStatic\\tUnrolling\\tThrough\\tTime\\n,\\t\\nTying\\tWeights\\ntf.truncated_normal()\\n,\\t\\nConstruction\\tPhase\\ntf.unstack()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nDynamic\\tUnrolling\\tThrough\\tTime\\n,\\t\\nTraining\\nto\\tPredict\\tTime\\tSeries\\n,\\t\\nChapter\\t14:\\tRecurrent\\tNeural\\tNetworks\\ntf.Variable\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\t\\nChapter\\t9:\\tUp\\tand\\nRunning\\twith\\tTensorFlow\\ntf.variable_scope()\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\n,\\t\\nReusing\\tModels\\tfrom\\tOther\\nFrameworks\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\n,\\t\\nTraining\\ta', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 696}), Document(page_content='Sequence\\tClassifier\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.zeros()\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n,\\t\\nTying\\tWeights\\ntruncated\\tbackpropagation\\tthrough\\ttime\\n,\\t\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\nSteps\\nvisualizing\\tgraph\\tand\\ttraining\\tcurves\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\nTensorBoard\\n-\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\nTensorFlow\\tServing\\n,\\t\\nOne\\tNeural\\tNetwork\\tper\\tDevice\\ntensorflow.contrib\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\ntest\\tset\\n,\\t\\nTesting\\tand\\tValidating\\n,\\t\\nCreate\\ta\\tTest\\tSet\\n-\\nCreate\\ta\\tTest\\tSet\\n,\\t\\nMNIST\\ntesting\\tand\\tvalidating\\n,\\t\\nTesting\\tand\\tValidating\\n-\\nTesting\\tand\\tValidating\\ntext\\tattributes\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\n-\\nHandling\\tText\\tand\\tCategorical\\nAttributes\\nTextLineReader\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\nTF-slim\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\ntf.layers.conv1d()\\n,\\t\\nResNet\\ntf.layers.conv2d()\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.layers.conv2d_transpose()\\n,\\t\\nResNet\\ntf.layers.conv3d()\\n,\\t\\nResNet\\ntf.layers.dense()\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\nTensorFlow\\ntf.layers.separable_conv2d()\\n,\\t\\nResNet\\nTF.Learn\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\nAPI\\ntf.nn.atrous_conv2d()\\n,\\t\\nResNet\\ntf.nn.depthwise_conv2d()\\n,\\t\\nResNet', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 697}), Document(page_content='thermal\\tequilibrium\\n,\\t\\nBoltzmann\\tMachines\\nthread\\tpools\\t(inter-op/intra-op,\\tin\\tTensorFlow\\n,\\t\\nParallel\\tExecution\\nthreshold\\t\\nvariable\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\nTikhonov\\tregularization\\n,\\t\\nRidge\\tRegression\\ntime\\tseries\\tdata\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\ntoarray()\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\ntolerance\\thyperparameter\\n,\\t\\nComputational\\tComplexity\\ntraining\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n-\\nImplementing\\tBatch\\nNormalization\\twith\\tTensorFlow\\n,\\t\\nApplying\\tDropout\\ntraining\\tdata\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\ninsufficient\\tquantities\\n,\\t\\nInsufficient\\tQuantity\\tof\\tTraining\\tData\\nirrelevant\\tfeatures\\n,\\t\\nIrrelevant\\tFeatures\\nloading\\n,\\t\\nLoading\\tData\\tDirectly\\tfrom\\tthe\\tGraph\\n-\\nOther\\tconvenience\\tfunctions\\nnonrepresentative\\n,\\t\\nNonrepresentative\\tTraining\\tData\\noverfitting\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n-\\nOverfitting\\tthe\\tTraining\\tData\\npoor\\tquality\\n,\\t\\nPoor-Quality\\tData\\nunderfitting\\n,\\t\\nUnderfitting\\tthe\\tTraining\\tData\\ntraining\\tinstance\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\ntraining\\tmodels\\n,\\t\\nModel-based\\tlearning\\n,\\t\\nTraining\\tModels\\n-\\nExercises\\nlearning\\tcurves\\tin\\n,\\t\\nLearning\\tCurves\\n-\\nLearning\\tCurves\\nLinear\\tRegression\\n,\\t\\nTraining\\tModels\\n,\\t\\nLinear\\tRegression\\n-\\nMini-batch\\tGradient\\tDescent\\nLogistic\\tRegression\\n,\\t\\nLogistic\\tRegression\\n-\\nSoftmax\\tRegression\\noverview\\n,\\t\\nTraining\\tModels\\n-\\nTraining\\tModels\\nPolynomial\\tRegression\\n,\\t\\nTraining\\tModels\\n,\\t\\nPolynomial\\tRegression\\n-\\nPolynomial\\tRegression', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 698}), Document(page_content='training\\tobjectives\\n,\\t\\nTraining\\tObjective\\n-\\nTraining\\tObjective\\ntraining\\tset\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\n,\\t\\nTesting\\tand\\tValidating\\n,\\t\\nDiscover\\tand\\tVisualize\\tthe\\nData\\tto\\tGain\\tInsights\\n,\\t\\nPrepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\tAlgorithms\\n,\\t\\nTraining\\tand\\nEvaluating\\ton\\tthe\\tTraining\\tSet\\n-\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\ncost\\tfunction\\tof\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\n-\\nTraining\\tand\\tCost\\tFunction\\nshuffling\\n,\\t\\nMNIST\\ntransfer\\tlearning\\n,\\t\\nReusing\\tPretrained\\tLayers\\n-\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\n(\\nsee\\talso\\n\\tpretrained\\tlayers\\treuse)\\ntransform()\\n,\\t\\nData\\tCleaning\\n,\\t\\nTransformation\\tPipelines\\ntransformation\\tpipelines\\n,\\t\\nTransformation\\tPipelines\\n-\\nSelect\\tand\\tTrain\\ta\\tModel\\ntransformers\\n,\\t\\nData\\tCleaning\\ntransformers,\\tcustom\\n,\\t\\nCustom\\tTransformers\\n-\\nCustom\\tTransformers\\ntranspose()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\ntrue\\tnegative\\trate\\t(TNR)\\n,\\t\\nThe\\tROC\\tCurve\\ntrue\\tpositive\\trate\\t(TPR)\\n,\\t\\nConfusion\\tMatrix\\n,\\t\\nThe\\tROC\\tCurve\\ntruncated\\tbackpropagation\\tthrough\\ttime\\n,\\t\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps\\ntuples\\n,\\t\\nQueues\\tof\\ttuples\\ntying\\tweights\\n,\\t\\nTying\\tWeights\\nU\\nunderfitting\\n,\\t\\nUnderfitting\\tthe\\tTraining\\tData\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n,\\nGaussian\\tRBF\\tKernel\\nunivariate\\tregression\\n,\\t\\nFrame\\tthe\\tProblem\\nunstack()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\nunsupervised\\tlearning\\n,\\t\\nUnsupervised\\tlearning\\n-\\nUnsupervised\\tlearning\\nanomaly\\tdetection\\n,\\t\\nUnsupervised\\tlearning', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 699}), Document(page_content='association\\trule\\tlearning\\n,\\t\\nUnsupervised\\tlearning\\n,\\t\\nUnsupervised\\tlearning\\nclustering\\n,\\t\\nUnsupervised\\tlearning\\ndimensionality\\treduction\\talgorithm\\n,\\t\\nUnsupervised\\tlearning\\nvisualization\\talgorithms\\n,\\t\\nUnsupervised\\tlearning\\nunsupervised\\tpretraining\\n,\\t\\nUnsupervised\\tPretraining\\n-\\nUnsupervised\\tPretraining\\n,\\t\\nUnsupervised\\nPretraining\\tUsing\\tStacked\\tAutoencoders\\n-\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\nAutoencoders\\nupsampling\\n,\\t\\nResNet\\nutility\\tfunction\\n,\\t\\nModel-based\\tlearning\\nV\\nvalidation\\tset\\n,\\t\\nTesting\\tand\\tValidating\\nValue\\tIteration\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nvalue_counts()\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\nvanishing\\tgradients\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n(\\nsee\\talso\\n\\tgradients,\\tvanishing\\tand\\texploding)\\nvariables,\\tsharing\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\nvariable_scope()\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\nvariance\\nbias/variance\\ttradeoff\\n,\\t\\nLearning\\tCurves\\nvariance\\tpreservation\\n,\\t\\nPreserving\\tthe\\tVariance\\n-\\nPreserving\\tthe\\tVariance\\nvariance_scaling_initializer()\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\nvariational\\tautoencoders\\n,\\t\\nVariational\\tAutoencoders\\n-\\nGenerating\\tDigits\\nVGGNet\\n,\\t\\nResNet\\nvisual\\tcortex\\n,\\t\\nThe\\tArchitecture\\tof\\tthe\\tVisual\\tCortex', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 700}), Document(page_content='visualization\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\n-\\nVisualizing\\tthe\\nGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\nvisualization\\talgorithms\\n,\\t\\nUnsupervised\\tlearning\\n-\\nUnsupervised\\tlearning\\nvoice\\trecognition\\n,\\t\\nConvolutional\\tNeural\\tNetworks\\nvoting\\tclassifiers\\n,\\t\\nVoting\\tClassifiers\\n-\\nVoting\\tClassifiers\\nW\\nwarmup\\tphase\\n,\\t\\nAsynchronous\\tupdates\\nweak\\tlearners\\n,\\t\\nVoting\\tClassifiers\\nweight-tying\\n,\\t\\nTying\\tWeights\\nweights\\n,\\t\\nConstruction\\tPhase\\nfreezing\\n,\\t\\nFreezing\\tthe\\tLower\\tLayers\\nwhile_loop()\\n,\\t\\nDynamic\\tUnrolling\\tThrough\\tTime\\nwhite\\tbox\\tmodels\\n,\\t\\nMaking\\tPredictions\\nworker\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\nworker\\tservice\\n,\\t\\nThe\\tMaster\\tand\\tWorker\\tServices\\nworker_device\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nworkspace\\tdirectory\\n,\\t\\nGet\\tthe\\tData\\n-\\nDownload\\tthe\\tData\\nX\\nXavier\\tinitialization\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n-\\nXavier\\tand\\tHe\\tInitialization\\nY\\nYouTube\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\nZ\\nzero\\tpadding\\n,\\t\\nConvolutional\\tLayer\\n,\\t\\nTensorFlow\\tImplementation', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 701}), Document(page_content='About\\tthe\\tAuthor\\nAurélien\\tGéron\\n\\tis\\ta\\tMachine\\tLearning\\tconsultant.\\tA\\tformer\\tGoogler,\\the\\tled\\tthe\\tYouTube\\tvideo\\nclassification\\tteam\\tfrom\\t2013\\tto\\t2016.\\tHe\\twas\\talso\\ta\\tfounder\\tand\\tCTO\\tof\\tWifirst\\tfrom\\t2002\\tto\\t2012,\\ta\\nleading\\tWireless\\tISP\\tin\\tFrance;\\tand\\ta\\tfounder\\tand\\tCTO\\tof\\tPolyconseil\\tin\\t2001,\\tthe\\tfirm\\tthat\\tnow\\nmanages\\tthe\\telectric\\tcar\\tsharing\\tservice\\tAutolib’.\\nBefore\\tthis\\the\\tworked\\tas\\tan\\tengineer\\tin\\ta\\tvariety\\tof\\tdomains:\\tfinance\\t(JP\\tMorgan\\tand\\tSociété\\tGénérale),\\ndefense\\t(Canada’s\\tDOD),\\tand\\thealthcare\\t(blood\\ttransfusion).\\tHe\\tpublished\\ta\\tfew\\ttechnical\\tbooks\\t(on\\nC++,\\tWiFi,\\tand\\tinternet\\tarchitectures),\\tand\\twas\\ta\\tComputer\\tScience\\tlecturer\\tin\\ta\\tFrench\\tengineering\\nschool.\\nA\\tfew\\tfun\\tfacts:\\the\\ttaught\\this\\tthree\\tchildren\\tto\\tcount\\tin\\tbinary\\twith\\ttheir\\tfingers\\t(up\\tto\\t1023),\\the\\tstudied\\nmicrobiology\\tand\\tevolutionary\\tgenetics\\tbefore\\tgoing\\tinto\\tsoftware\\tengineering,\\tand\\this\\tparachute\\tdidn’t\\nopen\\ton\\tthe\\tsecond\\tjump.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 702}), Document(page_content='Colophon\\nThe\\tanimal\\ton\\tthe\\tcover\\tof\\t\\nHands-On\\tMachine\\tLearning\\twith\\tScikit-Learn\\tand\\tTensorFlow\\n\\tis\\tthe\\tfar\\neastern\\tfire\\tsalamander\\t(\\nSalamandra\\tinfraimmaculata\\n),\\tan\\tamphibian\\tfound\\tin\\tthe\\tMiddle\\tEast.\\tThey\\nhave\\tblack\\tskin\\tfeaturing\\tlarge\\tyellow\\tspots\\ton\\ttheir\\tback\\tand\\thead.\\tThese\\tspots\\tare\\ta\\twarning\\tcoloration\\nmeant\\tto\\tkeep\\tpredators\\tat\\tbay.\\tFull-grown\\tsalamanders\\tcan\\tbe\\tover\\ta\\tfoot\\tin\\tlength.\\nFar\\teastern\\tfire\\tsalamanders\\tlive\\tin\\tsubtropical\\tshrubland\\tand\\tforests\\tnear\\trivers\\tor\\tother\\tfreshwater\\nbodies.\\tThey\\tspend\\tmost\\tof\\ttheir\\tlife\\ton\\tland,\\tbut\\tlay\\ttheir\\teggs\\tin\\tthe\\twater.\\tThey\\tsubsist\\tmostly\\ton\\ta\\tdiet\\nof\\tinsects,\\tworms,\\tand\\tsmall\\tcrustaceans,\\tbut\\toccasionally\\teat\\tother\\tsalamanders.\\tMales\\tof\\tthe\\tspecies\\nhave\\tbeen\\tknown\\tto\\tlive\\tup\\tto\\t23\\tyears,\\twhile\\tfemales\\tcan\\tlive\\tup\\tto\\t21\\tyears.\\nAlthough\\tnot\\tyet\\tendangered,\\tthe\\tfar\\teastern\\tfire\\tsalamander\\tpopulation\\tis\\tin\\tdecline.\\tPrimary\\tthreats\\ninclude\\tdamming\\tof\\trivers\\t(which\\tdisrupts\\tthe\\tsalamander’s\\tbreeding)\\tand\\tpollution.\\tThey\\tare\\talso\\nthreatened\\tby\\tthe\\trecent\\tintroduction\\tof\\tpredatory\\tfish,\\tsuch\\tas\\tthe\\tmosquitofish.\\tThese\\tfish\\twere\\tintended\\nto\\tcontrol\\tthe\\tmosquito\\tpopulation,\\tbut\\tthey\\talso\\tfeed\\ton\\tyoung\\tsalamanders.\\nMany\\tof\\tthe\\tanimals\\ton\\tO’Reilly\\tcovers\\tare\\tendangered;\\tall\\tof\\tthem\\tare\\timportant\\tto\\tthe\\tworld.\\tTo\\tlearn\\nmore\\tabout\\thow\\tyou\\tcan\\thelp,\\tgo\\tto\\t\\nanimals.oreilly.com\\n.\\nThe\\tcover\\timage\\tis\\tfrom\\t\\nWood’s\\tIllustrated\\tNatural\\tHistory\\n.\\tThe\\tcover\\tfonts\\tare\\tURW\\tTypewriter\\tand\\nGuardian\\tSans.\\tThe\\ttext\\tfont\\tis\\tAdobe\\tMinion\\tPro;\\tthe\\theading\\tfont\\tis\\tAdobe\\tMyriad\\tCondensed;\\tand\\tthe\\ncode\\tfont\\tis\\tDalton\\tMaag’s\\tUbuntu\\tMono.', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 703}), Document(page_content='Preface\\nThe\\tMachine\\tLearning\\tTsunami\\nMachine\\tLearning\\tin\\tYour\\tProjects\\nObjective\\tand\\tApproach\\nPrerequisites\\nRoadmap\\nOther\\tResources\\nConventions\\tUsed\\tin\\tThis\\tBook\\nUsing\\tCode\\tExamples\\nO’Reilly\\tSafari\\nHow\\tto\\tContact\\tUs\\nAcknowledgments\\nI.\\tThe\\tFundamentals\\tof\\tMachine\\tLearning\\n1.\\tThe\\tMachine\\tLearning\\tLandscape\\nWhat\\tIs\\tMachine\\tLearning?\\nWhy\\tUse\\tMachine\\tLearning?\\nTypes\\tof\\tMachine\\tLearning\\tSystems\\nSupervised/Unsupervised\\tLearning\\nBatch\\tand\\tOnline\\tLearning\\nInstance-Based\\tVersus\\tModel-Based\\tLearning\\nMain\\tChallenges\\tof\\tMachine\\tLearning\\nInsufficient\\tQuantity\\tof\\tTraining\\tData\\nNonrepresentative\\tTraining\\tData\\nPoor-Quality\\tData\\nIrrelevant\\tFeatures\\nOverfitting\\tthe\\tTraining\\tData', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 704}), Document(page_content='Underfitting\\tthe\\tTraining\\tData\\nStepping\\tBack\\nTesting\\tand\\tValidating\\nExercises\\n2.\\tEnd-to-End\\tMachine\\tLearning\\tProject\\nWorking\\twith\\tReal\\tData\\nLook\\tat\\tthe\\tBig\\tPicture\\nFrame\\tthe\\tProblem\\nSelect\\ta\\tPerformance\\tMeasure\\nCheck\\tthe\\tAssumptions\\nGet\\tthe\\tData\\nCreate\\tthe\\tWorkspace\\nDownload\\tthe\\tData\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\nCreate\\ta\\tTest\\tSet\\nDiscover\\tand\\tVisualize\\tthe\\tData\\tto\\tGain\\tInsights\\nVisualizing\\tGeographical\\tData\\nLooking\\tfor\\tCorrelations\\nExperimenting\\twith\\tAttribute\\tCombinations\\nPrepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\tAlgorithms\\nData\\tCleaning\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nCustom\\tTransformers\\nFeature\\tScaling\\nTransformation\\tPipelines\\nSelect\\tand\\tTrain\\ta\\tModel\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 705}), Document(page_content='Better\\tEvaluation\\tUsing\\tCross-Validation\\nFine-Tune\\tYour\\tModel\\nGrid\\tSearch\\nRandomized\\tSearch\\nEnsemble\\tMethods\\nAnalyze\\tthe\\tBest\\tModels\\tand\\tTheir\\tErrors\\nEvaluate\\tYour\\tSystem\\ton\\tthe\\tTest\\tSet\\nLaunch,\\tMonitor,\\tand\\tMaintain\\tYour\\tSystem\\nTry\\tIt\\tOut!\\nExercises\\n3.\\tClassification\\nMNIST\\nTraining\\ta\\tBinary\\tClassifier\\nPerformance\\tMeasures\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nConfusion\\tMatrix\\nPrecision\\tand\\tRecall\\nPrecision/Recall\\tTradeoff\\nThe\\tROC\\tCurve\\nMulticlass\\tClassification\\nError\\tAnalysis\\nMultilabel\\tClassification\\nMultioutput\\tClassification\\nExercises\\n4.\\tTraining\\tModels\\nLinear\\tRegression\\nThe\\tNormal\\tEquation', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 706}), Document(page_content='Computational\\tComplexity\\nGradient\\tDescent\\nBatch\\tGradient\\tDescent\\nStochastic\\tGradient\\tDescent\\nMini-batch\\tGradient\\tDescent\\nPolynomial\\tRegression\\nLearning\\tCurves\\nRegularized\\tLinear\\tModels\\nRidge\\tRegression\\nLasso\\tRegression\\nElastic\\tNet\\nEarly\\tStopping\\nLogistic\\tRegression\\nEstimating\\tProbabilities\\nTraining\\tand\\tCost\\tFunction\\nDecision\\tBoundaries\\nSoftmax\\tRegression\\nExercises\\n5.\\tSupport\\tVector\\tMachines\\nLinear\\tSVM\\tClassification\\nSoft\\tMargin\\tClassification\\nNonlinear\\tSVM\\tClassification\\nPolynomial\\tKernel\\nAdding\\tSimilarity\\tFeatures\\nGaussian\\tRBF\\tKernel\\nComputational\\tComplexity\\nSVM\\tRegression', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 707}), Document(page_content='Under\\tthe\\tHood\\nDecision\\tFunction\\tand\\tPredictions\\nTraining\\tObjective\\nQuadratic\\tProgramming\\nThe\\tDual\\tProblem\\nKernelized\\tSVM\\nOnline\\tSVMs\\nExercises\\n6.\\tDecision\\tTrees\\nTraining\\tand\\tVisualizing\\ta\\tDecision\\tTree\\nMaking\\tPredictions\\nEstimating\\tClass\\tProbabilities\\nThe\\tCART\\tTraining\\tAlgorithm\\nComputational\\tComplexity\\nGini\\tImpurity\\tor\\tEntropy?\\nRegularization\\tHyperparameters\\nRegression\\nInstability\\nExercises\\n7.\\tEnsemble\\tLearning\\tand\\tRandom\\tForests\\nVoting\\tClassifiers\\nBagging\\tand\\tPasting\\nBagging\\tand\\tPasting\\tin\\tScikit-Learn\\nOut-of-Bag\\tEvaluation\\nRandom\\tPatches\\tand\\tRandom\\tSubspaces\\nRandom\\tForests\\nExtra-Trees', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 708}), Document(page_content='Feature\\tImportance\\nBoosting\\nAdaBoost\\nGradient\\tBoosting\\nStacking\\nExercises\\n8.\\tDimensionality\\tReduction\\nThe\\tCurse\\tof\\tDimensionality\\nMain\\tApproaches\\tfor\\tDimensionality\\tReduction\\nProjection\\nManifold\\tLearning\\nPCA\\nPreserving\\tthe\\tVariance\\nPrincipal\\tComponents\\nProjecting\\tDown\\tto\\td\\tDimensions\\nUsing\\tScikit-Learn\\nExplained\\tVariance\\tRatio\\nChoosing\\tthe\\tRight\\tNumber\\tof\\tDimensions\\nPCA\\tfor\\tCompression\\nIncremental\\tPCA\\nRandomized\\tPCA\\nKernel\\tPCA\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nLLE\\nOther\\tDimensionality\\tReduction\\tTechniques\\nExercises\\nII.\\tNeural\\tNetworks\\tand\\tDeep\\tLearning', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 709}), Document(page_content='9.\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\nInstallation\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\nManaging\\tGraphs\\nLifecycle\\tof\\ta\\tNode\\tValue\\nLinear\\tRegression\\twith\\tTensorFlow\\nImplementing\\tGradient\\tDescent\\nManually\\tComputing\\tthe\\tGradients\\nUsing\\tautodiff\\nUsing\\tan\\tOptimizer\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\nSaving\\tand\\tRestoring\\tModels\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\nName\\tScopes\\nModularity\\nSharing\\tVariables\\nExercises\\n10.\\tIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\nFrom\\tBiological\\tto\\tArtificial\\tNeurons\\nBiological\\tNeurons\\nLogical\\tComputations\\twith\\tNeurons\\nThe\\tPerceptron\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nTraining\\ta\\tDNN\\tUsing\\tPlain\\tTensorFlow\\nConstruction\\tPhase\\nExecution\\tPhase', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 710}), Document(page_content='Using\\tthe\\tNeural\\tNetwork\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\nNumber\\tof\\tHidden\\tLayers\\nNumber\\tof\\tNeurons\\tper\\tHidden\\tLayer\\nActivation\\tFunctions\\nExercises\\n11.\\tTraining\\tDeep\\tNeural\\tNets\\nVanishing/Exploding\\tGradients\\tProblems\\nXavier\\tand\\tHe\\tInitialization\\nNonsaturating\\tActivation\\tFunctions\\nBatch\\tNormalization\\nGradient\\tClipping\\nReusing\\tPretrained\\tLayers\\nReusing\\ta\\tTensorFlow\\tModel\\nReusing\\tModels\\tfrom\\tOther\\tFrameworks\\nFreezing\\tthe\\tLower\\tLayers\\nCaching\\tthe\\tFrozen\\tLayers\\nTweaking,\\tDropping,\\tor\\tReplacing\\tthe\\tUpper\\tLayers\\nModel\\tZoos\\nUnsupervised\\tPretraining\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\nFaster\\tOptimizers\\nMomentum\\tOptimization\\nNesterov\\tAccelerated\\tGradient\\nAdaGrad\\nRMSProp\\nAdam\\tOptimization', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 711}), Document(page_content='Learning\\tRate\\tScheduling\\nAvoiding\\tOverfitting\\tThrough\\tRegularization\\nEarly\\tStopping\\nℓ1\\tand\\tℓ2\\tRegularization\\nDropout\\nMax-Norm\\tRegularization\\nData\\tAugmentation\\nPractical\\tGuidelines\\nExercises\\n12.\\tDistributing\\tTensorFlow\\tAcross\\tDevices\\tand\\tServers\\nMultiple\\tDevices\\ton\\ta\\tSingle\\tMachine\\nInstallation\\nManaging\\tthe\\tGPU\\tRAM\\nPlacing\\tOperations\\ton\\tDevices\\nParallel\\tExecution\\nControl\\tDependencies\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\nOpening\\ta\\tSession\\nThe\\tMaster\\tand\\tWorker\\tServices\\nPinning\\tOperations\\tAcross\\tTasks\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\nLoading\\tData\\tDirectly\\tfrom\\tthe\\tGraph\\nParallelizing\\tNeural\\tNetworks\\ton\\ta\\tTensorFlow\\tCluster\\nOne\\tNeural\\tNetwork\\tper\\tDevice\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 712}), Document(page_content='Model\\tParallelism\\nData\\tParallelism\\nExercises\\n13.\\tConvolutional\\tNeural\\tNetworks\\nThe\\tArchitecture\\tof\\tthe\\tVisual\\tCortex\\nConvolutional\\tLayer\\nFilters\\nStacking\\tMultiple\\tFeature\\tMaps\\nTensorFlow\\tImplementation\\nMemory\\tRequirements\\nPooling\\tLayer\\nCNN\\tArchitectures\\nLeNet-5\\nAlexNet\\nGoogLeNet\\nResNet\\nExercises\\n14.\\tRecurrent\\tNeural\\tNetworks\\nRecurrent\\tNeurons\\nMemory\\tCells\\nInput\\tand\\tOutput\\tSequences\\nBasic\\tRNNs\\tin\\tTensorFlow\\nStatic\\tUnrolling\\tThrough\\tTime\\nDynamic\\tUnrolling\\tThrough\\tTime\\nHandling\\tVariable\\tLength\\tInput\\tSequences\\nHandling\\tVariable-Length\\tOutput\\tSequences\\nTraining\\tRNNs', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 713}), Document(page_content='Training\\ta\\tSequence\\tClassifier\\nTraining\\tto\\tPredict\\tTime\\tSeries\\nCreative\\tRNN\\nDeep\\tRNNs\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\nApplying\\tDropout\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps\\nLSTM\\tCell\\nPeephole\\tConnections\\nGRU\\tCell\\nNatural\\tLanguage\\tProcessing\\nWord\\tEmbeddings\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nExercises\\n15.\\tAutoencoders\\nEfficient\\tData\\tRepresentations\\nPerforming\\tPCA\\twith\\tan\\tUndercomplete\\tLinear\\tAutoencoder\\nStacked\\tAutoencoders\\nTensorFlow\\tImplementation\\nTying\\tWeights\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\nVisualizing\\tthe\\tReconstructions\\nVisualizing\\tFeatures\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\nDenoising\\tAutoencoders\\nTensorFlow\\tImplementation\\nSparse\\tAutoencoders', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 714}), Document(page_content='TensorFlow\\tImplementation\\nVariational\\tAutoencoders\\nGenerating\\tDigits\\nOther\\tAutoencoders\\nExercises\\n16.\\tReinforcement\\tLearning\\nLearning\\tto\\tOptimize\\tRewards\\nPolicy\\tSearch\\nIntroduction\\tto\\tOpenAI\\tGym\\nNeural\\tNetwork\\tPolicies\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\nPolicy\\tGradients\\nMarkov\\tDecision\\tProcesses\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\nExploration\\tPolicies\\nApproximate\\tQ-Learning\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nExercises\\nThank\\tYou!\\nA.\\tExercise\\tSolutions\\nChapter\\t1:\\tThe\\tMachine\\tLearning\\tLandscape\\nChapter\\t2:\\tEnd-to-End\\tMachine\\tLearning\\tProject\\nChapter\\t3:\\tClassification\\nChapter\\t4:\\tTraining\\tModels\\nChapter\\t5:\\tSupport\\tVector\\tMachines\\nChapter\\t6:\\tDecision\\tTrees', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 715}), Document(page_content='Chapter\\t7:\\tEnsemble\\tLearning\\tand\\tRandom\\tForests\\nChapter\\t8:\\tDimensionality\\tReduction\\nChapter\\t9:\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\nChapter\\t10:\\tIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\nChapter\\t11:\\tTraining\\tDeep\\tNeural\\tNets\\nChapter\\t12:\\tDistributing\\tTensorFlow\\tAcross\\tDevices\\tand\\tServers\\nChapter\\t13:\\tConvolutional\\tNeural\\tNetworks\\nChapter\\t14:\\tRecurrent\\tNeural\\tNetworks\\nChapter\\t15:\\tAutoencoders\\nChapter\\t16:\\tReinforcement\\tLearning\\nB.\\tMachine\\tLearning\\tProject\\tChecklist\\nFrame\\tthe\\tProblem\\tand\\tLook\\tat\\tthe\\tBig\\tPicture\\nGet\\tthe\\tData\\nExplore\\tthe\\tData\\nPrepare\\tthe\\tData\\nShort-List\\tPromising\\tModels\\nFine-Tune\\tthe\\tSystem\\nPresent\\tYour\\tSolution\\nLaunch!\\nC.\\tSVM\\tDual\\tProblem\\nD.\\tAutodiff\\nManual\\tDifferentiation\\nSymbolic\\tDifferentiation\\nNumerical\\tDifferentiation\\nForward-Mode\\tAutodiff\\nReverse-Mode\\tAutodiff', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 716}), Document(page_content='E.\\tOther\\tPopular\\tANN\\tArchitectures\\nHopfield\\tNetworks\\nBoltzmann\\tMachines\\nRestricted\\tBoltzmann\\tMachines\\nDeep\\tBelief\\tNets\\nSelf-Organizing\\tMaps\\nIndex', metadata={'source': 'Hands on Machine Learning with Scikit Learn and Tensorflow.pdf', 'page': 717})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 10\n",
        ")\n",
        "text_chunks = splitter.split_documents(pdf_content)\n",
        "# print(text_chunks)\n",
        "text_data = [data.page_content for data in text_chunks]\n",
        "print(text_data)"
      ],
      "metadata": {
        "id": "fV0VpFdAGssY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe4e642-4552-4750-ce66-47d4d8a2c73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hands-On\\tMachine\\tLearning\\twith\\tScikit-Learn\\nand\\tTensorFlow\\nConcepts,\\tTools,\\tand\\tTechniques\\tto\\t\\nBuild\\tIntelligent\\tSystems\\nAurélien\\tGéron', 'Hands-On\\tMachine\\tLearning\\twith\\tScikit-Learn\\tand\\tTensorFlow\\nby\\t\\nAurélien\\t\\nGéron\\nCopyright\\t©\\t2017\\tAurélien\\tGéron.\\tAll\\trights\\treserved.\\nPrinted\\tin\\tthe\\tUnited\\tStates\\tof\\tAmerica.\\nPublished\\tby\\t\\nO’Reilly\\tMedia,\\tInc.\\n,\\t1005\\tGravenstein\\tHighway\\tNorth,\\tSebastopol,\\tCA\\t95472.\\nO’Reilly\\tbooks\\tmay\\tbe\\tpurchased\\tfor\\teducational,\\tbusiness,\\tor\\tsales\\tpromotional\\tuse.\\tOnline\\teditions\\tare\\nalso\\tavailable\\tfor\\tmost\\ttitles\\t(\\nhttp://oreilly.com/safari\\n).\\tFor\\tmore\\tinformation,\\tcontact\\tour\\ncorporate/institutional\\tsales\\tdepartment:\\t800-998-9938\\tor\\t\\ncorporate@oreilly.com\\n.\\nEditor:\\n\\tNicole\\tTache\\nProduction\\tEditor:\\n\\tNicholas\\tAdams\\nCopyeditor:\\n\\tRachel\\tMonaghan\\nProofreader:\\n\\tCharles\\tRoumeliotis\\nIndexer:\\n\\tWendy\\tCatalano\\nInterior\\tDesigner:\\n\\tDavid\\tFutato\\nCover\\tDesigner:\\n\\tRandy\\tComer\\nIllustrator:\\n\\tRebecca\\tDemarest\\nMarch\\t2017:\\n\\tFirst\\tEdition', 'Revision\\tHistory\\tfor\\tthe\\tFirst\\tEdition\\n2017-03-10:\\n\\tFirst\\tRelease\\n2017-06-09:\\n\\tSecond\\tRelease\\nSee\\t\\nhttp://oreilly.com/catalog/errata.csp?isbn=9781491962299\\n\\tfor\\trelease\\tdetails.\\nThe\\tO’Reilly\\tlogo\\tis\\ta\\tregistered\\ttrademark\\tof\\tO’Reilly\\tMedia,\\tInc.\\t\\nHands-On\\tMachine\\tLearning\\twith\\nScikit-Learn\\tand\\tTensorFlow\\n,\\tthe\\tcover\\timage,\\tand\\trelated\\ttrade\\tdress\\tare\\ttrademarks\\tof\\tO’Reilly\\nMedia,\\tInc.\\nWhile\\tthe\\tpublisher\\tand\\tthe\\tauthor\\thave\\tused\\tgood\\tfaith\\tefforts\\tto\\tensure\\tthat\\tthe\\tinformation\\tand\\ninstructions\\tcontained\\tin\\tthis\\twork\\tare\\taccurate,\\tthe\\tpublisher\\tand\\tthe\\tauthor\\tdisclaim\\tall\\tresponsibility\\nfor\\terrors\\tor\\tomissions,\\tincluding\\twithout\\tlimitation\\tresponsibility\\tfor\\tdamages\\tresulting\\tfrom\\tthe\\tuse\\tof\\nor\\treliance\\ton\\tthis\\twork.\\tUse\\tof\\tthe\\tinformation\\tand\\tinstructions\\tcontained\\tin\\tthis\\twork\\tis\\tat\\tyour\\town\\nrisk.\\tIf\\tany\\tcode\\tsamples\\tor\\tother\\ttechnology\\tthis\\twork\\tcontains\\tor\\tdescribes\\tis\\tsubject\\tto\\topen\\tsource', 'licenses\\tor\\tthe\\tintellectual\\tproperty\\trights\\tof\\tothers,\\tit\\tis\\tyour\\tresponsibility\\tto\\tensure\\tthat\\tyour\\tuse\\tthereof\\ncomplies\\twith\\tsuch\\tlicenses\\tand/or\\trights.\\n978-1-491-96229-9\\n[LSI]', 'Preface', 'The\\tMachine\\tLearning\\tTsunami\\nIn\\t2006,\\tGeoffrey\\tHinton\\tet\\tal.\\tpublished\\ta\\tpaper\\n1\\n\\tshowing\\thow\\tto\\ttrain\\ta\\tdeep\\tneural\\tnetwork\\tcapable\\tof\\nrecognizing\\thandwritten\\tdigits\\twith\\tstate-of-the-art\\tprecision\\t(>98%).\\tThey\\tbranded\\tthis\\ttechnique\\t“Deep\\nLearning.”\\t\\nTraining\\ta\\tdeep\\tneural\\tnet\\twas\\twidely\\tconsidered\\timpossible\\tat\\tthe\\ttime,\\n2\\n\\tand\\tmost\\nresearchers\\thad\\tabandoned\\tthe\\tidea\\tsince\\tthe\\t1990s.\\tThis\\tpaper\\trevived\\tthe\\tinterest\\tof\\tthe\\tscientific\\ncommunity\\tand\\tbefore\\tlong\\tmany\\tnew\\tpapers\\tdemonstrated\\tthat\\tDeep\\tLearning\\twas\\tnot\\tonly\\tpossible,\\tbut\\ncapable\\tof\\tmind-blowing\\tachievements\\tthat\\tno\\tother\\tMachine\\tLearning\\t(ML)\\ttechnique\\tcould\\thope\\tto\\nmatch\\t(with\\tthe\\thelp\\tof\\ttremendous\\tcomputing\\tpower\\tand\\tgreat\\tamounts\\tof\\tdata).\\tThis\\tenthusiasm\\tsoon\\nextended\\tto\\tmany\\tother\\tareas\\tof\\tMachine\\tLearning.\\nFast-forward\\t10\\tyears\\tand\\tMachine\\tLearning\\thas\\tconquered\\tthe\\tindustry:\\tit\\tis\\tnow\\tat\\tthe\\theart\\tof\\tmuch\\tof\\nthe\\tmagic\\tin\\ttoday’s\\thigh-tech\\tproducts,\\tranking\\tyour\\tweb\\tsearch\\tresults,\\tpowering\\tyour\\tsmartphone’s', 'speech\\trecognition,\\tand\\trecommending\\tvideos,\\tbeating\\tthe\\tworld\\tchampion\\tat\\tthe\\tgame\\tof\\tGo.\\tBefore\\tyou\\nknow\\tit,\\tit\\twill\\tbe\\tdriving\\tyour\\tcar.', 'Machine\\tLearning\\tin\\tYour\\tProjects\\nSo\\t\\nnaturally\\tyou\\tare\\texcited\\tabout\\tMachine\\tLearning\\tand\\tyou\\twould\\tlove\\tto\\tjoin\\tthe\\tparty!\\nPerhaps\\tyou\\twould\\tlike\\tto\\tgive\\tyour\\thomemade\\trobot\\ta\\tbrain\\tof\\tits\\town?\\tMake\\tit\\trecognize\\tfaces?\\tOr\\nlearn\\tto\\twalk\\taround?\\nOr\\tmaybe\\tyour\\tcompany\\thas\\ttons\\tof\\tdata\\t(user\\tlogs,\\tfinancial\\tdata,\\tproduction\\tdata,\\tmachine\\tsensor\\tdata,\\nhotline\\tstats,\\tHR\\treports,\\tetc.),\\tand\\tmore\\tthan\\tlikely\\tyou\\tcould\\tunearth\\tsome\\thidden\\tgems\\tif\\tyou\\tjust\\tknew\\nwhere\\tto\\tlook;\\tfor\\texample:\\nSegment\\tcustomers\\tand\\tfind\\tthe\\tbest\\tmarketing\\tstrategy\\tfor\\teach\\tgroup\\nRecommend\\tproducts\\tfor\\teach\\tclient\\tbased\\ton\\twhat\\tsimilar\\tclients\\tbought\\nDetect\\twhich\\ttransactions\\tare\\tlikely\\tto\\tbe\\tfraudulent\\nPredict\\tnext\\tyear’s\\trevenue\\nAnd\\tmore\\nWhatever\\tthe\\treason,\\tyou\\thave\\tdecided\\tto\\tlearn\\tMachine\\tLearning\\tand\\timplement\\tit\\tin\\tyour\\tprojects.\\nGreat\\tidea!', 'Objective\\tand\\tApproach\\nThis\\tbook\\tassumes\\tthat\\tyou\\tknow\\tclose\\tto\\tnothing\\tabout\\tMachine\\tLearning.\\tIts\\tgoal\\tis\\tto\\tgive\\tyou\\tthe\\nconcepts,\\tthe\\tintuitions,\\tand\\tthe\\ttools\\tyou\\tneed\\tto\\tactually\\timplement\\tprograms\\tcapable\\tof\\t\\nlearning\\tfrom\\ndata\\n.\\nWe\\twill\\tcover\\ta\\tlarge\\tnumber\\tof\\ttechniques,\\tfrom\\tthe\\tsimplest\\tand\\tmost\\tcommonly\\tused\\t(such\\tas\\tlinear\\nregression)\\tto\\tsome\\tof\\tthe\\tDeep\\tLearning\\ttechniques\\tthat\\tregularly\\twin\\tcompetitions.\\nRather\\tthan\\timplementing\\tour\\town\\ttoy\\tversions\\tof\\teach\\talgorithm,\\twe\\twill\\tbe\\tusing\\tactual\\tproduction-\\nready\\tPython\\tframeworks:\\nScikit-Learn\\n\\tis\\t\\nvery\\teasy\\tto\\tuse,\\tyet\\tit\\timplements\\tmany\\tMachine\\tLearning\\talgorithms\\tefficiently,\\tso\\nit\\tmakes\\tfor\\ta\\tgreat\\tentry\\tpoint\\tto\\tlearn\\tMachine\\tLearning.\\nTensorFlow\\n\\tis\\t\\na\\tmore\\tcomplex\\tlibrary\\tfor\\tdistributed\\tnumerical\\tcomputation\\tusing\\tdata\\tflow\\tgraphs.\\nIt\\tmakes\\tit\\tpossible\\tto\\ttrain\\tand\\trun\\tvery\\tlarge\\tneural\\tnetworks\\tefficiently\\tby\\tdistributing\\tthe', 'computations\\tacross\\tpotentially\\tthousands\\tof\\tmulti-GPU\\tservers.\\tTensorFlow\\twas\\tcreated\\tat\\tGoogle\\nand\\tsupports\\tmany\\tof\\ttheir\\tlarge-scale\\tMachine\\tLearning\\tapplications.\\tIt\\twas\\topen-sourced\\tin\\nNovember\\t2015.\\nThe\\tbook\\tfavors\\ta\\thands-on\\tapproach,\\tgrowing\\tan\\tintuitive\\tunderstanding\\tof\\tMachine\\tLearning\\tthrough\\nconcrete\\tworking\\texamples\\tand\\tjust\\ta\\tlittle\\tbit\\tof\\ttheory.\\tWhile\\tyou\\tcan\\tread\\tthis\\tbook\\twithout\\tpicking\\tup\\nyour\\tlaptop,\\twe\\thighly\\trecommend\\tyou\\texperiment\\twith\\tthe\\tcode\\texamples\\tavailable\\tonline\\tas\\tJupyter\\nnotebooks\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', 'Prerequisites\\nThis\\tbook\\tassumes\\tthat\\tyou\\thave\\tsome\\tPython\\tprogramming\\texperience\\tand\\tthat\\tyou\\tare\\tfamiliar\\twith\\nPython’s\\tmain\\tscientific\\tlibraries,\\tin\\tparticular\\t\\nNumPy\\n,\\t\\nPandas\\n,\\tand\\t\\nMatplotlib\\n.\\nAlso,\\tif\\tyou\\tcare\\tabout\\twhat’s\\tunder\\tthe\\thood\\tyou\\tshould\\thave\\ta\\treasonable\\tunderstanding\\tof\\tcollege-\\nlevel\\tmath\\tas\\twell\\t(calculus,\\tlinear\\talgebra,\\tprobabilities,\\tand\\tstatistics).\\nIf\\tyou\\tdon’t\\tknow\\tPython\\tyet,\\t\\nhttp://learnpython.org/\\n\\tis\\ta\\tgreat\\tplace\\tto\\tstart.\\tThe\\tofficial\\ttutorial\\ton\\npython.org\\n\\tis\\talso\\tquite\\tgood.\\nIf\\tyou\\thave\\tnever\\tused\\tJupyter,\\t\\nChapter\\t2\\n\\twill\\tguide\\tyou\\tthrough\\tinstallation\\tand\\tthe\\tbasics:\\tit\\tis\\ta\\tgreat\\ntool\\tto\\thave\\tin\\tyour\\ttoolbox.\\nIf\\tyou\\tare\\tnot\\tfamiliar\\twith\\tPython’s\\tscientific\\tlibraries,\\tthe\\tprovided\\tJupyter\\tnotebooks\\tinclude\\ta\\tfew\\ntutorials.\\tThere\\tis\\talso\\ta\\tquick\\tmath\\ttutorial\\tfor\\tlinear\\talgebra.', 'Roadmap\\nThis\\tbook\\tis\\torganized\\tin\\ttwo\\tparts.\\t\\nPart\\tI,\\t\\nThe\\tFundamentals\\tof\\t\\nMachine\\tLearning\\n,\\tcovers\\tthe\\nfollowing\\ttopics:\\nWhat\\tis\\tMachine\\tLearning?\\tWhat\\tproblems\\tdoes\\tit\\ttry\\tto\\tsolve?\\tWhat\\tare\\tthe\\tmain\\tcategories\\tand\\nfundamental\\tconcepts\\tof\\tMachine\\tLearning\\tsystems?\\nThe\\tmain\\tsteps\\tin\\ta\\ttypical\\tMachine\\tLearning\\tproject.\\nLearning\\tby\\tfitting\\ta\\tmodel\\tto\\tdata.\\nOptimizing\\ta\\tcost\\tfunction.\\nHandling,\\tcleaning,\\tand\\tpreparing\\tdata.\\nSelecting\\tand\\tengineering\\tfeatures.\\nSelecting\\ta\\tmodel\\tand\\ttuning\\thyperparameters\\tusing\\tcross-validation.\\nThe\\tmain\\tchallenges\\tof\\tMachine\\tLearning,\\tin\\tparticular\\tunderfitting\\tand\\toverfitting\\t(the\\nbias/variance\\ttradeoff).\\nReducing\\tthe\\tdimensionality\\tof\\tthe\\ttraining\\tdata\\tto\\tfight\\tthe\\tcurse\\tof\\tdimensionality.\\nThe\\tmost\\tcommon\\tlearning\\talgorithms:\\tLinear\\tand\\tPolynomial\\tRegression,\\tLogistic\\tRegression,\\tk-\\nNearest\\tNeighbors,\\tSupport\\tVector\\tMachines,\\tDecision\\tTrees,\\tRandom\\tForests,\\tand\\tEnsemble\\nmethods.\\nPart\\tII,\\t\\nNeural\\tNetworks\\tand\\tDeep\\tLearning', ',\\tcovers\\tthe\\tfollowing\\ttopics:\\nWhat\\tare\\tneural\\tnets?\\tWhat\\tare\\tthey\\tgood\\tfor?\\nBuilding\\tand\\ttraining\\tneural\\tnets\\tusing\\tTensorFlow.\\nThe\\tmost\\timportant\\tneural\\tnet\\tarchitectures:\\tfeedforward\\tneural\\tnets,\\tconvolutional\\tnets,\\trecurrent\\nnets,\\tlong\\tshort-term\\tmemory\\t(LSTM)\\tnets,\\tand\\tautoencoders.\\nTechniques\\tfor\\ttraining\\tdeep\\tneural\\tnets.\\nScaling\\tneural\\tnetworks\\tfor\\thuge\\tdatasets.\\nReinforcement\\tlearning.\\nThe\\tfirst\\tpart\\tis\\tbased\\tmostly\\ton\\tScikit-Learn\\twhile\\tthe\\tsecond\\tpart\\tuses\\tTensorFlow.', 'CAUTION\\nDon’t\\t\\njump\\tinto\\tdeep\\twaters\\ttoo\\thastily:\\twhile\\tDeep\\tLearning\\tis\\tno\\tdoubt\\tone\\tof\\tthe\\tmost\\texciting\\tareas\\tin\\tMachine\\tLearning,\\nyou\\tshould\\tmaster\\tthe\\tfundamentals\\tfirst.\\tMoreover,\\tmost\\tproblems\\tcan\\tbe\\tsolved\\tquite\\twell\\tusing\\tsimpler\\ttechniques\\tsuch\\tas\\nRandom\\tForests\\tand\\tEnsemble\\tmethods\\t(discussed\\tin\\t\\nPart\\tI\\n).\\tDeep\\tLearning\\tis\\tbest\\tsuited\\tfor\\tcomplex\\tproblems\\tsuch\\tas\\timage\\nrecognition,\\tspeech\\trecognition,\\tor\\tnatural\\tlanguage\\tprocessing,\\tprovided\\tyou\\thave\\tenough\\tdata,\\tcomputing\\tpower,\\tand\\tpatience.', 'Other\\tResources\\nMany\\t\\nresources\\tare\\tavailable\\tto\\tlearn\\tabout\\tMachine\\tLearning.\\tAndrew\\tNg’s\\t\\nML\\tcourse\\ton\\tCoursera\\n\\tand\\nGeoffrey\\tHinton’s\\t\\ncourse\\ton\\tneural\\tnetworks\\tand\\tDeep\\tLearning\\n\\tare\\tamazing,\\talthough\\tthey\\tboth\\trequire\\ta\\nsignificant\\ttime\\tinvestment\\t(think\\tmonths).\\nThere\\tare\\talso\\tmany\\tinteresting\\twebsites\\tabout\\tMachine\\tLearning,\\tincluding\\tof\\tcourse\\tScikit-Learn’s\\nexceptional\\t\\nUser\\tGuide\\n.\\tYou\\tmay\\talso\\tenjoy\\t\\nDataquest\\n,\\t\\nwhich\\tprovides\\tvery\\tnice\\tinteractive\\ttutorials,\\nand\\tML\\tblogs\\tsuch\\tas\\tthose\\tlisted\\ton\\t\\nQuora\\n.\\tFinally,\\tthe\\t\\nDeep\\tLearning\\twebsite\\n\\thas\\ta\\tgood\\tlist\\tof\\nresources\\tto\\tlearn\\tmore.\\nOf\\tcourse\\tthere\\tare\\talso\\tmany\\tother\\tintroductory\\tbooks\\tabout\\tMachine\\tLearning,\\tin\\tparticular:\\nJoel\\tGrus,\\t\\nData\\tScience\\tfrom\\tScratch\\n\\t(O’Reilly).\\tThis\\tbook\\tpresents\\tthe\\tfundamentals\\tof\\tMachine\\nLearning,\\tand\\timplements\\tsome\\tof\\tthe\\tmain\\talgorithms\\tin\\tpure\\tPython\\t(from\\tscratch,\\tas\\tthe\\tname\\nsuggests).\\nStephen\\tMarsland,\\t\\nMachine\\tLearning:\\tAn\\tAlgorithmic\\tPerspective\\n\\t(Chapman\\tand\\tHall).\\tThis\\tbook', 'is\\ta\\tgreat\\tintroduction\\tto\\tMachine\\tLearning,\\tcovering\\ta\\twide\\trange\\tof\\ttopics\\tin\\tdepth,\\twith\\tcode\\nexamples\\tin\\tPython\\t(also\\tfrom\\tscratch,\\tbut\\tusing\\tNumPy).\\nSebastian\\tRaschka,\\t\\nPython\\tMachine\\tLearning\\n\\t(Packt\\tPublishing).\\tAlso\\ta\\tgreat\\tintroduction\\tto\\nMachine\\tLearning,\\tthis\\tbook\\tleverages\\tPython\\topen\\tsource\\tlibraries\\t(Pylearn\\t2\\tand\\tTheano).\\nYaser\\tS.\\tAbu-Mostafa,\\tMalik\\tMagdon-Ismail,\\tand\\tHsuan-Tien\\tLin,\\t\\nLearning\\tfrom\\tData\\n(AMLBook).\\tA\\trather\\ttheoretical\\tapproach\\tto\\tML,\\tthis\\tbook\\tprovides\\tdeep\\tinsights,\\tin\\tparticular\\ton\\nthe\\tbias/variance\\ttradeoff\\t(see\\t\\nChapter\\t4\\n).\\nStuart\\tRussell\\tand\\tPeter\\tNorvig,\\t\\nArtificial\\tIntelligence:\\tA\\tModern\\tApproach,\\t3rd\\tEdition\\n(Pearson).\\tThis\\tis\\ta\\tgreat\\t(and\\thuge)\\tbook\\tcovering\\tan\\tincredible\\tamount\\tof\\ttopics,\\tincluding\\nMachine\\tLearning.\\tIt\\thelps\\tput\\tML\\tinto\\tperspective.\\nFinally,\\ta\\tgreat\\tway\\tto\\tlearn\\tis\\tto\\tjoin\\tML\\tcompetition\\twebsites\\tsuch\\tas\\t\\nKaggle.com\\n\\tthis\\twill\\tallow\\tyou', 'to\\tpractice\\tyour\\tskills\\ton\\treal-world\\tproblems,\\twith\\thelp\\tand\\tinsights\\tfrom\\tsome\\tof\\tthe\\tbest\\tML\\nprofessionals\\tout\\t\\nthere.', 'Conventions\\tUsed\\tin\\tThis\\tBook\\nThe\\tfollowing\\ttypographical\\tconventions\\tare\\tused\\tin\\tthis\\tbook:\\nItalic\\nIndicates\\tnew\\tterms,\\tURLs,\\temail\\taddresses,\\tfilenames,\\tand\\tfile\\textensions.\\nConstant\\twidth\\nUsed\\tfor\\tprogram\\tlistings,\\tas\\twell\\tas\\twithin\\tparagraphs\\tto\\trefer\\tto\\tprogram\\telements\\tsuch\\tas\\nvariable\\tor\\tfunction\\tnames,\\tdatabases,\\tdata\\ttypes,\\tenvironment\\tvariables,\\tstatements\\tand\\tkeywords.\\nConstant\\twidth\\tbold\\nShows\\tcommands\\tor\\tother\\ttext\\tthat\\tshould\\tbe\\ttyped\\tliterally\\tby\\tthe\\tuser.\\nConstant\\twidth\\titalic\\nShows\\ttext\\tthat\\tshould\\tbe\\treplaced\\twith\\tuser-supplied\\tvalues\\tor\\tby\\tvalues\\tdetermined\\tby\\tcontext.\\nTIP\\nThis\\telement\\tsignifies\\ta\\ttip\\tor\\tsuggestion.\\nNOTE\\nThis\\telement\\tsignifies\\ta\\tgeneral\\tnote.\\nWARNING\\nThis\\telement\\tindicates\\ta\\twarning\\tor\\tcaution.', 'Using\\tCode\\tExamples\\nSupplemental\\tmaterial\\t(code\\texamples,\\texercises,\\tetc.)\\tis\\tavailable\\tfor\\tdownload\\tat\\nhttps://github.com/ageron/handson-ml\\n.\\nThis\\tbook\\tis\\there\\tto\\thelp\\tyou\\tget\\tyour\\tjob\\tdone.\\tIn\\tgeneral,\\tif\\texample\\tcode\\tis\\toffered\\twith\\tthis\\tbook,\\tyou\\nmay\\tuse\\tit\\tin\\tyour\\tprograms\\tand\\tdocumentation.\\tYou\\tdo\\tnot\\tneed\\tto\\tcontact\\tus\\tfor\\tpermission\\tunless\\nyou’re\\treproducing\\ta\\tsignificant\\tportion\\tof\\tthe\\tcode.\\tFor\\texample,\\twriting\\ta\\tprogram\\tthat\\tuses\\tseveral\\nchunks\\tof\\tcode\\tfrom\\tthis\\tbook\\tdoes\\tnot\\trequire\\tpermission.\\tSelling\\tor\\tdistributing\\ta\\tCD-ROM\\tof\\nexamples\\tfrom\\tO’Reilly\\tbooks\\tdoes\\trequire\\tpermission.\\tAnswering\\ta\\tquestion\\tby\\tciting\\tthis\\tbook\\tand\\nquoting\\texample\\tcode\\tdoes\\tnot\\trequire\\tpermission.\\tIncorporating\\ta\\tsignificant\\tamount\\tof\\texample\\tcode\\nfrom\\tthis\\tbook\\tinto\\tyour\\tproduct’s\\tdocumentation\\tdoes\\trequire\\tpermission.\\nWe\\tappreciate,\\tbut\\tdo\\tnot\\trequire,\\tattribution.\\tAn\\tattribution\\tusually\\tincludes\\tthe\\ttitle,\\tauthor,\\tpublisher,\\nand\\tISBN.\\tFor\\texample:\\t“', 'Hands-On\\tMachine\\tLearning\\twith\\tScikit-Learn\\tand\\tTensorFlow\\n\\tby\\tAurélien\\nGéron\\t(O’Reilly).\\tCopyright\\t2017\\tAurélien\\tGéron,\\t978-1-491-96229-9.”\\nIf\\tyou\\tfeel\\tyour\\tuse\\tof\\tcode\\texamples\\tfalls\\toutside\\tfair\\tuse\\tor\\tthe\\tpermission\\tgiven\\tabove,\\tfeel\\tfree\\tto\\ncontact\\tus\\tat\\t\\npermissions@oreilly.com\\n.', 'O’Reilly\\tSafari\\nNOTE\\nSafari\\n\\t(formerly\\tSafari\\tBooks\\tOnline)\\tis\\ta\\tmembership-based\\ttraining\\tand\\treference\\tplatform\\tfor\\nenterprise,\\tgovernment,\\teducators,\\tand\\tindividuals.\\nMembers\\thave\\taccess\\tto\\tthousands\\tof\\tbooks,\\ttraining\\tvideos,\\tLearning\\tPaths,\\tinteractive\\ttutorials,\\tand\\ncurated\\tplaylists\\tfrom\\tover\\t250\\tpublishers,\\tincluding\\tO’Reilly\\tMedia,\\tHarvard\\tBusiness\\tReview,\\nPrentice\\tHall\\tProfessional,\\tAddison-Wesley\\tProfessional,\\tMicrosoft\\tPress,\\tSams,\\tQue,\\tPeachpit\\tPress,\\nAdobe,\\tFocal\\tPress,\\tCisco\\tPress,\\tJohn\\tWiley\\t&\\tSons,\\tSyngress,\\tMorgan\\tKaufmann,\\tIBM\\tRedbooks,\\nPackt,\\tAdobe\\tPress,\\tFT\\tPress,\\tApress,\\tManning,\\tNew\\tRiders,\\tMcGraw-Hill,\\tJones\\t&\\tBartlett,\\tand\\nCourse\\tTechnology,\\tamong\\tothers.\\nFor\\tmore\\tinformation,\\tplease\\tvisit\\t\\nhttp://oreilly.com/safari\\n.', 'How\\tto\\tContact\\tUs\\nPlease\\taddress\\tcomments\\tand\\tquestions\\tconcerning\\tthis\\tbook\\tto\\tthe\\tpublisher:\\nO’Reilly\\tMedia,\\tInc.\\n1005\\tGravenstein\\tHighway\\tNorth\\nSebastopol,\\tCA\\t95472\\n800-998-9938\\t(in\\tthe\\tUnited\\tStates\\tor\\tCanada)\\n707-829-0515\\t(international\\tor\\tlocal)\\n707-829-0104\\t(fax)\\nWe\\thave\\ta\\tweb\\tpage\\tfor\\tthis\\tbook,\\twhere\\twe\\tlist\\terrata,\\texamples,\\tand\\tany\\tadditional\\tinformation.\\tYou\\ncan\\taccess\\tthis\\tpage\\tat\\t\\nhttp://bit.ly/hands-on-machine-learning-with-scikit-learn-and-tensorflow\\n.\\nTo\\tcomment\\tor\\task\\ttechnical\\tquestions\\tabout\\tthis\\tbook,\\tsend\\temail\\tto\\t\\nbookquestions@oreilly.com\\n.\\nFor\\tmore\\tinformation\\tabout\\tour\\tbooks,\\tcourses,\\tconferences,\\tand\\tnews,\\tsee\\tour\\twebsite\\tat\\nhttp://www.oreilly.com\\n.\\nFind\\tus\\ton\\tFacebook:\\t\\nhttp://facebook.com/oreilly\\nFollow\\tus\\ton\\tTwitter:\\t\\nhttp://twitter.com/oreillymedia\\nWatch\\tus\\ton\\tYouTube:\\t\\nhttp://www.youtube.com/oreillymedia', 'Acknowledgments\\nI\\twould\\tlike\\tto\\tthank\\tmy\\tGoogle\\tcolleagues,\\tin\\tparticular\\tthe\\tYouTube\\tvideo\\tclassification\\tteam,\\tfor\\nteaching\\tme\\tso\\tmuch\\tabout\\tMachine\\tLearning.\\tI\\tcould\\tnever\\thave\\tstarted\\tthis\\tproject\\twithout\\tthem.\\nSpecial\\tthanks\\tto\\tmy\\tpersonal\\tML\\tgurus:\\tClément\\tCourbet,\\tJulien\\tDubois,\\tMathias\\tKende,\\tDaniel\\nKitachewsky,\\tJames\\tPack,\\tAlexander\\tPak,\\tAnosh\\tRaj,\\tVitor\\tSessak,\\tWiktor\\tTomczak,\\tIngrid\\tvon\\tGlehn,\\nRich\\tWashington,\\tand\\teveryone\\tat\\tYouTube\\tParis.\\nI\\tam\\tincredibly\\tgrateful\\tto\\tall\\tthe\\tamazing\\tpeople\\twho\\ttook\\ttime\\tout\\tof\\ttheir\\tbusy\\tlives\\tto\\treview\\tmy\\tbook\\nin\\tso\\tmuch\\tdetail.\\tThanks\\tto\\tPete\\tWarden\\tfor\\tanswering\\tall\\tmy\\tTensorFlow\\tquestions,\\treviewing\\t\\nPart\\tII\\n,\\nproviding\\tmany\\tinteresting\\tinsights,\\tand\\tof\\tcourse\\tfor\\tbeing\\tpart\\tof\\tthe\\tcore\\tTensorFlow\\tteam.\\tYou\\tshould\\ndefinitely\\tcheck\\tout\\t\\nhis\\tblog\\n!\\tMany\\tthanks\\tto\\tLukas\\tBiewald\\tfor\\this\\tvery\\tthorough\\treview\\tof\\t\\nPart\\tII\\n:\\the\\nleft\\tno\\tstone\\tunturned,\\ttested\\tall\\tthe\\tcode\\t(and\\tcaught\\ta\\tfew\\terrors),\\tmade\\tmany\\tgreat\\tsuggestions,\\tand\\this', 'enthusiasm\\twas\\tcontagious.\\tYou\\tshould\\tcheck\\tout\\t\\nhis\\tblog\\n\\tand\\this\\t\\ncool\\trobots\\n!\\tThanks\\tto\\tJustin\\tFrancis,\\nwho\\talso\\treviewed\\t\\nPart\\tII\\n\\tvery\\tthoroughly,\\tcatching\\terrors\\tand\\tproviding\\tgreat\\tinsights,\\tin\\tparticular\\tin\\nChapter\\t16\\n.\\tCheck\\tout\\t\\nhis\\tposts\\n\\ton\\tTensorFlow!\\nHuge\\tthanks\\tas\\twell\\tto\\tDavid\\tAndrzejewski,\\twho\\treviewed\\t\\nPart\\tI\\n\\tand\\tprovided\\tincredibly\\tuseful\\nfeedback,\\tidentifying\\tunclear\\tsections\\tand\\tsuggesting\\thow\\tto\\timprove\\tthem.\\tCheck\\tout\\t\\nhis\\twebsite\\n!\\nThanks\\tto\\tGrégoire\\tMesnil,\\twho\\treviewed\\t\\nPart\\tII\\n\\tand\\tcontributed\\tvery\\tinteresting\\tpractical\\tadvice\\ton\\ntraining\\tneural\\tnetworks.\\tThanks\\tas\\twell\\tto\\tEddy\\tHung,\\tSalim\\tSémaoune,\\tKarim\\tMatrah,\\tIngrid\\tvon\\nGlehn,\\tIain\\tSmears,\\tand\\tVincent\\tGuilbeau\\tfor\\treviewing\\t\\nPart\\tI\\n\\tand\\tmaking\\tmany\\tuseful\\tsuggestions.\\tAnd\\tI\\nalso\\twish\\tto\\tthank\\tmy\\tfather-in-law,\\tMichel\\tTessier,\\tformer\\tmathematics\\tteacher\\tand\\tnow\\ta\\tgreat\\ntranslator\\tof\\tAnton\\tChekhov,\\tfor\\thelping\\tme\\tiron\\tout\\tsome\\tof\\tthe\\tmathematics\\tand\\tnotations\\tin\\tthis\\tbook', 'and\\treviewing\\tthe\\tlinear\\talgebra\\tJupyter\\tnotebook.\\nAnd\\tof\\tcourse,\\ta\\tgigantic\\t“thank\\tyou”\\tto\\tmy\\tdear\\tbrother\\tSylvain,\\twho\\treviewed\\tevery\\tsingle\\tchapter,\\ntested\\tevery\\tline\\tof\\tcode,\\tprovided\\tfeedback\\ton\\tvirtually\\tevery\\tsection,\\tand\\tencouraged\\tme\\tfrom\\tthe\\tfirst\\nline\\tto\\tthe\\tlast.\\tLove\\tyou,\\tbro!\\nMany\\tthanks\\tas\\twell\\tto\\tO’Reilly’s\\tfantastic\\tstaff,\\tin\\tparticular\\tNicole\\tTache,\\twho\\tgave\\tme\\tinsightful\\nfeedback,\\talways\\tcheerful,\\tencouraging,\\tand\\thelpful.\\tThanks\\tas\\twell\\tto\\tMarie\\tBeaugureau,\\tBen\\tLorica,\\nMike\\tLoukides,\\tand\\tLaurel\\tRuma\\tfor\\tbelieving\\tin\\tthis\\tproject\\tand\\thelping\\tme\\tdefine\\tits\\tscope.\\tThanks\\tto\\nMatt\\tHacker\\tand\\tall\\tof\\tthe\\tAtlas\\tteam\\tfor\\tanswering\\tall\\tmy\\ttechnical\\tquestions\\tregarding\\tformatting,\\nasciidoc,\\tand\\tLaTeX,\\tand\\tthanks\\tto\\tRachel\\tMonaghan,\\tNick\\tAdams,\\tand\\tall\\tof\\tthe\\tproduction\\tteam\\tfor\\ntheir\\tfinal\\treview\\tand\\ttheir\\thundreds\\tof\\tcorrections.\\nLast\\tbut\\tnot\\tleast,\\tI\\tam\\tinfinitely\\tgrateful\\tto\\tmy\\tbeloved\\twife,\\tEmmanuelle,\\tand\\tto\\tour\\tthree\\twonderful', 'kids,\\tAlexandre,\\tRémi,\\tand\\tGabrielle,\\tfor\\tencouraging\\tme\\tto\\twork\\thard\\ton\\tthis\\tbook,\\tasking\\tmany\\nquestions\\t(who\\tsaid\\tyou\\tcan’t\\tteach\\tneural\\tnetworks\\tto\\ta\\tseven-year-old?),\\tand\\teven\\tbringing\\tme\\tcookies\\nand\\tcoffee.\\tWhat\\tmore\\tcan\\tone\\tdream\\tof?\\nAvailable\\ton\\tHinton’s\\thome\\tpage\\tat\\t\\nhttp://www.cs.toronto.edu/~hinton/\\n.\\nDespite\\tthe\\tfact\\tthat\\tYann\\tLecun’s\\tdeep\\tconvolutional\\tneural\\tnetworks\\thad\\tworked\\twell\\tfor\\timage\\trecognition\\tsince\\tthe\\t1990s,\\talthough\\nthey\\twere\\tnot\\tas\\tgeneral\\tpurpose.\\n1\\n2', 'Part\\tI.\\t\\nThe\\tFundamentals\\tof\\t\\nMachine\\tLearning', 'Chapter\\t1.\\t\\nThe\\tMachine\\tLearning\\tLandscape\\nWhen\\t\\nmost\\tpeople\\thear\\t“Machine\\tLearning,”\\tthey\\tpicture\\ta\\trobot:\\ta\\tdependable\\tbutler\\tor\\ta\\tdeadly\\nTerminator\\tdepending\\ton\\twho\\tyou\\task.\\tBut\\tMachine\\tLearning\\tis\\tnot\\tjust\\ta\\tfuturistic\\tfantasy,\\tit’s\\talready\\nhere.\\tIn\\tfact,\\tit\\thas\\tbeen\\taround\\tfor\\tdecades\\tin\\tsome\\tspecialized\\tapplications,\\tsuch\\tas\\t\\nOptical\\tCharacter\\nRecognition\\n\\t(OCR).\\t\\nBut\\tthe\\tfirst\\tML\\tapplication\\tthat\\treally\\tbecame\\tmainstream,\\timproving\\tthe\\tlives\\tof\\nhundreds\\tof\\tmillions\\tof\\tpeople,\\ttook\\tover\\tthe\\tworld\\tback\\tin\\tthe\\t1990s:\\tit\\twas\\t\\nthe\\t\\nspam\\tfilter\\n.\\tNot\\texactly\\na\\tself-aware\\tSkynet,\\tbut\\tit\\tdoes\\ttechnically\\tqualify\\tas\\tMachine\\tLearning\\t(it\\thas\\tactually\\tlearned\\tso\\twell\\nthat\\tyou\\tseldom\\tneed\\tto\\tflag\\tan\\temail\\tas\\tspam\\tanymore).\\tIt\\twas\\tfollowed\\tby\\thundreds\\tof\\tML\\tapplications\\nthat\\tnow\\tquietly\\tpower\\thundreds\\tof\\tproducts\\tand\\tfeatures\\tthat\\tyou\\tuse\\tregularly,\\tfrom\\tbetter\\nrecommendations\\tto\\tvoice\\tsearch.\\nWhere\\tdoes\\tMachine\\tLearning\\tstart\\tand\\twhere\\tdoes\\tit\\tend?\\tWhat\\texactly\\tdoes\\tit\\tmean\\tfor\\ta\\tmachine\\tto', 'learn\\n\\tsomething?\\tIf\\tI\\tdownload\\ta\\tcopy\\tof\\tWikipedia,\\thas\\tmy\\tcomputer\\treally\\t“learned”\\tsomething?\\tIs\\tit\\nsuddenly\\tsmarter?\\tIn\\tthis\\tchapter\\twe\\twill\\tstart\\tby\\tclarifying\\twhat\\tMachine\\tLearning\\tis\\tand\\twhy\\tyou\\tmay\\nwant\\tto\\tuse\\tit.\\nThen,\\tbefore\\twe\\tset\\tout\\tto\\texplore\\tthe\\tMachine\\tLearning\\tcontinent,\\twe\\twill\\ttake\\ta\\tlook\\tat\\tthe\\tmap\\tand\\nlearn\\tabout\\tthe\\tmain\\tregions\\tand\\tthe\\tmost\\tnotable\\tlandmarks:\\tsupervised\\tversus\\tunsupervised\\tlearning,\\nonline\\tversus\\tbatch\\tlearning,\\tinstance-based\\tversus\\tmodel-based\\tlearning.\\tThen\\twe\\twill\\tlook\\tat\\tthe\\nworkflow\\tof\\ta\\ttypical\\tML\\tproject,\\tdiscuss\\tthe\\tmain\\tchallenges\\tyou\\tmay\\tface,\\tand\\tcover\\thow\\tto\\tevaluate\\nand\\tfine-tune\\ta\\tMachine\\tLearning\\tsystem.\\nThis\\tchapter\\tintroduces\\ta\\tlot\\tof\\tfundamental\\tconcepts\\t(and\\tjargon)\\tthat\\tevery\\tdata\\tscientist\\tshould\\tknow\\nby\\theart.\\tIt\\twill\\tbe\\ta\\thigh-level\\toverview\\t(the\\tonly\\tchapter\\twithout\\tmuch\\tcode),\\tall\\trather\\tsimple,\\tbut\\tyou\\nshould\\tmake\\tsure\\teverything\\tis\\tcrystal-clear\\tto\\tyou\\tbefore\\tcontinuing\\tto\\tthe\\trest\\tof\\tthe\\tbook.\\tSo\\tgrab\\ta', 'coffee\\tand\\tlet’s\\tget\\tstarted!\\nTIP\\nIf\\tyou\\talready\\tknow\\tall\\tthe\\tMachine\\tLearning\\tbasics,\\tyou\\tmay\\twant\\tto\\tskip\\tdirectly\\tto\\t\\nChapter\\t2\\n.\\tIf\\tyou\\tare\\tnot\\tsure,\\ttry\\tto\\nanswer\\tall\\tthe\\tquestions\\tlisted\\tat\\tthe\\tend\\tof\\tthe\\tchapter\\tbefore\\tmoving\\ton.', 'What\\tIs\\tMachine\\tLearning?\\nMachine\\tLearning\\t\\nis\\tthe\\tscience\\t(and\\tart)\\tof\\tprogramming\\tcomputers\\tso\\tthey\\tcan\\t\\nlearn\\tfrom\\tdata\\n.\\nHere\\tis\\ta\\tslightly\\tmore\\tgeneral\\tdefinition:\\n[Machine\\tLearning\\tis\\tthe]\\tfield\\tof\\tstudy\\tthat\\tgives\\tcomputers\\tthe\\tability\\tto\\tlearn\\twithout\\tbeing\\nexplicitly\\tprogrammed.\\nArthur\\tSamuel,\\t\\n1959\\nAnd\\ta\\tmore\\tengineering-oriented\\tone:\\nA\\tcomputer\\tprogram\\tis\\tsaid\\tto\\tlearn\\tfrom\\texperience\\tE\\twith\\trespect\\tto\\tsome\\ttask\\tT\\tand\\tsome\\nperformance\\tmeasure\\tP,\\tif\\tits\\tperformance\\ton\\tT,\\tas\\tmeasured\\tby\\tP,\\timproves\\twith\\texperience\\tE.\\nTom\\tMitchell,\\t\\n1997\\nFor\\t\\nexample,\\tyour\\tspam\\tfilter\\tis\\ta\\tMachine\\tLearning\\tprogram\\tthat\\tcan\\tlearn\\tto\\tflag\\tspam\\tgiven\\texamples\\nof\\tspam\\temails\\t(e.g.,\\tflagged\\tby\\tusers)\\tand\\texamples\\tof\\tregular\\t(nonspam,\\talso\\tcalled\\t“ham”)\\temails.\\tThe\\nexamples\\tthat\\tthe\\tsystem\\tuses\\tto\\tlearn\\tare\\tcalled\\tthe\\t\\ntraining\\tset\\n.\\t\\nEach\\ttraining\\texample\\tis\\tcalled\\ta\\ntraining\\tinstance\\n\\t(or\\t\\nsample\\n).\\tIn\\tthis\\tcase,\\tthe\\ttask\\tT\\tis\\tto\\tflag\\tspam\\tfor\\tnew\\temails,\\tthe\\texperience\\tE\\tis\\nthe\\t\\ntraining\\tdata\\n,', ',\\t\\nand\\tthe\\tperformance\\tmeasure\\tP\\tneeds\\tto\\tbe\\tdefined;\\tfor\\texample,\\tyou\\tcan\\tuse\\tthe\\tratio\\nof\\tcorrectly\\tclassified\\temails.\\tThis\\tparticular\\tperformance\\tmeasure\\tis\\tcalled\\t\\naccuracy\\n\\t\\nand\\tit\\tis\\toften\\nused\\tin\\tclassification\\ttasks.\\nIf\\tyou\\tjust\\tdownload\\ta\\tcopy\\tof\\tWikipedia,\\tyour\\tcomputer\\thas\\ta\\tlot\\tmore\\tdata,\\tbut\\tit\\tis\\tnot\\tsuddenly\\tbetter\\nat\\tany\\ttask.\\tThus,\\tit\\tis\\tnot\\tMachine\\tLearning.', 'Why\\tUse\\tMachine\\tLearning?\\nConsider\\t\\nhow\\tyou\\twould\\twrite\\ta\\tspam\\tfilter\\tusing\\ttraditional\\tprogramming\\ttechniques\\t(\\nFigure\\t1-1\\n):\\n1\\n.\\t\\nFirst\\tyou\\twould\\tlook\\tat\\twhat\\tspam\\ttypically\\tlooks\\tlike.\\tYou\\tmight\\tnotice\\tthat\\tsome\\twords\\tor\\tphrases\\n(such\\tas\\t“4U,”\\t“credit\\tcard,”\\t“free,”\\tand\\t“amazing”)\\ttend\\tto\\tcome\\tup\\ta\\tlot\\tin\\tthe\\tsubject.\\tPerhaps\\nyou\\twould\\talso\\tnotice\\ta\\tfew\\tother\\tpatterns\\tin\\tthe\\tsender’s\\tname,\\tthe\\temail’s\\tbody,\\tand\\tso\\ton.\\n2\\n.\\t\\nYou\\twould\\twrite\\ta\\tdetection\\talgorithm\\tfor\\teach\\tof\\tthe\\tpatterns\\tthat\\tyou\\tnoticed,\\tand\\tyour\\tprogram\\nwould\\tflag\\temails\\tas\\tspam\\tif\\ta\\tnumber\\tof\\tthese\\tpatterns\\tare\\tdetected.\\n3\\n.\\t\\nYou\\twould\\ttest\\tyour\\tprogram,\\tand\\trepeat\\tsteps\\t1\\tand\\t2\\tuntil\\tit\\tis\\tgood\\tenough.\\nFigure\\t1-1.\\t\\nThe\\ttraditional\\tapproach\\nSince\\tthe\\tproblem\\tis\\tnot\\ttrivial,\\tyour\\tprogram\\twill\\tlikely\\tbecome\\ta\\tlong\\tlist\\tof\\tcomplex\\trules\\t—\\tpretty\\nhard\\tto\\tmaintain.\\nIn\\tcontrast,\\ta\\tspam\\tfilter\\tbased\\ton\\tMachine\\tLearning\\ttechniques\\tautomatically\\tlearns\\twhich\\twords\\tand', 'phrases\\tare\\tgood\\tpredictors\\tof\\tspam\\tby\\tdetecting\\tunusually\\tfrequent\\tpatterns\\tof\\twords\\tin\\tthe\\tspam\\nexamples\\tcompared\\tto\\tthe\\tham\\texamples\\t(\\nFigure\\t1-2\\n).\\tThe\\tprogram\\tis\\tmuch\\tshorter,\\teasier\\tto\\tmaintain,\\nand\\tmost\\tlikely\\tmore\\taccurate.', 'Figure\\t1-2.\\t\\nMachine\\tLearning\\tapproach\\nMoreover,\\tif\\tspammers\\tnotice\\tthat\\tall\\ttheir\\temails\\tcontaining\\t“4U”\\tare\\tblocked,\\tthey\\tmight\\tstart\\twriting\\n“For\\tU”\\tinstead.\\tA\\tspam\\tfilter\\tusing\\ttraditional\\tprogramming\\ttechniques\\twould\\tneed\\tto\\tbe\\tupdated\\tto\\tflag\\n“For\\tU”\\temails.\\tIf\\tspammers\\tkeep\\tworking\\taround\\tyour\\tspam\\tfilter,\\tyou\\twill\\tneed\\tto\\tkeep\\twriting\\tnew\\nrules\\tforever.\\nIn\\tcontrast,\\ta\\tspam\\tfilter\\tbased\\ton\\tMachine\\tLearning\\ttechniques\\tautomatically\\tnotices\\tthat\\t“For\\tU”\\thas\\nbecome\\tunusually\\tfrequent\\tin\\tspam\\tflagged\\tby\\tusers,\\tand\\tit\\tstarts\\tflagging\\tthem\\twithout\\tyour\\tintervention\\n(\\nFigure\\t1-3\\n).\\nFigure\\t1-3.\\t\\nAutomatically\\tadapting\\tto\\tchange\\nAnother\\t\\narea\\twhere\\tMachine\\tLearning\\tshines\\tis\\tfor\\tproblems\\tthat\\teither\\tare\\ttoo\\tcomplex\\tfor\\ttraditional\\napproaches\\tor\\thave\\tno\\tknown\\talgorithm.\\tFor\\texample,\\tconsider\\t\\nspeech\\trecognition:\\tsay\\tyou\\twant\\tto\\tstart\\nsimple\\tand\\twrite\\ta\\tprogram\\tcapable\\tof\\tdistinguishing\\tthe\\twords\\t“one”\\tand\\t“two.”\\tYou\\tmight\\tnotice\\tthat', 'the\\tword\\t“two”\\tstarts\\twith\\ta\\thigh-pitch\\tsound\\t(“T”),\\tso\\tyou\\tcould\\thardcode\\tan\\talgorithm\\tthat\\tmeasures\\nhigh-pitch\\tsound\\tintensity\\tand\\tuse\\tthat\\tto\\tdistinguish\\tones\\tand\\ttwos.\\tObviously\\tthis\\ttechnique\\twill\\tnot\\nscale\\tto\\tthousands\\tof\\twords\\tspoken\\tby\\tmillions\\tof\\tvery\\tdifferent\\tpeople\\tin\\tnoisy\\tenvironments\\tand\\tin', 'dozens\\tof\\tlanguages.\\tThe\\tbest\\tsolution\\t(at\\tleast\\ttoday)\\tis\\tto\\twrite\\tan\\talgorithm\\tthat\\tlearns\\tby\\titself,\\tgiven\\nmany\\texample\\trecordings\\tfor\\teach\\tword.\\nFinally,\\tMachine\\tLearning\\tcan\\thelp\\thumans\\tlearn\\t(\\nFigure\\t1-4\\n):\\tML\\talgorithms\\tcan\\tbe\\tinspected\\tto\\tsee\\nwhat\\tthey\\thave\\tlearned\\t(although\\tfor\\tsome\\talgorithms\\tthis\\tcan\\tbe\\ttricky).\\tFor\\tinstance,\\tonce\\tthe\\tspam\\nfilter\\thas\\tbeen\\ttrained\\ton\\tenough\\tspam,\\tit\\tcan\\teasily\\tbe\\tinspected\\tto\\treveal\\tthe\\tlist\\tof\\twords\\tand\\ncombinations\\tof\\twords\\tthat\\tit\\tbelieves\\tare\\tthe\\tbest\\tpredictors\\tof\\tspam.\\tSometimes\\tthis\\twill\\treveal\\nunsuspected\\tcorrelations\\tor\\tnew\\ttrends,\\tand\\tthereby\\tlead\\tto\\ta\\tbetter\\tunderstanding\\tof\\tthe\\tproblem.\\nApplying\\t\\nML\\ttechniques\\tto\\tdig\\tinto\\tlarge\\tamounts\\tof\\tdata\\tcan\\thelp\\tdiscover\\tpatterns\\tthat\\twere\\tnot\\nimmediately\\tapparent.\\tThis\\tis\\tcalled\\t\\ndata\\tmining\\n.\\nFigure\\t1-4.\\t\\nMachine\\tLearning\\tcan\\thelp\\thumans\\tlearn\\nTo\\tsummarize,\\tMachine\\tLearning\\tis\\tgreat\\tfor:\\nProblems\\tfor\\twhich\\texisting\\tsolutions\\trequire\\ta\\tlot\\tof\\thand-tuning\\tor\\tlong\\tlists\\tof\\trules:\\tone\\tMachine', 'Learning\\talgorithm\\tcan\\toften\\tsimplify\\tcode\\tand\\tperform\\tbetter.\\nComplex\\tproblems\\tfor\\twhich\\tthere\\tis\\tno\\tgood\\tsolution\\tat\\tall\\tusing\\ta\\ttraditional\\tapproach:\\tthe\\tbest\\nMachine\\tLearning\\ttechniques\\tcan\\tfind\\ta\\tsolution.\\nFluctuating\\tenvironments:\\ta\\tMachine\\tLearning\\tsystem\\tcan\\tadapt\\tto\\tnew\\tdata.\\nGetting\\tinsights\\tabout\\tcomplex\\tproblems\\tand\\tlarge\\tamounts\\t\\nof\\tdata.', 'Types\\tof\\tMachine\\tLearning\\tSystems\\nThere\\t\\nare\\tso\\tmany\\tdifferent\\ttypes\\tof\\tMachine\\tLearning\\tsystems\\tthat\\tit\\tis\\tuseful\\tto\\tclassify\\tthem\\tin\\tbroad\\ncategories\\tbased\\ton:\\nWhether\\tor\\tnot\\tthey\\tare\\ttrained\\twith\\thuman\\tsupervision\\t(supervised,\\tunsupervised,\\tsemisupervised,\\nand\\tReinforcement\\tLearning)\\nWhether\\tor\\tnot\\tthey\\tcan\\tlearn\\tincrementally\\ton\\tthe\\tfly\\t(online\\tversus\\tbatch\\tlearning)\\nWhether\\tthey\\twork\\tby\\tsimply\\tcomparing\\tnew\\tdata\\tpoints\\tto\\tknown\\tdata\\tpoints,\\tor\\tinstead\\tdetect\\npatterns\\tin\\tthe\\ttraining\\tdata\\tand\\tbuild\\ta\\tpredictive\\tmodel,\\tmuch\\tlike\\tscientists\\tdo\\t(instance-based\\nversus\\tmodel-based\\tlearning)\\nThese\\tcriteria\\tare\\tnot\\texclusive;\\tyou\\tcan\\tcombine\\tthem\\tin\\tany\\tway\\tyou\\tlike.\\tFor\\texample,\\ta\\tstate-of-the-\\nart\\tspam\\tfilter\\tmay\\tlearn\\ton\\tthe\\tfly\\tusing\\ta\\tdeep\\tneural\\tnetwork\\tmodel\\ttrained\\tusing\\texamples\\tof\\tspam\\tand\\nham;\\tthis\\tmakes\\tit\\tan\\tonline,\\tmodel-based,\\tsupervised\\tlearning\\tsystem.\\nLet’s\\tlook\\tat\\teach\\tof\\tthese\\tcriteria\\ta\\tbit\\tmore\\tclosely.', 'Supervised/Unsupervised\\tLearning\\nMachine\\t\\nLearning\\tsystems\\tcan\\tbe\\tclassified\\taccording\\tto\\tthe\\tamount\\tand\\ttype\\tof\\tsupervision\\tthey\\tget\\nduring\\ttraining.\\tThere\\tare\\tfour\\tmajor\\tcategories:\\tsupervised\\tlearning,\\tunsupervised\\tlearning,\\nsemisupervised\\tlearning,\\tand\\tReinforcement\\tLearning.\\nSupervised\\tlearning\\nIn\\t\\nsupervised\\tlearning\\n,\\tthe\\ttraining\\tdata\\tyou\\tfeed\\tto\\tthe\\talgorithm\\tincludes\\tthe\\tdesired\\tsolutions,\\t\\ncalled\\nlabels\\n\\t(\\nFigure\\t1-5\\n).\\nFigure\\t1-5.\\t\\nA\\tlabeled\\ttraining\\tset\\tfor\\tsupervised\\tlearning\\t(e.g.,\\tspam\\tclassification)\\nA\\ttypical\\tsupervised\\tlearning\\ttask\\t\\nis\\t\\nclassification\\n.\\tThe\\t\\nspam\\tfilter\\tis\\ta\\tgood\\texample\\tof\\tthis:\\tit\\tis\\ttrained\\nwith\\tmany\\texample\\temails\\talong\\twith\\ttheir\\t\\nclass\\n\\t(spam\\tor\\tham),\\tand\\tit\\tmust\\tlearn\\thow\\tto\\tclassify\\tnew\\nemails.\\nAnother\\ttypical\\ttask\\tis\\tto\\tpredict\\ta\\t\\ntarget\\n\\tnumeric\\tvalue,\\tsuch\\tas\\tthe\\tprice\\tof\\ta\\tcar,\\tgiven\\ta\\tset\\tof\\t\\nfeatures\\n(mileage,\\tage,\\tbrand,\\tetc.)\\tcalled\\t\\npredictors\\n.\\t\\nThis\\tsort\\tof\\ttask\\tis\\t\\ncalled\\t\\nregression\\n\\t(\\nFigure\\t1-6\\n).\\n1\\n\\tTo\\ttrain', 'To\\ttrain\\nthe\\tsystem,\\tyou\\tneed\\tto\\tgive\\tit\\tmany\\texamples\\tof\\tcars,\\tincluding\\tboth\\ttheir\\tpredictors\\tand\\ttheir\\tlabels\\n(i.e.,\\ttheir\\tprices).\\nNOTE\\nIn\\tMachine\\tLearning\\t\\nan\\t\\nattribute\\n\\tis\\ta\\tdata\\ttype\\t(e.g.,\\t“Mileage”),\\twhile\\ta\\t\\nfeature\\n\\t\\nhas\\tseveral\\tmeanings\\tdepending\\ton\\tthe\\ncontext,\\tbut\\tgenerally\\tmeans\\tan\\tattribute\\tplus\\tits\\tvalue\\t(e.g.,\\t“Mileage\\t=\\t15,000”).\\tMany\\tpeople\\tuse\\tthe\\twords\\t\\nattribute\\n\\tand\\nfeature\\n\\tinterchangeably,\\tthough.', 'Figure\\t1-6.\\t\\nRegression\\nNote\\tthat\\tsome\\tregression\\talgorithms\\tcan\\tbe\\tused\\tfor\\tclassification\\tas\\twell,\\tand\\tvice\\tversa.\\tFor\\texample,\\nLogistic\\tRegression\\n\\t\\nis\\tcommonly\\tused\\tfor\\tclassification,\\tas\\tit\\tcan\\toutput\\ta\\tvalue\\tthat\\tcorresponds\\tto\\tthe\\nprobability\\tof\\tbelonging\\tto\\ta\\tgiven\\tclass\\t(e.g.,\\t20%\\tchance\\tof\\tbeing\\tspam).\\nHere\\tare\\tsome\\tof\\tthe\\tmost\\timportant\\tsupervised\\tlearning\\talgorithms\\t(covered\\tin\\tthis\\tbook):\\nk-Nearest\\tNeighbors\\nLinear\\tRegression\\nLogistic\\tRegression\\nSupport\\tVector\\tMachines\\t(SVMs)\\nDecision\\tTrees\\tand\\tRandom\\tForests\\nNeural\\tnetworks\\n2\\nUnsupervised\\tlearning\\nIn\\t\\nunsupervised\\tlearning\\n,\\t\\nas\\tyou\\tmight\\tguess,\\tthe\\ttraining\\tdata\\tis\\tunlabeled\\t(\\nFigure\\t1-7\\n).\\tThe\\tsystem\\ttries\\nto\\tlearn\\twithout\\ta\\tteacher.', 'Figure\\t1-7.\\t\\nAn\\tunlabeled\\ttraining\\tset\\tfor\\tunsupervised\\tlearning\\nHere\\tare\\tsome\\tof\\tthe\\tmost\\timportant\\tunsupervised\\tlearning\\talgorithms\\t(we\\twill\\tcover\\tdimensionality\\nreduction\\tin\\t\\nChapter\\t8\\n):\\nClustering\\nk-Means\\nHierarchical\\tCluster\\tAnalysis\\t(HCA)\\nExpectation\\tMaximization\\nVisualization\\tand\\tdimensionality\\treduction\\nPrincipal\\tComponent\\tAnalysis\\t(PCA)\\nKernel\\tPCA\\nLocally-Linear\\tEmbedding\\t(LLE)\\nt-distributed\\tStochastic\\tNeighbor\\tEmbedding\\t(t-SNE)\\nAssociation\\trule\\t\\nlearning\\nApriori\\nEclat\\nFor\\texample,\\tsay\\tyou\\thave\\ta\\tlot\\tof\\tdata\\tabout\\tyour\\tblog’s\\tvisitors.\\tYou\\tmay\\twant\\tto\\trun\\ta\\t\\nclustering\\nalgorithm\\t\\nto\\ttry\\tto\\tdetect\\tgroups\\tof\\tsimilar\\tvisitors\\t(\\nFigure\\t1-8\\n).\\tAt\\tno\\tpoint\\tdo\\tyou\\ttell\\tthe\\talgorithm\\nwhich\\tgroup\\ta\\tvisitor\\tbelongs\\tto:\\tit\\tfinds\\tthose\\tconnections\\twithout\\tyour\\thelp.\\tFor\\texample,\\tit\\tmight\\nnotice\\tthat\\t40%\\tof\\tyour\\tvisitors\\tare\\tmales\\twho\\tlove\\tcomic\\tbooks\\tand\\tgenerally\\tread\\tyour\\tblog\\tin\\tthe\\nevening,\\twhile\\t20%\\tare\\tyoung\\tsci-fi\\tlovers\\twho\\tvisit\\tduring\\tthe\\tweekends,\\tand\\tso\\ton.\\tIf\\tyou\\tuse\\t\\na', 'a\\nhierarchical\\tclustering\\n\\talgorithm,\\tit\\tmay\\talso\\tsubdivide\\teach\\tgroup\\tinto\\tsmaller\\tgroups.\\tThis\\tmay\\thelp\\nyou\\ttarget\\tyour\\tposts\\tfor\\teach\\tgroup.', 'Figure\\t1-8.\\t\\nClustering\\nVisualization\\n\\talgorithms\\t\\nare\\talso\\tgood\\texamples\\tof\\tunsupervised\\tlearning\\talgorithms:\\tyou\\tfeed\\tthem\\ta\\tlot\\nof\\tcomplex\\tand\\tunlabeled\\tdata,\\tand\\tthey\\toutput\\ta\\t2D\\tor\\t3D\\trepresentation\\tof\\tyour\\tdata\\tthat\\tcan\\teasily\\tbe\\nplotted\\t(\\nFigure\\t1-9\\n).\\tThese\\talgorithms\\ttry\\tto\\tpreserve\\tas\\tmuch\\tstructure\\tas\\tthey\\tcan\\t(e.g.,\\ttrying\\tto\\tkeep\\nseparate\\tclusters\\tin\\tthe\\tinput\\tspace\\tfrom\\toverlapping\\tin\\tthe\\tvisualization),\\tso\\tyou\\tcan\\tunderstand\\thow\\tthe\\ndata\\tis\\torganized\\tand\\tperhaps\\tidentify\\tunsuspected\\tpatterns.\\nFigure\\t1-9.\\t\\nExample\\tof\\ta\\tt-SNE\\tvisualization\\thighlighting\\tsemantic\\tclusters\\n3\\nA\\t\\nrelated\\ttask\\tis\\t\\ndimensionality\\treduction\\n,\\tin\\twhich\\t\\nthe\\tgoal\\tis\\tto\\tsimplify\\tthe\\tdata\\twithout\\tlosing\\ttoo\\nmuch\\tinformation.\\tOne\\tway\\tto\\tdo\\tthis\\tis\\tto\\tmerge\\tseveral\\tcorrelated\\tfeatures\\tinto\\tone.\\tFor\\texample,\\ta\\ncar’s\\tmileage\\tmay\\tbe\\tvery\\tcorrelated\\twith\\tits\\tage,\\tso\\tthe\\tdimensionality\\treduction\\talgorithm\\twill\\tmerge\\nthem\\tinto\\tone\\tfeature\\tthat\\trepresents\\tthe\\tcar’s\\twear\\tand\\ttear.\\tThis\\tis\\t\\ncalled', 'called\\t\\nfeature\\textraction\\n.', 'TIP\\nIt\\tis\\toften\\ta\\tgood\\tidea\\tto\\ttry\\tto\\treduce\\tthe\\tdimension\\tof\\tyour\\ttraining\\tdata\\tusing\\ta\\tdimensionality\\treduction\\talgorithm\\tbefore\\tyou\\nfeed\\tit\\tto\\tanother\\tMachine\\tLearning\\talgorithm\\t(such\\tas\\ta\\tsupervised\\tlearning\\talgorithm).\\tIt\\twill\\trun\\tmuch\\tfaster,\\tthe\\tdata\\twill\\ttake\\nup\\tless\\tdisk\\tand\\tmemory\\tspace,\\tand\\tin\\tsome\\tcases\\tit\\tmay\\talso\\tperform\\tbetter.\\nYet\\tanother\\timportant\\t\\nunsupervised\\ttask\\tis\\t\\nanomaly\\tdetection\\n\\t—\\tfor\\texample,\\tdetecting\\tunusual\\tcredit\\ncard\\ttransactions\\tto\\tprevent\\tfraud,\\tcatching\\tmanufacturing\\tdefects,\\tor\\tautomatically\\tremoving\\toutliers\\nfrom\\ta\\tdataset\\tbefore\\tfeeding\\tit\\tto\\tanother\\tlearning\\talgorithm.\\tThe\\tsystem\\tis\\ttrained\\twith\\tnormal\\ninstances,\\tand\\twhen\\tit\\tsees\\ta\\tnew\\tinstance\\tit\\tcan\\ttell\\twhether\\tit\\tlooks\\tlike\\ta\\tnormal\\tone\\tor\\twhether\\tit\\tis\\nlikely\\tan\\tanomaly\\t(see\\t\\nFigure\\t1-10\\n).\\nFigure\\t1-10.\\t\\nAnomaly\\tdetection\\nFinally,\\tanother\\tcommon\\t\\nunsupervised\\ttask\\tis\\t\\nassociation\\trule\\tlearning\\n,\\tin\\twhich\\tthe\\tgoal\\tis\\tto\\tdig\\tinto', 'large\\tamounts\\tof\\tdata\\tand\\tdiscover\\tinteresting\\trelations\\tbetween\\tattributes.\\tFor\\texample,\\tsuppose\\tyou\\nown\\ta\\tsupermarket.\\tRunning\\tan\\tassociation\\trule\\ton\\tyour\\tsales\\tlogs\\tmay\\treveal\\tthat\\tpeople\\twho\\tpurchase\\nbarbecue\\tsauce\\tand\\tpotato\\tchips\\talso\\ttend\\tto\\tbuy\\tsteak.\\tThus,\\tyou\\tmay\\twant\\tto\\tplace\\tthese\\titems\\tclose\\tto\\neach\\t\\nother.\\nSemisupervised\\tlearning\\nSome\\t\\nalgorithms\\tcan\\tdeal\\twith\\tpartially\\tlabeled\\ttraining\\tdata,\\tusually\\ta\\tlot\\tof\\tunlabeled\\tdata\\tand\\ta\\tlittle\\nbit\\tof\\tlabeled\\tdata.\\tThis\\tis\\tcalled\\t\\nsemisupervised\\tlearning\\n\\t(\\nFigure\\t1-11\\n).\\nSome\\t\\nphoto-hosting\\tservices,\\tsuch\\tas\\t\\nGoogle\\tPhotos,\\tare\\tgood\\texamples\\tof\\tthis.\\tOnce\\tyou\\tupload\\tall\\tyour\\nfamily\\tphotos\\tto\\tthe\\tservice,\\tit\\tautomatically\\trecognizes\\tthat\\tthe\\tsame\\tperson\\tA\\tshows\\tup\\tin\\tphotos\\t1,\\t5,\\nand\\t11,\\twhile\\tanother\\tperson\\tB\\tshows\\tup\\tin\\tphotos\\t2,\\t5,\\tand\\t7.\\tThis\\tis\\tthe\\tunsupervised\\tpart\\tof\\tthe\\nalgorithm\\t(clustering).\\tNow\\tall\\tthe\\tsystem\\tneeds\\tis\\tfor\\tyou\\tto\\ttell\\tit\\twho\\tthese\\tpeople\\tare.\\tJust\\tone\\tlabel\\nper\\tperson,\\n4', '4\\n\\tand\\tit\\tis\\table\\tto\\tname\\teveryone\\tin\\tevery\\tphoto,\\twhich\\tis\\tuseful\\tfor\\tsearching\\tphotos.', 'Figure\\t1-11.\\t\\nSemisupervised\\tlearning\\nMost\\tsemisupervised\\tlearning\\talgorithms\\tare\\tcombinations\\tof\\tunsupervised\\tand\\tsupervised\\talgorithms.\\nFor\\texample,\\t\\ndeep\\tbelief\\tnetworks\\n\\t\\n(DBNs)\\tare\\tbased\\ton\\tunsupervised\\tcomponents\\tcalled\\t\\nrestricted\\nBoltzmann\\tmachines\\n\\t(RBMs)\\t\\nstacked\\ton\\ttop\\tof\\tone\\tanother.\\tRBMs\\tare\\ttrained\\tsequentially\\tin\\tan\\nunsupervised\\tmanner,\\tand\\tthen\\tthe\\twhole\\tsystem\\tis\\tfine-tuned\\tusing\\tsupervised\\tlearning\\ttechniques.\\nReinforcement\\tLearning\\nReinforcement\\tLearning\\n\\tis\\ta\\t\\nvery\\tdifferent\\tbeast.\\tThe\\tlearning\\tsystem,\\tcalled\\tan\\t\\nagent\\n\\tin\\tthis\\tcontext,\\ncan\\tobserve\\tthe\\tenvironment,\\tselect\\tand\\tperform\\tactions,\\tand\\tget\\t\\nrewards\\n\\tin\\treturn\\t(or\\t\\npenalties\\n\\tin\\tthe\\nform\\tof\\tnegative\\trewards,\\tas\\tin\\t\\nFigure\\t1-12\\n).\\tIt\\tmust\\tthen\\tlearn\\tby\\titself\\twhat\\tis\\tthe\\tbest\\tstrategy,\\tcalled\\ta\\npolicy\\n,\\tto\\tget\\tthe\\tmost\\treward\\tover\\ttime.\\tA\\tpolicy\\tdefines\\twhat\\taction\\tthe\\tagent\\tshould\\tchoose\\twhen\\tit\\tis\\nin\\ta\\tgiven\\tsituation.', 'Figure\\t1-12.\\t\\nReinforcement\\tLearning\\nFor\\texample,\\tmany\\trobots\\timplement\\tReinforcement\\tLearning\\talgorithms\\tto\\tlearn\\thow\\tto\\twalk.\\nDeepMind’s\\tAlphaGo\\t\\nprogram\\tis\\talso\\ta\\tgood\\texample\\tof\\tReinforcement\\tLearning:\\tit\\tmade\\tthe\\theadlines\\nin\\tMarch\\t2016\\twhen\\tit\\tbeat\\tthe\\tworld\\tchampion\\tLee\\tSedol\\tat\\tthe\\tgame\\tof\\t\\nGo\\n.\\tIt\\tlearned\\tits\\twinning\\npolicy\\tby\\tanalyzing\\tmillions\\tof\\tgames,\\tand\\tthen\\tplaying\\tmany\\tgames\\tagainst\\titself.\\tNote\\tthat\\tlearning\\twas\\nturned\\toff\\tduring\\tthe\\tgames\\tagainst\\tthe\\tchampion;\\tAlphaGo\\twas\\tjust\\tapplying\\tthe\\tpolicy\\tit\\thad\\t\\nlearned.', 'Batch\\tand\\tOnline\\tLearning\\nAnother\\t\\ncriterion\\tused\\tto\\tclassify\\tMachine\\tLearning\\tsystems\\tis\\twhether\\tor\\tnot\\tthe\\tsystem\\tcan\\tlearn\\nincrementally\\tfrom\\ta\\tstream\\tof\\tincoming\\tdata.\\nBatch\\tlearning\\nIn\\t\\nbatch\\tlearning\\n,\\tthe\\tsystem\\tis\\tincapable\\tof\\tlearning\\tincrementally:\\tit\\tmust\\tbe\\ttrained\\tusing\\tall\\tthe\\navailable\\tdata.\\tThis\\twill\\tgenerally\\ttake\\ta\\tlot\\tof\\ttime\\tand\\tcomputing\\tresources,\\tso\\tit\\tis\\ttypically\\tdone\\noffline.\\tFirst\\tthe\\tsystem\\tis\\ttrained,\\tand\\tthen\\tit\\tis\\tlaunched\\tinto\\tproduction\\tand\\truns\\twithout\\tlearning\\nanymore;\\tit\\tjust\\tapplies\\twhat\\tit\\thas\\tlearned.\\tThis\\tis\\t\\ncalled\\t\\noffline\\tlearning\\n.\\nIf\\tyou\\twant\\ta\\tbatch\\tlearning\\tsystem\\tto\\tknow\\tabout\\tnew\\tdata\\t(such\\tas\\ta\\tnew\\ttype\\tof\\tspam),\\tyou\\tneed\\tto\\ntrain\\ta\\tnew\\tversion\\tof\\tthe\\tsystem\\tfrom\\tscratch\\ton\\tthe\\tfull\\tdataset\\t(not\\tjust\\tthe\\tnew\\tdata,\\tbut\\talso\\tthe\\told\\ndata),\\tthen\\tstop\\tthe\\told\\tsystem\\tand\\treplace\\tit\\twith\\tthe\\tnew\\tone.\\nFortunately,\\tthe\\twhole\\tprocess\\tof\\ttraining,\\tevaluating,\\tand\\tlaunching\\ta\\tMachine\\tLearning\\tsystem\\tcan\\tbe\\nautomated\\tfairly\\teasily\\t(as\\tshown\\tin', 'Figure\\t1-3\\n),\\tso\\teven\\ta\\tbatch\\tlearning\\tsystem\\tcan\\tadapt\\tto\\tchange.\\nSimply\\tupdate\\tthe\\tdata\\tand\\ttrain\\ta\\tnew\\tversion\\tof\\tthe\\tsystem\\tfrom\\tscratch\\tas\\toften\\tas\\tneeded.\\nThis\\tsolution\\tis\\tsimple\\tand\\toften\\tworks\\tfine,\\tbut\\ttraining\\tusing\\tthe\\tfull\\tset\\tof\\tdata\\tcan\\ttake\\tmany\\thours,\\tso\\nyou\\twould\\ttypically\\ttrain\\ta\\tnew\\tsystem\\tonly\\tevery\\t24\\thours\\tor\\teven\\tjust\\tweekly.\\tIf\\tyour\\tsystem\\tneeds\\tto\\nadapt\\tto\\trapidly\\tchanging\\tdata\\t(e.g.,\\tto\\tpredict\\tstock\\tprices),\\tthen\\tyou\\tneed\\ta\\tmore\\treactive\\tsolution.\\nAlso,\\ttraining\\ton\\tthe\\tfull\\tset\\tof\\tdata\\trequires\\ta\\tlot\\tof\\tcomputing\\tresources\\t(CPU,\\tmemory\\tspace,\\tdisk\\nspace,\\tdisk\\tI/O,\\tnetwork\\tI/O,\\tetc.).\\tIf\\tyou\\thave\\ta\\tlot\\tof\\tdata\\tand\\tyou\\tautomate\\tyour\\tsystem\\tto\\ttrain\\tfrom\\nscratch\\tevery\\tday,\\tit\\twill\\tend\\tup\\tcosting\\tyou\\ta\\tlot\\tof\\tmoney.\\tIf\\tthe\\tamount\\tof\\tdata\\tis\\thuge,\\tit\\tmay\\teven\\tbe\\nimpossible\\tto\\tuse\\ta\\tbatch\\tlearning\\talgorithm.\\nFinally,\\tif\\tyour\\tsystem\\tneeds\\tto\\tbe\\table\\tto\\tlearn\\tautonomously\\tand\\tit\\thas\\tlimited\\tresources\\t(e.g.,\\ta', 'smartphone\\tapplication\\tor\\ta\\trover\\ton\\tMars),\\tthen\\tcarrying\\taround\\tlarge\\tamounts\\tof\\ttraining\\tdata\\tand\\ntaking\\tup\\ta\\tlot\\tof\\tresources\\tto\\ttrain\\tfor\\thours\\tevery\\tday\\tis\\ta\\tshowstopper.\\nFortunately,\\ta\\tbetter\\toption\\tin\\tall\\tthese\\tcases\\tis\\tto\\tuse\\talgorithms\\tthat\\tare\\tcapable\\tof\\tlearning\\nincrementally.\\nOnline\\tlearning\\nIn\\t\\nonline\\tlearning\\n,\\t\\nyou\\ttrain\\tthe\\tsystem\\tincrementally\\tby\\tfeeding\\tit\\tdata\\tinstances\\tsequentially,\\teither\\nindividually\\tor\\tby\\tsmall\\tgroups\\t\\ncalled\\t\\nmini-batches\\n.\\tEach\\tlearning\\tstep\\tis\\tfast\\tand\\tcheap,\\tso\\tthe\\tsystem\\ncan\\tlearn\\tabout\\tnew\\tdata\\ton\\tthe\\tfly,\\tas\\tit\\tarrives\\t(see\\t\\nFigure\\t1-13\\n).', 'Figure\\t1-13.\\t\\nOnline\\tlearning\\nOnline\\tlearning\\tis\\tgreat\\tfor\\tsystems\\tthat\\treceive\\tdata\\tas\\ta\\tcontinuous\\tflow\\t(e.g.,\\tstock\\tprices)\\tand\\tneed\\tto\\nadapt\\tto\\tchange\\trapidly\\tor\\tautonomously.\\tIt\\tis\\talso\\ta\\tgood\\toption\\tif\\tyou\\thave\\tlimited\\tcomputing\\tresources:\\nonce\\tan\\tonline\\tlearning\\tsystem\\thas\\tlearned\\tabout\\tnew\\tdata\\tinstances,\\tit\\tdoes\\tnot\\tneed\\tthem\\tanymore,\\tso\\nyou\\tcan\\tdiscard\\tthem\\t(unless\\tyou\\twant\\tto\\tbe\\table\\tto\\troll\\tback\\tto\\ta\\tprevious\\tstate\\tand\\t“replay”\\tthe\\tdata).\\nThis\\tcan\\tsave\\ta\\thuge\\tamount\\tof\\tspace.\\nOnline\\tlearning\\talgorithms\\tcan\\talso\\tbe\\tused\\tto\\ttrain\\tsystems\\ton\\thuge\\tdatasets\\tthat\\tcannot\\tfit\\tin\\tone\\nmachine’s\\tmain\\tmemory\\t(this\\tis\\tcalled\\t\\nout-of-core\\n\\t\\nlearning).\\tThe\\talgorithm\\tloads\\tpart\\tof\\tthe\\tdata,\\truns\\ta\\ntraining\\tstep\\ton\\tthat\\tdata,\\tand\\trepeats\\tthe\\tprocess\\tuntil\\tit\\thas\\trun\\ton\\tall\\tof\\tthe\\tdata\\t(see\\t\\nFigure\\t1-14\\n).\\nWARNING\\nThis\\twhole\\tprocess\\tis\\tusually\\tdone\\toffline\\t(i.e.,\\tnot\\ton\\tthe\\tlive\\tsystem),\\tso\\t\\nonline\\tlearning\\n\\tcan\\tbe\\ta\\tconfusing\\tname.\\tThink\\tof\\tit\\nas\\t\\nincremental\\tlearning\\n.', 'Figure\\t1-14.\\t\\nUsing\\tonline\\tlearning\\tto\\thandle\\thuge\\tdatasets\\nOne\\timportant\\tparameter\\tof\\tonline\\tlearning\\tsystems\\tis\\thow\\tfast\\tthey\\tshould\\tadapt\\tto\\tchanging\\tdata:\\tthis\\tis\\ncalled\\t\\nthe\\t\\nlearning\\trate\\n.\\tIf\\tyou\\tset\\ta\\thigh\\tlearning\\trate,\\tthen\\tyour\\tsystem\\twill\\trapidly\\tadapt\\tto\\tnew\\tdata,\\nbut\\tit\\twill\\talso\\ttend\\tto\\tquickly\\tforget\\tthe\\told\\tdata\\t(you\\tdon’t\\twant\\ta\\tspam\\tfilter\\tto\\tflag\\tonly\\tthe\\tlatest\\tkinds\\nof\\tspam\\tit\\twas\\tshown).\\tConversely,\\tif\\tyou\\tset\\ta\\tlow\\tlearning\\trate,\\tthe\\tsystem\\twill\\thave\\tmore\\tinertia;\\tthat\\nis,\\tit\\twill\\tlearn\\tmore\\tslowly,\\tbut\\tit\\twill\\talso\\tbe\\tless\\tsensitive\\tto\\tnoise\\tin\\tthe\\tnew\\tdata\\tor\\tto\\tsequences\\tof\\nnonrepresentative\\tdata\\tpoints.\\nA\\tbig\\tchallenge\\twith\\tonline\\tlearning\\tis\\tthat\\tif\\tbad\\tdata\\tis\\tfed\\tto\\tthe\\tsystem,\\tthe\\tsystem’s\\tperformance\\twill\\ngradually\\tdecline.\\tIf\\twe\\tare\\ttalking\\tabout\\ta\\tlive\\tsystem,\\tyour\\tclients\\twill\\tnotice.\\tFor\\texample,\\tbad\\tdata\\ncould\\tcome\\tfrom\\ta\\tmalfunctioning\\tsensor\\ton\\ta\\trobot,\\tor\\tfrom\\tsomeone\\tspamming\\ta\\tsearch\\tengine\\tto\\ttry\\tto', 'rank\\thigh\\tin\\tsearch\\tresults.\\tTo\\treduce\\tthis\\trisk,\\tyou\\tneed\\tto\\tmonitor\\tyour\\tsystem\\tclosely\\tand\\tpromptly\\nswitch\\tlearning\\toff\\t(and\\tpossibly\\trevert\\tto\\ta\\tpreviously\\tworking\\tstate)\\tif\\tyou\\tdetect\\ta\\tdrop\\tin\\nperformance.\\tYou\\tmay\\talso\\twant\\tto\\tmonitor\\tthe\\tinput\\tdata\\tand\\treact\\tto\\tabnormal\\t\\ndata\\t(e.g.,\\tusing\\tan\\nanomaly\\tdetection\\talgorithm).', 'Instance-Based\\tVersus\\tModel-Based\\tLearning\\nOne\\t\\nmore\\tway\\tto\\tcategorize\\tMachine\\tLearning\\tsystems\\tis\\tby\\thow\\tthey\\t\\ngeneralize\\n.\\tMost\\tMachine\\nLearning\\ttasks\\tare\\tabout\\tmaking\\tpredictions.\\tThis\\tmeans\\tthat\\tgiven\\ta\\tnumber\\tof\\ttraining\\texamples,\\tthe\\nsystem\\tneeds\\tto\\tbe\\table\\tto\\tgeneralize\\tto\\texamples\\tit\\thas\\tnever\\tseen\\tbefore.\\tHaving\\ta\\tgood\\tperformance\\nmeasure\\ton\\tthe\\ttraining\\tdata\\tis\\tgood,\\tbut\\tinsufficient;\\tthe\\ttrue\\tgoal\\tis\\tto\\tperform\\twell\\ton\\tnew\\tinstances.\\nThere\\tare\\ttwo\\tmain\\tapproaches\\tto\\tgeneralization:\\tinstance-based\\tlearning\\tand\\tmodel-based\\tlearning.\\nInstance-based\\tlearning\\nPossibly\\t\\nthe\\tmost\\ttrivial\\tform\\tof\\tlearning\\tis\\tsimply\\tto\\tlearn\\tby\\theart.\\tIf\\tyou\\twere\\tto\\tcreate\\ta\\tspam\\tfilter\\nthis\\tway,\\tit\\twould\\tjust\\tflag\\tall\\temails\\tthat\\tare\\tidentical\\tto\\temails\\tthat\\thave\\talready\\tbeen\\tflagged\\tby\\tusers\\n—\\tnot\\tthe\\tworst\\tsolution,\\tbut\\tcertainly\\tnot\\tthe\\tbest.\\nInstead\\tof\\tjust\\tflagging\\temails\\tthat\\tare\\tidentical\\tto\\tknown\\tspam\\temails,\\tyour\\tspam\\tfilter\\tcould\\tbe', 'programmed\\tto\\talso\\tflag\\temails\\tthat\\tare\\tvery\\tsimilar\\tto\\tknown\\tspam\\temails.\\tThis\\trequires\\t\\na\\t\\nmeasure\\tof\\nsimilarity\\n\\tbetween\\ttwo\\temails.\\tA\\t(very\\tbasic)\\tsimilarity\\tmeasure\\tbetween\\ttwo\\temails\\tcould\\tbe\\tto\\tcount\\nthe\\tnumber\\tof\\twords\\tthey\\thave\\tin\\tcommon.\\tThe\\tsystem\\twould\\tflag\\tan\\temail\\tas\\tspam\\tif\\tit\\thas\\tmany\\twords\\nin\\tcommon\\twith\\ta\\tknown\\tspam\\temail.\\nThis\\tis\\tcalled\\t\\ninstance-based\\tlearning\\n:\\tthe\\tsystem\\tlearns\\tthe\\texamples\\tby\\theart,\\tthen\\tgeneralizes\\tto\\tnew\\ncases\\tusing\\ta\\tsimilarity\\tmeasure\\t(\\nFigure\\t1-15\\n).\\nFigure\\t1-15.\\t\\nInstance-based\\tlearning\\nModel-based\\tlearning\\nAnother\\t\\nway\\tto\\tgeneralize\\tfrom\\ta\\tset\\tof\\texamples\\tis\\tto\\tbuild\\ta\\tmodel\\tof\\tthese\\texamples,\\tthen\\tuse\\tthat\\nmodel\\tto\\tmake\\t\\npredictions\\n.\\tThis\\tis\\tcalled\\t\\nmodel-based\\tlearning\\n\\t(\\nFigure\\t1-16\\n).', 'Figure\\t1-16.\\t\\nModel-based\\tlearning\\nFor\\texample,\\t\\nsuppose\\tyou\\twant\\tto\\tknow\\tif\\tmoney\\tmakes\\tpeople\\thappy,\\tso\\tyou\\tdownload\\tthe\\t\\nBetter\\tLife\\nIndex\\n\\tdata\\tfrom\\tthe\\t\\nOECD’s\\twebsite\\n\\tas\\twell\\tas\\tstats\\tabout\\tGDP\\tper\\tcapita\\tfrom\\tthe\\t\\nIMF’s\\twebsite\\n.\\tThen\\nyou\\tjoin\\tthe\\ttables\\tand\\tsort\\tby\\tGDP\\tper\\tcapita.\\t\\nTable\\t1-1\\n\\tshows\\tan\\texcerpt\\tof\\twhat\\tyou\\tget.\\nTable\\t1-1.\\t\\nDoes\\tmoney\\tmake\\tpeople\\nhappier?\\nCountry\\nGDP\\tper\\tcapita\\t(USD)\\nLife\\tsatisfaction\\nHungary\\n12,240\\n4.9\\nKorea\\n27,195\\n5.8\\nFrance\\n37,675\\n6.5\\nAustralia\\n50,962\\n7.3\\nUnited\\tStates\\n55,805\\n7.2\\nLet’s\\tplot\\tthe\\tdata\\tfor\\ta\\tfew\\trandom\\tcountries\\t(\\nFigure\\t1-17\\n).', 'Figure\\t1-17.\\t\\nDo\\tyou\\tsee\\ta\\ttrend\\there?\\nThere\\tdoes\\tseem\\tto\\tbe\\ta\\ttrend\\there!\\tAlthough\\tthe\\tdata\\tis\\t\\nnoisy\\n\\t(i.e.,\\tpartly\\trandom),\\tit\\tlooks\\tlike\\tlife\\nsatisfaction\\tgoes\\tup\\tmore\\tor\\tless\\tlinearly\\tas\\tthe\\tcountry’s\\tGDP\\tper\\tcapita\\tincreases.\\tSo\\tyou\\tdecide\\tto\\nmodel\\tlife\\tsatisfaction\\tas\\ta\\tlinear\\tfunction\\tof\\tGDP\\tper\\tcapita.\\tThis\\tstep\\tis\\tcalled\\t\\nmodel\\tselection\\n:\\t\\nyou\\nselected\\ta\\t\\nlinear\\tmodel\\n\\tof\\tlife\\tsatisfaction\\twith\\tjust\\tone\\tattribute,\\tGDP\\tper\\tcapita\\t(\\nEquation\\t1-1\\n).\\nEquation\\t1-1.\\t\\nA\\tsimple\\tlinear\\tmodel\\nThis\\tmodel\\thas\\t\\ntwo\\t\\nmodel\\tparameters\\n,\\t\\nθ\\n0\\n\\tand\\t\\nθ\\n1\\n.\\n5\\n\\tBy\\ttweaking\\tthese\\tparameters,\\tyou\\tcan\\tmake\\tyour\\nmodel\\trepresent\\tany\\tlinear\\tfunction,\\tas\\tshown\\tin\\t\\nFigure\\t1-18\\n.\\nFigure\\t1-18.\\t\\nA\\tfew\\tpossible\\tlinear\\tmodels\\nBefore\\tyou\\tcan\\tuse\\tyour\\tmodel,\\tyou\\tneed\\tto\\tdefine\\tthe\\tparameter\\tvalues\\t\\nθ\\n0\\n\\tand\\t\\nθ\\n1\\n.\\tHow\\tcan\\tyou\\tknow', 'which\\tvalues\\twill\\tmake\\tyour\\tmodel\\tperform\\tbest?\\tTo\\tanswer\\tthis\\tquestion,\\tyou\\tneed\\tto\\tspecify\\ta\\nperformance\\tmeasure.\\tYou\\tcan\\teither\\tdefine\\ta\\t\\nutility\\tfunction\\n\\t(or\\t\\nfitness\\tfunction\\n)\\t\\nthat\\t\\nmeasures\\thow\\ngood\\n\\tyour\\tmodel\\tis,\\tor\\tyou\\tcan\\tdefine\\ta\\t\\ncost\\tfunction\\n\\t\\nthat\\tmeasures\\thow\\t\\nbad\\n\\tit\\tis.\\tFor\\tlinear\\tregression\\nproblems,\\tpeople\\ttypically\\tuse\\ta\\tcost\\tfunction\\tthat\\tmeasures\\tthe\\tdistance\\tbetween\\tthe\\tlinear\\tmodel’s\\npredictions\\tand\\tthe\\ttraining\\texamples;\\tthe\\tobjective\\tis\\tto\\tminimize\\tthis\\tdistance.\\nThis\\tis\\twhere\\tthe\\t\\nLinear\\tRegression\\talgorithm\\tcomes\\tin:\\tyou\\tfeed\\tit\\tyour\\ttraining\\texamples\\tand\\tit\\tfinds\\nthe\\tparameters\\tthat\\tmake\\tthe\\tlinear\\tmodel\\tfit\\tbest\\tto\\tyour\\tdata.\\tThis\\tis\\tcalled\\t\\ntraining\\n\\t\\nthe\\tmodel.\\tIn\\tour\\ncase\\tthe\\talgorithm\\tfinds\\tthat\\tthe\\toptimal\\tparameter\\tvalues\\tare\\t\\nθ\\n0\\n\\t=\\t4.85\\tand\\t\\nθ\\n1\\n\\t=\\t4.91\\t×\\t10\\n–5\\n.\\nNow\\tthe\\tmodel\\tfits\\tthe\\ttraining\\tdata\\tas\\tclosely\\tas\\tpossible\\t(for\\ta\\tlinear\\tmodel),\\tas\\tyou\\tcan\\tsee\\tin\\nFigure\\t1-19\\n.\\nFigure\\t1-19.\\t\\nThe\\tlinear\\tmodel\\tthat\\tfits\\tthe\\ttraining\\tdata\\tbest', 'You\\tare\\tfinally\\tready\\tto\\trun\\tthe\\tmodel\\tto\\tmake\\tpredictions.\\tFor\\texample,\\tsay\\tyou\\twant\\tto\\tknow\\thow\\nhappy\\tCypriots\\tare,\\tand\\tthe\\tOECD\\tdata\\tdoes\\tnot\\thave\\tthe\\tanswer.\\tFortunately,\\tyou\\tcan\\tuse\\tyour\\tmodel\\tto\\nmake\\ta\\tgood\\tprediction:\\tyou\\tlook\\tup\\tCyprus’s\\tGDP\\tper\\tcapita,\\tfind\\t$22,587,\\tand\\tthen\\tapply\\tyour\\tmodel\\nand\\tfind\\tthat\\tlife\\tsatisfaction\\tis\\tlikely\\tto\\tbe\\tsomewhere\\taround\\t4.85\\t+\\t22,587\\t×\\t4.91\\t×\\t10\\n-5\\n\\t=\\t5.96.\\nTo\\twhet\\tyour\\tappetite,\\t\\nExample\\t1-1\\n\\tshows\\tthe\\tPython\\tcode\\tthat\\tloads\\tthe\\tdata,\\tprepares\\tit,\\n6\\n\\tcreates\\ta\\nscatterplot\\tfor\\tvisualization,\\tand\\tthen\\ttrains\\ta\\tlinear\\tmodel\\tand\\tmakes\\ta\\t\\nprediction.\\n7\\nExample\\t1-1.\\t\\nTraining\\tand\\trunning\\ta\\tlinear\\tmodel\\tusing\\tScikit-Learn\\nimport\\n\\t\\nmatplotlib\\nimport\\n\\t\\nmatplotlib.pyplot\\n\\t\\nas\\n\\t\\nplt\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nimport\\n\\t\\npandas\\n\\t\\nas\\n\\t\\npd\\nimport\\n\\t\\nsklearn\\n#\\tLoad\\tthe\\tdata\\noecd_bli\\n\\t\\n=\\n\\t\\npd\\n.\\nread_csv\\n(\\n\"oecd_bli_2015.csv\"\\n,\\n\\t\\nthousands\\n=\\n\\',\\'\\n)\\ngdp_per_capita\\n\\t\\n=\\n\\t\\npd\\n.\\nread_csv\\n(\\n\"gdp_per_capita.csv\"\\n,\\nthousands\\n=\\n\\',\\'\\n,\\ndelimiter\\n=\\n\\'\\n\\\\t\\n\\'\\n,', '\\'\\n\\\\t\\n\\'\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nencoding\\n=\\n\\'latin1\\'\\n,\\n\\t\\nna_values\\n=\\n\"n/a\"\\n)\\n#\\tPrepare\\tthe\\tdata\\ncountry_stats\\n\\t\\n=\\n\\t\\nprepare_country_stats\\n(\\noecd_bli\\n,\\n\\t\\ngdp_per_capita\\n)\\nX\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\ncountry_stats\\n[\\n\"GDP\\tper\\tcapita\"\\n]]', 'y\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\ncountry_stats\\n[\\n\"Life\\tsatisfaction\"\\n]]\\n#\\tVisualize\\tthe\\tdata\\ncountry_stats\\n.\\nplot\\n(\\nkind\\n=\\n\\'scatter\\'\\n,\\n\\t\\nx\\n=\\n\"GDP\\tper\\tcapita\"\\n,\\n\\t\\ny\\n=\\n\\'Life\\tsatisfaction\\'\\n)\\nplt\\n.\\nshow\\n()\\n#\\tSelect\\ta\\tlinear\\tmodel\\nmodel\\n\\t\\n=\\n\\t\\nsklearn\\n.\\nlinear_model\\n.\\nLinearRegression\\n()\\n#\\tTrain\\tthe\\tmodel\\nmodel\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\n#\\tMake\\ta\\tprediction\\tfor\\tCyprus\\nX_new\\n\\t\\n=\\n\\t\\n[[\\n22587\\n]]\\n\\t\\t\\n#\\tCyprus\\'\\tGDP\\tper\\tcapita\\nprint\\n(\\nmodel\\n.\\npredict\\n(\\nX_new\\n))\\n\\t\\n#\\toutputs\\t[[\\t5.96242338]]\\nNOTE\\nIf\\tyou\\thad\\tused\\t\\nan\\tinstance-based\\tlearning\\talgorithm\\tinstead,\\tyou\\twould\\thave\\tfound\\tthat\\tSlovenia\\thas\\tthe\\tclosest\\tGDP\\tper\\tcapita\\nto\\tthat\\tof\\tCyprus\\t($20,732),\\tand\\tsince\\tthe\\tOECD\\tdata\\ttells\\tus\\tthat\\tSlovenians’\\tlife\\tsatisfaction\\tis\\t5.7,\\tyou\\twould\\thave\\tpredicted\\ta\\nlife\\tsatisfaction\\tof\\t5.7\\tfor\\tCyprus.\\tIf\\tyou\\tzoom\\tout\\ta\\tbit\\tand\\tlook\\tat\\tthe\\ttwo\\tnext\\tclosest\\tcountries,\\tyou\\twill\\tfind\\tPortugal\\tand\\nSpain\\twith\\tlife\\tsatisfactions\\tof\\t5.1\\tand\\t6.5,\\trespectively.\\tAveraging\\tthese\\tthree\\tvalues,\\tyou\\tget\\t5.77,\\twhich\\tis\\tpretty\\tclose\\tto\\tyour', 'model-based\\tprediction.\\tThis\\tsimple\\talgorithm\\tis\\tcalled\\t\\nk-Nearest\\tNeighbors\\n\\t\\nregression\\t(in\\tthis\\texample,\\t\\nk\\n\\t=\\t3).\\nReplacing\\tthe\\tLinear\\tRegression\\tmodel\\twith\\tk-Nearest\\tNeighbors\\tregression\\tin\\tthe\\tprevious\\tcode\\tis\\tas\\tsimple\\tas\\treplacing\\tthis\\nline:\\nmodel\\n\\t\\n=\\n\\t\\nsklearn\\n.\\nlinear_model\\n.\\nLinearRegression\\n()\\nwith\\tthis\\t\\none:\\nmodel\\n\\t\\n=\\n\\t\\nsklearn\\n.\\nneighbors\\n.\\nKNeighborsRegressor\\n(\\nn_neighbors\\n=\\n3\\n)\\nIf\\t\\nall\\twent\\twell,\\tyour\\tmodel\\twill\\tmake\\tgood\\tpredictions.\\tIf\\tnot,\\tyou\\tmay\\tneed\\tto\\tuse\\tmore\\tattributes\\n(employment\\trate,\\thealth,\\tair\\tpollution,\\tetc.),\\tget\\tmore\\tor\\tbetter\\tquality\\ttraining\\tdata,\\tor\\tperhaps\\tselect\\ta\\nmore\\tpowerful\\tmodel\\t\\n(e.g.,\\ta\\tPolynomial\\tRegression\\tmodel).\\nIn\\tsummary:\\nYou\\tstudied\\tthe\\tdata.\\nYou\\tselected\\ta\\tmodel.\\nYou\\ttrained\\tit\\ton\\tthe\\ttraining\\tdata\\t(i.e.,\\tthe\\tlearning\\talgorithm\\tsearched\\tfor\\tthe\\tmodel\\tparameter\\nvalues\\tthat\\tminimize\\ta\\tcost\\tfunction).\\nFinally,\\tyou\\tapplied\\tthe\\tmodel\\tto\\tmake\\tpredictions\\ton\\tnew\\tcases\\t(this\\tis\\tcalled\\t\\ninference\\n),\\t\\nhoping', 'hoping\\nthat\\tthis\\tmodel\\twill\\tgeneralize\\twell.\\nThis\\tis\\twhat\\ta\\ttypical\\tMachine\\tLearning\\tproject\\tlooks\\tlike.\\tIn\\t\\nChapter\\t2\\n\\tyou\\twill\\texperience\\tthis\\tfirst-\\nhand\\tby\\tgoing\\tthrough\\tan\\tend-to-end\\tproject.\\nWe\\thave\\tcovered\\ta\\tlot\\tof\\tground\\tso\\tfar:\\tyou\\tnow\\tknow\\twhat\\tMachine\\tLearning\\tis\\treally\\tabout,\\twhy\\tit\\tis\\nuseful,\\twhat\\tsome\\tof\\tthe\\tmost\\tcommon\\tcategories\\tof\\tML\\tsystems\\tare,\\tand\\twhat\\ta\\ttypical\\tproject\\tworkflow\\nlooks\\tlike.\\tNow\\tlet’s\\tlook\\tat\\twhat\\tcan\\tgo\\twrong\\tin\\tlearning\\tand\\tprevent\\tyou\\tfrom\\tmaking\\taccurate', 'predictions.', 'Main\\tChallenges\\tof\\tMachine\\tLearning\\nIn\\t\\nshort,\\tsince\\tyour\\tmain\\ttask\\tis\\tto\\tselect\\ta\\tlearning\\talgorithm\\tand\\ttrain\\tit\\ton\\tsome\\tdata,\\tthe\\ttwo\\tthings\\tthat\\ncan\\tgo\\twrong\\tare\\t“bad\\talgorithm”\\tand\\t“bad\\tdata.”\\tLet’s\\tstart\\twith\\texamples\\tof\\tbad\\tdata.', 'Insufficient\\tQuantity\\tof\\tTraining\\tData\\nFor\\t\\na\\ttoddler\\tto\\tlearn\\twhat\\tan\\tapple\\tis,\\tall\\tit\\ttakes\\tis\\tfor\\tyou\\tto\\tpoint\\tto\\tan\\tapple\\tand\\tsay\\t“apple”\\n(possibly\\trepeating\\tthis\\tprocedure\\ta\\tfew\\ttimes).\\tNow\\tthe\\tchild\\tis\\table\\tto\\trecognize\\tapples\\tin\\tall\\tsorts\\tof\\ncolors\\tand\\tshapes.\\tGenius.\\nMachine\\tLearning\\tis\\tnot\\tquite\\tthere\\tyet;\\tit\\ttakes\\ta\\tlot\\tof\\tdata\\tfor\\tmost\\tMachine\\tLearning\\talgorithms\\tto\\nwork\\tproperly.\\tEven\\tfor\\tvery\\tsimple\\tproblems\\tyou\\ttypically\\tneed\\tthousands\\tof\\texamples,\\tand\\tfor\\ncomplex\\tproblems\\tsuch\\tas\\timage\\tor\\tspeech\\trecognition\\tyou\\tmay\\tneed\\tmillions\\tof\\texamples\\t(unless\\tyou\\ncan\\treuse\\tparts\\tof\\tan\\texisting\\tmodel).', 'THE\\tUNREASONABLE\\tEFFECTIVENESS\\tOF\\tDATA\\nIn\\ta\\t\\nfamous\\tpaper\\n\\tpublished\\tin\\t2001,\\tMicrosoft\\tresearchers\\tMichele\\tBanko\\tand\\tEric\\tBrill\\tshowed\\tthat\\tvery\\tdifferent\\tMachine\\tLearning\\nalgorithms,\\tincluding\\tfairly\\tsimple\\tones,\\tperformed\\talmost\\tidentically\\twell\\ton\\ta\\tcomplex\\tproblem\\tof\\tnatural\\tlanguage\\tdisambiguation\\n8\\n\\tonce\\nthey\\twere\\tgiven\\tenough\\tdata\\t(as\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t1-20\\n).\\nFigure\\t1-20.\\t\\nThe\\timportance\\tof\\tdata\\tversus\\talgorithms\\n9\\nAs\\tthe\\tauthors\\tput\\tit:\\t“these\\tresults\\tsuggest\\tthat\\twe\\tmay\\twant\\tto\\treconsider\\tthe\\ttrade-off\\tbetween\\tspending\\ttime\\tand\\tmoney\\ton\\talgorithm\\ndevelopment\\tversus\\tspending\\tit\\ton\\tcorpus\\tdevelopment.”\\nThe\\tidea\\tthat\\tdata\\tmatters\\tmore\\tthan\\talgorithms\\tfor\\tcomplex\\tproblems\\twas\\tfurther\\tpopularized\\tby\\tPeter\\tNorvig\\tet\\tal.\\tin\\ta\\tpaper\\ttitled\\n“The\\tUnreasonable\\tEffectiveness\\tof\\tData”\\n\\tpublished\\tin\\t2009.\\n10\\n\\t\\nIt\\tshould\\tbe\\tnoted,\\thowever,\\tthat\\tsmall-\\tand\\tmedium-sized\\tdatasets\\tare', 'still\\tvery\\tcommon,\\tand\\tit\\tis\\tnot\\talways\\teasy\\tor\\tcheap\\tto\\tget\\textra\\ttraining\\tdata,\\tso\\tdon’t\\tabandon\\talgorithms\\tjust\\tyet.', 'Nonrepresentative\\tTraining\\tData\\nIn\\t\\norder\\tto\\tgeneralize\\twell,\\tit\\tis\\tcrucial\\tthat\\tyour\\ttraining\\tdata\\tbe\\trepresentative\\tof\\tthe\\tnew\\tcases\\tyou\\nwant\\tto\\tgeneralize\\tto.\\tThis\\tis\\ttrue\\twhether\\tyou\\tuse\\tinstance-based\\tlearning\\tor\\tmodel-based\\tlearning.\\nFor\\texample,\\tthe\\tset\\tof\\tcountries\\twe\\tused\\tearlier\\tfor\\ttraining\\tthe\\tlinear\\tmodel\\twas\\tnot\\tperfectly\\nrepresentative;\\ta\\tfew\\tcountries\\twere\\tmissing.\\t\\nFigure\\t1-21\\n\\tshows\\twhat\\tthe\\tdata\\tlooks\\tlike\\twhen\\tyou\\tadd\\nthe\\tmissing\\tcountries.\\nFigure\\t1-21.\\t\\nA\\tmore\\trepresentative\\ttraining\\tsample\\nIf\\tyou\\ttrain\\ta\\tlinear\\tmodel\\ton\\tthis\\tdata,\\tyou\\tget\\tthe\\tsolid\\tline,\\twhile\\tthe\\told\\tmodel\\tis\\trepresented\\tby\\tthe\\ndotted\\tline.\\tAs\\tyou\\tcan\\tsee,\\tnot\\tonly\\tdoes\\tadding\\ta\\tfew\\tmissing\\tcountries\\tsignificantly\\talter\\tthe\\tmodel,\\tbut\\nit\\tmakes\\tit\\tclear\\tthat\\tsuch\\ta\\tsimple\\tlinear\\tmodel\\tis\\tprobably\\tnever\\tgoing\\tto\\twork\\twell.\\tIt\\tseems\\tthat\\tvery\\nrich\\tcountries\\tare\\tnot\\thappier\\tthan\\tmoderately\\trich\\tcountries\\t(in\\tfact\\tthey\\tseem\\tunhappier),\\tand', 'conversely\\tsome\\tpoor\\tcountries\\tseem\\thappier\\tthan\\tmany\\trich\\tcountries.\\nBy\\tusing\\ta\\tnonrepresentative\\ttraining\\tset,\\twe\\ttrained\\ta\\tmodel\\tthat\\tis\\tunlikely\\tto\\tmake\\taccurate\\npredictions,\\tespecially\\tfor\\tvery\\tpoor\\tand\\tvery\\trich\\tcountries.\\nIt\\tis\\tcrucial\\tto\\tuse\\ta\\ttraining\\tset\\tthat\\tis\\trepresentative\\tof\\tthe\\tcases\\tyou\\twant\\tto\\tgeneralize\\tto.\\tThis\\tis\\toften\\nharder\\tthan\\tit\\tsounds:\\tif\\tthe\\tsample\\tis\\ttoo\\tsmall,\\tyou\\twill\\t\\nhave\\t\\nsampling\\tnoise\\n\\t(i.e.,\\tnonrepresentative\\ndata\\tas\\ta\\tresult\\tof\\tchance),\\tbut\\teven\\tvery\\tlarge\\tsamples\\tcan\\tbe\\tnonrepresentative\\tif\\tthe\\tsampling\\tmethod\\nis\\tflawed.\\tThis\\t\\nis\\tcalled\\t\\nsampling\\tbias\\n.', 'A\\tFAMOUS\\tEXAMPLE\\tOF\\tSAMPLING\\tBIAS\\nPerhaps\\tthe\\tmost\\tfamous\\texample\\tof\\tsampling\\tbias\\thappened\\tduring\\tthe\\tUS\\tpresidential\\telection\\tin\\t1936,\\twhich\\tpitted\\tLandon\\tagainst\\nRoosevelt:\\tthe\\t\\nLiterary\\tDigest\\n\\tconducted\\ta\\tvery\\tlarge\\tpoll,\\tsending\\tmail\\tto\\tabout\\t10\\tmillion\\tpeople.\\tIt\\tgot\\t2.4\\tmillion\\tanswers,\\tand\\npredicted\\twith\\thigh\\tconfidence\\tthat\\tLandon\\twould\\tget\\t57%\\tof\\tthe\\tvotes.\\tInstead,\\tRoosevelt\\twon\\twith\\t62%\\tof\\tthe\\tvotes.\\tThe\\tflaw\\twas\\tin\\nthe\\t\\nLiterary\\tDigest\\n’s\\tsampling\\tmethod:\\nFirst,\\tto\\tobtain\\tthe\\taddresses\\tto\\tsend\\tthe\\tpolls\\tto,\\tthe\\t\\nLiterary\\tDigest\\n\\tused\\ttelephone\\tdirectories,\\tlists\\tof\\tmagazine\\tsubscribers,\\tclub\\nmembership\\tlists,\\tand\\tthe\\tlike.\\tAll\\tof\\tthese\\tlists\\ttend\\tto\\tfavor\\twealthier\\tpeople,\\twho\\tare\\tmore\\tlikely\\tto\\tvote\\tRepublican\\t(hence\\nLandon).\\nSecond,\\tless\\tthan\\t25%\\tof\\tthe\\tpeople\\twho\\treceived\\tthe\\tpoll\\tanswered.\\tAgain,\\tthis\\tintroduces\\ta\\tsampling\\tbias,\\tby\\truling\\tout\\tpeople\\nwho\\tdon’t\\tcare\\tmuch\\tabout\\tpolitics,\\tpeople\\twho\\tdon’t\\tlike\\tthe\\t\\nLiterary\\tDigest', ',\\tand\\tother\\tkey\\tgroups.\\tThis\\tis\\ta\\tspecial\\ttype\\tof\\nsampling\\tbias\\t\\ncalled\\t\\nnonresponse\\tbias\\n.\\nHere\\tis\\tanother\\texample:\\tsay\\tyou\\twant\\tto\\tbuild\\ta\\tsystem\\tto\\trecognize\\tfunk\\tmusic\\tvideos.\\tOne\\tway\\tto\\tbuild\\tyour\\ttraining\\tset\\tis\\tto\\tsearch\\n“funk\\tmusic”\\ton\\tYouTube\\tand\\tuse\\tthe\\tresulting\\tvideos.\\tBut\\tthis\\tassumes\\tthat\\tYouTube’s\\tsearch\\tengine\\treturns\\ta\\tset\\tof\\tvideos\\tthat\\tare\\nrepresentative\\tof\\tall\\tthe\\tfunk\\tmusic\\tvideos\\ton\\tYouTube.\\tIn\\treality,\\tthe\\tsearch\\tresults\\tare\\tlikely\\tto\\tbe\\tbiased\\ttoward\\tpopular\\tartists\\t(and\\tif\\nyou\\tlive\\tin\\tBrazil\\tyou\\twill\\tget\\ta\\tlot\\tof\\t“funk\\tcarioca”\\tvideos,\\twhich\\tsound\\tnothing\\tlike\\tJames\\tBrown).\\tOn\\tthe\\tother\\thand,\\thow\\telse\\tcan\\nyou\\tget\\ta\\tlarge\\ttraining\\tset?', 'Poor-Quality\\tData\\nObviously,\\t\\nif\\tyour\\ttraining\\tdata\\tis\\tfull\\tof\\terrors,\\toutliers,\\tand\\tnoise\\t(e.g.,\\tdue\\tto\\tpoor-quality\\nmeasurements),\\tit\\twill\\tmake\\tit\\tharder\\tfor\\tthe\\tsystem\\tto\\tdetect\\tthe\\tunderlying\\tpatterns,\\tso\\tyour\\tsystem\\tis\\nless\\tlikely\\tto\\tperform\\twell.\\tIt\\tis\\toften\\twell\\tworth\\tthe\\teffort\\tto\\tspend\\ttime\\tcleaning\\tup\\tyour\\ttraining\\tdata.\\nThe\\ttruth\\tis,\\tmost\\tdata\\tscientists\\tspend\\ta\\tsignificant\\tpart\\tof\\ttheir\\ttime\\tdoing\\tjust\\tthat.\\tFor\\texample:\\nIf\\tsome\\tinstances\\tare\\tclearly\\toutliers,\\tit\\tmay\\thelp\\tto\\tsimply\\tdiscard\\tthem\\tor\\ttry\\tto\\tfix\\tthe\\terrors\\nmanually.\\nIf\\tsome\\tinstances\\tare\\tmissing\\ta\\tfew\\tfeatures\\t(e.g.,\\t5%\\tof\\tyour\\tcustomers\\tdid\\tnot\\tspecify\\ttheir\\tage),\\nyou\\tmust\\tdecide\\twhether\\tyou\\twant\\tto\\tignore\\tthis\\tattribute\\taltogether,\\tignore\\tthese\\tinstances,\\tfill\\tin\\tthe\\nmissing\\tvalues\\t(e.g.,\\twith\\tthe\\tmedian\\tage),\\tor\\ttrain\\tone\\tmodel\\twith\\tthe\\tfeature\\tand\\tone\\tmodel\\twithout\\nit,\\tand\\tso\\t\\non.', 'Irrelevant\\tFeatures\\nAs\\t\\nthe\\tsaying\\tgoes:\\tgarbage\\tin,\\tgarbage\\tout.\\tYour\\tsystem\\twill\\tonly\\tbe\\tcapable\\tof\\tlearning\\tif\\tthe\\ttraining\\ndata\\tcontains\\tenough\\trelevant\\tfeatures\\tand\\tnot\\ttoo\\tmany\\tirrelevant\\tones.\\tA\\tcritical\\tpart\\tof\\tthe\\tsuccess\\tof\\ta\\nMachine\\tLearning\\tproject\\tis\\tcoming\\tup\\twith\\ta\\tgood\\tset\\tof\\tfeatures\\tto\\ttrain\\ton.\\tThis\\tprocess,\\t\\ncalled\\nfeature\\tengineering\\n,\\tinvolves:\\nFeature\\tselection\\n:\\tselecting\\t\\nthe\\tmost\\tuseful\\tfeatures\\tto\\ttrain\\ton\\tamong\\texisting\\tfeatures.\\nFeature\\textraction\\n:\\tcombining\\texisting\\tfeatures\\tto\\tproduce\\ta\\tmore\\tuseful\\tone\\t(as\\twe\\tsaw\\tearlier,\\ndimensionality\\treduction\\talgorithms\\tcan\\thelp).\\nCreating\\tnew\\tfeatures\\tby\\tgathering\\tnew\\tdata.\\nNow\\tthat\\twe\\thave\\tlooked\\tat\\tmany\\texamples\\tof\\tbad\\tdata,\\tlet’s\\tlook\\tat\\ta\\tcouple\\tof\\texamples\\tof\\tbad\\nalgorithms.', 'Overfitting\\tthe\\tTraining\\tData\\nSay\\t\\nyou\\tare\\tvisiting\\ta\\tforeign\\tcountry\\tand\\tthe\\ttaxi\\tdriver\\trips\\tyou\\toff.\\tYou\\tmight\\tbe\\ttempted\\tto\\tsay\\tthat\\t\\nall\\ntaxi\\tdrivers\\tin\\tthat\\tcountry\\tare\\tthieves.\\tOvergeneralizing\\tis\\tsomething\\tthat\\twe\\thumans\\tdo\\tall\\ttoo\\toften,\\tand\\nunfortunately\\tmachines\\tcan\\tfall\\tinto\\tthe\\tsame\\ttrap\\tif\\twe\\tare\\tnot\\tcareful.\\tIn\\tMachine\\tLearning\\tthis\\tis\\tcalled\\noverfitting\\n:\\tit\\tmeans\\tthat\\tthe\\tmodel\\tperforms\\twell\\ton\\tthe\\ttraining\\tdata,\\tbut\\tit\\tdoes\\tnot\\tgeneralize\\twell.\\nFigure\\t1-22\\n\\tshows\\tan\\texample\\tof\\ta\\thigh-degree\\tpolynomial\\tlife\\tsatisfaction\\tmodel\\tthat\\tstrongly\\toverfits\\nthe\\ttraining\\tdata.\\tEven\\tthough\\tit\\tperforms\\tmuch\\tbetter\\ton\\tthe\\ttraining\\tdata\\tthan\\tthe\\tsimple\\tlinear\\tmodel,\\nwould\\tyou\\treally\\ttrust\\tits\\tpredictions?\\nFigure\\t1-22.\\t\\nOverfitting\\tthe\\ttraining\\tdata\\nComplex\\tmodels\\tsuch\\tas\\tdeep\\tneural\\tnetworks\\tcan\\tdetect\\tsubtle\\tpatterns\\tin\\tthe\\tdata,\\tbut\\tif\\tthe\\ttraining\\tset\\nis\\tnoisy,\\tor\\tif\\tit\\tis\\ttoo\\tsmall\\t(which\\tintroduces\\tsampling\\tnoise),\\tthen\\tthe\\tmodel\\tis\\tlikely\\tto\\tdetect\\tpatterns', 'in\\tthe\\tnoise\\titself.\\tObviously\\tthese\\tpatterns\\twill\\tnot\\tgeneralize\\tto\\tnew\\tinstances.\\tFor\\texample,\\tsay\\tyou\\nfeed\\tyour\\tlife\\tsatisfaction\\tmodel\\tmany\\tmore\\tattributes,\\tincluding\\tuninformative\\tones\\tsuch\\tas\\tthe\\tcountry’s\\nname.\\tIn\\tthat\\tcase,\\ta\\tcomplex\\tmodel\\tmay\\tdetect\\tpatterns\\tlike\\tthe\\tfact\\tthat\\tall\\tcountries\\tin\\tthe\\ttraining\\tdata\\nwith\\ta\\t\\nw\\n\\tin\\ttheir\\tname\\thave\\ta\\tlife\\tsatisfaction\\tgreater\\tthan\\t7:\\tNew\\tZealand\\t(7.3),\\tNorway\\t(7.4),\\tSweden\\n(7.2),\\tand\\tSwitzerland\\t(7.5).\\tHow\\tconfident\\tare\\tyou\\tthat\\tthe\\tW-satisfaction\\trule\\tgeneralizes\\tto\\tRwanda\\tor\\nZimbabwe?\\tObviously\\tthis\\tpattern\\toccurred\\tin\\tthe\\ttraining\\tdata\\tby\\tpure\\tchance,\\tbut\\tthe\\tmodel\\thas\\tno\\tway\\nto\\ttell\\twhether\\ta\\tpattern\\tis\\treal\\tor\\tsimply\\tthe\\tresult\\tof\\tnoise\\tin\\tthe\\tdata.\\nWARNING\\nOverfitting\\thappens\\twhen\\tthe\\tmodel\\tis\\ttoo\\tcomplex\\trelative\\tto\\tthe\\tamount\\tand\\tnoisiness\\tof\\tthe\\ttraining\\tdata.\\tThe\\tpossible\\nsolutions\\tare:\\nTo\\tsimplify\\tthe\\tmodel\\tby\\tselecting\\tone\\twith\\tfewer\\tparameters\\t(e.g.,\\ta\\tlinear\\tmodel\\trather\\tthan\\ta\\thigh-degree\\tpolynomial', 'model),\\tby\\treducing\\tthe\\tnumber\\tof\\tattributes\\tin\\tthe\\ttraining\\tdata\\tor\\tby\\tconstraining\\tthe\\tmodel\\nTo\\tgather\\tmore\\ttraining\\tdata\\nTo\\treduce\\tthe\\tnoise\\tin\\tthe\\ttraining\\tdata\\t(e.g.,\\tfix\\tdata\\terrors\\tand\\tremove\\toutliers)\\nConstraining\\ta\\tmodel\\tto\\tmake\\tit\\tsimpler\\tand\\treduce\\tthe\\trisk\\tof\\toverfitting\\tis\\t\\ncalled\\t\\nregularization\\n.\\tFor', 'example,\\tthe\\tlinear\\tmodel\\twe\\tdefined\\tearlier\\thas\\ttwo\\tparameters,\\t\\nθ\\n0\\n\\tand\\t\\nθ\\n1\\n.\\tThis\\tgives\\tthe\\tlearning\\nalgorithm\\ttwo\\t\\ndegrees\\tof\\tfreedom\\n\\tto\\t\\nadapt\\tthe\\tmodel\\tto\\tthe\\ttraining\\tdata:\\tit\\tcan\\ttweak\\tboth\\tthe\\theight\\t(\\nθ\\n0\\n)\\nand\\tthe\\tslope\\t(\\nθ\\n1\\n)\\tof\\tthe\\tline.\\tIf\\twe\\tforced\\t\\nθ\\n1\\n\\t=\\t0,\\tthe\\talgorithm\\twould\\thave\\tonly\\tone\\tdegree\\tof\\tfreedom\\nand\\twould\\thave\\ta\\tmuch\\tharder\\ttime\\tfitting\\tthe\\tdata\\tproperly:\\tall\\tit\\tcould\\tdo\\tis\\tmove\\tthe\\tline\\tup\\tor\\tdown\\nto\\tget\\tas\\tclose\\tas\\tpossible\\tto\\tthe\\ttraining\\tinstances,\\tso\\tit\\twould\\tend\\tup\\taround\\tthe\\tmean.\\tA\\tvery\\tsimple\\nmodel\\tindeed!\\tIf\\twe\\tallow\\tthe\\talgorithm\\tto\\tmodify\\t\\nθ\\n1\\n\\tbut\\twe\\tforce\\tit\\tto\\tkeep\\tit\\tsmall,\\tthen\\tthe\\tlearning\\nalgorithm\\twill\\teffectively\\thave\\tsomewhere\\tin\\tbetween\\tone\\tand\\ttwo\\tdegrees\\tof\\tfreedom.\\tIt\\twill\\tproduce\\ta\\nsimpler\\tmodel\\tthan\\twith\\ttwo\\tdegrees\\tof\\tfreedom,\\tbut\\tmore\\tcomplex\\tthan\\twith\\tjust\\tone.\\tYou\\twant\\tto\\tfind\\nthe\\tright\\tbalance\\tbetween\\tfitting\\tthe\\tdata\\tperfectly\\tand\\tkeeping\\tthe\\tmodel\\tsimple\\tenough\\tto\\tensure\\tthat\\tit\\nwill\\tgeneralize\\twell.\\nFigure\\t1-23', 'shows\\tthree\\tmodels:\\tthe\\tdotted\\tline\\trepresents\\tthe\\toriginal\\tmodel\\tthat\\twas\\ttrained\\twith\\ta\\tfew\\ncountries\\tmissing,\\tthe\\tdashed\\tline\\tis\\tour\\tsecond\\tmodel\\ttrained\\twith\\tall\\tcountries,\\tand\\tthe\\tsolid\\tline\\tis\\ta\\nlinear\\tmodel\\ttrained\\twith\\tthe\\tsame\\tdata\\tas\\tthe\\tfirst\\tmodel\\tbut\\twith\\ta\\tregularization\\tconstraint.\\tYou\\tcan\\tsee\\nthat\\tregularization\\tforced\\tthe\\tmodel\\tto\\thave\\ta\\tsmaller\\tslope,\\twhich\\tfits\\ta\\tbit\\tless\\tthe\\ttraining\\tdata\\tthat\\tthe\\nmodel\\twas\\ttrained\\ton,\\tbut\\tactually\\tallows\\tit\\tto\\tgeneralize\\tbetter\\tto\\tnew\\texamples.\\nFigure\\t1-23.\\t\\nRegularization\\treduces\\tthe\\trisk\\tof\\toverfitting\\nThe\\tamount\\tof\\tregularization\\tto\\tapply\\tduring\\tlearning\\tcan\\tbe\\tcontrolled\\tby\\ta\\t\\nhyperparameter\\n.\\tA\\nhyperparameter\\tis\\ta\\tparameter\\tof\\ta\\tlearning\\talgorithm\\t(not\\tof\\tthe\\tmodel).\\tAs\\tsuch,\\tit\\tis\\tnot\\taffected\\tby\\tthe\\nlearning\\talgorithm\\titself;\\tit\\tmust\\tbe\\tset\\tprior\\tto\\ttraining\\tand\\tremains\\tconstant\\tduring\\ttraining.\\tIf\\tyou\\tset\\tthe\\nregularization\\thyperparameter\\tto\\ta\\tvery\\tlarge\\tvalue,\\tyou\\twill\\tget\\tan\\talmost\\tflat\\tmodel\\t(a\\tslope\\tclose\\tto', 'zero);\\tthe\\tlearning\\talgorithm\\twill\\talmost\\tcertainly\\tnot\\toverfit\\tthe\\ttraining\\tdata,\\tbut\\tit\\twill\\tbe\\tless\\tlikely\\tto\\nfind\\ta\\tgood\\tsolution.\\tTuning\\thyperparameters\\tis\\tan\\timportant\\tpart\\tof\\tbuilding\\ta\\t\\nMachine\\tLearning\\tsystem\\n(you\\twill\\tsee\\ta\\tdetailed\\texample\\tin\\tthe\\tnext\\tchapter).', 'Underfitting\\tthe\\tTraining\\tData\\nAs\\t\\nyou\\tmight\\tguess,\\t\\nunderfitting\\n\\tis\\tthe\\topposite\\tof\\toverfitting:\\tit\\toccurs\\twhen\\tyour\\tmodel\\tis\\ttoo\\tsimple\\tto\\nlearn\\tthe\\tunderlying\\tstructure\\tof\\tthe\\tdata.\\tFor\\texample,\\ta\\tlinear\\tmodel\\tof\\tlife\\tsatisfaction\\tis\\tprone\\tto\\nunderfit;\\treality\\tis\\tjust\\tmore\\tcomplex\\tthan\\tthe\\tmodel,\\tso\\tits\\tpredictions\\tare\\tbound\\tto\\tbe\\tinaccurate,\\teven\\non\\tthe\\ttraining\\texamples.\\nThe\\tmain\\toptions\\tto\\tfix\\tthis\\tproblem\\tare:\\nSelecting\\ta\\tmore\\tpowerful\\tmodel,\\twith\\tmore\\tparameters\\nFeeding\\tbetter\\tfeatures\\tto\\tthe\\tlearning\\talgorithm\\t(feature\\tengineering)\\nReducing\\tthe\\tconstraints\\ton\\tthe\\tmodel\\t\\n(e.g.,\\treducing\\tthe\\tregularization\\thyperparameter)', 'Stepping\\tBack\\nBy\\t\\nnow\\tyou\\talready\\tknow\\ta\\tlot\\tabout\\tMachine\\tLearning.\\tHowever,\\twe\\twent\\tthrough\\tso\\tmany\\tconcepts\\nthat\\tyou\\tmay\\tbe\\tfeeling\\ta\\tlittle\\tlost,\\tso\\tlet’s\\tstep\\tback\\tand\\tlook\\tat\\tthe\\tbig\\tpicture:\\nMachine\\tLearning\\tis\\tabout\\tmaking\\tmachines\\tget\\tbetter\\tat\\tsome\\ttask\\tby\\tlearning\\tfrom\\tdata,\\tinstead\\tof\\nhaving\\tto\\texplicitly\\tcode\\trules.\\nThere\\tare\\tmany\\tdifferent\\ttypes\\tof\\tML\\tsystems:\\tsupervised\\tor\\tnot,\\tbatch\\tor\\tonline,\\tinstance-based\\tor\\nmodel-based,\\tand\\tso\\ton.\\nIn\\ta\\tML\\tproject\\tyou\\tgather\\tdata\\tin\\ta\\ttraining\\tset,\\tand\\tyou\\tfeed\\tthe\\ttraining\\tset\\tto\\ta\\tlearning\\talgorithm.\\nIf\\tthe\\talgorithm\\tis\\tmodel-based\\tit\\ttunes\\tsome\\tparameters\\tto\\tfit\\tthe\\tmodel\\tto\\tthe\\ttraining\\tset\\t(i.e.,\\tto\\nmake\\tgood\\tpredictions\\ton\\tthe\\ttraining\\tset\\titself),\\tand\\tthen\\thopefully\\tit\\twill\\tbe\\table\\tto\\tmake\\tgood\\npredictions\\ton\\tnew\\tcases\\tas\\twell.\\tIf\\tthe\\talgorithm\\tis\\tinstance-based,\\tit\\tjust\\tlearns\\tthe\\texamples\\tby\\nheart\\tand\\tuses\\ta\\tsimilarity\\tmeasure\\tto\\tgeneralize\\tto\\tnew\\tinstances.', 'The\\tsystem\\twill\\tnot\\tperform\\twell\\tif\\tyour\\ttraining\\tset\\tis\\ttoo\\tsmall,\\tor\\tif\\tthe\\tdata\\tis\\tnot\\trepresentative,\\nnoisy,\\tor\\tpolluted\\twith\\tirrelevant\\tfeatures\\t(garbage\\tin,\\tgarbage\\tout).\\tLastly,\\tyour\\tmodel\\tneeds\\tto\\tbe\\nneither\\ttoo\\tsimple\\t(in\\twhich\\tcase\\tit\\twill\\tunderfit)\\tnor\\ttoo\\tcomplex\\t(in\\twhich\\tcase\\tit\\twill\\toverfit).\\nThere’s\\tjust\\tone\\tlast\\timportant\\ttopic\\tto\\tcover:\\tonce\\tyou\\thave\\ttrained\\ta\\tmodel,\\tyou\\tdon’t\\twant\\tto\\tjust\\n“hope”\\tit\\tgeneralizes\\tto\\tnew\\tcases.\\tYou\\twant\\tto\\tevaluate\\tit,\\tand\\tfine-tune\\tit\\tif\\tnecessary.\\t\\nLet’s\\tsee\\thow.', 'Testing\\tand\\tValidating\\nThe\\t\\nonly\\tway\\tto\\tknow\\thow\\twell\\ta\\tmodel\\twill\\tgeneralize\\tto\\tnew\\tcases\\tis\\tto\\tactually\\ttry\\tit\\tout\\ton\\tnew\\ncases.\\tOne\\tway\\tto\\tdo\\tthat\\tis\\tto\\tput\\tyour\\tmodel\\tin\\tproduction\\tand\\tmonitor\\thow\\twell\\tit\\tperforms.\\tThis\\nworks\\twell,\\tbut\\tif\\tyour\\tmodel\\tis\\thorribly\\tbad,\\tyour\\tusers\\twill\\tcomplain\\t—\\tnot\\tthe\\tbest\\tidea.\\nA\\tbetter\\toption\\tis\\tto\\tsplit\\tyour\\tdata\\tinto\\ttwo\\tsets:\\t\\nthe\\t\\ntraining\\tset\\n\\tand\\t\\nthe\\t\\ntest\\tset\\n.\\tAs\\tthese\\tnames\\timply,\\nyou\\ttrain\\tyour\\tmodel\\tusing\\tthe\\ttraining\\tset,\\tand\\tyou\\ttest\\tit\\tusing\\tthe\\ttest\\tset.\\tThe\\terror\\trate\\ton\\tnew\\tcases\\tis\\ncalled\\tthe\\t\\ngeneralization\\terror\\n\\t(or\\t\\nout-of-sample\\terror\\n),\\t\\nand\\tby\\tevaluating\\tyour\\tmodel\\ton\\tthe\\ttest\\tset,\\nyou\\tget\\tan\\testimation\\tof\\tthis\\terror.\\tThis\\tvalue\\ttells\\tyou\\thow\\twell\\tyour\\tmodel\\twill\\tperform\\ton\\tinstances\\tit\\nhas\\tnever\\tseen\\tbefore.\\nIf\\tthe\\ttraining\\terror\\tis\\tlow\\t(i.e.,\\tyour\\tmodel\\tmakes\\tfew\\tmistakes\\ton\\tthe\\ttraining\\tset)\\tbut\\tthe\\tgeneralization\\nerror\\tis\\thigh,\\tit\\tmeans\\tthat\\tyour\\tmodel\\tis\\toverfitting\\tthe\\ttraining\\tdata.\\nTIP', 'TIP\\nIt\\tis\\tcommon\\tto\\tuse\\t80%\\tof\\tthe\\tdata\\tfor\\ttraining\\tand\\t\\nhold\\tout\\n\\t20%\\tfor\\ttesting.\\nSo\\tevaluating\\ta\\tmodel\\tis\\tsimple\\tenough:\\tjust\\tuse\\ta\\ttest\\tset.\\tNow\\tsuppose\\tyou\\tare\\thesitating\\tbetween\\ttwo\\nmodels\\t(say\\ta\\tlinear\\tmodel\\tand\\ta\\tpolynomial\\tmodel):\\thow\\tcan\\tyou\\tdecide?\\tOne\\toption\\tis\\tto\\ttrain\\tboth\\nand\\tcompare\\thow\\twell\\tthey\\tgeneralize\\tusing\\tthe\\ttest\\tset.\\nNow\\tsuppose\\tthat\\tthe\\tlinear\\tmodel\\tgeneralizes\\tbetter,\\tbut\\tyou\\twant\\tto\\tapply\\tsome\\t\\nregularization\\tto\\tavoid\\noverfitting.\\tThe\\tquestion\\tis:\\thow\\tdo\\tyou\\tchoose\\tthe\\tvalue\\tof\\tthe\\tregularization\\thyperparameter?\\tOne\\noption\\tis\\tto\\ttrain\\t100\\tdifferent\\tmodels\\tusing\\t100\\tdifferent\\tvalues\\tfor\\tthis\\thyperparameter.\\tSuppose\\tyou\\nfind\\tthe\\tbest\\thyperparameter\\tvalue\\tthat\\tproduces\\ta\\tmodel\\twith\\tthe\\tlowest\\tgeneralization\\terror,\\tsay\\tjust\\n5%\\terror.\\nSo\\tyou\\tlaunch\\tthis\\tmodel\\tinto\\tproduction,\\tbut\\tunfortunately\\tit\\tdoes\\tnot\\tperform\\tas\\twell\\tas\\texpected\\tand\\nproduces\\t15%\\terrors.\\tWhat\\tjust\\thappened?', 'The\\tproblem\\tis\\tthat\\tyou\\tmeasured\\tthe\\tgeneralization\\terror\\tmultiple\\ttimes\\ton\\tthe\\ttest\\tset,\\tand\\tyou\\tadapted\\nthe\\tmodel\\tand\\thyperparameters\\tto\\tproduce\\tthe\\tbest\\tmodel\\t\\nfor\\tthat\\tset\\n.\\tThis\\tmeans\\tthat\\tthe\\tmodel\\tis\\nunlikely\\tto\\tperform\\tas\\twell\\ton\\tnew\\tdata.\\nA\\tcommon\\tsolution\\tto\\tthis\\tproblem\\tis\\tto\\thave\\ta\\tsecond\\tholdout\\tset\\tcalled\\tthe\\t\\nvalidation\\tset\\n.\\t\\nYou\\ttrain\\nmultiple\\tmodels\\twith\\tvarious\\thyperparameters\\tusing\\tthe\\ttraining\\tset,\\tyou\\tselect\\tthe\\tmodel\\tand\\nhyperparameters\\tthat\\tperform\\tbest\\ton\\tthe\\tvalidation\\tset,\\tand\\twhen\\tyou’re\\thappy\\twith\\tyour\\tmodel\\tyou\\trun\\na\\tsingle\\tfinal\\ttest\\tagainst\\tthe\\ttest\\tset\\tto\\tget\\tan\\testimate\\tof\\tthe\\tgeneralization\\terror.\\nTo\\tavoid\\t“wasting”\\ttoo\\tmuch\\ttraining\\tdata\\tin\\tvalidation\\tsets,\\ta\\tcommon\\ttechnique\\tis\\tto\\tuse\\t\\ncross-\\nvalidation\\n:\\t\\nthe\\ttraining\\tset\\tis\\tsplit\\tinto\\tcomplementary\\tsubsets,\\tand\\teach\\tmodel\\tis\\ttrained\\tagainst\\ta\\ndifferent\\tcombination\\tof\\tthese\\tsubsets\\tand\\tvalidated\\tagainst\\tthe\\tremaining\\tparts.\\tOnce\\tthe\\tmodel\\ttype\\tand', 'hyperparameters\\thave\\tbeen\\tselected,\\ta\\tfinal\\tmodel\\tis\\ttrained\\tusing\\tthese\\thyperparameters\\ton\\tthe\\tfull\\ntraining\\tset,\\tand\\tthe\\tgeneralized\\terror\\tis\\tmeasured\\ton\\tthe\\ttest\\tset.', 'NO\\tFREE\\tLUNCH\\tTHEOREM\\nA\\t\\nmodel\\t\\nis\\ta\\tsimplified\\tversion\\tof\\tthe\\tobservations.\\tThe\\tsimplifications\\tare\\tmeant\\tto\\tdiscard\\tthe\\tsuperfluous\\tdetails\\tthat\\tare\\tunlikely\\tto\\ngeneralize\\tto\\tnew\\tinstances.\\tHowever,\\tto\\tdecide\\twhat\\tdata\\tto\\tdiscard\\tand\\twhat\\tdata\\tto\\tkeep,\\tyou\\tmust\\tmake\\t\\nassumptions\\n.\\tFor\\texample,\\na\\tlinear\\tmodel\\tmakes\\tthe\\tassumption\\tthat\\tthe\\tdata\\tis\\tfundamentally\\tlinear\\tand\\tthat\\tthe\\tdistance\\tbetween\\tthe\\tinstances\\tand\\tthe\\tstraight\\tline\\nis\\tjust\\tnoise,\\twhich\\tcan\\tsafely\\tbe\\tignored.\\nIn\\ta\\t\\nfamous\\t1996\\tpaper\\n,\\n11\\n\\tDavid\\tWolpert\\tdemonstrated\\tthat\\tif\\tyou\\tmake\\tabsolutely\\tno\\tassumption\\tabout\\tthe\\tdata,\\tthen\\tthere\\tis\\tno\\treason\\nto\\tprefer\\tone\\tmodel\\tover\\tany\\tother.\\tThis\\tis\\tcalled\\tthe\\t\\nNo\\tFree\\tLunch\\n\\t(NFL)\\ttheorem.\\tFor\\tsome\\tdatasets\\tthe\\tbest\\tmodel\\tis\\ta\\tlinear\\nmodel,\\twhile\\tfor\\tother\\tdatasets\\tit\\tis\\ta\\tneural\\tnetwork.\\tThere\\tis\\tno\\tmodel\\tthat\\tis\\t\\na\\tpriori\\n\\tguaranteed\\tto\\twork\\tbetter\\t(hence\\tthe\\tname\\tof', 'the\\ttheorem).\\tThe\\tonly\\tway\\tto\\tknow\\tfor\\tsure\\twhich\\tmodel\\tis\\tbest\\tis\\tto\\tevaluate\\tthem\\tall.\\tSince\\tthis\\tis\\tnot\\tpossible,\\tin\\tpractice\\tyou\\tmake\\nsome\\treasonable\\tassumptions\\tabout\\tthe\\tdata\\tand\\tyou\\tevaluate\\tonly\\ta\\tfew\\treasonable\\tmodels.\\tFor\\texample,\\tfor\\tsimple\\ttasks\\tyou\\tmay\\nevaluate\\tlinear\\tmodels\\twith\\tvarious\\tlevels\\tof\\tregularization,\\tand\\tfor\\ta\\tcomplex\\tproblem\\tyou\\tmay\\tevaluate\\tvarious\\tneural\\t\\nnetworks.', 'Exercises\\nIn\\tthis\\tchapter\\twe\\thave\\tcovered\\tsome\\tof\\tthe\\tmost\\timportant\\tconcepts\\tin\\tMachine\\tLearning.\\tIn\\tthe\\tnext\\nchapters\\twe\\twill\\tdive\\tdeeper\\tand\\twrite\\tmore\\tcode,\\tbut\\tbefore\\twe\\tdo,\\tmake\\tsure\\tyou\\tknow\\thow\\tto\\tanswer\\nthe\\tfollowing\\tquestions:\\n1\\n.\\t\\nHow\\twould\\tyou\\tdefine\\tMachine\\tLearning?\\n2\\n.\\t\\nCan\\tyou\\tname\\tfour\\ttypes\\tof\\tproblems\\twhere\\tit\\tshines?\\n3\\n.\\t\\nWhat\\tis\\ta\\tlabeled\\ttraining\\tset?\\n4\\n.\\t\\nWhat\\tare\\tthe\\ttwo\\tmost\\tcommon\\tsupervised\\ttasks?\\n5\\n.\\t\\nCan\\tyou\\tname\\tfour\\tcommon\\tunsupervised\\ttasks?\\n6\\n.\\t\\nWhat\\ttype\\tof\\tMachine\\tLearning\\talgorithm\\twould\\tyou\\tuse\\tto\\tallow\\ta\\trobot\\tto\\twalk\\tin\\tvarious\\nunknown\\tterrains?\\n7\\n.\\t\\nWhat\\ttype\\tof\\talgorithm\\twould\\tyou\\tuse\\tto\\tsegment\\tyour\\tcustomers\\tinto\\tmultiple\\tgroups?\\n8\\n.\\t\\nWould\\tyou\\tframe\\tthe\\tproblem\\tof\\tspam\\tdetection\\tas\\ta\\tsupervised\\tlearning\\tproblem\\tor\\tan\\nunsupervised\\tlearning\\tproblem?\\n9\\n.\\t\\nWhat\\tis\\tan\\tonline\\tlearning\\tsystem?\\n10\\n.\\t\\nWhat\\tis\\tout-of-core\\tlearning?\\n11\\n.\\t\\nWhat\\ttype\\tof\\tlearning\\talgorithm\\trelies\\ton\\ta\\tsimilarity\\tmeasure\\tto\\tmake\\tpredictions?\\n12\\n.', '12\\n.\\t\\nWhat\\tis\\tthe\\tdifference\\tbetween\\ta\\tmodel\\tparameter\\tand\\ta\\tlearning\\talgorithm’s\\thyperparameter?\\n13\\n.\\t\\nWhat\\tdo\\tmodel-based\\tlearning\\talgorithms\\tsearch\\tfor?\\tWhat\\tis\\tthe\\tmost\\tcommon\\tstrategy\\tthey\\tuse\\tto\\nsucceed?\\tHow\\tdo\\tthey\\tmake\\tpredictions?\\n14\\n.\\t\\nCan\\tyou\\tname\\tfour\\tof\\tthe\\tmain\\tchallenges\\tin\\tMachine\\tLearning?\\n15\\n.\\t\\nIf\\tyour\\tmodel\\tperforms\\tgreat\\ton\\tthe\\ttraining\\tdata\\tbut\\tgeneralizes\\tpoorly\\tto\\tnew\\tinstances,\\twhat\\tis\\nhappening?\\tCan\\tyou\\tname\\tthree\\tpossible\\tsolutions?\\n16\\n.\\t\\nWhat\\tis\\ta\\ttest\\tset\\tand\\twhy\\twould\\tyou\\twant\\tto\\tuse\\tit?\\n17\\n.\\t\\nWhat\\tis\\tthe\\tpurpose\\tof\\ta\\tvalidation\\tset?\\n18\\n.\\t\\nWhat\\tcan\\tgo\\twrong\\tif\\tyou\\ttune\\thyperparameters\\tusing\\tthe\\ttest\\tset?\\n19\\n.\\t\\nWhat\\tis\\tcross-validation\\tand\\twhy\\twould\\tyou\\tprefer\\tit\\tto\\ta\\tvalidation\\tset?', 'Solutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\nFun\\tfact:\\tthis\\todd-sounding\\tname\\tis\\ta\\tstatistics\\tterm\\tintroduced\\tby\\tFrancis\\tGalton\\twhile\\the\\twas\\tstudying\\tthe\\tfact\\tthat\\tthe\\tchildren\\tof\\ttall\\npeople\\ttend\\tto\\tbe\\tshorter\\tthan\\ttheir\\tparents.\\tSince\\tchildren\\twere\\tshorter,\\the\\tcalled\\tthis\\t\\nregression\\tto\\tthe\\tmean\\n.\\tThis\\tname\\twas\\tthen\\napplied\\tto\\tthe\\tmethods\\the\\tused\\tto\\tanalyze\\tcorrelations\\tbetween\\tvariables.\\nSome\\tneural\\tnetwork\\tarchitectures\\tcan\\tbe\\tunsupervised,\\tsuch\\tas\\tautoencoders\\tand\\trestricted\\tBoltzmann\\tmachines.\\tThey\\tcan\\talso\\tbe\\nsemisupervised,\\tsuch\\tas\\tin\\tdeep\\tbelief\\tnetworks\\tand\\tunsupervised\\tpretraining.\\nNotice\\thow\\tanimals\\tare\\trather\\twell\\tseparated\\tfrom\\tvehicles,\\thow\\thorses\\tare\\tclose\\tto\\tdeer\\tbut\\tfar\\tfrom\\tbirds,\\tand\\tso\\ton.\\tFigure\\treproduced\\nwith\\tpermission\\tfrom\\tSocher,\\tGanjoo,\\tManning,\\tand\\tNg\\t(2013),\\t“T-SNE\\tvisualization\\tof\\tthe\\tsemantic\\tword\\tspace.”\\nThat’s\\twhen\\tthe\\tsystem\\tworks\\tperfectly.\\tIn\\tpractice\\tit\\toften\\tcreates\\ta\\tfew\\tclusters\\tper\\tperson,\\tand\\tsometimes\\tmixes\\tup\\ttwo\\tpeople\\twho', 'look\\talike,\\tso\\tyou\\tneed\\tto\\tprovide\\ta\\tfew\\tlabels\\tper\\tperson\\tand\\tmanually\\tclean\\tup\\tsome\\tclusters.\\nBy\\tconvention,\\tthe\\tGreek\\tletter\\tθ\\t(theta)\\tis\\tfrequently\\tused\\tto\\trepresent\\tmodel\\tparameters.\\nThe\\tcode\\tassumes\\tthat\\t\\nprepare_country_stats()\\n\\tis\\talready\\tdefined:\\tit\\tmerges\\tthe\\tGDP\\tand\\tlife\\tsatisfaction\\tdata\\tinto\\ta\\tsingle\\tPandas\\ndataframe.\\nIt’s\\tokay\\tif\\tyou\\tdon’t\\tunderstand\\tall\\tthe\\tcode\\tyet;\\twe\\twill\\tpresent\\tScikit-Learn\\tin\\tthe\\tfollowing\\tchapters.\\nFor\\texample,\\tknowing\\twhether\\tto\\twrite\\t“to,”\\t“two,”\\tor\\t“too”\\tdepending\\ton\\tthe\\tcontext.\\nFigure\\treproduced\\twith\\tpermission\\tfrom\\tBanko\\tand\\tBrill\\t(2001),\\t“Learning\\tCurves\\tfor\\tConfusion\\tSet\\tDisambiguation.”\\n“The\\tUnreasonable\\tEffectiveness\\tof\\tData,”\\tPeter\\tNorvig\\tet\\tal.\\t(2009).\\n“The\\tLack\\tof\\tA\\tPriori\\tDistinctions\\tBetween\\tLearning\\tAlgorithms,”\\tD.\\tWolperts\\t(1996).\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11', 'Chapter\\t2.\\t\\nEnd-to-End\\tMachine\\tLearning\\nProject\\nIn\\t\\nthis\\tchapter,\\tyou\\twill\\tgo\\tthrough\\tan\\texample\\tproject\\tend\\tto\\tend,\\tpretending\\tto\\tbe\\ta\\trecently\\thired\\tdata\\nscientist\\tin\\ta\\treal\\testate\\tcompany.\\n1\\n\\tHere\\tare\\tthe\\tmain\\tsteps\\tyou\\twill\\tgo\\tthrough:\\n1\\n.\\t\\nLook\\tat\\tthe\\tbig\\tpicture.\\n2\\n.\\t\\nGet\\tthe\\tdata.\\n3\\n.\\t\\nDiscover\\tand\\tvisualize\\tthe\\tdata\\tto\\tgain\\tinsights.\\n4\\n.\\t\\nPrepare\\tthe\\tdata\\tfor\\tMachine\\tLearning\\talgorithms.\\n5\\n.\\t\\nSelect\\ta\\tmodel\\tand\\ttrain\\tit.\\n6\\n.\\t\\nFine-tune\\tyour\\tmodel.\\n7\\n.\\t\\nPresent\\tyour\\tsolution.\\n8\\n.\\t\\nLaunch,\\tmonitor,\\tand\\tmaintain\\tyour\\tsystem.', 'Working\\twith\\tReal\\tData\\nWhen\\t\\nyou\\tare\\tlearning\\tabout\\tMachine\\tLearning\\tit\\tis\\tbest\\tto\\tactually\\texperiment\\twith\\treal-world\\tdata,\\tnot\\njust\\tartificial\\tdatasets.\\tFortunately,\\tthere\\tare\\tthousands\\tof\\topen\\tdatasets\\tto\\tchoose\\tfrom,\\tranging\\tacross\\tall\\nsorts\\tof\\tdomains.\\tHere\\tare\\ta\\tfew\\tplaces\\tyou\\tcan\\tlook\\tto\\tget\\tdata:\\nPopular\\topen\\tdata\\trepositories:\\nUC\\tIrvine\\tMachine\\tLearning\\tRepository\\nKaggle\\tdatasets\\nAmazon’s\\tAWS\\tdatasets\\nMeta\\tportals\\t(they\\tlist\\topen\\tdata\\trepositories):\\nhttp://dataportals.org/\\nhttp://opendatamonitor.eu/\\nhttp://quandl.com/\\nOther\\tpages\\tlisting\\tmany\\tpopular\\topen\\tdata\\trepositories:\\nWikipedia’s\\tlist\\tof\\tMachine\\tLearning\\tdatasets\\nQuora.com\\tquestion\\nDatasets\\tsubreddit\\nIn\\tthis\\tchapter\\twe\\tchose\\tthe\\tCalifornia\\tHousing\\tPrices\\tdataset\\tfrom\\tthe\\tStatLib\\trepository\\n2\\n\\t(see\\t\\nFigure\\t2-\\n1\\n).\\tThis\\tdataset\\twas\\tbased\\ton\\tdata\\tfrom\\tthe\\t1990\\tCalifornia\\tcensus.\\tIt\\tis\\tnot\\texactly\\trecent\\t(you\\tcould\\nstill\\tafford\\ta\\tnice\\thouse\\tin\\tthe\\tBay\\tArea\\tat\\tthe\\ttime),\\tbut\\tit\\thas\\tmany\\tqualities\\tfor\\tlearning,\\tso\\twe\\twill', 'pretend\\tit\\tis\\trecent\\tdata.\\tWe\\talso\\tadded\\ta\\tcategorical\\tattribute\\tand\\tremoved\\ta\\tfew\\tfeatures\\tfor\\tteaching\\npurposes.', 'Figure\\t2-1.\\t\\nCalifornia\\thousing\\tprices', 'Look\\tat\\tthe\\tBig\\tPicture\\nWelcome\\tto\\tMachine\\tLearning\\tHousing\\tCorporation!\\tThe\\tfirst\\ttask\\tyou\\tare\\tasked\\tto\\tperform\\tis\\tto\\tbuild\\ta\\nmodel\\tof\\thousing\\tprices\\tin\\tCalifornia\\tusing\\tthe\\tCalifornia\\tcensus\\tdata.\\tThis\\tdata\\thas\\tmetrics\\tsuch\\tas\\tthe\\npopulation,\\tmedian\\tincome,\\tmedian\\thousing\\tprice,\\tand\\tso\\ton\\tfor\\teach\\tblock\\tgroup\\tin\\tCalifornia.\\tBlock\\ngroups\\tare\\tthe\\tsmallest\\tgeographical\\tunit\\tfor\\twhich\\tthe\\tUS\\tCensus\\tBureau\\tpublishes\\tsample\\tdata\\t(a\\tblock\\ngroup\\ttypically\\thas\\ta\\tpopulation\\tof\\t600\\tto\\t3,000\\tpeople).\\tWe\\twill\\tjust\\tcall\\tthem\\t“districts”\\tfor\\tshort.\\nYour\\tmodel\\tshould\\tlearn\\tfrom\\tthis\\tdata\\tand\\tbe\\table\\tto\\tpredict\\tthe\\tmedian\\thousing\\tprice\\tin\\tany\\tdistrict,\\ngiven\\tall\\tthe\\tother\\tmetrics.\\nTIP\\nSince\\tyou\\tare\\ta\\twell-organized\\tdata\\tscientist,\\tthe\\tfirst\\tthing\\tyou\\tdo\\tis\\tto\\tpull\\tout\\tyour\\t\\nMachine\\tLearning\\tproject\\tchecklist.\\tYou\\tcan\\nstart\\twith\\tthe\\tone\\tin\\t\\nAppendix\\tB\\n;\\tit\\tshould\\twork\\treasonably\\twell\\tfor\\tmost\\tMachine\\tLearning\\tprojects\\tbut\\tmake\\tsure\\tto\\tadapt\\tit\\tto', 'your\\tneeds.\\tIn\\tthis\\tchapter\\twe\\twill\\tgo\\tthrough\\tmany\\tchecklist\\titems,\\tbut\\twe\\twill\\talso\\tskip\\ta\\tfew,\\teither\\tbecause\\tthey\\tare\\tself-\\nexplanatory\\tor\\tbecause\\tthey\\twill\\tbe\\tdiscussed\\tin\\tlater\\tchapters.', 'Frame\\tthe\\tProblem\\nThe\\t\\nfirst\\tquestion\\tto\\task\\tyour\\tboss\\tis\\twhat\\texactly\\tis\\tthe\\tbusiness\\tobjective;\\tbuilding\\ta\\tmodel\\tis\\tprobably\\nnot\\tthe\\tend\\tgoal.\\tHow\\tdoes\\tthe\\tcompany\\texpect\\tto\\tuse\\tand\\tbenefit\\tfrom\\tthis\\tmodel?\\tThis\\tis\\timportant\\nbecause\\tit\\twill\\tdetermine\\thow\\tyou\\tframe\\tthe\\tproblem,\\twhat\\talgorithms\\tyou\\twill\\tselect,\\twhat\\tperformance\\nmeasure\\tyou\\twill\\tuse\\tto\\tevaluate\\tyour\\tmodel,\\tand\\thow\\tmuch\\teffort\\tyou\\tshould\\tspend\\ttweaking\\tit.\\nYour\\tboss\\tanswers\\tthat\\tyour\\tmodel’s\\toutput\\t(a\\tprediction\\tof\\ta\\tdistrict’s\\tmedian\\thousing\\tprice)\\twill\\tbe\\tfed\\nto\\tanother\\tMachine\\tLearning\\tsystem\\t(see\\t\\nFigure\\t2-2\\n),\\talong\\twith\\tmany\\tother\\t\\nsignals\\n.\\n3\\n\\tThis\\tdownstream\\nsystem\\twill\\tdetermine\\twhether\\tit\\tis\\tworth\\tinvesting\\tin\\ta\\tgiven\\tarea\\tor\\tnot.\\tGetting\\tthis\\tright\\tis\\tcritical,\\tas\\nit\\tdirectly\\taffects\\trevenue.\\nFigure\\t2-2.\\t\\nA\\tMachine\\tLearning\\tpipeline\\tfor\\treal\\testate\\tinvestments\\nPIPELINES\\nA\\t\\nsequence\\tof\\tdata\\tprocessing\\t\\ncomponents\\n\\tis\\tcalled\\ta\\tdata\\t\\npipeline\\n.\\tPipelines\\tare\\tvery\\tcommon\\tin\\tMachine\\tLearning\\tsystems,\\tsince', 'there\\tis\\ta\\tlot\\tof\\tdata\\tto\\tmanipulate\\tand\\tmany\\tdata\\ttransformations\\tto\\tapply.\\nComponents\\ttypically\\trun\\tasynchronously.\\tEach\\tcomponent\\tpulls\\tin\\ta\\tlarge\\tamount\\tof\\tdata,\\tprocesses\\tit,\\tand\\tspits\\tout\\tthe\\tresult\\tin\\tanother\\ndata\\tstore,\\tand\\tthen\\tsome\\ttime\\tlater\\tthe\\tnext\\tcomponent\\tin\\tthe\\tpipeline\\tpulls\\tthis\\tdata\\tand\\tspits\\tout\\tits\\town\\toutput,\\tand\\tso\\ton.\\tEach\\ncomponent\\tis\\tfairly\\tself-contained:\\tthe\\tinterface\\tbetween\\tcomponents\\tis\\tsimply\\tthe\\tdata\\tstore.\\tThis\\tmakes\\tthe\\tsystem\\tquite\\tsimple\\tto\\ngrasp\\t(with\\tthe\\thelp\\tof\\ta\\tdata\\tflow\\tgraph),\\tand\\tdifferent\\tteams\\tcan\\tfocus\\ton\\tdifferent\\tcomponents.\\tMoreover,\\tif\\ta\\tcomponent\\tbreaks\\ndown,\\tthe\\tdownstream\\tcomponents\\tcan\\toften\\tcontinue\\tto\\trun\\tnormally\\t(at\\tleast\\tfor\\ta\\twhile)\\tby\\tjust\\tusing\\tthe\\tlast\\toutput\\tfrom\\tthe\\tbroken\\ncomponent.\\tThis\\tmakes\\tthe\\tarchitecture\\tquite\\trobust.\\nOn\\tthe\\tother\\thand,\\ta\\tbroken\\tcomponent\\tcan\\tgo\\tunnoticed\\tfor\\tsome\\ttime\\tif\\tproper\\tmonitoring\\tis\\tnot\\timplemented.\\tThe\\tdata\\tgets\\tstale\\tand\\nthe\\toverall\\tsystem’s\\tperformance\\tdrops.', 'The\\tnext\\tquestion\\tto\\task\\tis\\twhat\\tthe\\tcurrent\\tsolution\\tlooks\\tlike\\t(if\\tany).\\tIt\\twill\\toften\\tgive\\tyou\\ta\\treference\\nperformance,\\tas\\twell\\tas\\tinsights\\ton\\thow\\tto\\tsolve\\tthe\\tproblem.\\tYour\\tboss\\tanswers\\tthat\\tthe\\tdistrict\\thousing\\nprices\\tare\\tcurrently\\testimated\\tmanually\\tby\\texperts:\\ta\\tteam\\tgathers\\tup-to-date\\tinformation\\tabout\\ta\\tdistrict,\\nand\\twhen\\tthey\\tcannot\\tget\\tthe\\tmedian\\thousing\\tprice,\\tthey\\testimate\\tit\\tusing\\tcomplex\\trules.\\nThis\\tis\\tcostly\\tand\\ttime-consuming,\\tand\\ttheir\\testimates\\tare\\tnot\\tgreat;\\tin\\tcases\\twhere\\tthey\\tmanage\\tto\\tfind\\nout\\tthe\\tactual\\tmedian\\thousing\\tprice,\\tthey\\toften\\trealize\\tthat\\ttheir\\testimates\\twere\\toff\\tby\\tmore\\tthan\\t10%.\\nThis\\tis\\twhy\\tthe\\tcompany\\tthinks\\tthat\\tit\\twould\\tbe\\tuseful\\tto\\ttrain\\ta\\tmodel\\tto\\tpredict\\ta\\tdistrict’s\\tmedian\\nhousing\\tprice\\tgiven\\tother\\tdata\\tabout\\tthat\\tdistrict.\\tThe\\tcensus\\tdata\\tlooks\\tlike\\ta\\tgreat\\tdataset\\tto\\texploit\\tfor\\nthis\\tpurpose,\\tsince\\tit\\tincludes\\tthe\\tmedian\\thousing\\tprices\\tof\\tthousands\\tof\\tdistricts,\\tas\\twell\\tas\\tother\\tdata.', 'Okay,\\twith\\tall\\tthis\\tinformation\\tyou\\tare\\tnow\\tready\\tto\\tstart\\tdesigning\\tyour\\tsystem.\\tFirst,\\tyou\\tneed\\tto\\tframe', 'the\\tproblem:\\tis\\tit\\tsupervised,\\tunsupervised,\\tor\\tReinforcement\\tLearning?\\tIs\\tit\\ta\\tclassification\\ttask,\\ta\\nregression\\ttask,\\tor\\tsomething\\telse?\\tShould\\tyou\\tuse\\tbatch\\tlearning\\tor\\tonline\\tlearning\\ttechniques?\\tBefore\\nyou\\tread\\ton,\\tpause\\tand\\ttry\\tto\\tanswer\\tthese\\tquestions\\tfor\\tyourself.\\nHave\\tyou\\tfound\\tthe\\tanswers?\\tLet’s\\tsee:\\tit\\tis\\tclearly\\ta\\ttypical\\tsupervised\\tlearning\\ttask\\tsince\\tyou\\tare\\t\\ngiven\\nlabeled\\n\\ttraining\\texamples\\t(each\\tinstance\\tcomes\\twith\\tthe\\texpected\\toutput,\\ti.e.,\\tthe\\tdistrict’s\\tmedian\\nhousing\\tprice).\\tMoreover,\\tit\\tis\\talso\\ta\\ttypical\\tregression\\ttask,\\tsince\\tyou\\tare\\tasked\\tto\\tpredict\\ta\\tvalue.\\tMore\\nspecifically,\\tthis\\tis\\t\\na\\t\\nmultivariate\\tregression\\n\\tproblem\\tsince\\tthe\\tsystem\\twill\\tuse\\tmultiple\\tfeatures\\tto\\tmake\\na\\tprediction\\t(it\\twill\\tuse\\tthe\\tdistrict’s\\tpopulation,\\tthe\\tmedian\\tincome,\\tetc.).\\tIn\\tthe\\tfirst\\tchapter,\\tyou\\npredicted\\tlife\\tsatisfaction\\tbased\\ton\\tjust\\tone\\tfeature,\\tthe\\tGDP\\tper\\tcapita,\\tso\\tit\\twas\\t\\na\\t\\nunivariate\\tregression', 'problem.\\tFinally,\\tthere\\tis\\tno\\tcontinuous\\tflow\\tof\\tdata\\tcoming\\tin\\tthe\\tsystem,\\tthere\\tis\\tno\\tparticular\\tneed\\tto\\nadjust\\tto\\tchanging\\tdata\\trapidly,\\tand\\tthe\\tdata\\tis\\tsmall\\tenough\\tto\\tfit\\tin\\tmemory,\\tso\\tplain\\tbatch\\tlearning\\nshould\\tdo\\tjust\\tfine.\\nTIP\\nIf\\tthe\\tdata\\twas\\thuge,\\tyou\\tcould\\teither\\tsplit\\tyour\\tbatch\\tlearning\\twork\\tacross\\tmultiple\\tservers\\t(using\\t\\nthe\\t\\nMapReduce\\n\\ttechnique,\\tas\\nwe\\twill\\tsee\\tlater),\\tor\\tyou\\tcould\\tuse\\tan\\tonline\\tlearning\\t\\ntechnique\\tinstead.', 'Select\\ta\\tPerformance\\tMeasure\\nYour\\t\\nnext\\tstep\\tis\\tto\\tselect\\ta\\tperformance\\tmeasure.\\tA\\ttypical\\tperformance\\tmeasure\\tfor\\tregression\\nproblems\\tis\\tthe\\t\\nRoot\\tMean\\tSquare\\tError\\t(RMSE).\\tIt\\tgives\\tan\\tidea\\tof\\thow\\tmuch\\terror\\tthe\\tsystem\\ttypically\\nmakes\\tin\\tits\\tpredictions,\\twith\\ta\\thigher\\tweight\\tfor\\tlarge\\terrors.\\t\\nEquation\\t2-1\\n\\tshows\\tthe\\tmathematical\\nformula\\tto\\tcompute\\tthe\\tRMSE.\\nEquation\\t2-1.\\t\\nRoot\\tMean\\tSquare\\tError\\t(RMSE)\\nNOTATIONS\\nThis\\tequation\\tintroduces\\tseveral\\tvery\\tcommon\\t\\nMachine\\tLearning\\tnotations\\tthat\\twe\\twill\\tuse\\tthroughout\\tthis\\tbook:\\nm\\n\\tis\\tthe\\tnumber\\tof\\tinstances\\tin\\tthe\\tdataset\\tyou\\tare\\tmeasuring\\tthe\\tRMSE\\ton.\\nFor\\texample,\\tif\\tyou\\tare\\tevaluating\\tthe\\tRMSE\\ton\\ta\\tvalidation\\tset\\tof\\t2,000\\tdistricts,\\tthen\\t\\nm\\n\\t=\\t2,000.\\nx\\n(i)\\n\\tis\\ta\\tvector\\tof\\tall\\tthe\\tfeature\\tvalues\\t(excluding\\tthe\\tlabel)\\tof\\tthe\\t\\ni\\nth\\n\\tinstance\\tin\\tthe\\tdataset,\\tand\\t\\ny\\n(i)\\n\\tis\\tits\\tlabel\\t(the\\tdesired\\noutput\\tvalue\\tfor\\tthat\\tinstance).', 'For\\texample,\\tif\\tthe\\tfirst\\tdistrict\\tin\\tthe\\tdataset\\tis\\tlocated\\tat\\tlongitude\\t–118.29°,\\tlatitude\\t33.91°,\\tand\\tit\\thas\\t1,416\\tinhabitants\\nwith\\ta\\tmedian\\tincome\\tof\\t$38,372,\\tand\\tthe\\tmedian\\thouse\\tvalue\\tis\\t$156,400\\t(ignoring\\tthe\\tother\\tfeatures\\tfor\\tnow),\\tthen:\\nand:\\nX\\n\\tis\\ta\\tmatrix\\tcontaining\\tall\\tthe\\tfeature\\tvalues\\t(excluding\\tlabels)\\tof\\tall\\tinstances\\tin\\tthe\\tdataset.\\tThere\\tis\\tone\\trow\\tper\\tinstance\\tand\\nthe\\t\\ni\\nth\\n\\trow\\tis\\tequal\\tto\\tthe\\ttranspose\\tof\\t\\nx\\n(i)\\n,\\tnoted\\t(\\nx\\n(i)\\n)\\nT\\n.\\n4\\nFor\\texample,\\tif\\tthe\\tfirst\\tdistrict\\tis\\tas\\tjust\\tdescribed,\\tthen\\tthe\\tmatrix\\t\\nX\\n\\tlooks\\tlike\\tthis:', 'h\\n\\tis\\tyour\\tsystem’s\\tprediction\\tfunction,\\talso\\tcalled\\t\\na\\t\\nhypothesis\\n.\\tWhen\\tyour\\tsystem\\tis\\tgiven\\tan\\t\\ninstance’s\\tfeature\\tvector\\t\\nx\\n(i)\\n,\\tit\\noutputs\\ta\\tpredicted\\tvalue\\t\\nŷ\\n(i)\\n\\t=\\t\\nh\\n(\\nx\\n(i)\\n)\\tfor\\tthat\\tinstance\\t(\\nŷ\\n\\tis\\tpronounced\\t“y-hat”).\\nFor\\texample,\\tif\\tyour\\tsystem\\tpredicts\\tthat\\tthe\\tmedian\\thousing\\tprice\\tin\\tthe\\tfirst\\tdistrict\\tis\\t$158,400,\\tthen\\t\\nŷ\\n(1)\\n\\t=\\t\\nh\\n(\\nx\\n(1)\\n)\\t=\\n158,400.\\tThe\\tprediction\\terror\\tfor\\tthis\\tdistrict\\tis\\t\\nŷ\\n(1)\\n\\t–\\t\\ny\\n(1)\\n\\t=\\t2,000.\\nRMSE(\\nX\\n,\\nh\\n)\\tis\\tthe\\t\\ncost\\tfunction\\tmeasured\\ton\\tthe\\tset\\tof\\texamples\\tusing\\tyour\\thypothesis\\t\\nh\\n.\\nWe\\tuse\\tlowercase\\titalic\\tfont\\tfor\\tscalar\\tvalues\\t(such\\tas\\t\\nm\\n\\tor\\t\\ny\\n(i)\\n)\\tand\\tfunction\\tnames\\t(such\\tas\\t\\nh\\n),\\tlowercase\\tbold\\tfont\\tfor\\tvectors\\t(such\\nas\\t\\nx\\n(i)\\n),\\tand\\tuppercase\\t\\nbold\\tfont\\tfor\\tmatrices\\t(such\\tas\\t\\nX\\n).\\nEven\\tthough\\tthe\\tRMSE\\tis\\tgenerally\\tthe\\tpreferred\\tperformance\\tmeasure\\tfor\\tregression\\ttasks,\\tin\\tsome\\ncontexts\\tyou\\tmay\\tprefer\\tto\\tuse\\tanother\\tfunction.\\tFor\\texample,\\tsuppose\\tthat\\tthere\\tare\\tmany\\toutlier\\tdistricts.\\nIn\\tthat\\tcase,\\tyou\\tmay\\tconsider\\tusing\\t\\nthe', 'the\\t\\nMean\\tAbsolute\\tError\\n\\t(also\\tcalled\\tthe\\tAverage\\tAbsolute\\nDeviation;\\tsee\\t\\nEquation\\t2-2\\n):\\nEquation\\t2-2.\\t\\nMean\\tAbsolute\\tError\\nBoth\\tthe\\tRMSE\\tand\\tthe\\tMAE\\tare\\tways\\tto\\tmeasure\\tthe\\tdistance\\tbetween\\ttwo\\tvectors:\\tthe\\tvector\\tof\\npredictions\\tand\\tthe\\tvector\\tof\\ttarget\\tvalues.\\tVarious\\tdistance\\tmeasures,\\tor\\t\\nnorms\\n,\\t\\nare\\tpossible:\\nComputing\\tthe\\troot\\tof\\ta\\tsum\\tof\\tsquares\\t(RMSE)\\tcorresponds\\tto\\tthe\\t\\nEuclidian\\tnorm\\n:\\tit\\t\\nis\\tthe\\tnotion\\nof\\tdistance\\tyou\\tare\\tfamiliar\\twith.\\tIt\\t\\nis\\talso\\tcalled\\tthe\\tℓ\\n2\\n\\t\\nnorm\\n,\\tnoted\\t\\t·\\t\\n2\\n\\t(or\\tjust\\t\\t·\\t).\\nComputing\\tthe\\tsum\\tof\\tabsolutes\\t(MAE)\\tcorresponds\\tto\\tthe\\tℓ\\n1\\n\\t\\nnorm\\n,\\t\\nnoted\\t\\t·\\t\\n1\\n.\\tIt\\tis\\tsometimes\\ncalled\\t\\nthe\\t\\nManhattan\\tnorm\\n\\tbecause\\tit\\tmeasures\\tthe\\tdistance\\tbetween\\ttwo\\tpoints\\tin\\ta\\tcity\\tif\\tyou\\tcan\\nonly\\ttravel\\talong\\torthogonal\\tcity\\tblocks.\\nMore\\tgenerally,\\t\\nthe\\tℓ\\nk\\n\\t\\nnorm\\n\\tof\\ta\\tvector\\t\\nv\\n\\tcontaining\\t\\nn\\n\\telements\\tis\\tdefined\\tas\\t\\n.\\tℓ\\n0\\n\\tjust\\t\\ngives\\tthe\\tnumber\\tof\\tnon-zero\\telements\\tin\\tthe\\tvector,\\nand\\tℓ\\n∞\\n\\t\\ngives\\tthe\\tmaximum\\tabsolute\\tvalue\\tin\\tthe\\tvector.', 'The\\thigher\\tthe\\tnorm\\tindex,\\tthe\\tmore\\tit\\tfocuses\\ton\\tlarge\\tvalues\\tand\\tneglects\\tsmall\\tones.\\tThis\\tis\\twhy\\nthe\\tRMSE\\tis\\tmore\\tsensitive\\tto\\toutliers\\tthan\\tthe\\tMAE.\\tBut\\twhen\\toutliers\\tare\\texponentially\\trare\\t(like\\nin\\ta\\tbell-shaped\\tcurve),\\tthe\\tRMSE\\tperforms\\tvery\\twell\\tand\\tis\\t\\ngenerally\\tpreferred.', 'Check\\tthe\\tAssumptions\\nLastly,\\t\\nit\\tis\\tgood\\tpractice\\tto\\tlist\\tand\\tverify\\tthe\\tassumptions\\tthat\\twere\\tmade\\tso\\tfar\\t(by\\tyou\\tor\\tothers);\\tthis\\ncan\\tcatch\\tserious\\tissues\\tearly\\ton.\\tFor\\texample,\\tthe\\tdistrict\\tprices\\tthat\\tyour\\tsystem\\toutputs\\tare\\tgoing\\tto\\tbe\\nfed\\tinto\\ta\\tdownstream\\tMachine\\tLearning\\tsystem,\\tand\\twe\\tassume\\tthat\\tthese\\tprices\\tare\\tgoing\\tto\\tbe\\tused\\tas\\nsuch.\\tBut\\twhat\\tif\\tthe\\tdownstream\\tsystem\\tactually\\tconverts\\tthe\\tprices\\tinto\\tcategories\\t(e.g.,\\t“cheap,”\\n“medium,”\\tor\\t“expensive”)\\tand\\tthen\\tuses\\tthose\\tcategories\\tinstead\\tof\\tthe\\tprices\\tthemselves?\\tIn\\tthis\\tcase,\\ngetting\\tthe\\tprice\\tperfectly\\tright\\tis\\tnot\\timportant\\tat\\tall;\\tyour\\tsystem\\tjust\\tneeds\\tto\\tget\\tthe\\tcategory\\tright.\\tIf\\nthat’s\\tso,\\tthen\\tthe\\tproblem\\tshould\\thave\\tbeen\\tframed\\tas\\ta\\tclassification\\ttask,\\tnot\\ta\\tregression\\ttask.\\tYou\\ndon’t\\twant\\tto\\tfind\\tthis\\tout\\tafter\\tworking\\ton\\ta\\tregression\\tsystem\\tfor\\tmonths.\\nFortunately,\\tafter\\ttalking\\twith\\tthe\\tteam\\tin\\tcharge\\tof\\tthe\\tdownstream\\tsystem,\\tyou\\tare\\tconfident\\tthat\\tthey\\tdo', 'indeed\\tneed\\tthe\\tactual\\tprices,\\tnot\\tjust\\tcategories.\\tGreat!\\tYou’re\\tall\\tset,\\tthe\\tlights\\tare\\tgreen,\\tand\\tyou\\tcan\\nstart\\tcoding\\tnow!', 'Get\\tthe\\tData\\nIt’s\\t\\ntime\\tto\\tget\\tyour\\thands\\tdirty.\\tDon’t\\thesitate\\tto\\tpick\\tup\\tyour\\tlaptop\\tand\\twalk\\tthrough\\tthe\\tfollowing\\tcode\\nexamples\\tin\\ta\\tJupyter\\tnotebook.\\tThe\\tfull\\tJupyter\\tnotebook\\tis\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', 'Create\\tthe\\tWorkspace\\nFirst\\tyou\\twill\\tneed\\tto\\thave\\tPython\\tinstalled.\\tIt\\tis\\tprobably\\talready\\tinstalled\\ton\\tyour\\tsystem.\\tIf\\tnot,\\tyou\\ncan\\tget\\tit\\tat\\t\\nhttps://www.python.org/\\n.\\n5\\nNext\\tyou\\tneed\\tto\\tcreate\\ta\\tworkspace\\tdirectory\\tfor\\tyour\\tMachine\\tLearning\\tcode\\tand\\tdatasets.\\tOpen\\ta\\nterminal\\tand\\ttype\\tthe\\tfollowing\\tcommands\\t(after\\tthe\\t\\n$\\n\\tprompts):\\n$\\texport\\tML_PATH=\"$HOME/ml\"\\t\\t\\t\\t\\t\\t#\\tYou\\tcan\\tchange\\tthe\\tpath\\tif\\tyou\\tprefer\\n$\\tmkdir\\t-p\\t$ML_PATH\\nYou\\twill\\tneed\\ta\\tnumber\\tof\\tPython\\tmodules:\\t\\nJupyter,\\tNumPy,\\tPandas,\\tMatplotlib,\\tand\\tScikit-Learn.\\tIf\\tyou\\nalready\\thave\\tJupyter\\trunning\\twith\\tall\\tthese\\tmodules\\tinstalled,\\tyou\\tcan\\tsafely\\tskip\\tto\\t\\n“Download\\tthe\\nData”\\n.\\tIf\\tyou\\tdon’t\\thave\\tthem\\tyet,\\tthere\\tare\\tmany\\tways\\tto\\tinstall\\tthem\\t(and\\ttheir\\tdependencies).\\tYou\\tcan\\nuse\\tyour\\tsystem’s\\tpackaging\\tsystem\\t(e.g.,\\tapt-get\\ton\\tUbuntu,\\tor\\tMacPorts\\tor\\tHomeBrew\\ton\\tmacOS),\\ninstall\\ta\\tScientific\\tPython\\tdistribution\\tsuch\\tas\\tAnaconda\\t\\nand\\tuse\\tits\\tpackaging\\tsystem,\\tor\\tjust\\tuse\\nPython’s\\town\\tpackaging\\tsystem,', 'pip,\\twhich\\tis\\tincluded\\tby\\tdefault\\twith\\tthe\\tPython\\tbinary\\tinstallers\\t(since\\nPython\\t2.7.9).\\n6\\n\\tYou\\tcan\\tcheck\\tto\\tsee\\tif\\tpip\\tis\\tinstalled\\tby\\ttyping\\tthe\\tfollowing\\tcommand:\\n$\\tpip3\\t--version\\npip\\t9.0.1\\tfrom\\t[...]/lib/python3.5/site-packages\\t(python\\t3.5)\\nYou\\tshould\\tmake\\tsure\\tyou\\thave\\ta\\trecent\\tversion\\tof\\tpip\\tinstalled,\\tat\\tthe\\tvery\\tleast\\t>1.4\\tto\\tsupport\\tbinary\\nmodule\\tinstallation\\t(a.k.a.\\twheels).\\tTo\\tupgrade\\tthe\\tpip\\tmodule,\\ttype:\\n7\\n$\\tpip3\\tinstall\\t--upgrade\\tpip\\nCollecting\\tpip\\n[...]\\nSuccessfully\\tinstalled\\tpip-9.0.1', \"CREATING\\tAN\\tISOLATED\\tENVIRONMENT\\nIf\\t\\nyou\\twould\\tlike\\tto\\twork\\tin\\tan\\tisolated\\tenvironment\\t(which\\tis\\tstrongly\\trecommended\\tso\\tyou\\tcan\\twork\\ton\\tdifferent\\tprojects\\twithout\\nhaving\\tconflicting\\tlibrary\\tversions),\\tinstall\\tvirtualenv\\tby\\trunning\\tthe\\tfollowing\\tpip\\tcommand:\\n$\\tpip3\\tinstall\\t--user\\t--upgrade\\tvirtualenv\\nCollecting\\tvirtualenv\\n[...]\\nSuccessfully\\tinstalled\\tvirtualenv\\nNow\\tyou\\tcan\\tcreate\\tan\\tisolated\\tPython\\tenvironment\\tby\\ttyping:\\n$\\tcd\\t$ML_PATH\\n$\\tvirtualenv\\tenv\\nUsing\\tbase\\tprefix\\t'[...]'\\nNew\\tpython\\texecutable\\tin\\t[...]/ml/env/bin/python3.5\\nAlso\\tcreating\\texecutable\\tin\\t[...]/ml/env/bin/python\\nInstalling\\tsetuptools,\\tpip,\\twheel...done.\\nNow\\tevery\\ttime\\tyou\\twant\\tto\\tactivate\\tthis\\tenvironment,\\tjust\\topen\\ta\\tterminal\\tand\\ttype:\\n$\\tcd\\t$ML_PATH\\n$\\tsource\\tenv/bin/activate\\nWhile\\tthe\\tenvironment\\tis\\tactive,\\tany\\tpackage\\tyou\\tinstall\\tusing\\tpip\\twill\\tbe\\tinstalled\\tin\\tthis\\tisolated\\tenvironment,\\tand\\tPython\\twill\\tonly\\thave\", 'access\\tto\\tthese\\tpackages\\t(if\\tyou\\talso\\twant\\taccess\\tto\\tthe\\tsystem’s\\tsite\\tpackages,\\tyou\\tshould\\tcreate\\tthe\\tenvironment\\tusing\\tvirtualenv’s\\t\\n--\\nsystem-site-packages\\n\\toption).\\tCheck\\tout\\tvirtualenv’s\\tdocumentation\\tfor\\tmore\\t\\ninformation.\\nNow\\tyou\\tcan\\tinstall\\tall\\tthe\\trequired\\tmodules\\tand\\ttheir\\tdependencies\\tusing\\tthis\\tsimple\\tpip\\tcommand:\\n$\\tpip3\\tinstall\\t--upgrade\\tjupyter\\tmatplotlib\\tnumpy\\tpandas\\tscipy\\tscikit-learn\\nCollecting\\tjupyter\\n\\t\\tDownloading\\tjupyter-1.0.0-py2.py3-none-any.whl\\nCollecting\\tmatplotlib\\n\\t\\t[...]\\nTo\\tcheck\\tyour\\tinstallation,\\ttry\\tto\\timport\\tevery\\tmodule\\tlike\\tthis:\\n$\\tpython3\\t-c\\t\"import\\tjupyter,\\tmatplotlib,\\tnumpy,\\tpandas,\\tscipy,\\tsklearn\"\\nThere\\tshould\\tbe\\tno\\toutput\\tand\\tno\\terror.\\tNow\\tyou\\tcan\\tfire\\tup\\t\\nJupyter\\tby\\ttyping:\\n$\\tjupyter\\tnotebook\\n[I\\t15:24\\tNotebookApp]\\tServing\\tnotebooks\\tfrom\\tlocal\\tdirectory:\\t[...]/ml\\n[I\\t15:24\\tNotebookApp]\\t0\\tactive\\tkernels\\n[I\\t15:24\\tNotebookApp]\\tThe\\tJupyter\\tNotebook\\tis\\trunning\\tat:\\thttp://localhost:8888/', '[I\\t15:24\\tNotebookApp]\\tUse\\tControl-C\\tto\\tstop\\tthis\\tserver\\tand\\tshut\\tdown\\tall\\nkernels\\t(twice\\tto\\tskip\\tconfirmation).\\nA\\tJupyter\\tserver\\tis\\tnow\\trunning\\tin\\tyour\\tterminal,\\tlistening\\tto\\tport\\t8888.\\tYou\\tcan\\tvisit\\tthis\\tserver\\tby\\nopening\\tyour\\tweb\\tbrowser\\tto\\t\\nhttp://localhost:8888/\\n\\t(this\\tusually\\thappens\\tautomatically\\twhen\\tthe\\tserver\\nstarts).\\tYou\\tshould\\tsee\\tyour\\tempty\\tworkspace\\tdirectory\\t(containing\\tonly\\tthe\\t\\nenv\\n\\tdirectory\\tif\\tyou\\tfollowed\\nthe\\tpreceding\\tvirtualenv\\tinstructions).\\nNow\\tcreate\\ta\\tnew\\t\\nPython\\tnotebook\\tby\\tclicking\\ton\\tthe\\tNew\\tbutton\\tand\\tselecting\\tthe\\tappropriate\\tPython\\nversion\\n8\\n\\t(see\\t\\nFigure\\t2-3\\n).', 'This\\tdoes\\tthree\\tthings:\\tfirst,\\tit\\tcreates\\ta\\tnew\\tnotebook\\tfile\\tcalled\\t\\nUntitled.ipynb\\n\\tin\\tyour\\tworkspace;\\nsecond,\\tit\\tstarts\\ta\\tJupyter\\tPython\\tkernel\\tto\\trun\\tthis\\tnotebook;\\tand\\tthird,\\tit\\topens\\tthis\\tnotebook\\tin\\ta\\tnew\\ntab.\\tYou\\tshould\\tstart\\tby\\trenaming\\tthis\\tnotebook\\tto\\t“Housing”\\t(this\\twill\\tautomatically\\trename\\tthe\\tfile\\tto\\nHousing.ipynb\\n)\\tby\\tclicking\\tUntitled\\tand\\ttyping\\tthe\\tnew\\tname.\\nFigure\\t2-3.\\t\\nYour\\tworkspace\\tin\\tJupyter\\nA\\tnotebook\\tcontains\\ta\\tlist\\tof\\tcells.\\tEach\\tcell\\tcan\\tcontain\\texecutable\\tcode\\tor\\tformatted\\ttext.\\tRight\\tnow\\tthe\\nnotebook\\tcontains\\tonly\\tone\\tempty\\tcode\\tcell,\\tlabeled\\t“In\\t[1]:”.\\tTry\\ttyping\\t\\nprint(\"Hello\\tworld!\")\\n\\tin\\nthe\\tcell,\\tand\\tclick\\ton\\tthe\\tplay\\tbutton\\t(see\\t\\nFigure\\t2-4\\n)\\tor\\tpress\\tShift-Enter.\\tThis\\tsends\\tthe\\tcurrent\\tcell\\tto\\nthis\\tnotebook’s\\tPython\\tkernel,\\twhich\\truns\\tit\\tand\\treturns\\tthe\\toutput.\\tThe\\tresult\\tis\\tdisplayed\\tbelow\\tthe\\tcell,\\nand\\tsince\\twe\\treached\\tthe\\tend\\tof\\tthe\\tnotebook,\\ta\\tnew\\tcell\\tis\\tautomatically\\tcreated.\\tGo\\tthrough\\tthe\\tUser', 'Interface\\tTour\\tfrom\\tJupyter’s\\tHelp\\tmenu\\tto\\tlearn\\tthe\\tbasics.\\nFigure\\t2-4.\\t\\nHello\\tworld\\tPython\\tnotebook', 'Download\\tthe\\tData\\nIn\\t\\ntypical\\tenvironments\\tyour\\tdata\\twould\\tbe\\tavailable\\tin\\ta\\trelational\\tdatabase\\t(or\\tsome\\tother\\tcommon\\ndatastore)\\tand\\tspread\\tacross\\tmultiple\\ttables/documents/files.\\tTo\\taccess\\tit,\\tyou\\twould\\tfirst\\tneed\\tto\\tget\\nyour\\tcredentials\\tand\\taccess\\tauthorizations,\\n9\\n\\tand\\tfamiliarize\\tyourself\\twith\\tthe\\tdata\\tschema.\\tIn\\tthis\\tproject,\\nhowever,\\tthings\\tare\\tmuch\\tsimpler:\\tyou\\twill\\tjust\\tdownload\\ta\\tsingle\\tcompressed\\tfile,\\t\\nhousing.tgz\\n,\\twhich\\ncontains\\ta\\tcomma-separated\\tvalue\\t(CSV)\\tfile\\tcalled\\t\\nhousing.csv\\n\\twith\\tall\\tthe\\tdata.\\nYou\\tcould\\tuse\\tyour\\tweb\\tbrowser\\tto\\tdownload\\tit,\\tand\\trun\\t\\ntar\\txzf\\thousing.tgz\\n\\tto\\tdecompress\\tthe\\tfile\\nand\\textract\\tthe\\tCSV\\tfile,\\tbut\\tit\\tis\\tpreferable\\tto\\tcreate\\ta\\tsmall\\tfunction\\tto\\tdo\\tthat.\\tIt\\tis\\tuseful\\tin\\tparticular\\tif\\ndata\\tchanges\\tregularly,\\tas\\tit\\tallows\\tyou\\tto\\twrite\\ta\\tsmall\\tscript\\tthat\\tyou\\tcan\\trun\\twhenever\\tyou\\tneed\\tto\\nfetch\\tthe\\tlatest\\tdata\\t(or\\tyou\\tcan\\tset\\tup\\ta\\tscheduled\\tjob\\tto\\tdo\\tthat\\tautomatically\\tat\\tregular\\tintervals).', 'Automating\\tthe\\tprocess\\tof\\tfetching\\tthe\\tdata\\tis\\talso\\tuseful\\tif\\tyou\\tneed\\tto\\tinstall\\tthe\\tdataset\\ton\\tmultiple\\nmachines.\\nHere\\tis\\tthe\\tfunction\\tto\\tfetch\\tthe\\tdata:\\n10\\nimport\\n\\t\\nos\\nimport\\n\\t\\ntarfile\\nfrom\\n\\t\\nsix.moves\\n\\t\\nimport\\n\\t\\nurllib\\nDOWNLOAD_ROOT\\n\\t\\n=\\n\\t\\n\"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\\nHOUSING_PATH\\n\\t\\n=\\n\\t\\nos\\n.\\npath\\n.\\njoin\\n(\\n\"datasets\"\\n,\\n\\t\\n\"housing\"\\n)\\nHOUSING_URL\\n\\t\\n=\\n\\t\\nDOWNLOAD_ROOT\\n\\t\\n+\\n\\t\\n\"datasets/housing/housing.tgz\"\\ndef\\n\\t\\nfetch_housing_data\\n(\\nhousing_url\\n=\\nHOUSING_URL\\n,\\n\\t\\nhousing_path\\n=\\nHOUSING_PATH\\n):\\n\\t\\t\\t\\t\\nif\\n\\t\\nnot\\n\\t\\nos\\n.\\npath\\n.\\nisdir\\n(\\nhousing_path\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nos\\n.\\nmakedirs\\n(\\nhousing_path\\n)\\n\\t\\t\\t\\t\\ntgz_path\\n\\t\\n=\\n\\t\\nos\\n.\\npath\\n.\\njoin\\n(\\nhousing_path\\n,\\n\\t\\n\"housing.tgz\"\\n)\\n\\t\\t\\t\\t\\nurllib\\n.\\nrequest\\n.\\nurlretrieve\\n(\\nhousing_url\\n,\\n\\t\\ntgz_path\\n)\\n\\t\\t\\t\\t\\nhousing_tgz\\n\\t\\n=\\n\\t\\ntarfile\\n.\\nopen\\n(\\ntgz_path\\n)\\n\\t\\t\\t\\t\\nhousing_tgz\\n.\\nextractall\\n(\\npath\\n=\\nhousing_path\\n)\\n\\t\\t\\t\\t\\nhousing_tgz\\n.\\nclose\\n()\\nNow\\twhen\\tyou\\tcall\\t\\nfetch_housing_data()\\n,\\tit\\tcreates\\ta\\t\\ndatasets/housing\\n\\tdirectory\\tin\\tyour\\tworkspace,', 'downloads\\tthe\\t\\nhousing.tgz\\n\\tfile,\\tand\\textracts\\tthe\\t\\nhousing.csv\\n\\tfrom\\tit\\tin\\tthis\\tdirectory.\\nNow\\tlet’s\\tload\\tthe\\tdata\\tusing\\t\\nPandas.\\tOnce\\tagain\\tyou\\tshould\\twrite\\ta\\tsmall\\tfunction\\tto\\tload\\tthe\\tdata:\\nimport\\n\\t\\npandas\\n\\t\\nas\\n\\t\\npd\\ndef\\n\\t\\nload_housing_data\\n(\\nhousing_path\\n=\\nHOUSING_PATH\\n):\\n\\t\\t\\t\\t\\ncsv_path\\n\\t\\n=\\n\\t\\nos\\n.\\npath\\n.\\njoin\\n(\\nhousing_path\\n,\\n\\t\\n\"housing.csv\"\\n)\\n\\t\\t\\t\\t\\nreturn\\n\\t\\npd\\n.\\nread_csv\\n(\\ncsv_path\\n)\\nThis\\tfunction\\treturns\\ta\\tPandas\\tDataFrame\\tobject\\tcontaining\\tall\\tthe\\t\\ndata.', 'Take\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\nLet’s\\t\\ntake\\ta\\tlook\\tat\\tthe\\ttop\\tfive\\trows\\tusing\\tthe\\tDataFrame’s\\t\\nhead()\\n\\tmethod\\t(see\\t\\nFigure\\t2-5\\n).\\nFigure\\t2-5.\\t\\nTop\\tfive\\trows\\tin\\tthe\\tdataset\\nEach\\trow\\trepresents\\tone\\tdistrict.\\tThere\\tare\\t10\\tattributes\\t(you\\tcan\\tsee\\tthe\\tfirst\\t6\\tin\\tthe\\tscreenshot):\\nlongitude\\n,\\t\\nlatitude\\n,\\t\\nhousing_median_age\\n,\\t\\ntotal_rooms\\n,\\t\\ntotal_bedrooms\\n,\\t\\npopulation\\n,\\nhouseholds\\n,\\t\\nmedian_income\\n,\\t\\nmedian_house_value\\n,\\tand\\t\\nocean_proximity\\n.\\nThe\\t\\ninfo()\\n\\t\\nmethod\\tis\\tuseful\\tto\\tget\\ta\\tquick\\tdescription\\tof\\tthe\\tdata,\\tin\\tparticular\\tthe\\ttotal\\tnumber\\tof\\trows,\\nand\\teach\\tattribute’s\\ttype\\tand\\tnumber\\tof\\tnon-null\\tvalues\\t(see\\t\\nFigure\\t2-6\\n).\\nFigure\\t2-6.\\t\\nHousing\\tinfo\\nThere\\tare\\t20,640\\tinstances\\tin\\tthe\\tdataset,\\twhich\\tmeans\\tthat\\tit\\tis\\tfairly\\tsmall\\tby\\tMachine\\tLearning\\nstandards,\\tbut\\tit’s\\tperfect\\tto\\tget\\tstarted.\\tNotice\\tthat\\tthe\\t\\ntotal_bedrooms\\n\\tattribute\\thas\\tonly\\t20,433\\tnon-\\nnull\\tvalues,\\tmeaning\\tthat\\t207\\tdistricts\\tare\\tmissing\\tthis\\tfeature.\\tWe\\twill\\tneed\\tto\\ttake\\tcare\\tof\\tthis\\tlater.', 'All\\tattributes\\tare\\tnumerical,\\texcept\\tthe\\t\\nocean_proximity\\n\\tfield.\\tIts\\ttype\\tis\\t\\nobject\\n,\\tso\\tit\\tcould\\thold\\tany\\nkind\\tof\\tPython\\tobject,\\tbut\\tsince\\tyou\\tloaded\\tthis\\tdata\\tfrom\\ta\\tCSV\\tfile\\tyou\\tknow\\tthat\\tit\\tmust\\tbe\\ta\\ttext\\nattribute.\\tWhen\\tyou\\tlooked\\tat\\tthe\\ttop\\tfive\\trows,\\tyou\\tprobably\\tnoticed\\tthat\\tthe\\tvalues\\tin\\tthe\\nocean_proximity\\n\\tcolumn\\twere\\trepetitive,\\twhich\\tmeans\\tthat\\tit\\tis\\tprobably\\ta\\tcategorical\\tattribute.\\tYou\\ncan\\tfind\\tout\\twhat\\tcategories\\texist\\tand\\thow\\tmany\\tdistricts\\tbelong\\tto\\teach\\tcategory\\tby\\tusing\\tthe', 'value_counts()\\n\\t\\nmethod:\\n>>>\\t\\nhousing\\n[\\n\"ocean_proximity\"\\n]\\n.\\nvalue_counts\\n()\\n<1H\\tOCEAN\\t\\t\\t\\t\\t9136\\nINLAND\\t\\t\\t\\t\\t\\t\\t\\t6551\\nNEAR\\tOCEAN\\t\\t\\t\\t2658\\nNEAR\\tBAY\\t\\t\\t\\t\\t\\t2290\\nISLAND\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t5\\nName:\\tocean_proximity,\\tdtype:\\tint64\\nLet’s\\tlook\\tat\\tthe\\tother\\tfields.\\tThe\\t\\ndescribe()\\n\\t\\nmethod\\tshows\\ta\\tsummary\\tof\\tthe\\tnumerical\\tattributes\\n(\\nFigure\\t2-7\\n).\\nFigure\\t2-7.\\t\\nSummary\\tof\\teach\\tnumerical\\tattribute\\nThe\\t\\ncount\\n,\\t\\nmean\\n,\\t\\nmin\\n,\\tand\\t\\nmax\\n\\trows\\tare\\tself-explanatory.\\tNote\\tthat\\tthe\\tnull\\tvalues\\tare\\tignored\\t(so,\\tfor\\nexample,\\t\\ncount\\n\\tof\\t\\ntotal_bedrooms\\n\\tis\\t20,433,\\tnot\\t20,640).\\tThe\\t\\nstd\\n\\trow\\tshows\\tthe\\t\\nstandard\\tdeviation\\n,\\nwhich\\tmeasures\\thow\\tdispersed\\tthe\\tvalues\\tare.\\n11\\n\\tThe\\t25%,\\t50%,\\tand\\t75%\\trows\\tshow\\tthe\\tcorresponding\\npercentiles\\n:\\ta\\t\\npercentile\\tindicates\\tthe\\tvalue\\tbelow\\twhich\\ta\\tgiven\\tpercentage\\tof\\tobservations\\tin\\ta\\tgroup\\nof\\tobservations\\tfalls.\\tFor\\texample,\\t25%\\tof\\tthe\\tdistricts\\thave\\ta\\t\\nhousing_median_age\\n\\tlower\\tthan\\t18,\\nwhile\\t50%\\tare\\tlower\\tthan\\t29\\tand\\t75%\\tare\\tlower\\tthan\\t37.\\tThese\\tare\\toften\\tcalled\\tthe\\t25\\nth', 'th\\n\\tpercentile\\t(or\\n1\\nst\\n\\t\\nquartile\\n),\\tthe\\tmedian,\\tand\\tthe\\t75\\nth\\n\\tpercentile\\t(or\\t3\\nrd\\n\\tquartile).\\nAnother\\tquick\\tway\\tto\\tget\\ta\\tfeel\\tof\\tthe\\ttype\\tof\\tdata\\tyou\\tare\\tdealing\\twith\\tis\\tto\\tplot\\ta\\t\\nhistogram\\tfor\\teach\\nnumerical\\tattribute.\\tA\\thistogram\\tshows\\tthe\\tnumber\\tof\\tinstances\\t(on\\tthe\\tvertical\\taxis)\\tthat\\thave\\ta\\tgiven\\nvalue\\trange\\t(on\\tthe\\thorizontal\\taxis).\\tYou\\tcan\\teither\\tplot\\tthis\\tone\\tattribute\\tat\\ta\\ttime,\\tor\\tyou\\tcan\\tcall\\tthe\\nhist()\\n\\tmethod\\ton\\tthe\\twhole\\tdataset,\\tand\\tit\\twill\\tplot\\ta\\thistogram\\tfor\\teach\\tnumerical\\tattribute\\t(see\\nFigure\\t2-8\\n).\\tFor\\texample,\\tyou\\tcan\\tsee\\tthat\\tslightly\\tover\\t800\\tdistricts\\thave\\ta\\t\\nmedian_house_value\\n\\tequal\\nto\\tabout\\t$100,000.\\n%\\nmatplotlib\\n\\t\\ninline\\n\\t\\t\\t\\n#\\tonly\\tin\\ta\\tJupyter\\tnotebook\\nimport\\n\\t\\nmatplotlib.pyplot\\n\\t\\nas\\n\\t\\nplt\\nhousing\\n.\\nhist\\n(\\nbins\\n=\\n50\\n,\\n\\t\\nfigsize\\n=\\n(\\n20\\n,\\n15\\n))\\nplt\\n.\\nshow\\n()', 'NOTE\\nThe\\t\\nhist()\\n\\tmethod\\trelies\\ton\\t\\nMatplotlib,\\twhich\\tin\\tturn\\trelies\\ton\\ta\\tuser-specified\\tgraphical\\tbackend\\tto\\tdraw\\ton\\tyour\\tscreen.\\tSo\\nbefore\\tyou\\tcan\\tplot\\tanything,\\tyou\\tneed\\tto\\tspecify\\twhich\\tbackend\\tMatplotlib\\tshould\\tuse.\\tThe\\tsimplest\\toption\\tis\\tto\\tuse\\t\\nJupyter’s\\nmagic\\tcommand\\t\\n%matplotlib\\tinline\\n.\\tThis\\ttells\\tJupyter\\tto\\tset\\tup\\tMatplotlib\\tso\\tit\\tuses\\tJupyter’s\\town\\tbackend.\\tPlots\\tare\\tthen\\nrendered\\twithin\\tthe\\tnotebook\\titself.\\tNote\\tthat\\tcalling\\t\\nshow()\\n\\tis\\t\\noptional\\tin\\ta\\tJupyter\\tnotebook,\\tas\\tJupyter\\twill\\tautomatically\\ndisplay\\tplots\\twhen\\ta\\tcell\\tis\\texecuted.\\nFigure\\t2-8.\\t\\nA\\thistogram\\tfor\\teach\\tnumerical\\tattribute\\nNotice\\ta\\tfew\\tthings\\tin\\tthese\\thistograms:\\n1\\n.\\t\\nFirst,\\tthe\\tmedian\\tincome\\tattribute\\tdoes\\tnot\\tlook\\tlike\\tit\\tis\\texpressed\\tin\\tUS\\tdollars\\t(USD).\\tAfter\\nchecking\\twith\\tthe\\tteam\\tthat\\tcollected\\tthe\\tdata,\\tyou\\tare\\ttold\\tthat\\tthe\\tdata\\thas\\tbeen\\tscaled\\tand\\tcapped\\nat\\t15\\t(actually\\t15.0001)\\tfor\\thigher\\tmedian\\tincomes,\\tand\\tat\\t0.5\\t(actually\\t0.4999)\\tfor\\tlower\\tmedian\\nincomes.\\tWorking\\twith', 'preprocessed\\tattributes\\tis\\tcommon\\tin\\tMachine\\tLearning,\\tand\\tit\\tis\\tnot\\nnecessarily\\ta\\tproblem,\\tbut\\tyou\\tshould\\ttry\\tto\\tunderstand\\thow\\tthe\\tdata\\twas\\tcomputed.\\n2\\n.\\t\\nThe\\thousing\\tmedian\\tage\\tand\\tthe\\tmedian\\thouse\\tvalue\\twere\\talso\\tcapped.\\tThe\\tlatter\\tmay\\tbe\\ta\\tserious\\nproblem\\tsince\\tit\\tis\\tyour\\t\\ntarget\\tattribute\\t(your\\tlabels).\\tYour\\tMachine\\tLearning\\talgorithms\\tmay\\tlearn\\nthat\\tprices\\tnever\\tgo\\tbeyond\\tthat\\tlimit.\\tYou\\tneed\\tto\\tcheck\\twith\\tyour\\tclient\\tteam\\t(the\\tteam\\tthat\\twill\\tuse\\nyour\\tsystem’s\\toutput)\\tto\\tsee\\tif\\tthis\\tis\\ta\\tproblem\\tor\\tnot.\\tIf\\tthey\\ttell\\tyou\\tthat\\tthey\\tneed\\tprecise\\npredictions\\teven\\tbeyond\\t$500,000,\\tthen\\tyou\\thave\\tmainly\\ttwo\\toptions:\\na\\n.\\t\\nCollect\\tproper\\tlabels\\tfor\\tthe\\tdistricts\\twhose\\tlabels\\twere\\tcapped.\\nb\\n.\\t\\nRemove\\tthose\\tdistricts\\tfrom\\tthe\\ttraining\\tset\\t(and\\talso\\tfrom\\tthe\\ttest\\tset,\\tsince\\tyour\\tsystem\\tshould\\nnot\\tbe\\tevaluated\\tpoorly\\tif\\tit\\tpredicts\\tvalues\\tbeyond\\t$500,000).', '3\\n.\\t\\nThese\\tattributes\\thave\\tvery\\tdifferent\\tscales.\\tWe\\twill\\tdiscuss\\tthis\\tlater\\tin\\tthis\\tchapter\\twhen\\twe\\nexplore\\tfeature\\tscaling.\\n4\\n.\\t\\nFinally,\\tmany\\thistograms\\tare\\t\\ntail\\theavy\\n:\\t\\nthey\\textend\\tmuch\\tfarther\\tto\\tthe\\tright\\tof\\tthe\\tmedian\\tthan\\tto\\nthe\\tleft.\\tThis\\tmay\\tmake\\tit\\ta\\tbit\\tharder\\tfor\\tsome\\tMachine\\tLearning\\talgorithms\\tto\\tdetect\\tpatterns.\\tWe\\nwill\\ttry\\ttransforming\\tthese\\tattributes\\tlater\\ton\\tto\\thave\\tmore\\tbell-shaped\\tdistributions.\\nHopefully\\tyou\\tnow\\thave\\ta\\tbetter\\tunderstanding\\tof\\tthe\\tkind\\tof\\tdata\\tyou\\t\\nare\\tdealing\\twith.\\nWARNING\\nWait!\\tBefore\\tyou\\tlook\\tat\\tthe\\tdata\\tany\\tfurther,\\tyou\\tneed\\tto\\tcreate\\ta\\ttest\\tset,\\tput\\tit\\taside,\\tand\\tnever\\tlook\\tat\\tit.', 'Create\\ta\\tTest\\tSet\\nIt\\t\\nmay\\tsound\\tstrange\\tto\\tvoluntarily\\tset\\taside\\tpart\\tof\\tthe\\tdata\\tat\\tthis\\tstage.\\tAfter\\tall,\\tyou\\thave\\tonly\\ttaken\\ta\\nquick\\tglance\\tat\\tthe\\tdata,\\tand\\tsurely\\tyou\\tshould\\tlearn\\ta\\twhole\\tlot\\tmore\\tabout\\tit\\tbefore\\tyou\\tdecide\\twhat\\nalgorithms\\tto\\tuse,\\tright?\\tThis\\tis\\ttrue,\\tbut\\tyour\\tbrain\\tis\\tan\\tamazing\\tpattern\\tdetection\\tsystem,\\twhich\\tmeans\\nthat\\tit\\tis\\thighly\\tprone\\tto\\t\\noverfitting:\\tif\\tyou\\tlook\\tat\\tthe\\ttest\\tset,\\tyou\\tmay\\tstumble\\tupon\\tsome\\tseemingly\\ninteresting\\tpattern\\tin\\tthe\\ttest\\tdata\\tthat\\tleads\\tyou\\tto\\tselect\\ta\\tparticular\\tkind\\tof\\tMachine\\tLearning\\tmodel.\\nWhen\\tyou\\testimate\\tthe\\tgeneralization\\terror\\tusing\\tthe\\ttest\\tset,\\tyour\\testimate\\twill\\tbe\\ttoo\\toptimistic\\tand\\tyou\\nwill\\tlaunch\\ta\\tsystem\\tthat\\twill\\tnot\\tperform\\tas\\twell\\tas\\texpected.\\tThis\\tis\\t\\ncalled\\t\\ndata\\tsnooping\\n\\tbias.\\nCreating\\ta\\ttest\\tset\\tis\\ttheoretically\\tquite\\tsimple:\\tjust\\tpick\\tsome\\tinstances\\trandomly,\\ttypically\\t20%\\tof\\tthe\\ndataset,\\tand\\tset\\tthem\\taside:\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\ndef\\n\\t\\nsplit_train_test\\n(\\ndata\\n,\\n\\t\\ntest_ratio\\n):\\n\\t\\t\\t\\t\\nshuffled_indices\\n\\t\\n=', '=\\n\\t\\nnp\\n.\\nrandom\\n.\\npermutation\\n(\\nlen\\n(\\ndata\\n))\\n\\t\\t\\t\\t\\ntest_set_size\\n\\t\\n=\\n\\t\\nint\\n(\\nlen\\n(\\ndata\\n)\\n\\t\\n*\\n\\t\\ntest_ratio\\n)\\n\\t\\t\\t\\t\\ntest_indices\\n\\t\\n=\\n\\t\\nshuffled_indices\\n[:\\ntest_set_size\\n]\\n\\t\\t\\t\\t\\ntrain_indices\\n\\t\\n=\\n\\t\\nshuffled_indices\\n[\\ntest_set_size\\n:]\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ndata\\n.\\niloc\\n[\\ntrain_indices\\n],\\n\\t\\ndata\\n.\\niloc\\n[\\ntest_indices\\n]\\nYou\\tcan\\tthen\\tuse\\tthis\\tfunction\\tlike\\tthis:\\n>>>\\t\\ntrain_set\\n,\\n\\t\\ntest_set\\n\\t\\n=\\n\\t\\nsplit_train_test\\n(\\nhousing\\n,\\n\\t\\n0.2\\n)\\n>>>\\t\\nprint\\n(\\nlen\\n(\\ntrain_set\\n),\\n\\t\\n\"train\\t+\"\\n,\\n\\t\\nlen\\n(\\ntest_set\\n),\\n\\t\\n\"test\"\\n)\\n16512\\ttrain\\t+\\t4128\\ttest\\nWell,\\tthis\\tworks,\\tbut\\tit\\tis\\tnot\\tperfect:\\tif\\tyou\\trun\\tthe\\tprogram\\tagain,\\tit\\twill\\tgenerate\\ta\\tdifferent\\ttest\\tset!\\nOver\\ttime,\\tyou\\t(or\\tyour\\tMachine\\tLearning\\talgorithms)\\twill\\tget\\tto\\tsee\\tthe\\twhole\\tdataset,\\twhich\\tis\\twhat\\nyou\\twant\\tto\\tavoid.\\nOne\\tsolution\\tis\\tto\\tsave\\tthe\\ttest\\tset\\ton\\tthe\\tfirst\\trun\\tand\\tthen\\tload\\tit\\tin\\tsubsequent\\truns.\\tAnother\\toption\\tis\\tto\\nset\\tthe\\trandom\\tnumber\\tgenerator’s\\tseed\\t(e.g.,\\t\\nnp.random.seed(42)\\n)\\n12\\n\\tbefore\\tcalling\\nnp.random.permutation()\\n,\\tso', ',\\tso\\t\\nthat\\tit\\talways\\tgenerates\\tthe\\tsame\\tshuffled\\tindices.\\nBut\\tboth\\tthese\\tsolutions\\twill\\tbreak\\tnext\\ttime\\tyou\\tfetch\\tan\\tupdated\\tdataset.\\tA\\tcommon\\tsolution\\tis\\tto\\tuse\\neach\\tinstance’s\\tidentifier\\tto\\tdecide\\twhether\\tor\\tnot\\tit\\tshould\\tgo\\tin\\tthe\\ttest\\tset\\t(assuming\\tinstances\\thave\\ta\\nunique\\tand\\timmutable\\tidentifier).\\tFor\\texample,\\tyou\\tcould\\tcompute\\ta\\thash\\tof\\teach\\tinstance’s\\tidentifier,\\nkeep\\tonly\\tthe\\tlast\\tbyte\\tof\\tthe\\thash,\\tand\\tput\\tthe\\tinstance\\tin\\tthe\\ttest\\tset\\tif\\tthis\\tvalue\\tis\\tlower\\tor\\tequal\\tto\\t51\\n(~20%\\tof\\t256).\\tThis\\tensures\\tthat\\tthe\\ttest\\tset\\twill\\tremain\\tconsistent\\tacross\\tmultiple\\truns,\\teven\\tif\\tyou\\nrefresh\\tthe\\tdataset.\\tThe\\tnew\\ttest\\tset\\twill\\tcontain\\t20%\\tof\\tthe\\tnew\\tinstances,\\tbut\\tit\\twill\\tnot\\tcontain\\tany\\ninstance\\tthat\\twas\\tpreviously\\tin\\tthe\\ttraining\\tset.\\tHere\\tis\\ta\\tpossible\\timplementation:\\nimport\\n\\t\\nhashlib\\ndef\\n\\t\\ntest_set_check\\n(\\nidentifier\\n,\\n\\t\\ntest_ratio\\n,\\n\\t\\nhash\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nhash\\n(\\nnp\\n.\\nint64\\n(\\nidentifier\\n))\\n.\\ndigest\\n()[\\n-\\n1\\n]\\n\\t\\n<\\n\\t\\n256\\n\\t\\n*\\n\\t\\ntest_ratio\\ndef\\n\\t\\nsplit_train_test_by_id\\n(\\ndata\\n,', 'data\\n,\\n\\t\\ntest_ratio\\n,\\n\\t\\nid_column\\n,\\n\\t\\nhash\\n=\\nhashlib\\n.\\nmd5\\n):\\n\\t\\t\\t\\t\\nids\\n\\t\\n=\\n\\t\\ndata\\n[\\nid_column\\n]\\n\\t\\t\\t\\t\\nin_test_set\\n\\t\\n=\\n\\t\\nids\\n.\\napply\\n(\\nlambda\\n\\t\\nid_\\n:\\n\\t\\ntest_set_check\\n(\\nid_\\n,\\n\\t\\ntest_ratio\\n,\\n\\t\\nhash\\n))\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ndata\\n.\\nloc\\n[\\n~\\nin_test_set\\n],\\n\\t\\ndata\\n.\\nloc\\n[\\nin_test_set\\n]', 'Unfortunately,\\tthe\\thousing\\tdataset\\tdoes\\tnot\\thave\\tan\\tidentifier\\tcolumn.\\tThe\\tsimplest\\tsolution\\tis\\tto\\tuse\\tthe\\nrow\\tindex\\tas\\tthe\\tID:\\nhousing_with_id\\n\\t\\n=\\n\\t\\nhousing\\n.\\nreset_index\\n()\\n\\t\\t\\t\\n#\\tadds\\tan\\t`index`\\tcolumn\\ntrain_set\\n,\\n\\t\\ntest_set\\n\\t\\n=\\n\\t\\nsplit_train_test_by_id\\n(\\nhousing_with_id\\n,\\n\\t\\n0.2\\n,\\n\\t\\n\"index\"\\n)\\nIf\\tyou\\tuse\\tthe\\trow\\tindex\\tas\\ta\\tunique\\tidentifier,\\tyou\\tneed\\tto\\tmake\\tsure\\tthat\\tnew\\tdata\\tgets\\tappended\\tto\\tthe\\nend\\tof\\tthe\\tdataset,\\tand\\tno\\trow\\tever\\tgets\\tdeleted.\\tIf\\tthis\\tis\\tnot\\tpossible,\\tthen\\tyou\\tcan\\ttry\\tto\\tuse\\tthe\\tmost\\nstable\\tfeatures\\tto\\tbuild\\ta\\tunique\\tidentifier.\\tFor\\texample,\\ta\\tdistrict’s\\tlatitude\\tand\\tlongitude\\tare\\tguaranteed\\nto\\tbe\\tstable\\tfor\\ta\\tfew\\tmillion\\tyears,\\tso\\tyou\\tcould\\tcombine\\tthem\\tinto\\tan\\tID\\tlike\\tso:\\n13\\nhousing_with_id\\n[\\n\"id\"\\n]\\n\\t\\n=\\n\\t\\nhousing\\n[\\n\"longitude\"\\n]\\n\\t\\n*\\n\\t\\n1000\\n\\t\\n+\\n\\t\\nhousing\\n[\\n\"latitude\"\\n]\\ntrain_set\\n,\\n\\t\\ntest_set\\n\\t\\n=\\n\\t\\nsplit_train_test_by_id\\n(\\nhousing_with_id\\n,\\n\\t\\n0.2\\n,\\n\\t\\n\"id\"\\n)\\nScikit-Learn\\tprovides\\ta\\tfew\\tfunctions\\tto\\tsplit\\tdatasets\\tinto\\tmultiple\\tsubsets\\tin\\tvarious\\tways.\\tThe\\t\\nsimplest', 'simplest\\nfunction\\tis\\t\\ntrain_test_split\\n,\\twhich\\tdoes\\tpretty\\tmuch\\tthe\\tsame\\tthing\\tas\\tthe\\tfunction\\nsplit_train_test\\n\\tdefined\\tearlier,\\twith\\ta\\tcouple\\tof\\tadditional\\tfeatures.\\tFirst\\tthere\\tis\\ta\\t\\nrandom_state\\nparameter\\tthat\\tallows\\tyou\\tto\\tset\\tthe\\trandom\\tgenerator\\tseed\\tas\\texplained\\tpreviously,\\tand\\tsecond\\tyou\\tcan\\npass\\tit\\tmultiple\\tdatasets\\twith\\tan\\tidentical\\tnumber\\tof\\trows,\\tand\\tit\\twill\\tsplit\\tthem\\ton\\tthe\\tsame\\tindices\\t(this\\nis\\tvery\\tuseful,\\tfor\\texample,\\tif\\tyou\\thave\\ta\\tseparate\\tDataFrame\\tfor\\tlabels):\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\ntrain_test_split\\ntrain_set\\n,\\n\\t\\ntest_set\\n\\t\\n=\\n\\t\\ntrain_test_split\\n(\\nhousing\\n,\\n\\t\\ntest_size\\n=\\n0.2\\n,\\n\\t\\nrandom_state\\n=\\n42\\n)\\nSo\\tfar\\twe\\thave\\tconsidered\\tpurely\\trandom\\tsampling\\tmethods.\\tThis\\tis\\tgenerally\\tfine\\tif\\tyour\\tdataset\\tis\\tlarge\\nenough\\t(especially\\trelative\\tto\\tthe\\tnumber\\tof\\tattributes),\\tbut\\tif\\tit\\tis\\tnot,\\tyou\\trun\\tthe\\trisk\\tof\\tintroducing\\ta\\nsignificant\\t\\nsampling\\tbias.\\tWhen\\ta\\tsurvey\\tcompany\\tdecides\\tto\\tcall\\t1,000\\tpeople\\tto\\task\\tthem\\ta\\tfew', 'questions,\\tthey\\tdon’t\\tjust\\tpick\\t1,000\\tpeople\\trandomly\\tin\\ta\\tphone\\tbooth.\\tThey\\ttry\\tto\\tensure\\tthat\\tthese\\t1,000\\npeople\\tare\\trepresentative\\tof\\tthe\\twhole\\tpopulation.\\tFor\\texample,\\tthe\\tUS\\tpopulation\\tis\\tcomposed\\tof\\t51.3%\\nfemale\\tand\\t48.7%\\tmale,\\tso\\ta\\twell-conducted\\tsurvey\\tin\\tthe\\tUS\\twould\\ttry\\tto\\tmaintain\\tthis\\tratio\\tin\\tthe\\nsample:\\t513\\tfemale\\tand\\t487\\tmale.\\tThis\\tis\\tcalled\\t\\nstratified\\tsampling\\n:\\t\\nthe\\tpopulation\\tis\\tdivided\\tinto\\nhomogeneous\\tsubgroups\\tcalled\\t\\nstrata\\n,\\tand\\tthe\\tright\\tnumber\\tof\\tinstances\\tis\\tsampled\\tfrom\\teach\\tstratum\\tto\\nguarantee\\tthat\\tthe\\ttest\\tset\\tis\\trepresentative\\tof\\tthe\\toverall\\tpopulation.\\tIf\\tthey\\tused\\tpurely\\trandom\\tsampling,\\nthere\\twould\\tbe\\tabout\\t12%\\tchance\\tof\\tsampling\\ta\\tskewed\\ttest\\tset\\twith\\teither\\tless\\tthan\\t49%\\tfemale\\tor\\tmore\\nthan\\t54%\\tfemale.\\tEither\\tway,\\tthe\\tsurvey\\tresults\\twould\\tbe\\tsignificantly\\tbiased.\\nSuppose\\tyou\\tchatted\\twith\\texperts\\twho\\ttold\\tyou\\tthat\\tthe\\tmedian\\tincome\\tis\\ta\\tvery\\timportant\\tattribute\\tto', 'predict\\tmedian\\thousing\\tprices.\\tYou\\tmay\\twant\\tto\\tensure\\tthat\\tthe\\ttest\\tset\\tis\\trepresentative\\tof\\tthe\\tvarious\\ncategories\\tof\\tincomes\\tin\\tthe\\twhole\\tdataset.\\tSince\\tthe\\tmedian\\tincome\\tis\\ta\\tcontinuous\\tnumerical\\tattribute,\\nyou\\tfirst\\tneed\\tto\\tcreate\\tan\\tincome\\tcategory\\tattribute.\\tLet’s\\tlook\\tat\\tthe\\tmedian\\tincome\\thistogram\\tmore\\nclosely\\t(see\\t\\nFigure\\t2-8\\n):\\tmost\\tmedian\\tincome\\tvalues\\tare\\tclustered\\taround\\t$20,000–$50,000,\\tbut\\tsome\\nmedian\\tincomes\\tgo\\tfar\\tbeyond\\t$60,000.\\tIt\\tis\\timportant\\tto\\thave\\ta\\tsufficient\\tnumber\\tof\\tinstances\\tin\\tyour\\ndataset\\tfor\\teach\\tstratum,\\tor\\telse\\tthe\\testimate\\tof\\tthe\\tstratum’s\\timportance\\tmay\\tbe\\tbiased.\\tThis\\tmeans\\tthat\\nyou\\tshould\\tnot\\thave\\ttoo\\tmany\\tstrata,\\tand\\teach\\tstratum\\tshould\\tbe\\tlarge\\tenough.\\tThe\\tfollowing\\tcode\\tcreates\\nan\\tincome\\tcategory\\tattribute\\tby\\tdividing\\tthe\\tmedian\\tincome\\tby\\t1.5\\t(to\\tlimit\\tthe\\tnumber\\tof\\tincome', 'categories),\\tand\\trounding\\tup\\tusing\\t\\nceil\\n\\t(to\\thave\\tdiscrete\\tcategories),\\tand\\tthen\\tmerging\\tall\\tthe\\tcategories\\ngreater\\tthan\\t5\\tinto\\tcategory\\t5:\\nhousing\\n[\\n\"income_cat\"\\n]\\n\\t\\n=\\n\\t\\nnp\\n.\\nceil\\n(\\nhousing\\n[\\n\"median_income\"\\n]\\n\\t\\n/\\n\\t\\n1.5\\n)\\nhousing\\n[\\n\"income_cat\"\\n]\\n.\\nwhere\\n(\\nhousing\\n[\\n\"income_cat\"\\n]\\n\\t\\n<\\n\\t\\n5\\n,\\n\\t\\n5.0\\n,\\n\\t\\ninplace\\n=\\nTrue\\n)\\nThese\\tincome\\tcategories\\tare\\trepresented\\ton\\t\\nFigure\\t2-9\\n):\\nFigure\\t2-9.\\t\\nHistogram\\tof\\tincome\\tcategories\\nNow\\tyou\\tare\\tready\\tto\\tdo\\t\\nstratified\\tsampling\\tbased\\ton\\tthe\\tincome\\tcategory.\\tFor\\tthis\\tyou\\tcan\\tuse\\tScikit-\\nLearn’s\\t\\nStratifiedShuffleSplit\\n\\tclass:\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\nStratifiedShuffleSplit\\nsplit\\n\\t\\n=\\n\\t\\nStratifiedShuffleSplit\\n(\\nn_splits\\n=\\n1\\n,\\n\\t\\ntest_size\\n=\\n0.2\\n,\\n\\t\\nrandom_state\\n=\\n42\\n)\\nfor\\n\\t\\ntrain_index\\n,\\n\\t\\ntest_index\\n\\t\\nin\\n\\t\\nsplit\\n.\\nsplit\\n(\\nhousing\\n,\\n\\t\\nhousing\\n[\\n\"income_cat\"\\n]):\\n\\t\\t\\t\\t\\nstrat_train_set\\n\\t\\n=\\n\\t\\nhousing\\n.\\nloc\\n[\\ntrain_index\\n]\\n\\t\\t\\t\\t\\nstrat_test_set\\n\\t\\n=\\n\\t\\nhousing\\n.\\nloc\\n[\\ntest_index\\n]', ']\\nLet’s\\tsee\\tif\\tthis\\tworked\\tas\\texpected.\\tYou\\tcan\\tstart\\tby\\tlooking\\tat\\tthe\\tincome\\tcategory\\tproportions\\tin\\tthe\\nfull\\thousing\\tdataset:\\n>>>\\t\\nhousing\\n[\\n\"income_cat\"\\n]\\n.\\nvalue_counts\\n()\\n\\t\\n/\\n\\t\\nlen\\n(\\nhousing\\n)\\n3.0\\t\\t\\t\\t0.350581\\n2.0\\t\\t\\t\\t0.318847\\n4.0\\t\\t\\t\\t0.176308\\n5.0\\t\\t\\t\\t0.114438\\n1.0\\t\\t\\t\\t0.039826\\nName:\\tincome_cat,\\tdtype:\\tfloat64\\nWith\\tsimilar\\tcode\\tyou\\tcan\\tmeasure\\tthe\\tincome\\tcategory\\tproportions\\tin\\tthe\\ttest\\tset.\\t\\nFigure\\t2-10\\n\\tcompares\\nthe\\tincome\\tcategory\\tproportions\\tin\\tthe\\toverall\\tdataset,\\tin\\tthe\\ttest\\tset\\tgenerated\\twith\\tstratified\\tsampling,\\nand\\tin\\ta\\ttest\\tset\\tgenerated\\tusing\\tpurely\\trandom\\tsampling.\\tAs\\tyou\\tcan\\tsee,\\tthe\\ttest\\tset\\tgenerated\\tusing', 'stratified\\tsampling\\thas\\tincome\\tcategory\\tproportions\\talmost\\tidentical\\tto\\tthose\\tin\\tthe\\tfull\\tdataset,\\twhereas\\nthe\\ttest\\tset\\tgenerated\\tusing\\tpurely\\trandom\\tsampling\\tis\\tquite\\tskewed.\\nFigure\\t2-10.\\t\\nSampling\\tbias\\tcomparison\\tof\\tstratified\\tversus\\tpurely\\trandom\\tsampling\\nNow\\tyou\\tshould\\tremove\\tthe\\t\\nincome_cat\\n\\tattribute\\tso\\tthe\\tdata\\tis\\tback\\tto\\tits\\toriginal\\tstate:\\nfor\\n\\t\\nset_\\n\\t\\nin\\n\\t\\n(\\nstrat_train_set\\n,\\n\\t\\nstrat_test_set\\n):\\n\\t\\t\\t\\t\\nset_\\n.\\ndrop\\n(\\n\"income_cat\"\\n,\\n\\t\\naxis\\n=\\n1\\n,\\n\\t\\ninplace\\n=\\nTrue\\n)\\nWe\\tspent\\tquite\\ta\\tbit\\tof\\ttime\\ton\\ttest\\tset\\tgeneration\\tfor\\ta\\tgood\\treason:\\tthis\\tis\\tan\\toften\\tneglected\\tbut\\tcritical\\npart\\tof\\ta\\tMachine\\tLearning\\tproject.\\tMoreover,\\tmany\\tof\\tthese\\tideas\\twill\\tbe\\tuseful\\tlater\\twhen\\twe\\tdiscuss\\ncross-validation.\\tNow\\tit’s\\ttime\\tto\\tmove\\ton\\tto\\tthe\\tnext\\tstage:\\texploring\\tthe\\t\\ndata.', 'Discover\\tand\\tVisualize\\tthe\\tData\\tto\\tGain\\tInsights\\nSo\\tfar\\tyou\\thave\\tonly\\ttaken\\ta\\tquick\\tglance\\tat\\tthe\\tdata\\tto\\tget\\ta\\tgeneral\\tunderstanding\\tof\\tthe\\tkind\\tof\\tdata\\tyou\\nare\\tmanipulating.\\tNow\\tthe\\tgoal\\tis\\tto\\tgo\\ta\\tlittle\\tbit\\tmore\\tin\\tdepth.\\nFirst,\\tmake\\tsure\\tyou\\thave\\tput\\tthe\\ttest\\tset\\taside\\tand\\tyou\\tare\\tonly\\texploring\\tthe\\ttraining\\tset.\\tAlso,\\tif\\tthe\\ntraining\\tset\\tis\\tvery\\tlarge,\\tyou\\tmay\\twant\\tto\\tsample\\tan\\texploration\\tset,\\tto\\tmake\\tmanipulations\\teasy\\tand\\tfast.\\nIn\\tour\\tcase,\\tthe\\tset\\tis\\tquite\\tsmall\\tso\\tyou\\tcan\\tjust\\twork\\tdirectly\\ton\\tthe\\tfull\\tset.\\tLet’s\\tcreate\\ta\\tcopy\\tso\\tyou\\ncan\\tplay\\twith\\tit\\twithout\\tharming\\tthe\\ttraining\\tset:\\nhousing\\n\\t\\n=\\n\\t\\nstrat_train_set\\n.\\ncopy\\n()', 'Visualizing\\tGeographical\\tData\\nSince\\t\\nthere\\tis\\tgeographical\\tinformation\\t(latitude\\tand\\tlongitude),\\tit\\tis\\ta\\tgood\\tidea\\tto\\tcreate\\ta\\tscatterplot\\tof\\nall\\tdistricts\\tto\\tvisualize\\tthe\\tdata\\t(\\nFigure\\t2-11\\n):\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n\\t\\nx\\n=\\n\"longitude\"\\n,\\n\\t\\ny\\n=\\n\"latitude\"\\n)\\nFigure\\t2-11.\\t\\nA\\tgeographical\\tscatterplot\\tof\\tthe\\tdata\\nThis\\tlooks\\tlike\\tCalifornia\\tall\\tright,\\tbut\\tother\\tthan\\tthat\\tit\\tis\\thard\\tto\\tsee\\tany\\tparticular\\tpattern.\\tSetting\\tthe\\nalpha\\n\\toption\\tto\\t\\n0.1\\n\\tmakes\\tit\\tmuch\\teasier\\tto\\tvisualize\\tthe\\tplaces\\twhere\\tthere\\tis\\ta\\thigh\\tdensity\\tof\\tdata\\npoints\\t(\\nFigure\\t2-12\\n):\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n\\t\\nx\\n=\\n\"longitude\"\\n,\\n\\t\\ny\\n=\\n\"latitude\"\\n,\\n\\t\\nalpha\\n=\\n0.1\\n)', 'Figure\\t2-12.\\t\\nA\\tbetter\\tvisualization\\thighlighting\\thigh-density\\tareas\\nNow\\tthat’s\\tmuch\\tbetter:\\tyou\\tcan\\tclearly\\tsee\\tthe\\thigh-density\\tareas,\\tnamely\\tthe\\tBay\\tArea\\tand\\taround\\tLos\\nAngeles\\tand\\tSan\\tDiego,\\tplus\\ta\\tlong\\tline\\tof\\tfairly\\thigh\\tdensity\\tin\\tthe\\tCentral\\tValley,\\tin\\tparticular\\taround\\nSacramento\\tand\\tFresno.\\nMore\\tgenerally,\\tour\\tbrains\\tare\\tvery\\tgood\\tat\\tspotting\\tpatterns\\ton\\tpictures,\\tbut\\tyou\\tmay\\tneed\\tto\\tplay\\taround\\nwith\\tvisualization\\tparameters\\tto\\tmake\\tthe\\tpatterns\\tstand\\tout.\\nNow\\tlet’s\\tlook\\tat\\tthe\\thousing\\tprices\\t(\\nFigure\\t2-13\\n).\\tThe\\tradius\\tof\\teach\\tcircle\\trepresents\\tthe\\tdistrict’s\\npopulation\\t(option\\t\\ns\\n),\\tand\\tthe\\tcolor\\trepresents\\tthe\\tprice\\t(option\\t\\nc\\n).\\tWe\\twill\\tuse\\ta\\tpredefined\\tcolor\\tmap\\n(option\\t\\ncmap\\n)\\tcalled\\t\\njet\\n,\\twhich\\tranges\\tfrom\\tblue\\t(low\\tvalues)\\tto\\tred\\t(high\\tprices):\\n14\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n\\t\\nx\\n=\\n\"longitude\"\\n,\\n\\t\\ny\\n=\\n\"latitude\"\\n,\\n\\t\\nalpha\\n=\\n0.4\\n,\\n\\t\\t\\t\\t\\ns\\n=\\nhousing\\n[\\n\"population\"\\n]\\n/\\n100\\n,\\n\\t\\nlabel\\n=\\n\"population\"\\n,\\n\\t\\nfigsize\\n=\\n(\\n10\\n,\\n7\\n),\\n\\t\\t\\t\\t\\nc\\n=\\n\"median_house_value\"\\n,\\n\\t\\ncmap\\n=', 'cmap\\n=\\nplt\\n.\\nget_cmap\\n(\\n\"jet\"\\n),\\n\\t\\ncolorbar\\n=\\nTrue\\n,\\n)\\nplt\\n.\\nlegend\\n()', 'Figure\\t2-13.\\t\\nCalifornia\\thousing\\tprices\\nThis\\timage\\ttells\\tyou\\tthat\\tthe\\thousing\\tprices\\tare\\tvery\\tmuch\\trelated\\tto\\tthe\\tlocation\\t(e.g.,\\tclose\\tto\\tthe\\tocean)\\nand\\tto\\tthe\\tpopulation\\tdensity,\\tas\\tyou\\tprobably\\tknew\\talready.\\tIt\\twill\\tprobably\\tbe\\tuseful\\tto\\tuse\\ta\\tclustering\\nalgorithm\\tto\\tdetect\\tthe\\tmain\\tclusters,\\tand\\tadd\\tnew\\tfeatures\\tthat\\tmeasure\\tthe\\tproximity\\tto\\tthe\\tcluster\\ncenters.\\tThe\\tocean\\tproximity\\tattribute\\tmay\\tbe\\tuseful\\tas\\twell,\\talthough\\tin\\tNorthern\\tCalifornia\\tthe\\thousing\\nprices\\tin\\tcoastal\\tdistricts\\tare\\tnot\\ttoo\\thigh,\\tso\\tit\\tis\\tnot\\ta\\t\\nsimple\\trule.', 'Looking\\tfor\\tCorrelations\\nSince\\t\\nthe\\tdataset\\tis\\tnot\\ttoo\\tlarge,\\tyou\\tcan\\teasily\\tcompute\\tthe\\t\\nstandard\\tcorrelation\\tcoefficient\\n\\t(also\\ncalled\\t\\nPearson’s\\tr\\n)\\tbetween\\t\\nevery\\tpair\\tof\\tattributes\\tusing\\tthe\\t\\ncorr()\\n\\tmethod:\\ncorr_matrix\\n\\t\\n=\\n\\t\\nhousing\\n.\\ncorr\\n()\\nNow\\tlet’s\\tlook\\tat\\thow\\tmuch\\teach\\tattribute\\tcorrelates\\twith\\tthe\\tmedian\\thouse\\tvalue:\\n>>>\\t\\ncorr_matrix\\n[\\n\"median_house_value\"\\n]\\n.\\nsort_values\\n(\\nascending\\n=\\nFalse\\n)\\nmedian_house_value\\t\\t\\t\\t1.000000\\nmedian_income\\t\\t\\t\\t\\t\\t\\t\\t\\t0.687170\\ntotal_rooms\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.135231\\nhousing_median_age\\t\\t\\t\\t0.114220\\nhouseholds\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.064702\\ntotal_bedrooms\\t\\t\\t\\t\\t\\t\\t\\t0.047865\\npopulation\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.026699\\nlongitude\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.047279\\nlatitude\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.142826\\nName:\\tmedian_house_value,\\tdtype:\\tfloat64\\nThe\\tcorrelation\\tcoefficient\\tranges\\tfrom\\t–1\\tto\\t1.\\tWhen\\tit\\tis\\tclose\\tto\\t1,\\tit\\tmeans\\tthat\\tthere\\tis\\ta\\tstrong\\npositive\\tcorrelation;\\tfor\\texample,\\tthe\\tmedian\\thouse\\tvalue\\ttends\\tto\\tgo\\tup\\twhen\\tthe\\tmedian\\tincome\\tgoes\\tup.', 'When\\tthe\\tcoefficient\\tis\\tclose\\tto\\t–1,\\tit\\tmeans\\tthat\\tthere\\tis\\ta\\tstrong\\tnegative\\tcorrelation;\\tyou\\tcan\\tsee\\ta\\nsmall\\tnegative\\tcorrelation\\tbetween\\tthe\\tlatitude\\tand\\tthe\\tmedian\\thouse\\tvalue\\t(i.e.,\\tprices\\thave\\ta\\tslight\\ntendency\\tto\\tgo\\tdown\\twhen\\tyou\\tgo\\tnorth).\\tFinally,\\tcoefficients\\tclose\\tto\\tzero\\tmean\\tthat\\tthere\\tis\\tno\\tlinear\\ncorrelation.\\t\\nFigure\\t2-14\\n\\tshows\\tvarious\\tplots\\talong\\twith\\tthe\\tcorrelation\\tcoefficient\\tbetween\\ttheir\\nhorizontal\\tand\\tvertical\\taxes.\\nFigure\\t2-14.\\t\\nStandard\\tcorrelation\\tcoefficient\\tof\\tvarious\\tdatasets\\t(source:\\tWikipedia;\\tpublic\\tdomain\\timage)', 'WARNING\\nThe\\tcorrelation\\tcoefficient\\tonly\\tmeasures\\tlinear\\tcorrelations\\t(“if\\t\\nx\\n\\tgoes\\tup,\\tthen\\t\\ny\\n\\tgenerally\\tgoes\\tup/down”).\\tIt\\tmay\\tcompletely\\nmiss\\tout\\ton\\tnonlinear\\trelationships\\t(e.g.,\\t“if\\t\\nx\\n\\tis\\tclose\\tto\\tzero\\tthen\\t\\ny\\n\\tgenerally\\tgoes\\tup”).\\tNote\\thow\\tall\\tthe\\tplots\\tof\\tthe\\tbottom\\trow\\nhave\\ta\\tcorrelation\\tcoefficient\\tequal\\tto\\tzero\\tdespite\\tthe\\tfact\\tthat\\ttheir\\taxes\\tare\\tclearly\\tnot\\tindependent:\\tthese\\tare\\texamples\\tof\\nnonlinear\\trelationships.\\tAlso,\\tthe\\tsecond\\trow\\tshows\\texamples\\twhere\\tthe\\tcorrelation\\tcoefficient\\tis\\tequal\\tto\\t1\\tor\\t–1;\\tnotice\\tthat\\tthis\\nhas\\tnothing\\tto\\tdo\\twith\\tthe\\tslope.\\tFor\\texample,\\tyour\\theight\\tin\\tinches\\thas\\ta\\tcorrelation\\tcoefficient\\tof\\t1\\twith\\tyour\\theight\\tin\\tfeet\\tor\\tin\\nnanometers.\\nAnother\\tway\\tto\\tcheck\\tfor\\tcorrelation\\tbetween\\tattributes\\tis\\tto\\tuse\\t\\nPandas’\\t\\nscatter_matrix\\n\\tfunction,\\nwhich\\tplots\\tevery\\tnumerical\\tattribute\\tagainst\\tevery\\tother\\tnumerical\\tattribute.\\tSince\\tthere\\tare\\tnow\\t11\\nnumerical\\tattributes,\\tyou\\twould\\tget\\t11\\n2', '2\\n\\t=\\t121\\tplots,\\twhich\\twould\\tnot\\tfit\\ton\\ta\\tpage,\\tso\\tlet’s\\tjust\\tfocus\\ton\\ta\\nfew\\tpromising\\tattributes\\tthat\\tseem\\tmost\\tcorrelated\\twith\\tthe\\tmedian\\thousing\\tvalue\\t(\\nFigure\\t2-15\\n):\\nfrom\\n\\t\\npandas.tools.plotting\\n\\t\\nimport\\n\\t\\nscatter_matrix\\nattributes\\n\\t\\n=\\n\\t\\n[\\n\"median_house_value\"\\n,\\n\\t\\n\"median_income\"\\n,\\n\\t\\n\"total_rooms\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\"housing_median_age\"\\n]\\nscatter_matrix\\n(\\nhousing\\n[\\nattributes\\n],\\n\\t\\nfigsize\\n=\\n(\\n12\\n,\\n\\t\\n8\\n))\\nFigure\\t2-15.\\t\\nScatter\\tmatrix\\nThe\\tmain\\tdiagonal\\t(top\\tleft\\tto\\tbottom\\tright)\\twould\\tbe\\tfull\\tof\\tstraight\\tlines\\tif\\tPandas\\tplotted\\teach\\tvariable\\nagainst\\titself,\\twhich\\twould\\tnot\\tbe\\tvery\\tuseful.\\tSo\\tinstead\\tPandas\\tdisplays\\ta\\thistogram\\tof\\teach\\tattribute\\n(other\\toptions\\tare\\tavailable;\\tsee\\t\\nPandas’\\tdocumentation\\tfor\\tmore\\tdetails).\\nThe\\tmost\\tpromising\\tattribute\\tto\\tpredict\\tthe\\tmedian\\thouse\\tvalue\\tis\\tthe\\tmedian\\tincome,\\tso\\tlet’s\\tzoom\\tin\\ton\\ntheir\\tcorrelation\\tscatterplot\\t(\\nFigure\\t2-16\\n):\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n\\t\\nx\\n=\\n\"median_income\"\\n,\\n\\t\\ny\\n=\\n\"median_house_value\"\\n,', 'alpha\\n=\\n0.1\\n)\\nThis\\tplot\\treveals\\ta\\tfew\\tthings.\\tFirst,\\tthe\\tcorrelation\\tis\\tindeed\\tvery\\tstrong;\\tyou\\tcan\\tclearly\\tsee\\tthe\\tupward\\ntrend\\tand\\tthe\\tpoints\\tare\\tnot\\ttoo\\tdispersed.\\tSecond,\\tthe\\tprice\\tcap\\tthat\\twe\\tnoticed\\tearlier\\tis\\tclearly\\tvisible\\nas\\ta\\thorizontal\\tline\\tat\\t$500,000.\\tBut\\tthis\\tplot\\treveals\\tother\\tless\\tobvious\\tstraight\\tlines:\\ta\\thorizontal\\tline\\naround\\t$450,000,\\tanother\\taround\\t$350,000,\\tperhaps\\tone\\taround\\t$280,000,\\tand\\ta\\tfew\\tmore\\tbelow\\tthat.\\nYou\\tmay\\twant\\tto\\ttry\\tremoving\\tthe\\tcorresponding\\tdistricts\\tto\\tprevent\\tyour\\talgorithms\\tfrom\\tlearning\\tto\\nreproduce\\tthese\\t\\ndata\\tquirks.\\nFigure\\t2-16.\\t\\nMedian\\tincome\\tversus\\tmedian\\thouse\\tvalue', 'Experimenting\\twith\\tAttribute\\tCombinations\\nHopefully\\t\\nthe\\tprevious\\tsections\\tgave\\tyou\\tan\\tidea\\tof\\ta\\tfew\\tways\\tyou\\tcan\\texplore\\tthe\\tdata\\tand\\tgain\\ninsights.\\tYou\\tidentified\\ta\\tfew\\tdata\\tquirks\\tthat\\tyou\\tmay\\twant\\tto\\tclean\\tup\\tbefore\\tfeeding\\tthe\\tdata\\tto\\ta\\nMachine\\tLearning\\talgorithm,\\tand\\tyou\\tfound\\tinteresting\\tcorrelations\\tbetween\\tattributes,\\tin\\tparticular\\twith\\nthe\\ttarget\\tattribute.\\tYou\\talso\\tnoticed\\tthat\\tsome\\tattributes\\thave\\ta\\ttail-heavy\\tdistribution,\\tso\\tyou\\tmay\\twant\\nto\\ttransform\\tthem\\t(e.g.,\\tby\\tcomputing\\ttheir\\tlogarithm).\\tOf\\tcourse,\\tyour\\tmileage\\twill\\tvary\\tconsiderably\\nwith\\teach\\tproject,\\tbut\\tthe\\tgeneral\\tideas\\tare\\tsimilar.\\nOne\\tlast\\tthing\\tyou\\tmay\\twant\\tto\\tdo\\tbefore\\tactually\\tpreparing\\tthe\\tdata\\tfor\\tMachine\\tLearning\\talgorithms\\tis\\nto\\ttry\\tout\\tvarious\\tattribute\\tcombinations.\\tFor\\texample,\\tthe\\ttotal\\tnumber\\tof\\trooms\\tin\\ta\\tdistrict\\tis\\tnot\\tvery\\nuseful\\tif\\tyou\\tdon’t\\tknow\\thow\\tmany\\thouseholds\\tthere\\tare.\\tWhat\\tyou\\treally\\twant\\tis\\tthe\\tnumber\\tof\\trooms', 'per\\thousehold.\\tSimilarly,\\tthe\\ttotal\\tnumber\\tof\\tbedrooms\\tby\\titself\\tis\\tnot\\tvery\\tuseful:\\tyou\\tprobably\\twant\\tto\\ncompare\\tit\\tto\\tthe\\tnumber\\tof\\trooms.\\tAnd\\tthe\\tpopulation\\tper\\thousehold\\talso\\tseems\\tlike\\tan\\tinteresting\\nattribute\\tcombination\\tto\\tlook\\tat.\\tLet’s\\tcreate\\tthese\\tnew\\tattributes:\\nhousing\\n[\\n\"rooms_per_household\"\\n]\\n\\t\\n=\\n\\t\\nhousing\\n[\\n\"total_rooms\"\\n]\\n/\\nhousing\\n[\\n\"households\"\\n]\\nhousing\\n[\\n\"bedrooms_per_room\"\\n]\\n\\t\\n=\\n\\t\\nhousing\\n[\\n\"total_bedrooms\"\\n]\\n/\\nhousing\\n[\\n\"total_rooms\"\\n]\\nhousing\\n[\\n\"population_per_household\"\\n]\\n=\\nhousing\\n[\\n\"population\"\\n]\\n/\\nhousing\\n[\\n\"households\"\\n]\\nAnd\\tnow\\tlet’s\\tlook\\tat\\tthe\\tcorrelation\\tmatrix\\tagain:\\n>>>\\t\\ncorr_matrix\\n\\t\\n=\\n\\t\\nhousing\\n.\\ncorr\\n()\\n>>>\\t\\ncorr_matrix\\n[\\n\"median_house_value\"\\n]\\n.\\nsort_values\\n(\\nascending\\n=\\nFalse\\n)\\nmedian_house_value\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1.000000\\nmedian_income\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.687160\\nrooms_per_household\\t\\t\\t\\t\\t\\t\\t\\t\\t0.146285\\ntotal_rooms\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.135097\\nhousing_median_age\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.114110\\nhouseholds\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.064506\\ntotal_bedrooms\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.047689', 'population_per_household\\t\\t\\t-0.021985\\npopulation\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.026920\\nlongitude\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.047432\\nlatitude\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.142724\\nbedrooms_per_room\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.259984\\nName:\\tmedian_house_value,\\tdtype:\\tfloat64\\nHey,\\tnot\\tbad!\\tThe\\tnew\\t\\nbedrooms_per_room\\n\\tattribute\\tis\\tmuch\\tmore\\tcorrelated\\twith\\tthe\\tmedian\\thouse\\nvalue\\tthan\\tthe\\ttotal\\tnumber\\tof\\trooms\\tor\\tbedrooms.\\tApparently\\thouses\\twith\\ta\\tlower\\tbedroom/room\\tratio\\ntend\\tto\\tbe\\tmore\\texpensive.\\tThe\\tnumber\\tof\\trooms\\tper\\thousehold\\tis\\talso\\tmore\\tinformative\\tthan\\tthe\\ttotal\\nnumber\\tof\\trooms\\tin\\ta\\tdistrict\\t—\\tobviously\\tthe\\tlarger\\tthe\\thouses,\\tthe\\tmore\\texpensive\\tthey\\tare.\\nThis\\tround\\tof\\texploration\\tdoes\\tnot\\thave\\tto\\tbe\\tabsolutely\\tthorough;\\tthe\\tpoint\\tis\\tto\\tstart\\toff\\ton\\tthe\\tright\\tfoot\\nand\\tquickly\\tgain\\tinsights\\tthat\\twill\\thelp\\tyou\\tget\\ta\\tfirst\\treasonably\\tgood\\tprototype.\\tBut\\tthis\\tis\\tan\\titerative\\nprocess:\\tonce\\tyou\\tget\\ta\\tprototype\\tup\\tand\\trunning,\\tyou\\tcan\\tanalyze\\tits\\toutput\\tto\\tgain\\tmore\\tinsights\\tand\\ncome\\tback\\tto\\tthis\\texploration\\t\\nstep.', 'Prepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\tAlgorithms\\nIt’s\\t\\ntime\\tto\\tprepare\\tthe\\tdata\\tfor\\tyour\\tMachine\\tLearning\\talgorithms.\\tInstead\\tof\\tjust\\tdoing\\tthis\\tmanually,\\tyou\\nshould\\twrite\\tfunctions\\tto\\tdo\\tthat,\\tfor\\tseveral\\tgood\\treasons:\\nThis\\twill\\tallow\\tyou\\tto\\treproduce\\tthese\\ttransformations\\teasily\\ton\\tany\\tdataset\\t(e.g.,\\tthe\\tnext\\ttime\\tyou\\nget\\ta\\tfresh\\tdataset).\\nYou\\twill\\tgradually\\tbuild\\ta\\tlibrary\\tof\\ttransformation\\tfunctions\\tthat\\tyou\\tcan\\treuse\\tin\\tfuture\\tprojects.\\nYou\\tcan\\tuse\\tthese\\tfunctions\\tin\\tyour\\tlive\\tsystem\\tto\\ttransform\\tthe\\tnew\\tdata\\tbefore\\tfeeding\\tit\\tto\\tyour\\nalgorithms.\\nThis\\twill\\tmake\\tit\\tpossible\\tfor\\tyou\\tto\\teasily\\ttry\\tvarious\\ttransformations\\tand\\tsee\\twhich\\tcombination\\nof\\ttransformations\\tworks\\tbest.\\nBut\\tfirst\\tlet’s\\trevert\\tto\\ta\\tclean\\t\\ntraining\\tset\\t(by\\tcopying\\t\\nstrat_train_set\\n\\tonce\\tagain),\\tand\\tlet’s\\tseparate\\nthe\\tpredictors\\tand\\tthe\\tlabels\\tsince\\twe\\tdon’t\\tnecessarily\\twant\\tto\\tapply\\tthe\\tsame\\ttransformations\\tto\\tthe\\npredictors\\tand\\tthe\\ttarget\\tvalues\\t(note\\tthat\\t\\ndrop()', 'drop()\\n\\t\\ncreates\\ta\\tcopy\\tof\\tthe\\tdata\\tand\\tdoes\\tnot\\taffect\\nstrat_train_set\\n):\\nhousing\\n\\t\\n=\\n\\t\\nstrat_train_set\\n.\\ndrop\\n(\\n\"median_house_value\"\\n,\\n\\t\\naxis\\n=\\n1\\n)\\nhousing_labels\\n\\t\\n=\\n\\t\\nstrat_train_set\\n[\\n\"median_house_value\"\\n]\\n.\\ncopy\\n()', 'Data\\tCleaning\\nMost\\t\\nMachine\\tLearning\\talgorithms\\tcannot\\twork\\twith\\tmissing\\tfeatures,\\tso\\tlet’s\\tcreate\\ta\\tfew\\tfunctions\\tto\\ntake\\tcare\\tof\\tthem.\\tYou\\tnoticed\\tearlier\\tthat\\tthe\\t\\ntotal_bedrooms\\n\\tattribute\\thas\\tsome\\tmissing\\tvalues,\\tso\\nlet’s\\tfix\\tthis.\\tYou\\thave\\tthree\\toptions:\\nGet\\trid\\tof\\tthe\\tcorresponding\\tdistricts.\\nGet\\trid\\tof\\tthe\\twhole\\tattribute.\\nSet\\tthe\\tvalues\\tto\\tsome\\tvalue\\t(zero,\\tthe\\tmean,\\tthe\\tmedian,\\tetc.).\\nYou\\tcan\\taccomplish\\tthese\\teasily\\tusing\\t\\nDataFrame’s\\t\\ndropna()\\n,\\t\\ndrop()\\n,\\t\\nand\\t\\nfillna()\\n\\tmethods:\\nhousing\\n.\\ndropna\\n(\\nsubset\\n=\\n[\\n\"total_bedrooms\"\\n])\\n\\t\\t\\t\\t\\n#\\toption\\t1\\nhousing\\n.\\ndrop\\n(\\n\"total_bedrooms\"\\n,\\n\\t\\naxis\\n=\\n1\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\n#\\toption\\t2\\nmedian\\n\\t\\n=\\n\\t\\nhousing\\n[\\n\"total_bedrooms\"\\n]\\n.\\nmedian\\n()\\n\\t\\t\\n#\\toption\\t3\\nhousing\\n[\\n\"total_bedrooms\"\\n]\\n.\\nfillna\\n(\\nmedian\\n,\\n\\t\\ninplace\\n=\\nTrue\\n)\\nIf\\tyou\\tchoose\\toption\\t3,\\tyou\\tshould\\tcompute\\tthe\\tmedian\\tvalue\\ton\\tthe\\ttraining\\tset,\\tand\\tuse\\tit\\tto\\tfill\\tthe\\nmissing\\tvalues\\tin\\tthe\\ttraining\\tset,\\tbut\\talso\\tdon’t\\tforget\\tto\\tsave\\tthe\\tmedian\\tvalue\\tthat\\tyou\\thave\\tcomputed.', 'You\\twill\\tneed\\tit\\tlater\\tto\\treplace\\tmissing\\tvalues\\tin\\tthe\\ttest\\tset\\twhen\\tyou\\twant\\tto\\tevaluate\\tyour\\tsystem,\\tand\\nalso\\tonce\\tthe\\tsystem\\tgoes\\tlive\\tto\\treplace\\tmissing\\tvalues\\tin\\tnew\\tdata.\\nScikit-Learn\\t\\nprovides\\ta\\thandy\\tclass\\tto\\ttake\\tcare\\tof\\tmissing\\tvalues:\\t\\nImputer\\n.\\tHere\\tis\\thow\\tto\\tuse\\tit.\\tFirst,\\nyou\\tneed\\tto\\tcreate\\tan\\t\\nImputer\\n\\tinstance,\\tspecifying\\tthat\\tyou\\twant\\tto\\treplace\\teach\\tattribute’s\\tmissing\\nvalues\\twith\\tthe\\tmedian\\tof\\tthat\\tattribute:\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nImputer\\nimputer\\n\\t\\n=\\n\\t\\nImputer\\n(\\nstrategy\\n=\\n\"median\"\\n)\\nSince\\tthe\\tmedian\\tcan\\tonly\\tbe\\tcomputed\\ton\\tnumerical\\tattributes,\\twe\\tneed\\tto\\tcreate\\ta\\tcopy\\tof\\tthe\\tdata\\nwithout\\tthe\\ttext\\tattribute\\t\\nocean_proximity\\n:\\nhousing_num\\n\\t\\n=\\n\\t\\nhousing\\n.\\ndrop\\n(\\n\"ocean_proximity\"\\n,\\n\\t\\naxis\\n=\\n1\\n)\\nNow\\tyou\\tcan\\tfit\\tthe\\t\\nimputer\\n\\tinstance\\tto\\tthe\\ttraining\\tdata\\tusing\\tthe\\t\\nfit()\\n\\t\\nmethod:\\nimputer\\n.\\nfit\\n(\\nhousing_num\\n)\\nThe\\t\\nimputer\\n\\thas\\tsimply\\tcomputed\\tthe\\tmedian\\tof\\teach\\tattribute\\tand\\tstored\\tthe\\tresult\\tin\\tits\\t\\nstatistics_\\ninstance\\tvariable.\\tOnly\\tthe', 'total_bedrooms\\n\\tattribute\\thad\\tmissing\\tvalues,\\tbut\\twe\\tcannot\\tbe\\tsure\\tthat\\nthere\\twon’t\\tbe\\tany\\tmissing\\tvalues\\tin\\tnew\\tdata\\tafter\\tthe\\tsystem\\tgoes\\tlive,\\tso\\tit\\tis\\tsafer\\tto\\tapply\\tthe\\nimputer\\n\\tto\\tall\\tthe\\tnumerical\\tattributes:\\n>>>\\t\\nimputer\\n.\\nstatistics_\\narray([\\t-118.51\\t,\\t34.26\\t,\\t29.\\t,\\t2119.5\\t,\\t433.\\t,\\t1164.\\t,\\t408.\\t,\\t3.5409])\\n>>>\\t\\nhousing_num\\n.\\nmedian\\n()\\n.\\nvalues', 'array([\\t-118.51\\t,\\t34.26\\t,\\t29.\\t,\\t2119.5\\t,\\t433.\\t,\\t1164.\\t,\\t408.\\t,\\t3.5409])\\nNow\\tyou\\tcan\\tuse\\tthis\\t“trained”\\t\\nimputer\\n\\tto\\ttransform\\tthe\\ttraining\\tset\\tby\\treplacing\\tmissing\\tvalues\\tby\\tthe\\nlearned\\tmedians:\\nX\\n\\t\\n=\\n\\t\\nimputer\\n.\\ntransform\\n(\\nhousing_num\\n)\\nThe\\tresult\\tis\\ta\\tplain\\tNumpy\\tarray\\tcontaining\\tthe\\ttransformed\\tfeatures.\\tIf\\tyou\\twant\\tto\\tput\\tit\\tback\\tinto\\ta\\nPandas\\tDataFrame,\\tit’s\\tsimple:\\nhousing_tr\\n\\t\\n=\\n\\t\\npd\\n.\\nDataFrame\\n(\\nX\\n,\\n\\t\\ncolumns\\n=\\nhousing_num\\n.\\ncolumns\\n)\\nSCIKIT-LEARN\\tDESIGN\\nScikit-Learn’s\\t\\nAPI\\tis\\tremarkably\\twell\\tdesigned.\\tThe\\t\\nmain\\tdesign\\tprinciples\\n\\tare:\\n15\\nConsistency\\n.\\tAll\\tobjects\\tshare\\ta\\tconsistent\\tand\\tsimple\\tinterface:\\nEstimators\\n.\\tAny\\t\\nobject\\tthat\\tcan\\testimate\\tsome\\tparameters\\tbased\\ton\\ta\\tdataset\\tis\\tcalled\\tan\\t\\nestimator\\n\\t(e.g.,\\tan\\t\\nimputer\\n\\tis\\nan\\testimator).\\tThe\\testimation\\titself\\tis\\tperformed\\tby\\tthe\\t\\nfit()\\n\\tmethod,\\tand\\tit\\ttakes\\tonly\\ta\\tdataset\\tas\\ta\\tparameter\\t(or\\ttwo\\nfor\\tsupervised\\tlearning\\talgorithms;\\tthe\\tsecond\\tdataset\\tcontains\\tthe\\tlabels).\\tAny\\tother\\tparameter\\tneeded\\tto\\tguide\\tthe', 'estimation\\tprocess\\tis\\tconsidered\\ta\\thyperparameter\\t(such\\tas\\tan\\t\\nimputer\\n’s\\t\\nstrategy\\n),\\tand\\tit\\tmust\\tbe\\tset\\tas\\tan\\tinstance\\nvariable\\t(generally\\tvia\\ta\\tconstructor\\tparameter).\\nTransformers\\n.\\tSome\\t\\nestimators\\t(such\\tas\\tan\\t\\nimputer\\n)\\tcan\\talso\\ttransform\\ta\\tdataset;\\tthese\\tare\\tcalled\\t\\ntransformers\\n.\\tOnce\\nagain,\\tthe\\tAPI\\tis\\tquite\\tsimple:\\tthe\\ttransformation\\tis\\tperformed\\tby\\tthe\\t\\ntransform()\\n\\t\\nmethod\\twith\\tthe\\tdataset\\tto\\ttransform\\tas\\na\\tparameter.\\tIt\\treturns\\tthe\\ttransformed\\tdataset.\\tThis\\ttransformation\\tgenerally\\trelies\\ton\\tthe\\tlearned\\tparameters,\\tas\\tis\\tthe\\ncase\\tfor\\tan\\t\\nimputer\\n.\\tAll\\ttransformers\\talso\\thave\\ta\\tconvenience\\tmethod\\tcalled\\t\\nfit_transform()\\n\\t\\nthat\\tis\\tequivalent\\tto\\tcalling\\nfit()\\n\\tand\\tthen\\t\\ntransform()\\n\\t(but\\tsometimes\\t\\nfit_transform()\\n\\tis\\toptimized\\tand\\truns\\tmuch\\tfaster).\\nPredictors\\n.\\tFinally,\\t\\nsome\\testimators\\tare\\tcapable\\tof\\tmaking\\tpredictions\\tgiven\\ta\\tdataset;\\tthey\\tare\\tcalled\\t\\npredictors\\n.\\tFor\\nexample,\\tthe\\t\\nLinearRegression\\n\\tmodel', 'model\\t\\nin\\tthe\\tprevious\\tchapter\\twas\\ta\\tpredictor:\\tit\\tpredicted\\tlife\\tsatisfaction\\tgiven\\ta\\ncountry’s\\tGDP\\tper\\tcapita.\\tA\\tpredictor\\thas\\ta\\t\\npredict()\\n\\t\\nmethod\\tthat\\ttakes\\ta\\tdataset\\tof\\tnew\\tinstances\\tand\\treturns\\ta\\tdataset\\nof\\tcorresponding\\tpredictions.\\tIt\\talso\\thas\\ta\\t\\nscore()\\n\\t\\nmethod\\tthat\\tmeasures\\tthe\\tquality\\tof\\tthe\\tpredictions\\tgiven\\ta\\ttest\\tset\\t(and\\nthe\\tcorresponding\\tlabels\\tin\\tthe\\tcase\\tof\\tsupervised\\tlearning\\talgorithms).\\n16\\nInspection\\n.\\tAll\\tthe\\testimator’s\\thyperparameters\\tare\\taccessible\\tdirectly\\tvia\\tpublic\\tinstance\\tvariables\\t(e.g.,\\t\\nimputer.strategy\\n),\\nand\\tall\\tthe\\testimator’s\\tlearned\\tparameters\\tare\\talso\\taccessible\\tvia\\tpublic\\tinstance\\tvariables\\twith\\tan\\tunderscore\\tsuffix\\t(e.g.,\\nimputer.statistics_\\n).\\nNonproliferation\\tof\\tclasses\\n.\\tDatasets\\tare\\trepresented\\tas\\tNumPy\\tarrays\\tor\\tSciPy\\tsparse\\tmatrices,\\tinstead\\tof\\thomemade\\nclasses.\\tHyperparameters\\tare\\tjust\\tregular\\tPython\\tstrings\\tor\\tnumbers.\\nComposition\\n.\\tExisting\\tbuilding\\tblocks\\tare\\treused\\tas\\tmuch\\tas\\tpossible.\\tFor\\texample,\\tit\\tis\\teasy\\tto\\tcreate\\ta\\t\\nPipeline', 'Pipeline\\n\\testimator\\nfrom\\tan\\tarbitrary\\tsequence\\tof\\ttransformers\\tfollowed\\tby\\ta\\tfinal\\testimator,\\tas\\twe\\twill\\tsee.\\nSensible\\tdefaults\\n.\\tScikit-Learn\\tprovides\\treasonable\\tdefault\\tvalues\\tfor\\tmost\\tparameters,\\tmaking\\tit\\teasy\\tto\\tcreate\\ta\\tbaseline\\nworking\\tsystem\\tquickly.', 'Handling\\tText\\tand\\tCategorical\\tAttributes\\nEarlier\\t\\nwe\\tleft\\tout\\tthe\\tcategorical\\tattribute\\t\\nocean_proximity\\n\\tbecause\\tit\\tis\\ta\\ttext\\tattribute\\tso\\twe\\tcannot\\ncompute\\tits\\tmedian.\\tMost\\tMachine\\tLearning\\talgorithms\\tprefer\\tto\\twork\\twith\\tnumbers\\tanyway,\\tso\\tlet’s\\nconvert\\tthese\\ttext\\tlabels\\tto\\tnumbers.\\nScikit-Learn\\tprovides\\ta\\ttransformer\\tfor\\tthis\\ttask\\t\\ncalled\\t\\nLabelEncoder\\n:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nLabelEncoder\\n>>>\\t\\nencoder\\n\\t\\n=\\n\\t\\nLabelEncoder\\n()\\n>>>\\t\\nhousing_cat\\n\\t\\n=\\n\\t\\nhousing\\n[\\n\"ocean_proximity\"\\n]\\n>>>\\t\\nhousing_cat_encoded\\n\\t\\n=\\n\\t\\nencoder\\n.\\nfit_transform\\n(\\nhousing_cat\\n)\\n>>>\\t\\nhousing_cat_encoded\\narray([0,\\t0,\\t4,\\t...,\\t1,\\t0,\\t3])\\nThis\\tis\\tbetter:\\tnow\\twe\\tcan\\tuse\\tthis\\tnumerical\\tdata\\tin\\tany\\tML\\talgorithm.\\tYou\\tcan\\tlook\\tat\\tthe\\tmapping\\tthat\\nthis\\tencoder\\thas\\tlearned\\tusing\\tthe\\t\\nclasses_\\n\\tattribute\\t(“<1H\\tOCEAN”\\tis\\tmapped\\tto\\t0,\\t“INLAND”\\tis\\nmapped\\tto\\t1,\\tetc.):\\n>>>\\t\\nprint\\n(\\nencoder\\n.\\nclasses_\\n)\\n[\\'<1H\\tOCEAN\\'\\t\\'INLAND\\'\\t\\'ISLAND\\'\\t\\'NEAR\\tBAY\\'\\t\\'NEAR\\tOCEAN\\']', 'One\\tissue\\twith\\tthis\\trepresentation\\tis\\tthat\\tML\\talgorithms\\twill\\tassume\\tthat\\ttwo\\tnearby\\tvalues\\tare\\tmore\\nsimilar\\tthan\\ttwo\\tdistant\\tvalues.\\tObviously\\tthis\\tis\\tnot\\tthe\\tcase\\t(for\\texample,\\tcategories\\t0\\tand\\t4\\tare\\tmore\\nsimilar\\tthan\\tcategories\\t0\\tand\\t1).\\tTo\\tfix\\tthis\\tissue,\\ta\\tcommon\\tsolution\\tis\\tto\\tcreate\\tone\\tbinary\\tattribute\\tper\\ncategory:\\tone\\tattribute\\tequal\\tto\\t1\\twhen\\tthe\\tcategory\\tis\\t“<1H\\tOCEAN”\\t(and\\t0\\totherwise),\\tanother\\nattribute\\tequal\\tto\\t1\\twhen\\tthe\\tcategory\\tis\\t“INLAND”\\t(and\\t0\\totherwise),\\tand\\tso\\ton.\\tThis\\tis\\tcalled\\t\\none-hot\\nencoding\\n,\\t\\nbecause\\tonly\\tone\\tattribute\\twill\\tbe\\tequal\\tto\\t1\\t(hot),\\twhile\\tthe\\tothers\\twill\\tbe\\t0\\t(cold).\\nScikit-Learn\\tprovides\\ta\\t\\nOneHotEncoder\\n\\t\\nencoder\\tto\\tconvert\\tinteger\\tcategorical\\tvalues\\tinto\\tone-hot\\nvectors.\\tLet’s\\tencode\\tthe\\tcategories\\tas\\tone-hot\\tvectors.\\tNote\\tthat\\t\\nfit_transform()\\n\\texpects\\ta\\t2D\\tarray,\\nbut\\t\\nhousing_cat_encoded\\n\\tis\\ta\\t1D\\tarray,\\tso\\twe\\tneed\\tto\\treshape\\tit:\\n17\\n>>>\\t\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nOneHotEncoder\\n>>>\\t\\nencoder\\n\\t\\n=\\n\\t\\nOneHotEncoder\\n()\\n>>>', \"()\\n>>>\\t\\nhousing_cat_1hot\\n\\t\\n=\\n\\t\\nencoder\\n.\\nfit_transform\\n(\\nhousing_cat_encoded\\n.\\nreshape\\n(\\n-\\n1\\n,\\n1\\n))\\n>>>\\t\\nhousing_cat_1hot\\n<16512x5\\tsparse\\tmatrix\\tof\\ttype\\t'<class\\t'numpy.float64'>'\\n\\t with\\t16512\\tstored\\telements\\tin\\tCompressed\\tSparse\\tRow\\tformat>\\nNotice\\tthat\\tthe\\toutput\\tis\\ta\\tSciPy\\t\\nsparse\\tmatrix\\n,\\t\\ninstead\\tof\\ta\\t\\nNumPy\\tarray.\\tThis\\tis\\tvery\\tuseful\\twhen\\tyou\\nhave\\tcategorical\\tattributes\\twith\\tthousands\\tof\\tcategories.\\tAfter\\tone-hot\\tencoding\\twe\\tget\\ta\\tmatrix\\twith\\nthousands\\tof\\tcolumns,\\tand\\tthe\\tmatrix\\tis\\tfull\\tof\\tzeros\\texcept\\tfor\\tone\\t1\\tper\\trow.\\tUsing\\tup\\ttons\\tof\\tmemory\\nmostly\\tto\\tstore\\tzeros\\twould\\tbe\\tvery\\twasteful,\\tso\\tinstead\\ta\\tsparse\\tmatrix\\tonly\\tstores\\tthe\\tlocation\\tof\\tthe\\nnonzero\\telements.\\tYou\\tcan\\tuse\\tit\\tmostly\\tlike\\ta\\tnormal\\t2D\\tarray,\\n18\\n\\tbut\\tif\\tyou\\treally\\twant\\tto\\tconvert\\tit\\tto\\ta\\n(dense)\\tNumPy\\tarray,\\tjust\\tcall\\tthe\\t\\ntoarray()\\n\\t\\nmethod:\\n>>>\\t\\nhousing_cat_1hot\\n.\\ntoarray\\n()\\narray([[\\t1.,\\t\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t0.],\\n\\t\\t\\t\\t\\t\\t\\t[\\t1.,\\t\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t0.],\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t1.],\", '...,\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.,\\t\\t1.,\\t\\t0.,\\t\\t0.,\\t\\t0.],\\n\\t\\t\\t\\t\\t\\t\\t[\\t1.,\\t\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t0.],\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.,\\t\\t0.,\\t\\t0.,\\t\\t1.,\\t\\t0.]])\\nWe\\tcan\\tapply\\tboth\\ttransformations\\t(from\\ttext\\tcategories\\tto\\tinteger\\tcategories,\\tthen\\tfrom\\tinteger\\tcategories\\nto\\tone-hot\\tvectors)\\tin\\tone\\tshot\\tusing\\tthe\\t\\nLabelBinarizer\\n\\t\\nclass:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nLabelBinarizer\\n>>>\\t\\nencoder\\n\\t\\n=\\n\\t\\nLabelBinarizer\\n()\\n>>>\\t\\nhousing_cat_1hot\\n\\t\\n=\\n\\t\\nencoder\\n.\\nfit_transform\\n(\\nhousing_cat\\n)\\n>>>\\t\\nhousing_cat_1hot\\narray([[1,\\t0,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[1,\\t0,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t0,\\t1],\\n\\t\\t\\t\\t\\t\\t\\t...,\\n\\t\\t\\t\\t\\t\\t\\t[0,\\t1,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[1,\\t0,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t1,\\t0]])\\nNote\\tthat\\tthis\\treturns\\ta\\tdense\\tNumPy\\tarray\\tby\\tdefault.\\tYou\\tcan\\tget\\ta\\tsparse\\tmatrix\\tinstead\\tby\\tpassing\\nsparse_output=True\\n\\tto\\tthe\\t\\nLabelBinarizer\\n\\t\\nconstructor.', 'Custom\\tTransformers\\nAlthough\\t\\nScikit-Learn\\tprovides\\tmany\\tuseful\\ttransformers,\\tyou\\twill\\tneed\\tto\\twrite\\tyour\\town\\tfor\\ttasks\\tsuch\\nas\\tcustom\\tcleanup\\toperations\\tor\\tcombining\\tspecific\\tattributes.\\tYou\\twill\\twant\\tyour\\ttransformer\\tto\\twork\\nseamlessly\\twith\\tScikit-Learn\\tfunctionalities\\t(such\\tas\\tpipelines),\\tand\\tsince\\tScikit-Learn\\trelies\\ton\\tduck\\ntyping\\t(not\\tinheritance),\\tall\\tyou\\tneed\\tis\\tto\\tcreate\\ta\\tclass\\tand\\timplement\\tthree\\tmethods:\\t\\nfit()\\n\\t(returning\\nself\\n),\\t\\ntransform()\\n,\\tand\\t\\nfit_transform()\\n.\\tYou\\tcan\\tget\\tthe\\tlast\\tone\\tfor\\tfree\\tby\\tsimply\\tadding\\nTransformerMixin\\n\\t\\nas\\ta\\tbase\\tclass.\\tAlso,\\tif\\tyou\\tadd\\t\\nBaseEstimator\\n\\tas\\ta\\t\\nbase\\tclass\\t(and\\tavoid\\t\\n*args\\nand\\t\\n**kargs\\n\\tin\\tyour\\tconstructor)\\tyou\\twill\\tget\\ttwo\\textra\\tmethods\\t(\\nget_params()\\n\\tand\\t\\nset_params()\\n)\\nthat\\twill\\tbe\\tuseful\\tfor\\tautomatic\\thyperparameter\\ttuning.\\tFor\\texample,\\there\\tis\\ta\\tsmall\\ttransformer\\tclass\\nthat\\tadds\\tthe\\tcombined\\tattributes\\twe\\tdiscussed\\tearlier:\\nfrom\\n\\t\\nsklearn.base\\n\\t\\nimport\\n\\t\\nBaseEstimator\\n,\\n\\t\\nTransformerMixin\\nrooms_ix\\n,\\n\\t\\nbedrooms_ix\\n,', ',\\n\\t\\npopulation_ix\\n,\\n\\t\\nhousehold_ix\\n\\t\\n=\\n\\t\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n5\\n,\\n\\t\\n6\\nclass\\n\\t\\nCombinedAttributesAdder\\n(\\nBaseEstimator\\n,\\n\\t\\nTransformerMixin\\n):\\n\\t\\t\\t\\t\\ndef\\n\\t\\n__init__\\n(\\nself\\n,\\n\\t\\nadd_bedrooms_per_room\\n\\t\\n=\\n\\t\\nTrue\\n):\\n\\t\\n#\\tno\\t*args\\tor\\t**kargs\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nself\\n.\\nadd_bedrooms_per_room\\n\\t\\n=\\n\\t\\nadd_bedrooms_per_room\\n\\t\\t\\t\\t\\ndef\\n\\t\\nfit\\n(\\nself\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nself\\n\\t\\t\\n#\\tnothing\\telse\\tto\\tdo\\n\\t\\t\\t\\t\\ndef\\n\\t\\ntransform\\n(\\nself\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nrooms_per_household\\n\\t\\n=\\n\\t\\nX\\n[:,\\n\\t\\nrooms_ix\\n]\\n\\t\\n/\\n\\t\\nX\\n[:,\\n\\t\\nhousehold_ix\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npopulation_per_household\\n\\t\\n=\\n\\t\\nX\\n[:,\\n\\t\\npopulation_ix\\n]\\n\\t\\n/\\n\\t\\nX\\n[:,\\n\\t\\nhousehold_ix\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nself\\n.\\nadd_bedrooms_per_room\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbedrooms_per_room\\n\\t\\n=\\n\\t\\nX\\n[:,\\n\\t\\nbedrooms_ix\\n]\\n\\t\\n/\\n\\t\\nX\\n[:,\\n\\t\\nrooms_ix\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nnp\\n.\\nc_\\n[\\nX\\n,\\n\\t\\nrooms_per_household\\n,\\n\\t\\npopulation_per_household\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbedrooms_per_room\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nnp\\n.\\nc_\\n[\\nX\\n,\\n\\t\\nrooms_per_household\\n,\\n\\t\\npopulation_per_household\\n]\\nattr_adder\\n\\t\\n=', '=\\n\\t\\nCombinedAttributesAdder\\n(\\nadd_bedrooms_per_room\\n=\\nFalse\\n)\\nhousing_extra_attribs\\n\\t\\n=\\n\\t\\nattr_adder\\n.\\ntransform\\n(\\nhousing\\n.\\nvalues\\n)\\nIn\\tthis\\texample\\tthe\\ttransformer\\thas\\tone\\thyperparameter,\\t\\nadd_bedrooms_per_room\\n,\\tset\\tto\\t\\nTrue\\n\\tby\\tdefault\\n(it\\tis\\toften\\thelpful\\tto\\tprovide\\tsensible\\tdefaults).\\tThis\\t\\nhyperparameter\\twill\\tallow\\tyou\\tto\\teasily\\tfind\\tout\\nwhether\\tadding\\tthis\\tattribute\\thelps\\tthe\\tMachine\\tLearning\\talgorithms\\tor\\tnot.\\tMore\\tgenerally,\\tyou\\tcan\\tadd\\ta\\nhyperparameter\\tto\\tgate\\tany\\tdata\\tpreparation\\tstep\\tthat\\tyou\\tare\\tnot\\t100%\\tsure\\tabout.\\tThe\\tmore\\tyou\\nautomate\\tthese\\tdata\\tpreparation\\tsteps,\\tthe\\tmore\\tcombinations\\tyou\\tcan\\tautomatically\\ttry\\tout,\\tmaking\\tit\\nmuch\\tmore\\tlikely\\tthat\\tyou\\twill\\tfind\\ta\\tgreat\\t\\ncombination\\t(and\\tsaving\\tyou\\ta\\tlot\\tof\\ttime).', 'Feature\\tScaling\\nOne\\t\\nof\\tthe\\tmost\\timportant\\ttransformations\\tyou\\tneed\\tto\\tapply\\tto\\tyour\\tdata\\tis\\t\\nfeature\\tscaling\\n.\\tWith\\tfew\\nexceptions,\\tMachine\\tLearning\\talgorithms\\tdon’t\\tperform\\twell\\twhen\\tthe\\tinput\\tnumerical\\tattributes\\thave\\nvery\\tdifferent\\tscales.\\tThis\\tis\\tthe\\tcase\\tfor\\tthe\\thousing\\tdata:\\tthe\\ttotal\\tnumber\\tof\\trooms\\tranges\\tfrom\\tabout\\t6\\nto\\t39,320,\\twhile\\tthe\\tmedian\\tincomes\\tonly\\trange\\tfrom\\t0\\tto\\t15.\\tNote\\tthat\\tscaling\\tthe\\ttarget\\tvalues\\tis\\ngenerally\\tnot\\trequired.\\nThere\\tare\\ttwo\\tcommon\\tways\\tto\\tget\\tall\\tattributes\\tto\\thave\\tthe\\tsame\\tscale:\\t\\nmin-max\\tscaling\\n\\t\\nand\\nstandardization\\n.\\nMin-max\\tscaling\\t\\n(many\\tpeople\\tcall\\tthis\\t\\nnormalization\\n)\\tis\\tquite\\tsimple:\\tvalues\\tare\\tshifted\\tand\\trescaled\\nso\\tthat\\tthey\\tend\\tup\\tranging\\tfrom\\t0\\tto\\t1.\\tWe\\tdo\\tthis\\tby\\tsubtracting\\tthe\\tmin\\tvalue\\tand\\tdividing\\tby\\tthe\\tmax\\nminus\\tthe\\tmin.\\t\\nScikit-Learn\\tprovides\\ta\\ttransformer\\tcalled\\t\\nMinMaxScaler\\n\\tfor\\tthis.\\tIt\\thas\\ta\\nfeature_range\\n\\thyperparameter\\tthat\\tlets\\tyou\\tchange\\tthe\\trange\\tif\\tyou\\tdon’t\\twant\\t0–1\\tfor\\tsome\\treason.', 'Standardization\\tis\\tquite\\tdifferent:\\tfirst\\tit\\tsubtracts\\tthe\\tmean\\tvalue\\t(so\\tstandardized\\tvalues\\talways\\thave\\ta\\nzero\\tmean),\\tand\\tthen\\tit\\tdivides\\tby\\tthe\\tvariance\\tso\\tthat\\tthe\\tresulting\\tdistribution\\thas\\tunit\\tvariance.\\tUnlike\\nmin-max\\tscaling,\\tstandardization\\tdoes\\tnot\\tbound\\tvalues\\tto\\ta\\tspecific\\trange,\\twhich\\tmay\\tbe\\ta\\tproblem\\tfor\\nsome\\talgorithms\\t(e.g.,\\tneural\\tnetworks\\toften\\texpect\\tan\\tinput\\tvalue\\tranging\\tfrom\\t0\\tto\\t1).\\tHowever,\\nstandardization\\tis\\tmuch\\tless\\taffected\\tby\\toutliers.\\tFor\\texample,\\tsuppose\\ta\\tdistrict\\thad\\ta\\tmedian\\tincome\\nequal\\tto\\t100\\t(by\\tmistake).\\tMin-max\\tscaling\\twould\\tthen\\tcrush\\tall\\tthe\\tother\\tvalues\\tfrom\\t0–15\\tdown\\tto\\t0–\\n0.15,\\twhereas\\tstandardization\\twould\\tnot\\tbe\\tmuch\\taffected.\\tScikit-Learn\\tprovides\\ta\\ttransformer\\tcalled\\nStandardScaler\\n\\t\\nfor\\tstandardization.\\nWARNING\\nAs\\twith\\tall\\tthe\\ttransformations,\\tit\\tis\\timportant\\tto\\tfit\\tthe\\tscalers\\tto\\tthe\\ttraining\\tdata\\tonly,\\tnot\\tto\\tthe\\tfull\\tdataset\\t(including\\tthe\\ttest', 'set).\\tOnly\\tthen\\tcan\\tyou\\tuse\\tthem\\tto\\ttransform\\tthe\\ttraining\\tset\\tand\\tthe\\ttest\\tset\\t(and\\tnew\\tdata).', 'Transformation\\tPipelines\\nAs\\t\\nyou\\tcan\\tsee,\\tthere\\tare\\tmany\\tdata\\ttransformation\\tsteps\\tthat\\tneed\\tto\\tbe\\texecuted\\tin\\tthe\\tright\\torder.\\nFortunately,\\tScikit-Learn\\tprovides\\tthe\\t\\nPipeline\\n\\tclass\\tto\\thelp\\twith\\tsuch\\tsequences\\tof\\ttransformations.\\nHere\\tis\\ta\\tsmall\\tpipeline\\tfor\\tthe\\t\\nnumerical\\tattributes:\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nPipeline\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nStandardScaler\\nnum_pipeline\\n\\t\\n=\\n\\t\\nPipeline\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'imputer\\'\\n,\\n\\t\\nImputer\\n(\\nstrategy\\n=\\n\"median\"\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'attribs_adder\\'\\n,\\n\\t\\nCombinedAttributesAdder\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'std_scaler\\'\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\n])\\nhousing_num_tr\\n\\t\\n=\\n\\t\\nnum_pipeline\\n.\\nfit_transform\\n(\\nhousing_num\\n)\\nThe\\t\\nPipeline\\n\\tconstructor\\ttakes\\ta\\tlist\\tof\\tname/estimator\\tpairs\\tdefining\\ta\\tsequence\\tof\\tsteps.\\tAll\\tbut\\tthe\\nlast\\testimator\\tmust\\tbe\\ttransformers\\t(i.e.,\\tthey\\tmust\\thave\\ta\\t\\nfit_transform()\\n\\tmethod).\\tThe\\tnames\\tcan\\tbe\\nanything\\tyou\\tlike\\t(as\\tlong\\tas\\tthey\\tdon’t\\tcontain\\tdouble\\tunderscores\\t“\\n__\\n”).\\nWhen\\tyou\\tcall\\tthe\\t\\npipeline’s\\t\\nfit()', 'fit()\\n\\tmethod,\\tit\\tcalls\\t\\nfit_transform()\\n\\tsequentially\\ton\\tall\\ttransformers,\\npassing\\tthe\\toutput\\tof\\teach\\tcall\\tas\\tthe\\tparameter\\tto\\tthe\\tnext\\tcall,\\tuntil\\tit\\treaches\\tthe\\tfinal\\testimator,\\tfor\\nwhich\\tit\\tjust\\tcalls\\tthe\\t\\nfit()\\n\\tmethod.\\nThe\\tpipeline\\texposes\\tthe\\tsame\\tmethods\\tas\\tthe\\tfinal\\testimator.\\tIn\\tthis\\texample,\\tthe\\tlast\\testimator\\tis\\t\\na\\nStandardScaler\\n,\\twhich\\tis\\ta\\ttransformer,\\tso\\tthe\\tpipeline\\thas\\t\\na\\t\\ntransform()\\n\\tmethod\\tthat\\tapplies\\tall\\tthe\\ntransforms\\tto\\tthe\\tdata\\tin\\tsequence\\t(it\\talso\\thas\\ta\\t\\nfit_transform\\n\\tmethod\\tthat\\twe\\tcould\\thave\\tused\\tinstead\\nof\\tcalling\\t\\nfit()\\n\\tand\\tthen\\t\\ntransform()\\n).\\nNow\\tit\\twould\\tbe\\tnice\\tif\\twe\\tcould\\tfeed\\ta\\tPandas\\tDataFrame\\tdirectly\\tinto\\tour\\tpipeline,\\tinstead\\tof\\thaving\\nto\\tfirst\\tmanually\\textract\\tthe\\tnumerical\\tcolumns\\tinto\\ta\\tNumPy\\tarray.\\tThere\\tis\\tnothing\\tin\\tScikit-Learn\\tto\\nhandle\\tPandas\\tDataFrames,\\n19\\n\\tbut\\twe\\tcan\\twrite\\ta\\tcustom\\ttransformer\\t\\nfor\\tthis\\ttask:\\nfrom\\n\\t\\nsklearn.base\\n\\t\\nimport\\n\\t\\nBaseEstimator\\n,\\n\\t\\nTransformerMixin\\nclass\\n\\t\\nDataFrameSelector\\n(\\nBaseEstimator\\n,', ',\\n\\t\\nTransformerMixin\\n):\\n\\t\\t\\t\\t\\ndef\\n\\t\\n__init__\\n(\\nself\\n,\\n\\t\\nattribute_names\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nself\\n.\\nattribute_names\\n\\t\\n=\\n\\t\\nattribute_names\\n\\t\\t\\t\\t\\ndef\\n\\t\\nfit\\n(\\nself\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nself\\n\\t\\t\\t\\t\\ndef\\n\\t\\ntransform\\n(\\nself\\n,\\n\\t\\nX\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nX\\n[\\nself\\n.\\nattribute_names\\n]\\n.\\nvalues\\nOur\\t\\nDataFrameSelector\\n\\twill\\ttransform\\tthe\\tdata\\tby\\tselecting\\tthe\\tdesired\\tattributes,\\tdropping\\tthe\\trest,\\nand\\tconverting\\tthe\\tresulting\\tDataFrame\\tto\\ta\\tNumPy\\tarray.\\tWith\\tthis,\\tyou\\tcan\\teasily\\twrite\\ta\\tpipeline\\tthat\\nwill\\ttake\\ta\\tPandas\\tDataFrame\\tand\\thandle\\tonly\\tthe\\tnumerical\\tvalues:\\tthe\\tpipeline\\twould\\tjust\\tstart\\twith\\ta\\nDataFrameSelector\\n\\tto\\tpick\\tonly\\tthe\\tnumerical\\tattributes,\\tfollowed\\tby\\tthe\\tother\\tpreprocessing\\tsteps\\twe\\ndiscussed\\tearlier.\\tAnd\\tyou\\tcan\\tjust\\tas\\teasily\\twrite\\tanother\\tpipeline\\tfor\\tthe\\tcategorical\\tattributes\\tas\\twell\\nby\\tsimply\\tselecting\\tthe\\tcategorical\\tattributes\\tusing\\ta\\t\\nDataFrameSelector\\n\\tand\\tthen\\tapplying\\ta\\nLabelBinarizer\\n\\t\\n.', 'num_attribs\\n\\t\\n=\\n\\t\\nlist\\n(\\nhousing_num\\n)\\ncat_attribs\\n\\t\\n=\\n\\t\\n[\\n\"ocean_proximity\"\\n]\\nnum_pipeline\\n\\t\\n=\\n\\t\\nPipeline\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'selector\\'\\n,\\n\\t\\nDataFrameSelector\\n(\\nnum_attribs\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'imputer\\'\\n,\\n\\t\\nImputer\\n(\\nstrategy\\n=\\n\"median\"\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'attribs_adder\\'\\n,\\n\\t\\nCombinedAttributesAdder\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'std_scaler\\'\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\n])\\ncat_pipeline\\n\\t\\n=\\n\\t\\nPipeline\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'selector\\'\\n,\\n\\t\\nDataFrameSelector\\n(\\ncat_attribs\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\\'label_binarizer\\'\\n,\\n\\t\\nLabelBinarizer\\n()),\\n\\t\\t\\t\\t\\n])\\nBut\\thow\\tcan\\tyou\\tjoin\\tthese\\ttwo\\tpipelines\\tinto\\ta\\tsingle\\tpipeline?\\tThe\\tanswer\\tis\\tto\\tuse\\tScikit-Learn’s\\nFeatureUnion\\n\\t\\nclass.\\tYou\\tgive\\tit\\ta\\tlist\\tof\\ttransformers\\t(which\\tcan\\tbe\\tentire\\ttransformer\\tpipelines);\\twhen\\nits\\t\\ntransform()\\n\\tmethod\\tis\\tcalled,\\tit\\truns\\teach\\ttransformer’s\\t\\ntransform()\\n\\tmethod\\tin\\tparallel,\\twaits\\tfor\\ntheir\\toutput,\\tand\\tthen\\tconcatenates\\tthem\\tand\\treturns\\tthe\\tresult\\t(and\\tof\\tcourse\\tcalling\\tits\\t\\nfit()\\n\\tmethod\\ncalls\\teach\\ttransformer’s\\t\\nfit()', 'fit()\\n\\tmethod).\\tA\\tfull\\tpipeline\\thandling\\tboth\\tnumerical\\tand\\tcategorical\\nattributes\\tmay\\tlook\\tlike\\t\\nthis:\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nFeatureUnion\\nfull_pipeline\\n\\t\\n=\\n\\t\\nFeatureUnion\\n(\\ntransformer_list\\n=\\n[\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"num_pipeline\"\\n,\\n\\t\\nnum_pipeline\\n),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"cat_pipeline\"\\n,\\n\\t\\ncat_pipeline\\n),\\n\\t\\t\\t\\t\\n])\\nAnd\\tyou\\tcan\\trun\\tthe\\twhole\\tpipeline\\tsimply:\\n>>>\\t\\nhousing_prepared\\n\\t\\n=\\n\\t\\nfull_pipeline\\n.\\nfit_transform\\n(\\nhousing\\n)\\n>>>\\t\\nhousing_prepared\\narray([[-1.15604281,\\t\\t0.77194962,\\t\\t0.74333089,\\t...,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t],\\n\\t\\t\\t\\t\\t\\t\\t[-1.17602483,\\t\\t0.6596948\\t,\\t-1.1653172\\t,\\t...,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t],\\n\\t\\t\\t\\t\\t\\t\\t[...]\\n>>>\\t\\nhousing_prepared\\n.\\nshape\\n(16512,\\t16)', 'Select\\tand\\tTrain\\ta\\tModel\\nAt\\tlast!\\t\\nYou\\tframed\\tthe\\tproblem,\\tyou\\tgot\\tthe\\tdata\\tand\\texplored\\tit,\\tyou\\tsampled\\ta\\ttraining\\tset\\tand\\ta\\ttest\\tset,\\nand\\tyou\\twrote\\ttransformation\\tpipelines\\tto\\tclean\\tup\\tand\\tprepare\\tyour\\tdata\\tfor\\tMachine\\tLearning\\nalgorithms\\tautomatically.\\tYou\\tare\\tnow\\tready\\tto\\tselect\\tand\\ttrain\\ta\\tMachine\\tLearning\\tmodel.', 'Training\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\nThe\\t\\ngood\\tnews\\tis\\tthat\\tthanks\\tto\\tall\\tthese\\tprevious\\tsteps,\\tthings\\tare\\tnow\\tgoing\\tto\\tbe\\tmuch\\tsimpler\\tthan\\tyou\\nmight\\tthink.\\tLet’s\\tfirst\\ttrain\\ta\\t\\nLinear\\tRegression\\tmodel,\\tlike\\twe\\tdid\\tin\\tthe\\tprevious\\tchapter:\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLinearRegression\\nlin_reg\\n\\t\\n=\\n\\t\\nLinearRegression\\n()\\nlin_reg\\n.\\nfit\\n(\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n)\\nDone!\\tYou\\tnow\\thave\\ta\\tworking\\tLinear\\tRegression\\tmodel.\\tLet’s\\ttry\\tit\\tout\\ton\\ta\\tfew\\tinstances\\tfrom\\tthe\\ntraining\\tset:\\n>>>\\t\\nsome_data\\n\\t\\n=\\n\\t\\nhousing\\n.\\niloc\\n[:\\n5\\n]\\n>>>\\t\\nsome_labels\\n\\t\\n=\\n\\t\\nhousing_labels\\n.\\niloc\\n[:\\n5\\n]\\n>>>\\t\\nsome_data_prepared\\n\\t\\n=\\n\\t\\nfull_pipeline\\n.\\ntransform\\n(\\nsome_data\\n)\\n>>>\\t\\nprint\\n(\\n\"Predictions:\"\\n,\\n\\t\\nlin_reg\\n.\\npredict\\n(\\nsome_data_prepared\\n))\\nPredictions:\\t[\\t210644.6045\\t\\t317768.8069\\t\\t210956.4333\\t\\t59218.9888\\t\\t189747.5584]\\n>>>\\t\\nprint\\n(\\n\"Labels:\"\\n,\\n\\t\\nlist\\n(\\nsome_labels\\n))\\nLabels:\\t[286600.0,\\t340600.0,\\t196900.0,\\t46300.0,\\t254500.0]', 'It\\tworks,\\talthough\\tthe\\tpredictions\\tare\\tnot\\texactly\\taccurate\\t(e.g.,\\tthe\\tfirst\\tprediction\\tis\\toff\\tby\\tclose\\tto\\n40%!).\\tLet’s\\tmeasure\\tthis\\tregression\\tmodel’s\\tRMSE\\ton\\tthe\\twhole\\ttraining\\tset\\tusing\\tScikit-Learn’s\\nmean_squared_error\\n\\t\\nfunction:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nmean_squared_error\\n>>>\\t\\nhousing_predictions\\n\\t\\n=\\n\\t\\nlin_reg\\n.\\npredict\\n(\\nhousing_prepared\\n)\\n>>>\\t\\nlin_mse\\n\\t\\n=\\n\\t\\nmean_squared_error\\n(\\nhousing_labels\\n,\\n\\t\\nhousing_predictions\\n)\\n>>>\\t\\nlin_rmse\\n\\t\\n=\\n\\t\\nnp\\n.\\nsqrt\\n(\\nlin_mse\\n)\\n>>>\\t\\nlin_rmse\\n68628.198198489219\\nOkay,\\tthis\\tis\\tbetter\\tthan\\tnothing\\tbut\\tclearly\\tnot\\ta\\tgreat\\tscore:\\tmost\\tdistricts’\\t\\nmedian_housing_values\\nrange\\tbetween\\t$120,000\\tand\\t$265,000,\\tso\\ta\\ttypical\\tprediction\\terror\\tof\\t$68,628\\tis\\tnot\\tvery\\tsatisfying.\\nThis\\tis\\tan\\texample\\tof\\ta\\tmodel\\t\\nunderfitting\\tthe\\ttraining\\tdata.\\tWhen\\tthis\\thappens\\tit\\tcan\\tmean\\tthat\\tthe\\nfeatures\\tdo\\tnot\\tprovide\\tenough\\tinformation\\tto\\tmake\\tgood\\tpredictions,\\tor\\tthat\\tthe\\tmodel\\tis\\tnot\\tpowerful', 'enough.\\tAs\\twe\\tsaw\\tin\\tthe\\tprevious\\tchapter,\\tthe\\tmain\\tways\\tto\\tfix\\tunderfitting\\tare\\tto\\tselect\\ta\\tmore\\npowerful\\tmodel,\\tto\\tfeed\\tthe\\ttraining\\talgorithm\\twith\\tbetter\\tfeatures,\\tor\\tto\\treduce\\tthe\\tconstraints\\ton\\tthe\\nmodel.\\tThis\\tmodel\\tis\\tnot\\tregularized,\\tso\\tthis\\trules\\tout\\tthe\\tlast\\toption.\\tYou\\tcould\\ttry\\tto\\tadd\\tmore\\tfeatures\\n(e.g.,\\tthe\\tlog\\tof\\tthe\\tpopulation),\\tbut\\tfirst\\tlet’s\\ttry\\ta\\tmore\\tcomplex\\tmodel\\tto\\tsee\\thow\\tit\\tdoes.\\nLet’s\\ttrain\\ta\\t\\nDecisionTreeRegressor\\n.\\tThis\\tis\\ta\\tpowerful\\tmodel,\\tcapable\\tof\\tfinding\\tcomplex\\tnonlinear\\nrelationships\\tin\\tthe\\t\\ndata\\t(Decision\\tTrees\\tare\\tpresented\\tin\\tmore\\tdetail\\tin\\t\\nChapter\\t6\\n).\\tThe\\tcode\\tshould\\tlook\\nfamiliar\\tby\\tnow:\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nDecisionTreeRegressor\\ntree_reg\\n\\t\\n=\\n\\t\\nDecisionTreeRegressor\\n()\\ntree_reg\\n.\\nfit\\n(\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n)\\nNow\\tthat\\tthe\\tmodel\\tis\\ttrained,\\tlet’s\\tevaluate\\tit\\ton\\tthe\\ttraining\\t\\nset:', '>>>\\t\\nhousing_predictions\\n\\t\\n=\\n\\t\\ntree_reg\\n.\\npredict\\n(\\nhousing_prepared\\n)\\n>>>\\t\\ntree_mse\\n\\t\\n=\\n\\t\\nmean_squared_error\\n(\\nhousing_labels\\n,\\n\\t\\nhousing_predictions\\n)\\n>>>\\t\\ntree_rmse\\n\\t\\n=\\n\\t\\nnp\\n.\\nsqrt\\n(\\ntree_mse\\n)\\n>>>\\t\\ntree_rmse\\n0.0\\nWait,\\twhat!?\\tNo\\terror\\tat\\tall?\\tCould\\tthis\\tmodel\\treally\\tbe\\tabsolutely\\tperfect?\\tOf\\tcourse,\\tit\\tis\\tmuch\\tmore\\nlikely\\tthat\\tthe\\tmodel\\thas\\tbadly\\toverfit\\tthe\\tdata.\\tHow\\tcan\\tyou\\tbe\\tsure?\\tAs\\twe\\tsaw\\tearlier,\\tyou\\tdon’t\\twant\\nto\\ttouch\\tthe\\ttest\\tset\\tuntil\\tyou\\tare\\tready\\tto\\tlaunch\\ta\\tmodel\\tyou\\tare\\tconfident\\tabout,\\tso\\tyou\\tneed\\tto\\tuse\\tpart\\nof\\tthe\\ttraining\\tset\\tfor\\ttraining,\\tand\\tpart\\tfor\\t\\nmodel\\tvalidation.', 'Better\\tEvaluation\\tUsing\\tCross-Validation\\nOne\\t\\nway\\tto\\tevaluate\\tthe\\tDecision\\tTree\\tmodel\\twould\\tbe\\tto\\tuse\\tthe\\t\\ntrain_test_split\\n\\tfunction\\tto\\tsplit\\nthe\\ttraining\\tset\\tinto\\ta\\tsmaller\\ttraining\\tset\\tand\\ta\\tvalidation\\tset,\\tthen\\ttrain\\tyour\\tmodels\\tagainst\\tthe\\tsmaller\\ntraining\\tset\\tand\\tevaluate\\tthem\\tagainst\\tthe\\tvalidation\\tset.\\tIt’s\\ta\\tbit\\tof\\twork,\\tbut\\tnothing\\ttoo\\tdifficult\\tand\\tit\\nwould\\twork\\tfairly\\twell.\\nA\\tgreat\\talternative\\tis\\tto\\tuse\\t\\nScikit-Learn’s\\t\\ncross-validation\\n\\tfeature.\\tThe\\tfollowing\\tcode\\tperforms\\t\\nK-fold\\ncross-validation\\n:\\tit\\trandomly\\tsplits\\tthe\\ttraining\\tset\\tinto\\t10\\tdistinct\\tsubsets\\tcalled\\t\\nfolds\\n,\\t\\nthen\\tit\\ttrains\\tand\\nevaluates\\tthe\\tDecision\\tTree\\tmodel\\t10\\ttimes,\\tpicking\\ta\\tdifferent\\tfold\\tfor\\tevaluation\\tevery\\ttime\\tand\\ntraining\\ton\\tthe\\tother\\t9\\tfolds.\\tThe\\tresult\\tis\\tan\\tarray\\tcontaining\\tthe\\t\\n10\\tevaluation\\tscores:\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\ncross_val_score\\nscores\\n\\t\\n=\\n\\t\\ncross_val_score\\n(\\ntree_reg\\n,\\n\\t\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscoring\\n=', 'scoring\\n=\\n\"neg_mean_squared_error\"\\n,\\n\\t\\ncv\\n=\\n10\\n)\\ntree_rmse_scores\\n\\t\\n=\\n\\t\\nnp\\n.\\nsqrt\\n(\\n-\\nscores\\n)\\nWARNING\\nScikit-Learn\\tcross-validation\\tfeatures\\texpect\\ta\\tutility\\tfunction\\t(greater\\tis\\tbetter)\\trather\\tthan\\ta\\tcost\\tfunction\\t(lower\\tis\\tbetter),\\tso\\nthe\\tscoring\\tfunction\\tis\\tactually\\tthe\\topposite\\tof\\tthe\\tMSE\\t(i.e.,\\ta\\tnegative\\tvalue),\\twhich\\tis\\twhy\\tthe\\tpreceding\\tcode\\tcomputes\\t\\n-\\nscores\\n\\tbefore\\tcalculating\\tthe\\tsquare\\troot.\\nLet’s\\tlook\\tat\\tthe\\tresults:\\n>>>\\t\\ndef\\n\\t\\ndisplay_scores\\n(\\nscores\\n):\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\n\"Scores:\"\\n,\\n\\t\\nscores\\n)\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\n\"Mean:\"\\n,\\n\\t\\nscores\\n.\\nmean\\n())\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\n\"Standard\\tdeviation:\"\\n,\\n\\t\\nscores\\n.\\nstd\\n())\\n...\\n>>>\\t\\ndisplay_scores\\n(\\ntree_rmse_scores\\n)\\nScores:\\t[\\t70232.0136482\\t\\t\\t66828.46839892\\t\\t72444.08721003\\t\\t70761.50186201\\n\\t\\t71125.52697653\\t\\t75581.29319857\\t\\t70169.59286164\\t\\t70055.37863456\\n\\t\\t75370.49116773\\t\\t71222.39081244]\\nMean:\\t71379.0744771\\nStandard\\tdeviation:\\t2458.31882043', 'Now\\tthe\\tDecision\\tTree\\tdoesn’t\\tlook\\tas\\tgood\\tas\\tit\\tdid\\tearlier.\\tIn\\tfact,\\tit\\tseems\\tto\\tperform\\tworse\\tthan\\tthe\\nLinear\\tRegression\\tmodel!\\tNotice\\tthat\\tcross-validation\\tallows\\tyou\\tto\\tget\\tnot\\tonly\\tan\\testimate\\tof\\tthe\\nperformance\\tof\\tyour\\tmodel,\\tbut\\talso\\ta\\tmeasure\\tof\\thow\\tprecise\\tthis\\testimate\\tis\\t(i.e.,\\tits\\tstandard\\ndeviation).\\tThe\\tDecision\\tTree\\thas\\ta\\tscore\\tof\\tapproximately\\t71,379,\\tgenerally\\t±2,458.\\tYou\\twould\\tnot\\nhave\\tthis\\tinformation\\tif\\tyou\\tjust\\tused\\tone\\tvalidation\\tset.\\tBut\\tcross-validation\\tcomes\\tat\\tthe\\tcost\\tof\\ttraining\\nthe\\tmodel\\tseveral\\ttimes,\\tso\\tit\\tis\\tnot\\talways\\tpossible.\\nLet’s\\tcompute\\tthe\\tsame\\tscores\\tfor\\tthe\\tLinear\\tRegression\\tmodel\\tjust\\tto\\tbe\\t\\nsure:\\n>>>\\t\\nlin_scores\\n\\t\\n=\\n\\t\\ncross_val_score\\n(\\nlin_reg\\n,\\n\\t\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n,\\n...\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscoring\\n=\\n\"neg_mean_squared_error\"\\n,\\n\\t\\ncv\\n=\\n10\\n)\\n...\\n>>>\\t\\nlin_rmse_scores\\n\\t\\n=\\n\\t\\nnp\\n.\\nsqrt\\n(\\n-\\nlin_scores\\n)\\n>>>\\t\\ndisplay_scores\\n(\\nlin_rmse_scores\\n)\\nScores:\\t[\\t66760.97371572\\t\\t66962.61914244\\t\\t70349.94853401\\t\\t74757.02629506', '68031.13388938\\t\\t71193.84183426\\t\\t64968.13706527\\t\\t68261.95557897', '71527.64217874\\t\\t67665.10082067]\\nMean:\\t69047.8379055\\nStandard\\tdeviation:\\t2735.51074287\\nThat’s\\tright:\\tthe\\tDecision\\tTree\\tmodel\\tis\\toverfitting\\tso\\tbadly\\tthat\\tit\\tperforms\\tworse\\tthan\\t\\nthe\\tLinear\\nRegression\\tmodel.\\nLet’s\\ttry\\tone\\tlast\\tmodel\\tnow:\\tthe\\t\\nRandomForestRegressor\\n.\\tAs\\twe\\twill\\tsee\\tin\\t\\nChapter\\t7\\n,\\t\\nRandom\\nForests\\twork\\tby\\ttraining\\tmany\\tDecision\\tTrees\\ton\\trandom\\tsubsets\\tof\\tthe\\tfeatures,\\tthen\\taveraging\\tout\\ttheir\\npredictions.\\tBuilding\\ta\\tmodel\\ton\\ttop\\tof\\tmany\\tother\\tmodels\\tis\\t\\ncalled\\t\\nEnsemble\\tLearning\\n,\\tand\\tit\\tis\\toften\\ta\\ngreat\\tway\\tto\\tpush\\tML\\talgorithms\\teven\\tfurther.\\tWe\\twill\\tskip\\tmost\\tof\\tthe\\tcode\\tsince\\tit\\tis\\tessentially\\tthe\\nsame\\tas\\tfor\\tthe\\tother\\tmodels:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nRandomForestRegressor\\n>>>\\t\\nforest_reg\\n\\t\\n=\\n\\t\\nRandomForestRegressor\\n()\\n>>>\\t\\nforest_reg\\n.\\nfit\\n(\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n)\\n>>>\\t\\n[\\n...\\n]\\n>>>\\t\\nforest_rmse\\n21941.911027380233\\n>>>\\t\\ndisplay_scores\\n(\\nforest_rmse_scores\\n)\\nScores:\\t[\\t51650.94405471\\t\\t48920.80645498\\t\\t52979.16096752\\t\\t54412.74042021', '50861.29381163\\t\\t56488.55699727\\t\\t51866.90120786\\t\\t49752.24599537\\n\\t\\t55399.50713191\\t\\t53309.74548294]\\nMean:\\t52564.1902524\\nStandard\\tdeviation:\\t2301.87380392\\nWow,\\tthis\\tis\\tmuch\\tbetter:\\tRandom\\tForests\\tlook\\tvery\\tpromising.\\tHowever,\\tnote\\tthat\\tthe\\tscore\\ton\\tthe\\ntraining\\tset\\tis\\tstill\\tmuch\\tlower\\tthan\\ton\\tthe\\tvalidation\\tsets,\\tmeaning\\tthat\\tthe\\tmodel\\tis\\tstill\\toverfitting\\tthe\\ntraining\\tset.\\tPossible\\tsolutions\\tfor\\toverfitting\\tare\\tto\\tsimplify\\tthe\\tmodel,\\tconstrain\\tit\\t(i.e.,\\tregularize\\tit),\\tor\\nget\\ta\\tlot\\tmore\\ttraining\\tdata.\\tHowever,\\tbefore\\tyou\\tdive\\tmuch\\tdeeper\\tin\\tRandom\\tForests,\\tyou\\tshould\\ttry\\tout\\nmany\\tother\\tmodels\\tfrom\\tvarious\\tcategories\\tof\\tMachine\\tLearning\\talgorithms\\t(several\\tSupport\\tVector\\nMachines\\twith\\tdifferent\\tkernels,\\tpossibly\\ta\\tneural\\tnetwork,\\tetc.),\\twithout\\tspending\\ttoo\\tmuch\\ttime\\ntweaking\\tthe\\thyperparameters.\\tThe\\tgoal\\tis\\tto\\tshortlist\\ta\\tfew\\t(two\\tto\\tfive)\\tpromising\\tmodels.\\nTIP\\nYou\\tshould\\tsave\\tevery\\tmodel\\tyou\\texperiment\\twith,\\tso\\tyou\\tcan\\tcome\\tback\\teasily\\tto\\tany\\tmodel\\tyou\\twant.\\tMake\\tsure\\tyou\\tsave', 'both\\tthe\\thyperparameters\\tand\\tthe\\ttrained\\tparameters,\\tas\\twell\\tas\\tthe\\tcross-validation\\tscores\\tand\\tperhaps\\tthe\\tactual\\tpredictions\\tas\\nwell.\\tThis\\twill\\tallow\\tyou\\tto\\teasily\\tcompare\\tscores\\tacross\\tmodel\\ttypes,\\tand\\tcompare\\tthe\\ttypes\\tof\\terrors\\tthey\\tmake.\\tYou\\tcan\\neasily\\tsave\\tScikit-Learn\\tmodels\\tby\\tusing\\t\\nPython’s\\t\\npickle\\n\\tmodule,\\tor\\t\\nusing\\t\\nsklearn.externals.joblib\\n,\\twhich\\tis\\tmore\\tefficient\\nat\\tserializing\\t\\nlarge\\tNumPy\\tarrays:\\nfrom\\n\\t\\nsklearn.externals\\n\\t\\nimport\\n\\t\\njoblib\\njoblib\\n.\\ndump\\n(\\nmy_model\\n,\\n\\t\\n\"my_model.pkl\"\\n)\\n#\\tand\\tlater...\\nmy_model_loaded\\n\\t\\n=\\n\\t\\njoblib\\n.\\nload\\n(\\n\"my_model.pkl\"\\n)', 'Fine-Tune\\tYour\\tModel\\nLet’s\\t\\nassume\\tthat\\tyou\\tnow\\thave\\ta\\tshortlist\\tof\\tpromising\\tmodels.\\tYou\\tnow\\tneed\\tto\\tfine-tune\\tthem.\\tLet’s\\nlook\\tat\\ta\\tfew\\tways\\tyou\\tcan\\tdo\\tthat.', \"Grid\\tSearch\\nOne\\tway\\tto\\tdo\\tthat\\twould\\tbe\\tto\\tfiddle\\twith\\tthe\\t\\nhyperparameters\\tmanually,\\tuntil\\tyou\\tfind\\ta\\tgreat\\ncombination\\tof\\thyperparameter\\tvalues.\\tThis\\twould\\tbe\\tvery\\ttedious\\twork,\\tand\\tyou\\tmay\\tnot\\thave\\ttime\\tto\\nexplore\\tmany\\tcombinations.\\nInstead\\tyou\\tshould\\tget\\t\\nScikit-Learn’s\\t\\nGridSearchCV\\n\\tto\\tsearch\\tfor\\tyou.\\tAll\\tyou\\tneed\\tto\\tdo\\tis\\ttell\\tit\\twhich\\nhyperparameters\\tyou\\twant\\tit\\tto\\texperiment\\twith,\\tand\\twhat\\tvalues\\tto\\ttry\\tout,\\tand\\tit\\twill\\tevaluate\\tall\\tthe\\npossible\\tcombinations\\tof\\thyperparameter\\tvalues,\\tusing\\tcross-validation.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\nsearches\\tfor\\tthe\\tbest\\tcombination\\tof\\thyperparameter\\tvalues\\t\\nfor\\tthe\\t\\nRandomForestRegressor\\n:\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\nGridSearchCV\\nparam_grid\\n\\t\\n=\\n\\t\\n[\\n\\t\\t\\t\\t\\n{\\n'n_estimators'\\n:\\n\\t\\n[\\n3\\n,\\n\\t\\n10\\n,\\n\\t\\n30\\n],\\n\\t\\n'max_features'\\n:\\n\\t\\n[\\n2\\n,\\n\\t\\n4\\n,\\n\\t\\n6\\n,\\n\\t\\n8\\n]},\\n\\t\\t\\t\\t\\n{\\n'bootstrap'\\n:\\n\\t\\n[\\nFalse\\n],\\n\\t\\n'n_estimators'\\n:\\n\\t\\n[\\n3\\n,\\n\\t\\n10\\n],\\n\\t\\n'max_features'\\n:\\n\\t\\n[\\n2\\n,\\n\\t\\n3\\n,\\n\\t\\n4\\n]},\\n\\t\\t\\n]\\nforest_reg\\n\\t\\n=\\n\\t\\nRandomForestRegressor\\n()\\ngrid_search\\n\\t\\n=\", \"=\\n\\t\\nGridSearchCV\\n(\\nforest_reg\\n,\\n\\t\\nparam_grid\\n,\\n\\t\\ncv\\n=\\n5\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscoring\\n=\\n'neg_mean_squared_error'\\n)\\ngrid_search\\n.\\nfit\\n(\\nhousing_prepared\\n,\\n\\t\\nhousing_labels\\n)\\nTIP\\nWhen\\tyou\\thave\\tno\\tidea\\twhat\\tvalue\\ta\\thyperparameter\\tshould\\thave,\\ta\\tsimple\\tapproach\\tis\\tto\\ttry\\tout\\tconsecutive\\tpowers\\tof\\t10\\t(or\\ta\\nsmaller\\tnumber\\tif\\tyou\\twant\\ta\\tmore\\tfine-grained\\tsearch,\\tas\\tshown\\tin\\tthis\\texample\\twith\\tthe\\t\\nn_estimators\\n\\thyperparameter).\\nThis\\t\\nparam_grid\\n\\ttells\\tScikit-Learn\\tto\\tfirst\\tevaluate\\tall\\t3\\t×\\t4\\t=\\t12\\tcombinations\\tof\\t\\nn_estimators\\n\\tand\\nmax_features\\n\\thyperparameter\\tvalues\\tspecified\\tin\\tthe\\tfirst\\t\\ndict\\n\\t(don’t\\tworry\\tabout\\twhat\\tthese\\nhyperparameters\\tmean\\tfor\\tnow;\\tthey\\twill\\tbe\\texplained\\tin\\t\\nChapter\\t7\\n),\\tthen\\ttry\\tall\\t2\\t×\\t3\\t=\\t6\\tcombinations\\nof\\thyperparameter\\tvalues\\tin\\tthe\\tsecond\\t\\ndict\\n,\\tbut\\tthis\\ttime\\twith\\tthe\\t\\nbootstrap\\n\\t\\nhyperparameter\\tset\\tto\\nFalse\\n\\tinstead\\tof\\t\\nTrue\\n\\t(which\\tis\\tthe\\tdefault\\tvalue\\tfor\\tthis\\thyperparameter).\\nAll\\tin\\tall,\\tthe\\tgrid\\tsearch\\twill\\texplore\\t12\\t+\\t6\\t=\\t18\\tcombinations\\tof\", \"RandomForestRegressor\\nhyperparameter\\tvalues,\\tand\\tit\\twill\\ttrain\\teach\\tmodel\\tfive\\ttimes\\t(since\\twe\\tare\\tusing\\tfive-fold\\tcross\\nvalidation).\\tIn\\tother\\twords,\\tall\\tin\\tall,\\tthere\\twill\\tbe\\t18\\t×\\t5\\t=\\t90\\trounds\\tof\\ttraining!\\tIt\\tmay\\ttake\\tquite\\ta\\tlong\\ntime,\\tbut\\twhen\\tit\\tis\\tdone\\tyou\\tcan\\tget\\tthe\\tbest\\tcombination\\tof\\tparameters\\tlike\\tthis:\\n>>>\\t\\ngrid_search\\n.\\nbest_params_\\n{'max_features':\\t8,\\t'n_estimators':\\t30}\\nTIP\\nSince\\t8\\tand\\t30\\tare\\tthe\\tmaximum\\tvalues\\tthat\\twere\\tevaluated,\\tyou\\tshould\\tprobably\\ttry\\tsearching\\tagain\\twith\\thigher\\tvalues,\\tsince\\nthe\\tscore\\tmay\\tcontinue\\tto\\timprove.\", 'You\\tcan\\talso\\tget\\tthe\\tbest\\testimator\\tdirectly:\\n>>>\\t\\ngrid_search\\n.\\nbest_estimator_\\nRandomForestRegressor(bootstrap=True,\\tcriterion=\\'mse\\',\\tmax_depth=None,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tmax_features=8,\\tmax_leaf_nodes=None,\\tmin_impurity_split=1e-07,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tmin_samples_leaf=1,\\tmin_samples_split=2,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tmin_weight_fraction_leaf=0.0,\\tn_estimators=30,\\tn_jobs=1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toob_score=False,\\trandom_state=42,\\tverbose=0,\\twarm_start=False)\\nNOTE\\nIf\\t\\nGridSearchCV\\n\\tis\\tinitialized\\twith\\t\\nrefit=True\\n\\t(which\\tis\\tthe\\tdefault),\\tthen\\tonce\\tit\\tfinds\\tthe\\tbest\\testimator\\tusing\\tcross-validation,\\tit\\nretrains\\tit\\ton\\tthe\\twhole\\ttraining\\tset.\\tThis\\tis\\tusually\\ta\\tgood\\tidea\\tsince\\tfeeding\\tit\\tmore\\tdata\\twill\\tlikely\\timprove\\tits\\tperformance.\\nAnd\\tof\\tcourse\\tthe\\tevaluation\\tscores\\tare\\talso\\tavailable:\\n>>>\\t\\ncvres\\n\\t\\n=\\n\\t\\ngrid_search\\n.\\ncv_results_\\n>>>\\t\\nfor\\n\\t\\nmean_score\\n,\\n\\t\\nparams\\n\\t\\nin\\n\\t\\nzip\\n(\\ncvres\\n[\\n\"mean_test_score\"\\n],\\n\\t\\ncvres\\n[\\n\"params\"\\n]):\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\nnp\\n.\\nsqrt\\n(\\n-\\nmean_score\\n),\\n\\t\\nparams\\n)\\n...', \")\\n...\\n63825.0479302\\t{'max_features':\\t2,\\t'n_estimators':\\t3}\\n55643.8429091\\t{'max_features':\\t2,\\t'n_estimators':\\t10}\\n53380.6566859\\t{'max_features':\\t2,\\t'n_estimators':\\t30}\\n60959.1388585\\t{'max_features':\\t4,\\t'n_estimators':\\t3}\\n52740.5841667\\t{'max_features':\\t4,\\t'n_estimators':\\t10}\\n50374.1421461\\t{'max_features':\\t4,\\t'n_estimators':\\t30}\\n58661.2866462\\t{'max_features':\\t6,\\t'n_estimators':\\t3}\\n52009.9739798\\t{'max_features':\\t6,\\t'n_estimators':\\t10}\\n50154.1177737\\t{'max_features':\\t6,\\t'n_estimators':\\t30}\\n57865.3616801\\t{'max_features':\\t8,\\t'n_estimators':\\t3}\\n51730.0755087\\t{'max_features':\\t8,\\t'n_estimators':\\t10}\\n49694.8514333\\t{'max_features':\\t8,\\t'n_estimators':\\t30}\\n62874.4073931\\t{'max_features':\\t2,\\t'n_estimators':\\t3,\\t'bootstrap':\\tFalse}\\n54561.9398157\\t{'max_features':\\t2,\\t'n_estimators':\\t10,\\t'bootstrap':\\tFalse}\\n59416.6463145\\t{'max_features':\\t3,\\t'n_estimators':\\t3,\\t'bootstrap':\\tFalse}\\n52660.245911\\t{'max_features':\\t3,\\t'n_estimators':\\t10,\\t'bootstrap':\\tFalse}\", \"57490.0168279\\t{'max_features':\\t4,\\t'n_estimators':\\t3,\\t'bootstrap':\\tFalse}\\n51093.9059428\\t{'max_features':\\t4,\\t'n_estimators':\\t10,\\t'bootstrap':\\tFalse}\\nIn\\tthis\\texample,\\twe\\tobtain\\tthe\\tbest\\tsolution\\tby\\tsetting\\tthe\\t\\nmax_features\\n\\thyperparameter\\tto\\t\\n8\\n,\\tand\\tthe\\nn_estimators\\n\\thyperparameter\\tto\\t\\n30\\n.\\tThe\\tRMSE\\tscore\\tfor\\tthis\\tcombination\\tis\\t49,694,\\twhich\\tis\\tslightly\\nbetter\\tthan\\tthe\\tscore\\tyou\\tgot\\tearlier\\tusing\\tthe\\tdefault\\thyperparameter\\tvalues\\t(which\\twas\\t52,564).\\nCongratulations,\\tyou\\thave\\tsuccessfully\\tfine-tuned\\tyour\\tbest\\tmodel!\\nTIP\\nDon’t\\tforget\\tthat\\tyou\\tcan\\ttreat\\tsome\\tof\\tthe\\tdata\\tpreparation\\tsteps\\tas\\thyperparameters.\\tFor\\texample,\\tthe\\tgrid\\tsearch\\twill\\nautomatically\\tfind\\tout\\twhether\\tor\\tnot\\tto\\tadd\\ta\\tfeature\\tyou\\twere\\tnot\\tsure\\tabout\\t(e.g.,\\tusing\\tthe\\t\\nadd_bedrooms_per_room\\nhyperparameter\\tof\\tyour\\t\\nCombinedAttributesAdder\\n\\ttransformer).\\tIt\\tmay\\tsimilarly\\tbe\\tused\\tto\\tautomatically\\tfind\\tthe\\tbest\\tway\\tto\\nhandle\\toutliers,\\tmissing\\tfeatures,\\tfeature\\tselection,\\t\\nand\\t\\nmore.\", 'Randomized\\tSearch\\nThe\\t\\ngrid\\tsearch\\tapproach\\t\\nis\\tfine\\twhen\\tyou\\tare\\texploring\\trelatively\\tfew\\tcombinations,\\tlike\\tin\\tthe\\tprevious\\nexample,\\tbut\\twhen\\tthe\\thyperparameter\\t\\nsearch\\tspace\\n\\t\\nis\\tlarge,\\tit\\tis\\toften\\tpreferable\\tto\\tuse\\nRandomizedSearchCV\\n\\tinstead.\\tThis\\tclass\\tcan\\tbe\\tused\\tin\\tmuch\\tthe\\tsame\\tway\\tas\\tthe\\t\\nGridSearchCV\\n\\tclass,\\nbut\\tinstead\\tof\\ttrying\\tout\\tall\\tpossible\\tcombinations,\\tit\\tevaluates\\ta\\tgiven\\tnumber\\tof\\trandom\\tcombinations\\nby\\tselecting\\ta\\trandom\\tvalue\\tfor\\teach\\thyperparameter\\tat\\tevery\\titeration.\\tThis\\tapproach\\thas\\ttwo\\tmain\\nbenefits:\\nIf\\tyou\\tlet\\tthe\\trandomized\\tsearch\\trun\\tfor,\\tsay,\\t1,000\\titerations,\\tthis\\tapproach\\twill\\texplore\\t1,000\\ndifferent\\tvalues\\tfor\\teach\\thyperparameter\\t(instead\\tof\\tjust\\ta\\tfew\\tvalues\\tper\\thyperparameter\\twith\\tthe\\ngrid\\tsearch\\tapproach).\\nYou\\thave\\tmore\\tcontrol\\tover\\tthe\\tcomputing\\tbudget\\tyou\\twant\\tto\\tallocate\\tto\\thyperparameter\\tsearch,\\nsimply\\tby\\tsetting\\tthe\\tnumber\\tof\\titerations.', 'Ensemble\\tMethods\\nAnother\\t\\nway\\tto\\tfine-tune\\tyour\\tsystem\\tis\\tto\\ttry\\tto\\tcombine\\tthe\\tmodels\\tthat\\tperform\\tbest.\\tThe\\tgroup\\t(or\\n“ensemble”)\\twill\\toften\\tperform\\tbetter\\tthan\\tthe\\tbest\\tindividual\\tmodel\\t(just\\tlike\\tRandom\\tForests\\tperform\\nbetter\\tthan\\tthe\\tindividual\\tDecision\\tTrees\\tthey\\trely\\ton),\\tespecially\\tif\\tthe\\tindividual\\tmodels\\tmake\\tvery\\ndifferent\\ttypes\\tof\\terrors.\\tWe\\twill\\tcover\\tthis\\ttopic\\tin\\tmore\\tdetail\\tin\\t\\nChapter\\t7\\n.', 'Analyze\\tthe\\tBest\\tModels\\tand\\tTheir\\tErrors\\nYou\\t\\nwill\\toften\\tgain\\tgood\\tinsights\\ton\\tthe\\tproblem\\tby\\tinspecting\\tthe\\tbest\\tmodels.\\tFor\\texample,\\tthe\\nRandomForestRegressor\\n\\t\\ncan\\tindicate\\tthe\\trelative\\timportance\\tof\\teach\\tattribute\\tfor\\tmaking\\taccurate\\npredictions:\\n>>>\\t\\nfeature_importances\\n\\t\\n=\\n\\t\\ngrid_search\\n.\\nbest_estimator_\\n.\\nfeature_importances_\\n>>>\\t\\nfeature_importances\\narray([\\t\\t7.33442355e-02,\\t\\t\\t6.29090705e-02,\\t\\t\\t4.11437985e-02,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t1.46726854e-02,\\t\\t\\t1.41064835e-02,\\t\\t\\t1.48742809e-02,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t1.42575993e-02,\\t\\t\\t3.66158981e-01,\\t\\t\\t5.64191792e-02,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t1.08792957e-01,\\t\\t\\t5.33510773e-02,\\t\\t\\t1.03114883e-02,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t1.64780994e-01,\\t\\t\\t6.02803867e-05,\\t\\t\\t1.96041560e-03,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t2.85647464e-03])\\nLet’s\\tdisplay\\tthese\\timportance\\tscores\\tnext\\tto\\ttheir\\tcorresponding\\tattribute\\tnames:\\n>>>\\t\\nextra_attribs\\n\\t\\n=\\n\\t\\n[\\n\"rooms_per_hhold\"\\n,\\n\\t\\n\"pop_per_hhold\"\\n,\\n\\t\\n\"bedrooms_per_room\"\\n]\\n>>>\\t\\ncat_one_hot_attribs\\n\\t\\n=\\n\\t\\nlist\\n(\\nencoder\\n.\\nclasses_\\n)\\n>>>\\t\\nattributes\\n\\t\\n=\\n\\t\\nnum_attribs\\n\\t\\n+\\n\\t\\nextra_attribs\\n\\t\\n+', \"+\\n\\t\\ncat_one_hot_attribs\\n>>>\\t\\nsorted\\n(\\nzip\\n(\\nfeature_importances\\n,\\n\\t\\nattributes\\n),\\n\\t\\nreverse\\n=\\nTrue\\n)\\n[(0.36615898061813418,\\t'median_income'),\\n\\t(0.16478099356159051,\\t'INLAND'),\\n\\t(0.10879295677551573,\\t'pop_per_hhold'),\\n\\t(0.073344235516012421,\\t'longitude'),\\n\\t(0.062909070482620302,\\t'latitude'),\\n\\t(0.056419179181954007,\\t'rooms_per_hhold'),\\n\\t(0.053351077347675809,\\t'bedrooms_per_room'),\\n\\t(0.041143798478729635,\\t'housing_median_age'),\\n\\t(0.014874280890402767,\\t'population'),\\n\\t(0.014672685420543237,\\t'total_rooms'),\\n\\t(0.014257599323407807,\\t'households'),\\n\\t(0.014106483453584102,\\t'total_bedrooms'),\\n\\t(0.010311488326303787,\\t'<1H\\tOCEAN'),\\n\\t(0.0028564746373201579,\\t'NEAR\\tOCEAN'),\\n\\t(0.0019604155994780701,\\t'NEAR\\tBAY'),\\n\\t(6.0280386727365991e-05,\\t'ISLAND')]\\nWith\\tthis\\tinformation,\\tyou\\tmay\\twant\\tto\\ttry\\tdropping\\tsome\\tof\\tthe\\tless\\tuseful\\tfeatures\\t(e.g.,\\tapparently\\tonly\\none\\t\\nocean_proximity\\n\\tcategory\\tis\\treally\\tuseful,\\tso\\tyou\\tcould\\ttry\\tdropping\\tthe\\tothers).\", 'You\\tshould\\talso\\tlook\\tat\\tthe\\tspecific\\terrors\\tthat\\tyour\\tsystem\\tmakes,\\tthen\\ttry\\tto\\tunderstand\\twhy\\tit\\tmakes\\nthem\\tand\\twhat\\tcould\\tfix\\tthe\\tproblem\\t(adding\\textra\\tfeatures\\tor,\\ton\\tthe\\tcontrary,\\t\\ngetting\\trid\\tof\\tuninformative\\nones,\\tcleaning\\tup\\toutliers,\\tetc.).', 'Evaluate\\tYour\\tSystem\\ton\\tthe\\tTest\\tSet\\nAfter\\t\\ntweaking\\tyour\\tmodels\\tfor\\ta\\twhile,\\tyou\\teventually\\thave\\ta\\tsystem\\tthat\\tperforms\\tsufficiently\\twell.\\nNow\\tis\\tthe\\ttime\\tto\\tevaluate\\tthe\\tfinal\\tmodel\\ton\\tthe\\ttest\\tset.\\tThere\\tis\\tnothing\\tspecial\\tabout\\tthis\\tprocess;\\tjust\\nget\\tthe\\tpredictors\\tand\\tthe\\tlabels\\tfrom\\tyour\\ttest\\tset,\\trun\\tyour\\t\\nfull_pipeline\\n\\tto\\ttransform\\tthe\\tdata\\t(call\\ntransform()\\n,\\t\\nnot\\n\\t\\nfit_transform()\\n!),\\tand\\tevaluate\\tthe\\tfinal\\tmodel\\ton\\tthe\\ttest\\tset:\\nfinal_model\\n\\t\\n=\\n\\t\\ngrid_search\\n.\\nbest_estimator_\\nX_test\\n\\t\\n=\\n\\t\\nstrat_test_set\\n.\\ndrop\\n(\\n\"median_house_value\"\\n,\\n\\t\\naxis\\n=\\n1\\n)\\ny_test\\n\\t\\n=\\n\\t\\nstrat_test_set\\n[\\n\"median_house_value\"\\n]\\n.\\ncopy\\n()\\nX_test_prepared\\n\\t\\n=\\n\\t\\nfull_pipeline\\n.\\ntransform\\n(\\nX_test\\n)\\nfinal_predictions\\n\\t\\n=\\n\\t\\nfinal_model\\n.\\npredict\\n(\\nX_test_prepared\\n)\\nfinal_mse\\n\\t\\n=\\n\\t\\nmean_squared_error\\n(\\ny_test\\n,\\n\\t\\nfinal_predictions\\n)\\nfinal_rmse\\n\\t\\n=\\n\\t\\nnp\\n.\\nsqrt\\n(\\nfinal_mse\\n)\\n\\t\\t\\t\\n#\\t=>\\tevaluates\\tto\\t47,766.0\\nThe\\t\\nperformance\\twill\\tusually\\tbe\\tslightly\\tworse\\tthan\\twhat\\tyou\\tmeasured\\tusing\\tcross-validation\\tif\\tyou\\tdid', 'a\\tlot\\tof\\thyperparameter\\ttuning\\t(because\\tyour\\tsystem\\tends\\tup\\tfine-tuned\\tto\\tperform\\twell\\ton\\tthe\\tvalidation\\ndata,\\tand\\twill\\tlikely\\tnot\\tperform\\tas\\twell\\ton\\tunknown\\tdatasets).\\tIt\\tis\\tnot\\tthe\\tcase\\tin\\tthis\\texample,\\tbut\\twhen\\nthis\\thappens\\tyou\\tmust\\tresist\\tthe\\ttemptation\\tto\\ttweak\\tthe\\t\\nhyperparameters\\tto\\tmake\\tthe\\tnumbers\\tlook\\tgood\\non\\tthe\\ttest\\tset;\\tthe\\timprovements\\twould\\tbe\\tunlikely\\tto\\tgeneralize\\tto\\tnew\\tdata.\\nNow\\tcomes\\tthe\\tproject\\tprelaunch\\tphase:\\tyou\\tneed\\tto\\tpresent\\tyour\\tsolution\\t(highlighting\\twhat\\tyou\\thave\\nlearned,\\twhat\\tworked\\tand\\twhat\\tdid\\tnot,\\twhat\\tassumptions\\twere\\tmade,\\tand\\twhat\\tyour\\tsystem’s\\tlimitations\\nare),\\tdocument\\teverything,\\tand\\tcreate\\tnice\\tpresentations\\twith\\tclear\\tvisualizations\\tand\\teasy-to-remember\\nstatements\\t(e.g.,\\t“the\\tmedian\\tincome\\tis\\tthe\\tnumber\\tone\\tpredictor\\tof\\thousing\\tprices”).', 'Launch,\\tMonitor,\\tand\\tMaintain\\tYour\\tSystem\\nPerfect,\\tyou\\tgot\\tapproval\\tto\\tlaunch!\\tYou\\tneed\\tto\\tget\\tyour\\tsolution\\tready\\tfor\\tproduction,\\tin\\tparticular\\tby\\nplugging\\tthe\\tproduction\\tinput\\tdata\\tsources\\tinto\\tyour\\tsystem\\tand\\twriting\\ttests.\\nYou\\talso\\tneed\\tto\\twrite\\tmonitoring\\tcode\\tto\\tcheck\\tyour\\tsystem’s\\tlive\\tperformance\\tat\\tregular\\tintervals\\tand\\ntrigger\\talerts\\twhen\\tit\\tdrops.\\tThis\\tis\\timportant\\tto\\tcatch\\tnot\\tonly\\tsudden\\tbreakage,\\tbut\\talso\\tperformance\\ndegradation.\\tThis\\tis\\tquite\\tcommon\\tbecause\\tmodels\\ttend\\tto\\t“rot”\\tas\\tdata\\tevolves\\tover\\ttime,\\tunless\\tthe\\nmodels\\tare\\tregularly\\ttrained\\ton\\tfresh\\tdata.\\nEvaluating\\tyour\\tsystem’s\\tperformance\\twill\\trequire\\tsampling\\tthe\\tsystem’s\\tpredictions\\tand\\tevaluating\\tthem.\\nThis\\twill\\tgenerally\\trequire\\ta\\thuman\\tanalysis.\\tThese\\tanalysts\\tmay\\tbe\\tfield\\texperts,\\tor\\tworkers\\ton\\ta\\ncrowdsourcing\\tplatform\\t(such\\tas\\tAmazon\\tMechanical\\tTurk\\tor\\tCrowdFlower).\\tEither\\tway,\\tyou\\tneed\\tto\\nplug\\tthe\\thuman\\tevaluation\\tpipeline\\tinto\\tyour\\tsystem.', 'You\\tshould\\talso\\tmake\\tsure\\tyou\\tevaluate\\tthe\\tsystem’s\\tinput\\tdata\\tquality.\\tSometimes\\tperformance\\twill\\ndegrade\\tslightly\\tbecause\\tof\\ta\\tpoor\\tquality\\tsignal\\t(e.g.,\\ta\\tmalfunctioning\\tsensor\\tsending\\trandom\\tvalues,\\tor\\nanother\\tteam’s\\toutput\\tbecoming\\tstale),\\tbut\\tit\\tmay\\ttake\\ta\\twhile\\tbefore\\tyour\\tsystem’s\\tperformance\\tdegrades\\nenough\\tto\\ttrigger\\tan\\talert.\\tIf\\tyou\\tmonitor\\tyour\\tsystem’s\\tinputs,\\tyou\\tmay\\tcatch\\tthis\\tearlier.\\tMonitoring\\tthe\\ninputs\\tis\\tparticularly\\timportant\\tfor\\tonline\\tlearning\\tsystems.\\nFinally,\\tyou\\twill\\tgenerally\\twant\\tto\\ttrain\\tyour\\tmodels\\ton\\ta\\tregular\\tbasis\\tusing\\tfresh\\tdata.\\tYou\\tshould\\nautomate\\tthis\\tprocess\\tas\\tmuch\\tas\\tpossible.\\tIf\\tyou\\tdon’t,\\tyou\\tare\\tvery\\tlikely\\tto\\trefresh\\tyour\\tmodel\\tonly\\nevery\\tsix\\tmonths\\t(at\\tbest),\\tand\\tyour\\tsystem’s\\tperformance\\tmay\\tfluctuate\\tseverely\\tover\\ttime.\\tIf\\tyour\\nsystem\\tis\\tan\\tonline\\tlearning\\tsystem,\\tyou\\tshould\\tmake\\tsure\\tyou\\tsave\\tsnapshots\\tof\\tits\\tstate\\tat\\tregular\\nintervals\\tso\\tyou\\tcan\\teasily\\troll\\tback\\tto\\ta\\tpreviously\\tworking\\tstate.', 'Try\\tIt\\tOut!\\nHopefully\\tthis\\tchapter\\tgave\\tyou\\ta\\tgood\\tidea\\tof\\twhat\\ta\\tMachine\\tLearning\\tproject\\tlooks\\tlike,\\tand\\tshowed\\nyou\\tsome\\tof\\tthe\\ttools\\tyou\\tcan\\tuse\\tto\\ttrain\\ta\\tgreat\\tsystem.\\tAs\\tyou\\tcan\\tsee,\\tmuch\\tof\\tthe\\twork\\tis\\tin\\tthe\\tdata\\npreparation\\tstep,\\tbuilding\\tmonitoring\\ttools,\\tsetting\\tup\\thuman\\tevaluation\\tpipelines,\\tand\\tautomating\\tregular\\nmodel\\ttraining.\\tThe\\tMachine\\tLearning\\talgorithms\\tare\\talso\\timportant,\\tof\\tcourse,\\tbut\\tit\\tis\\tprobably\\npreferable\\tto\\tbe\\tcomfortable\\twith\\tthe\\toverall\\tprocess\\tand\\tknow\\tthree\\tor\\tfour\\talgorithms\\twell\\trather\\tthan\\nto\\tspend\\tall\\tyour\\ttime\\texploring\\tadvanced\\talgorithms\\tand\\tnot\\tenough\\ttime\\ton\\tthe\\toverall\\tprocess.\\nSo,\\tif\\tyou\\thave\\tnot\\talready\\tdone\\tso,\\tnow\\tis\\ta\\tgood\\ttime\\tto\\tpick\\tup\\ta\\tlaptop,\\tselect\\ta\\tdataset\\tthat\\tyou\\tare\\ninterested\\tin,\\tand\\ttry\\tto\\tgo\\tthrough\\tthe\\twhole\\tprocess\\tfrom\\tA\\tto\\tZ.\\tA\\tgood\\tplace\\tto\\tstart\\tis\\ton\\ta\\ncompetition\\twebsite\\tsuch\\tas\\t\\nhttp://kaggle.com/\\n:\\tyou\\twill\\thave\\ta\\tdataset\\tto\\tplay\\twith,\\ta\\tclear\\tgoal,\\tand\\npeople\\tto\\tshare\\tthe\\texperience\\twith.', 'Exercises\\nUsing\\tthis\\tchapter’s\\thousing\\tdataset:\\n1\\n.\\t\\nTry\\ta\\tSupport\\tVector\\tMachine\\tregressor\\t(\\nsklearn.svm.SVR\\n),\\t\\nwith\\tvarious\\thyperparameters\\tsuch\\tas\\nkernel=\"linear\"\\n\\t(with\\tvarious\\tvalues\\tfor\\tthe\\t\\nC\\n\\thyperparameter)\\tor\\t\\nkernel=\"rbf\"\\n\\t(with\\tvarious\\nvalues\\tfor\\tthe\\t\\nC\\n\\tand\\t\\ngamma\\n\\thyperparameters).\\tDon’t\\tworry\\tabout\\twhat\\tthese\\thyperparameters\\tmean\\nfor\\tnow.\\tHow\\tdoes\\tthe\\tbest\\t\\nSVR\\n\\tpredictor\\tperform?\\n2\\n.\\t\\nTry\\treplacing\\t\\nGridSearchCV\\n\\t\\nwith\\t\\nRandomizedSearchCV\\n.\\n3\\n.\\t\\nTry\\tadding\\ta\\ttransformer\\tin\\tthe\\tpreparation\\tpipeline\\tto\\tselect\\tonly\\tthe\\tmost\\timportant\\tattributes.\\n4\\n.\\t\\nTry\\tcreating\\ta\\tsingle\\tpipeline\\tthat\\tdoes\\tthe\\tfull\\tdata\\tpreparation\\tplus\\tthe\\tfinal\\tprediction.\\n5\\n.\\t\\nAutomatically\\texplore\\tsome\\tpreparation\\toptions\\tusing\\t\\nGridSearchCV\\n.\\nSolutions\\t\\nto\\tthese\\texercises\\tare\\tavailable\\tin\\tthe\\tonline\\tJupyter\\tnotebooks\\tat\\nhttps://github.com/ageron/handson-ml\\n.', '.\\nThe\\texample\\tproject\\tis\\tcompletely\\tfictitious;\\tthe\\tgoal\\tis\\tjust\\tto\\tillustrate\\tthe\\tmain\\tsteps\\tof\\ta\\tMachine\\tLearning\\tproject,\\tnot\\tto\\tlearn\\tanything\\nabout\\tthe\\treal\\testate\\tbusiness.\\nThe\\toriginal\\tdataset\\tappeared\\tin\\tR.\\tKelley\\tPace\\tand\\tRonald\\tBarry,\\t“Sparse\\tSpatial\\tAutoregressions,”\\t\\nStatistics\\t&\\tProbability\\tLetters\\n\\t33,\\nno.\\t3\\t(1997):\\t291–297.\\nA\\tpiece\\tof\\tinformation\\tfed\\tto\\ta\\tMachine\\tLearning\\tsystem\\tis\\toften\\tcalled\\ta\\t\\nsignal\\n\\tin\\treference\\tto\\tShannon’s\\tinformation\\ttheory:\\tyou\\twant\\ta\\nhigh\\tsignal/noise\\tratio.\\nRecall\\tthat\\tthe\\ttranspose\\toperator\\tflips\\ta\\tcolumn\\tvector\\tinto\\ta\\trow\\tvector\\t(and\\tvice\\tversa).\\nThe\\tlatest\\tversion\\tof\\tPython\\t3\\tis\\trecommended.\\tPython\\t2.7+\\tshould\\twork\\tfine\\ttoo,\\tbut\\tit\\tis\\tdeprecated.\\tIf\\tyou\\tuse\\tPython\\t2,\\tyou\\tmust\\tadd\\nfrom\\t__future__\\timport\\tdivision,\\tprint_function,\\tunicode_literals\\n\\tat\\tthe\\tbeginning\\tof\\tyour\\tcode.\\nWe\\twill\\tshow\\tthe\\tinstallation\\tsteps\\tusing\\tpip\\tin\\ta\\tbash\\tshell\\ton\\ta\\tLinux\\tor\\tmacOS\\tsystem.\\tYou\\tmay\\tneed\\tto\\tadapt\\tthese\\tcommands\\tto\\tyour', 'own\\tsystem.\\tOn\\tWindows,\\twe\\trecommend\\tinstalling\\tAnaconda\\tinstead.\\nYou\\tmay\\tneed\\tto\\thave\\tadministrator\\trights\\tto\\trun\\tthis\\tcommand;\\tif\\tso,\\ttry\\tprefixing\\tit\\twith\\t\\nsudo\\n.\\nNote\\tthat\\tJupyter\\tcan\\thandle\\tmultiple\\tversions\\tof\\tPython,\\tand\\teven\\tmany\\tother\\tlanguages\\tsuch\\tas\\tR\\tor\\tOctave.\\nYou\\tmight\\talso\\tneed\\tto\\tcheck\\tlegal\\tconstraints,\\tsuch\\tas\\tprivate\\tfields\\tthat\\tshould\\tnever\\tbe\\tcopied\\tto\\tunsafe\\tdatastores.\\nIn\\ta\\treal\\tproject\\tyou\\twould\\tsave\\tthis\\tcode\\tin\\ta\\tPython\\tfile,\\tbut\\tfor\\tnow\\tyou\\tcan\\tjust\\twrite\\tit\\tin\\tyour\\tJupyter\\tnotebook.\\nThe\\tstandard\\tdeviation\\tis\\tgenerally\\tdenoted\\tσ\\t(the\\tGreek\\tletter\\tsigma),\\tand\\tit\\tis\\tthe\\tsquare\\troot\\tof\\tthe\\t\\nvariance\\n,\\twhich\\tis\\tthe\\taverage\\tof\\nthe\\tsquared\\tdeviation\\tfrom\\tthe\\tmean.\\tWhen\\ta\\tfeature\\thas\\ta\\tbell-shaped\\t\\nnormal\\tdistribution\\n\\t(also\\tcalled\\ta\\t\\nGaussian\\tdistribution\\n),\\twhich\\nis\\tvery\\tcommon,\\tthe\\t“68-95-99.7”\\trule\\tapplies:\\tabout\\t68%\\tof\\tthe\\tvalues\\tfall\\twithin\\t1σ\\tof\\tthe\\tmean,\\t95%\\twithin\\t2σ,\\tand\\t99.7%\\twithin\\t3σ.', 'You\\twill\\toften\\tsee\\tpeople\\tset\\tthe\\trandom\\tseed\\tto\\t42.\\tThis\\tnumber\\thas\\tno\\tspecial\\tproperty,\\tother\\tthan\\tto\\tbe\\tThe\\tAnswer\\tto\\tthe\\tUltimate\\nQuestion\\tof\\tLife,\\tthe\\tUniverse,\\tand\\tEverything.\\nThe\\tlocation\\tinformation\\tis\\tactually\\tquite\\tcoarse,\\tand\\tas\\ta\\tresult\\tmany\\tdistricts\\twill\\thave\\tthe\\texact\\tsame\\tID,\\tso\\tthey\\twill\\tend\\tup\\tin\\tthe\\tsame\\nset\\t(test\\tor\\ttrain).\\tThis\\tintroduces\\tsome\\tunfortunate\\tsampling\\tbias.\\nIf\\tyou\\tare\\treading\\tthis\\tin\\tgrayscale,\\tgrab\\ta\\tred\\tpen\\tand\\tscribble\\tover\\tmost\\tof\\tthe\\tcoastline\\tfrom\\tthe\\tBay\\tArea\\tdown\\tto\\tSan\\tDiego\\t(as\\tyou\\nmight\\texpect).\\tYou\\tcan\\tadd\\ta\\tpatch\\tof\\tyellow\\taround\\tSacramento\\tas\\twell.\\nFor\\tmore\\tdetails\\ton\\tthe\\tdesign\\tprinciples,\\tsee\\t“API\\tdesign\\tfor\\tmachine\\tlearning\\tsoftware:\\texperiences\\tfrom\\tthe\\tscikit-learn\\tproject,”\\tL.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15', 'Buitinck,\\tG.\\tLouppe,\\tM.\\tBlondel,\\tF.\\tPedregosa,\\tA.\\tMüller,\\tet\\tal.\\t(2013).\\nSome\\tpredictors\\talso\\tprovide\\tmethods\\tto\\tmeasure\\tthe\\tconfidence\\tof\\ttheir\\tpredictions.\\nNumPy’s\\t\\nreshape()\\n\\tfunction\\tallows\\tone\\tdimension\\tto\\tbe\\t–1,\\twhich\\tmeans\\t“unspecified”:\\tthe\\tvalue\\tis\\tinferred\\tfrom\\tthe\\tlength\\tof\\tthe\\tarray\\nand\\tthe\\tremaining\\tdimensions.\\nSee\\tSciPy’s\\tdocumentation\\tfor\\tmore\\tdetails.\\nBut\\tcheck\\tout\\tPull\\tRequest\\t#3886,\\twhich\\tmay\\tintroduce\\ta\\t\\nColumnTransformer\\n\\tclass\\tmaking\\tattribute-specific\\ttransformations\\teasy.\\tYou\\ncould\\talso\\trun\\t\\npip3\\tinstall\\tsklearn-pandas\\n\\tto\\tget\\ta\\t\\nDataFrameMapper\\n\\tclass\\twith\\ta\\tsimilar\\tobjective.\\n16\\n17\\n18\\n19', 'Chapter\\t3.\\t\\nClassification\\nIn\\t\\nChapter\\t1\\n\\twe\\tmentioned\\tthat\\tthe\\tmost\\tcommon\\tsupervised\\tlearning\\ttasks\\tare\\tregression\\t(predicting\\nvalues)\\tand\\tclassification\\t(predicting\\tclasses).\\tIn\\t\\nChapter\\t2\\n\\twe\\texplored\\ta\\tregression\\ttask,\\tpredicting\\nhousing\\tvalues,\\tusing\\tvarious\\talgorithms\\tsuch\\tas\\tLinear\\tRegression,\\tDecision\\tTrees,\\tand\\tRandom\\tForests\\n(which\\twill\\tbe\\texplained\\tin\\tfurther\\tdetail\\tin\\tlater\\tchapters).\\tNow\\twe\\twill\\tturn\\tour\\tattention\\tto\\nclassification\\tsystems.', \"MNIST\\nIn\\tthis\\t\\nchapter,\\twe\\twill\\tbe\\tusing\\tthe\\tMNIST\\tdataset,\\twhich\\tis\\ta\\tset\\tof\\t70,000\\tsmall\\timages\\tof\\tdigits\\nhandwritten\\tby\\thigh\\tschool\\tstudents\\tand\\temployees\\tof\\tthe\\tUS\\tCensus\\tBureau.\\tEach\\timage\\tis\\tlabeled\\twith\\nthe\\tdigit\\tit\\trepresents.\\tThis\\tset\\thas\\tbeen\\tstudied\\tso\\tmuch\\tthat\\tit\\tis\\toften\\tcalled\\tthe\\t“Hello\\tWorld”\\tof\\nMachine\\tLearning:\\twhenever\\tpeople\\tcome\\tup\\twith\\ta\\tnew\\tclassification\\talgorithm,\\tthey\\tare\\tcurious\\tto\\tsee\\nhow\\tit\\twill\\tperform\\ton\\tMNIST.\\tWhenever\\tsomeone\\tlearns\\tMachine\\tLearning,\\tsooner\\tor\\tlater\\tthey\\ttackle\\nMNIST.\\nScikit-Learn\\tprovides\\tmany\\thelper\\tfunctions\\tto\\tdownload\\tpopular\\tdatasets.\\tMNIST\\tis\\tone\\tof\\tthem.\\tThe\\nfollowing\\tcode\\tfetches\\tthe\\tMNIST\\tdataset:\\n1\\n>>>\\t\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nfetch_mldata\\n>>>\\t\\nmnist\\n\\t\\n=\\n\\t\\nfetch_mldata\\n(\\n'MNIST\\toriginal'\\n)\\n>>>\\t\\nmnist\\n{'COL_NAMES':\\t['label',\\t'data'],\\n\\t'DESCR':\\t'mldata.org\\tdataset:\\tmnist-original',\\n\\t'data':\\tarray([[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t\\t...,\", '[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0],\\n\\t\\t\\t\\t\\t\\t\\t\\t[0,\\t0,\\t0,\\t...,\\t0,\\t0,\\t0]],\\tdtype=uint8),\\n\\t\\'target\\':\\tarray([\\t0.,\\t\\t0.,\\t\\t0.,\\t...,\\t\\t9.,\\t\\t9.,\\t\\t9.])}\\nDatasets\\tloaded\\tby\\tScikit-Learn\\tgenerally\\thave\\ta\\tsimilar\\tdictionary\\tstructure\\tincluding:\\nA\\t\\nDESCR\\n\\tkey\\tdescribing\\tthe\\tdataset\\nA\\t\\ndata\\n\\tkey\\tcontaining\\tan\\tarray\\twith\\tone\\trow\\tper\\tinstance\\tand\\tone\\tcolumn\\tper\\tfeature\\nA\\t\\ntarget\\n\\tkey\\tcontaining\\tan\\tarray\\twith\\tthe\\tlabels\\nLet’s\\tlook\\tat\\tthese\\tarrays:\\n>>>\\t\\nX\\n,\\n\\t\\ny\\n\\t\\n=\\n\\t\\nmnist\\n[\\n\"data\"\\n],\\n\\t\\nmnist\\n[\\n\"target\"\\n]\\n>>>\\t\\nX\\n.\\nshape\\n(70000,\\t784)\\n>>>\\t\\ny\\n.\\nshape\\n(70000,)\\nThere\\tare\\t70,000\\timages,\\tand\\teach\\timage\\thas\\t784\\tfeatures.\\tThis\\tis\\tbecause\\teach\\timage\\tis\\t28×28\\tpixels,\\nand\\teach\\tfeature\\tsimply\\trepresents\\tone\\tpixel’s\\tintensity,\\tfrom\\t0\\t(white)\\tto\\t255\\t(black).\\tLet’s\\ttake\\ta\\tpeek\\nat\\tone\\tdigit\\tfrom\\tthe\\tdataset.\\tAll\\tyou\\tneed\\tto\\tdo\\tis\\tgrab\\tan\\tinstance’s\\tfeature\\tvector,\\treshape\\tit\\tto\\ta\\t28×28\\narray,\\tand\\tdisplay\\tit\\tusing\\tMatplotlib’s\\t\\nimshow()\\n\\tfunction:\\n%\\nmatplotlib\\n\\t\\ninline\\nimport\\n\\t\\nmatplotlib\\nimport', 'import\\n\\t\\nmatplotlib.pyplot\\n\\t\\nas\\n\\t\\nplt\\nsome_digit\\n\\t\\n=\\n\\t\\nX\\n[\\n36000\\n]\\nsome_digit_image\\n\\t\\n=\\n\\t\\nsome_digit\\n.\\nreshape\\n(\\n28\\n,\\n\\t\\n28\\n)\\nplt\\n.\\nimshow\\n(\\nsome_digit_image\\n,\\n\\t\\ncmap\\n\\t\\n=\\n\\t\\nmatplotlib\\n.\\ncm\\n.\\nbinary\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ninterpolation\\n=\\n\"nearest\"\\n)', 'plt\\n.\\naxis\\n(\\n\"off\"\\n)\\nplt\\n.\\nshow\\n()\\nThis\\tlooks\\tlike\\ta\\t5,\\tand\\tindeed\\tthat’s\\twhat\\tthe\\tlabel\\ttells\\tus:\\n>>>\\t\\ny\\n[\\n36000\\n]\\n5.0\\nFigure\\t3-1\\n\\tshows\\ta\\tfew\\tmore\\timages\\tfrom\\tthe\\tMNIST\\tdataset\\tto\\tgive\\tyou\\ta\\tfeel\\tfor\\tthe\\tcomplexity\\tof\\tthe\\nclassification\\ttask.', 'Figure\\t3-1.\\t\\nA\\tfew\\tdigits\\tfrom\\tthe\\tMNIST\\tdataset\\nBut\\twait!\\tYou\\tshould\\talways\\tcreate\\ta\\t\\ntest\\tset\\tand\\tset\\tit\\taside\\tbefore\\tinspecting\\tthe\\tdata\\tclosely.\\tThe\\nMNIST\\tdataset\\tis\\tactually\\talready\\tsplit\\tinto\\ta\\ttraining\\tset\\t(the\\tfirst\\t60,000\\timages)\\tand\\ta\\ttest\\tset\\t(the\\tlast\\n10,000\\timages):\\nX_train\\n,\\n\\t\\nX_test\\n,\\n\\t\\ny_train\\n,\\n\\t\\ny_test\\n\\t\\n=\\n\\t\\nX\\n[:\\n60000\\n],\\n\\t\\nX\\n[\\n60000\\n:],\\n\\t\\ny\\n[:\\n60000\\n],\\n\\t\\ny\\n[\\n60000\\n:]\\nLet’s\\talso\\tshuffle\\tthe\\t\\ntraining\\tset;\\tthis\\twill\\tguarantee\\tthat\\tall\\tcross-validation\\t\\nfolds\\twill\\tbe\\tsimilar\\t(you\\ndon’t\\twant\\tone\\tfold\\tto\\tbe\\tmissing\\tsome\\tdigits).\\tMoreover,\\tsome\\tlearning\\talgorithms\\tare\\tsensitive\\tto\\tthe\\norder\\tof\\tthe\\ttraining\\tinstances,\\tand\\tthey\\tperform\\tpoorly\\tif\\tthey\\tget\\tmany\\tsimilar\\tinstances\\tin\\ta\\trow.\\nShuffling\\tthe\\tdataset\\tensures\\tthat\\tthis\\twon’t\\t\\nhappen:\\n2\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nshuffle_index\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\npermutation\\n(\\n60000\\n)\\nX_train\\n,\\n\\t\\ny_train\\n\\t\\n=\\n\\t\\nX_train\\n[\\nshuffle_index\\n],\\n\\t\\ny_train\\n[\\nshuffle_index\\n]', 'Training\\ta\\tBinary\\tClassifier\\nLet’s\\t\\nsimplify\\tthe\\tproblem\\tfor\\tnow\\tand\\tonly\\ttry\\tto\\tidentify\\tone\\tdigit\\t—\\tfor\\texample,\\tthe\\tnumber\\t5.\\tThis\\n“5-detector”\\twill\\tbe\\tan\\texample\\tof\\ta\\t\\nbinary\\tclassifier\\n,\\tcapable\\tof\\tdistinguishing\\tbetween\\tjust\\ttwo\\nclasses,\\t5\\tand\\tnot-5.\\tLet’s\\tcreate\\tthe\\ttarget\\tvectors\\tfor\\tthis\\tclassification\\ttask:\\ny_train_5\\n\\t\\n=\\n\\t\\n(\\ny_train\\n\\t\\n==\\n\\t\\n5\\n)\\n\\t\\t\\n#\\tTrue\\tfor\\tall\\t5s,\\tFalse\\tfor\\tall\\tother\\tdigits.\\ny_test_5\\n\\t\\n=\\n\\t\\n(\\ny_test\\n\\t\\n==\\n\\t\\n5\\n)\\nOkay,\\tnow\\tlet’s\\tpick\\ta\\tclassifier\\tand\\ttrain\\tit.\\tA\\tgood\\tplace\\tto\\tstart\\tis\\twith\\t\\na\\t\\nStochastic\\tGradient\\tDescent\\n(SGD)\\tclassifier,\\tusing\\t\\nScikit-Learn’s\\t\\nSGDClassifier\\n\\tclass.\\tThis\\tclassifier\\thas\\tthe\\tadvantage\\tof\\tbeing\\ncapable\\tof\\thandling\\tvery\\tlarge\\tdatasets\\tefficiently.\\tThis\\tis\\tin\\tpart\\tbecause\\tSGD\\tdeals\\twith\\ttraining\\ninstances\\tindependently,\\tone\\tat\\ta\\ttime\\t(which\\talso\\tmakes\\tSGD\\twell\\tsuited\\tfor\\t\\nonline\\tlearning\\n),\\tas\\twe\\nwill\\tsee\\tlater.\\tLet’s\\tcreate\\tan\\t\\nSGDClassifier\\n\\t\\nand\\ttrain\\tit\\ton\\tthe\\twhole\\ttraining\\tset:\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport', 'import\\n\\t\\nSGDClassifier\\nsgd_clf\\n\\t\\n=\\n\\t\\nSGDClassifier\\n(\\nrandom_state\\n=\\n42\\n)\\nsgd_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train_5\\n)\\nTIP\\nThe\\t\\nSGDClassifier\\n\\trelies\\ton\\trandomness\\tduring\\ttraining\\t(hence\\tthe\\tname\\t“stochastic”).\\tIf\\tyou\\twant\\treproducible\\tresults,\\tyou\\nshould\\tset\\tthe\\t\\nrandom_state\\n\\tparameter.\\nNow\\tyou\\tcan\\tuse\\tit\\tto\\tdetect\\timages\\tof\\tthe\\tnumber\\t5:\\n>>>\\t\\nsgd_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([\\tTrue],\\tdtype=bool)\\nThe\\tclassifier\\tguesses\\tthat\\tthis\\timage\\trepresents\\ta\\t5\\t(\\nTrue\\n).\\tLooks\\tlike\\tit\\tguessed\\tright\\tin\\tthis\\tparticular\\ncase!\\tNow,\\tlet’s\\tevaluate\\tthis\\tmodel’s\\tperformance.', 'Performance\\tMeasures\\nEvaluating\\t\\na\\tclassifier\\tis\\toften\\tsignificantly\\ttrickier\\tthan\\tevaluating\\ta\\tregressor,\\tso\\twe\\twill\\tspend\\ta\\tlarge\\npart\\tof\\tthis\\tchapter\\ton\\tthis\\ttopic.\\tThere\\tare\\tmany\\tperformance\\tmeasures\\tavailable,\\tso\\tgrab\\tanother\\tcoffee\\nand\\tget\\tready\\tto\\tlearn\\tmany\\tnew\\tconcepts\\tand\\tacronyms!', 'Measuring\\tAccuracy\\tUsing\\tCross-Validation\\nA\\t\\ngood\\tway\\tto\\tevaluate\\ta\\tmodel\\tis\\tto\\tuse\\tcross-validation,\\tjust\\tas\\tyou\\tdid\\tin\\t\\nChapter\\t2\\n.\\nIMPLEMENTING\\tCROSS-VALIDATION\\nOccasionally\\tyou\\twill\\tneed\\tmore\\tcontrol\\tover\\tthe\\tcross-validation\\tprocess\\tthan\\twhat\\tScikit-Learn\\tprovides\\toff-the-shelf.\\tIn\\tthese\\tcases,\\nyou\\tcan\\timplement\\tcross-validation\\tyourself;\\tit\\tis\\tactually\\tfairly\\tstraightforward.\\tThe\\tfollowing\\tcode\\tdoes\\troughly\\tthe\\tsame\\tthing\\tas\\nScikit-Learn’s\\t\\ncross_val_score()\\n\\tfunction,\\tand\\tprints\\tthe\\t\\nsame\\tresult:\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\nStratifiedKFold\\nfrom\\n\\t\\nsklearn.base\\n\\t\\nimport\\n\\t\\nclone\\nskfolds\\n\\t\\n=\\n\\t\\nStratifiedKFold\\n(\\nn_splits\\n=\\n3\\n,\\n\\t\\nrandom_state\\n=\\n42\\n)\\nfor\\n\\t\\ntrain_index\\n,\\n\\t\\ntest_index\\n\\t\\nin\\n\\t\\nskfolds\\n.\\nsplit\\n(\\nX_train\\n,\\n\\t\\ny_train_5\\n):\\n\\t\\t\\t\\t\\nclone_clf\\n\\t\\n=\\n\\t\\nclone\\n(\\nsgd_clf\\n)\\n\\t\\t\\t\\t\\nX_train_folds\\n\\t\\n=\\n\\t\\nX_train\\n[\\ntrain_index\\n]\\n\\t\\t\\t\\t\\ny_train_folds\\n\\t\\n=\\n\\t\\n(\\ny_train_5\\n[\\ntrain_index\\n])\\n\\t\\t\\t\\t\\nX_test_fold\\n\\t\\n=\\n\\t\\nX_train\\n[\\ntest_index\\n]\\n\\t\\t\\t\\t\\ny_test_fold\\n\\t\\n=\\n\\t\\n(\\ny_train_5\\n[\\ntest_index\\n])', '])\\n\\t\\t\\t\\t\\nclone_clf\\n.\\nfit\\n(\\nX_train_folds\\n,\\n\\t\\ny_train_folds\\n)\\n\\t\\t\\t\\t\\ny_pred\\n\\t\\n=\\n\\t\\nclone_clf\\n.\\npredict\\n(\\nX_test_fold\\n)\\n\\t\\t\\t\\t\\nn_correct\\n\\t\\n=\\n\\t\\nsum\\n(\\ny_pred\\n\\t\\n==\\n\\t\\ny_test_fold\\n)\\n\\t\\t\\t\\t\\nprint\\n(\\nn_correct\\n\\t\\n/\\n\\t\\nlen\\n(\\ny_pred\\n))\\n\\t\\t\\n#\\tprints\\t0.9502,\\t0.96565\\tand\\t0.96495\\nThe\\t\\nStratifiedKFold\\n\\tclass\\tperforms\\t\\nstratified\\tsampling\\t(as\\texplained\\tin\\t\\nChapter\\t2\\n)\\tto\\tproduce\\tfolds\\tthat\\tcontain\\ta\\trepresentative\\tratio\\nof\\teach\\tclass.\\tAt\\teach\\titeration\\tthe\\tcode\\tcreates\\ta\\tclone\\tof\\tthe\\tclassifier,\\ttrains\\tthat\\tclone\\ton\\tthe\\ttraining\\tfolds,\\tand\\tmakes\\tpredictions\\ton\\nthe\\ttest\\tfold.\\tThen\\tit\\tcounts\\tthe\\tnumber\\tof\\tcorrect\\tpredictions\\tand\\toutputs\\tthe\\tratio\\tof\\tcorrect\\tpredictions.\\nLet’s\\tuse\\tthe\\t\\ncross_val_score()\\n\\tfunction\\tto\\tevaluate\\tyour\\t\\nSGDClassifier\\n\\tmodel\\tusing\\t\\nK-fold\\tcross-\\nvalidation,\\twith\\tthree\\tfolds.\\tRemember\\tthat\\tK-fold\\tcross-validation\\tmeans\\tsplitting\\tthe\\ttraining\\tset\\tinto\\nK-folds\\t(in\\tthis\\tcase,\\tthree),\\tthen\\tmaking\\tpredictions\\tand\\tevaluating\\tthem\\ton\\teach\\tfold\\tusing\\ta\\tmodel\\ntrained\\ton\\tthe\\tremaining\\tfolds\\t(see', 'Chapter\\t2\\n):\\n>>>\\t\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\ncross_val_score\\n>>>\\t\\ncross_val_score\\n(\\nsgd_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train_5\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\nscoring\\n=\\n\"accuracy\"\\n)\\narray([\\t0.9502\\t,\\t\\t0.96565,\\t\\t0.96495])\\nWow!\\tAbove\\t95%\\t\\naccuracy\\n\\t(ratio\\tof\\tcorrect\\tpredictions)\\ton\\tall\\tcross-validation\\tfolds?\\t\\nThis\\tlooks\\namazing,\\tdoesn’t\\tit?\\tWell,\\tbefore\\tyou\\tget\\ttoo\\texcited,\\tlet’s\\tlook\\tat\\ta\\tvery\\tdumb\\tclassifier\\tthat\\tjust\\nclassifies\\n\\tevery\\tsingle\\timage\\tin\\tthe\\t“not-5”\\tclass:\\nfrom\\n\\t\\nsklearn.base\\n\\t\\nimport\\n\\t\\nBaseEstimator\\nclass\\n\\t\\nNever5Classifier\\n(\\nBaseEstimator\\n):\\n\\t\\t\\t\\t\\ndef\\n\\t\\nfit\\n(\\nself\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npass\\n\\t\\t\\t\\t\\ndef\\n\\t\\npredict\\n(\\nself\\n,\\n\\t\\nX\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nnp\\n.\\nzeros\\n((\\nlen\\n(\\nX\\n),\\n\\t\\n1\\n),\\n\\t\\ndtype\\n=\\nbool\\n)\\nCan\\tyou\\tguess\\tthis\\tmodel’s\\taccuracy?\\tLet’s\\tfind\\tout:\\n>>>\\t\\nnever_5_clf\\n\\t\\n=\\n\\t\\nNever5Classifier\\n()\\n>>>\\t\\ncross_val_score\\n(\\nnever_5_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train_5\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\nscoring\\n=\\n\"accuracy\"\\n)\\narray([\\t0.909\\t\\t,\\t\\t0.90715,\\t\\t0.9128\\t])', 'That’s\\tright,\\tit\\thas\\tover\\t90%\\taccuracy!\\tThis\\tis\\tsimply\\tbecause\\tonly\\tabout\\t10%\\tof\\tthe\\timages\\tare\\t5s,\\tso\\tif\\nyou\\talways\\tguess\\tthat\\tan\\timage\\tis\\t\\nnot\\n\\ta\\t5,\\tyou\\twill\\tbe\\tright\\tabout\\t90%\\tof\\tthe\\ttime.\\tBeats\\tNostradamus.\\nThis\\tdemonstrates\\twhy\\taccuracy\\tis\\tgenerally\\tnot\\tthe\\tpreferred\\tperformance\\tmeasure\\tfor\\tclassifiers,\\nespecially\\twhen\\tyou\\tare\\t\\ndealing\\t\\nwith\\t\\nskewed\\tdatasets\\n\\t(i.e.,\\twhen\\tsome\\tclasses\\tare\\tmuch\\tmore\\tfrequent\\nthan\\tothers).', 'Confusion\\tMatrix\\nA\\t\\nmuch\\tbetter\\tway\\tto\\tevaluate\\tthe\\tperformance\\tof\\ta\\tclassifier\\tis\\tto\\tlook\\tat\\tthe\\t\\nconfusion\\tmatrix\\n.\\tThe\\ngeneral\\tidea\\tis\\tto\\tcount\\tthe\\tnumber\\tof\\ttimes\\tinstances\\tof\\tclass\\tA\\tare\\tclassified\\tas\\tclass\\tB.\\tFor\\texample,\\tto\\nknow\\tthe\\tnumber\\tof\\ttimes\\tthe\\tclassifier\\tconfused\\timages\\tof\\t5s\\twith\\t3s,\\tyou\\twould\\tlook\\tin\\tthe\\t5\\nth\\n\\trow\\tand\\n3\\nrd\\n\\tcolumn\\tof\\tthe\\tconfusion\\tmatrix.\\nTo\\tcompute\\tthe\\tconfusion\\tmatrix,\\tyou\\tfirst\\tneed\\tto\\thave\\ta\\tset\\tof\\tpredictions,\\t\\nso\\tthey\\tcan\\tbe\\tcompared\\tto\\nthe\\tactual\\ttargets.\\tYou\\tcould\\tmake\\tpredictions\\ton\\tthe\\ttest\\tset,\\tbut\\tlet’s\\tkeep\\tit\\tuntouched\\tfor\\tnow\\n(remember\\tthat\\tyou\\twant\\tto\\tuse\\tthe\\ttest\\tset\\tonly\\tat\\tthe\\tvery\\tend\\tof\\tyour\\tproject,\\tonce\\tyou\\thave\\ta\\tclassifier\\nthat\\tyou\\tare\\tready\\tto\\tlaunch).\\tInstead,\\tyou\\tcan\\tuse\\tthe\\t\\ncross_val_predict()\\n\\t\\nfunction:\\nfrom\\tsklearn.model_selection\\timport\\tcross_val_predict\\ny_train_pred\\t=\\tcross_val_predict(sgd_clf,\\tX_train,\\ty_train_5,\\tcv=3)\\nJust\\tlike\\tthe\\t\\ncross_val_score()\\n\\t\\nfunction,\\t\\ncross_val_predict()', 'performs\\tK-fold\\tcross-validation,\\nbut\\tinstead\\tof\\treturning\\tthe\\tevaluation\\tscores,\\tit\\treturns\\tthe\\tpredictions\\tmade\\ton\\teach\\ttest\\tfold.\\tThis\\tmeans\\nthat\\tyou\\tget\\ta\\tclean\\tprediction\\tfor\\teach\\tinstance\\tin\\tthe\\ttraining\\tset\\t(“clean”\\tmeaning\\tthat\\tthe\\tprediction\\tis\\nmade\\tby\\ta\\tmodel\\tthat\\tnever\\tsaw\\tthe\\tdata\\tduring\\ttraining).\\nNow\\tyou\\tare\\tready\\tto\\tget\\tthe\\t\\nconfusion\\tmatrix\\tusing\\tthe\\t\\nconfusion_matrix()\\n\\tfunction.\\tJust\\tpass\\tit\\tthe\\ntarget\\tclasses\\t(\\ny_train_5\\n)\\tand\\tthe\\tpredicted\\tclasses\\t(\\ny_train_pred\\n):\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nconfusion_matrix\\n>>>\\t\\nconfusion_matrix\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred\\n)\\narray([[53272,\\t\\t1307],\\n\\t\\t\\t\\t\\t\\t\\t[\\t1077,\\t\\t4344]])\\nEach\\trow\\tin\\ta\\tconfusion\\tmatrix\\trepresents\\tan\\t\\nactual\\tclass\\n,\\twhile\\teach\\tcolumn\\trepresents\\ta\\t\\npredicted\\nclass\\n.\\t\\nThe\\tfirst\\trow\\tof\\tthis\\tmatrix\\tconsiders\\tnon-5\\timages\\t(the\\t\\nnegative\\tclass\\n):\\t53,272\\tof\\tthem\\twere\\ncorrectly\\tclassified\\tas\\tnon-5s\\t(they\\tare\\tcalled\\t\\ntrue\\tnegatives\\n),\\twhile\\tthe\\tremaining\\t1,307\\twere\\twrongly\\nclassified\\tas\\t5s\\t(\\nfalse\\tpositives', ').\\tThe\\tsecond\\trow\\tconsiders\\tthe\\timages\\tof\\t5s\\t(the\\t\\npositive\\tclass\\n):\\t1,077\\nwere\\twrongly\\tclassified\\tas\\tnon-5s\\t(\\nfalse\\tnegatives\\n),\\twhile\\tthe\\tremaining\\t4,344\\twere\\tcorrectly\\tclassified\\nas\\t5s\\t(\\ntrue\\tpositives\\n).\\tA\\tperfect\\tclassifier\\twould\\thave\\tonly\\ttrue\\tpositives\\tand\\ttrue\\tnegatives,\\tso\\tits\\nconfusion\\tmatrix\\twould\\thave\\tnonzero\\tvalues\\tonly\\ton\\tits\\tmain\\tdiagonal\\t(top\\tleft\\tto\\tbottom\\tright):\\n>>>\\t\\nconfusion_matrix\\n(\\ny_train_5\\n,\\n\\t\\ny_train_perfect_predictions\\n)\\narray([[54579,\\t\\t\\t\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t\\t\\t0,\\t5421]])\\nThe\\tconfusion\\tmatrix\\tgives\\tyou\\ta\\tlot\\tof\\tinformation,\\tbut\\tsometimes\\tyou\\tmay\\tprefer\\ta\\tmore\\tconcise\\tmetric.\\nAn\\tinteresting\\tone\\tto\\tlook\\tat\\tis\\tthe\\taccuracy\\tof\\tthe\\tpositive\\tpredictions;\\tthis\\tis\\tcalled\\t\\nthe\\t\\nprecision\\n\\tof\\tthe\\nclassifier\\t(\\nEquation\\t3-1\\n).\\nEquation\\t3-1.\\t\\nPrecision', 'TP\\tis\\tthe\\tnumber\\tof\\ttrue\\tpositives,\\tand\\tFP\\tis\\tthe\\tnumber\\tof\\tfalse\\tpositives.\\nA\\ttrivial\\tway\\tto\\thave\\tperfect\\tprecision\\tis\\tto\\tmake\\tone\\tsingle\\tpositive\\tprediction\\tand\\tensure\\tit\\tis\\tcorrect\\n(precision\\t=\\t1/1\\t=\\t100%).\\tThis\\twould\\tnot\\tbe\\tvery\\tuseful\\tsince\\tthe\\tclassifier\\twould\\tignore\\tall\\tbut\\tone\\npositive\\tinstance.\\tSo\\tprecision\\tis\\ttypically\\tused\\talong\\twith\\tanother\\tmetric\\tnamed\\t\\nrecall\\n,\\t\\nalso\\tcalled\\nsensitivity\\n\\tor\\t\\ntrue\\tpositive\\trate\\n\\t(\\nTPR\\n):\\tthis\\tis\\tthe\\tratio\\tof\\tpositive\\tinstances\\tthat\\tare\\tcorrectly\\tdetected\\tby\\nthe\\tclassifier\\t(\\nEquation\\t3-2\\n).\\nEquation\\t3-2.\\t\\nRecall\\nFN\\tis\\tof\\tcourse\\tthe\\tnumber\\tof\\tfalse\\tnegatives.\\nIf\\tyou\\tare\\tconfused\\tabout\\tthe\\tconfusion\\tmatrix,\\t\\nFigure\\t3-2\\n\\tmay\\t\\nhelp.\\nFigure\\t3-2.\\t\\nAn\\tillustrated\\tconfusion\\tmatrix', 'Precision\\tand\\tRecall\\nScikit-Learn\\t\\nprovides\\tseveral\\tfunctions\\tto\\tcompute\\tclassifier\\tmetrics,\\tincluding\\t\\nprecision\\tand\\trecall:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nprecision_score\\n,\\n\\t\\nrecall_score\\n>>>\\t\\nprecision_score\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred\\n)\\n\\t\\n#\\t==\\t4344\\t/\\t(4344\\t+\\t1307)\\n0.76871350203503808\\n>>>\\t\\nrecall_score\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred\\n)\\n\\t\\n#\\t==\\t4344\\t/\\t(4344\\t+\\t1077)\\n0.80132816823464303\\nNow\\tyour\\t5-detector\\tdoes\\tnot\\tlook\\tas\\tshiny\\tas\\tit\\tdid\\twhen\\tyou\\tlooked\\tat\\tits\\taccuracy.\\tWhen\\tit\\tclaims\\tan\\nimage\\trepresents\\ta\\t5,\\tit\\tis\\tcorrect\\tonly\\t77%\\tof\\tthe\\ttime.\\tMoreover,\\tit\\tonly\\tdetects\\t80%\\tof\\tthe\\t5s.\\nIt\\tis\\toften\\tconvenient\\tto\\tcombine\\tprecision\\tand\\trecall\\tinto\\ta\\tsingle\\tmetric\\tcalled\\tthe\\t\\nF\\n1\\n\\tscore\\n,\\tin\\nparticular\\tif\\tyou\\tneed\\ta\\tsimple\\tway\\tto\\tcompare\\ttwo\\tclassifiers.\\tThe\\t\\nF\\n1\\n\\tscore\\tis\\t\\nthe\\t\\nharmonic\\tmean\\n\\tof\\nprecision\\tand\\trecall\\t(\\nEquation\\t3-3\\n).\\tWhereas\\tthe\\tregular\\tmean\\ttreats\\tall\\tvalues\\tequally,\\tthe\\tharmonic', 'mean\\tgives\\tmuch\\tmore\\tweight\\tto\\tlow\\tvalues.\\tAs\\ta\\tresult,\\tthe\\tclassifier\\twill\\tonly\\tget\\ta\\thigh\\tF\\n1\\n\\tscore\\tif\\nboth\\trecall\\tand\\tprecision\\tare\\thigh.\\nEquation\\t3-3.\\t\\nF\\n1\\n\\tscore\\nTo\\tcompute\\t\\nthe\\tF\\n1\\n\\tscore,\\tsimply\\tcall\\tthe\\t\\nf1_score()\\n\\tfunction:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nf1_score\\n>>>\\t\\nf1_score\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred\\n)\\n0.78468208092485547\\nThe\\tF\\n1\\n\\tscore\\tfavors\\tclassifiers\\tthat\\thave\\tsimilar\\tprecision\\tand\\trecall.\\tThis\\tis\\tnot\\talways\\twhat\\tyou\\twant:\\nin\\tsome\\tcontexts\\tyou\\tmostly\\tcare\\tabout\\tprecision,\\tand\\tin\\tother\\tcontexts\\tyou\\treally\\tcare\\tabout\\trecall.\\tFor\\nexample,\\tif\\tyou\\ttrained\\ta\\tclassifier\\tto\\tdetect\\tvideos\\tthat\\tare\\tsafe\\tfor\\tkids,\\tyou\\twould\\tprobably\\tprefer\\ta\\nclassifier\\tthat\\trejects\\tmany\\tgood\\tvideos\\t(low\\trecall)\\tbut\\tkeeps\\tonly\\tsafe\\tones\\t(high\\tprecision),\\trather\\tthan\\na\\tclassifier\\tthat\\thas\\ta\\tmuch\\thigher\\trecall\\tbut\\tlets\\ta\\tfew\\treally\\tbad\\tvideos\\tshow\\tup\\tin\\tyour\\tproduct\\t(in\\tsuch\\ncases,\\tyou\\tmay\\teven\\twant\\tto\\tadd\\ta\\thuman\\tpipeline\\tto\\tcheck\\tthe\\tclassifier’s\\tvideo\\tselection).\\tOn\\tthe\\tother', 'hand,\\tsuppose\\tyou\\ttrain\\ta\\tclassifier\\tto\\tdetect\\tshoplifters\\ton\\tsurveillance\\timages:\\tit\\tis\\tprobably\\tfine\\tif\\tyour\\nclassifier\\thas\\tonly\\t30%\\tprecision\\tas\\tlong\\tas\\tit\\thas\\t99%\\trecall\\t(sure,\\tthe\\tsecurity\\tguards\\twill\\tget\\ta\\tfew\\nfalse\\talerts,\\tbut\\talmost\\tall\\tshoplifters\\twill\\tget\\tcaught).\\nUnfortunately,\\tyou\\tcan’t\\thave\\tit\\tboth\\tways:\\tincreasing\\tprecision\\treduces\\trecall,\\tand\\tvice\\tversa.\\tThis\\t\\nis\\ncalled\\tthe\\t\\nprecision/recall\\ttradeoff\\n.', 'Precision/Recall\\tTradeoff\\nTo\\t\\nunderstand\\tthis\\ttradeoff,\\tlet’s\\tlook\\tat\\thow\\tthe\\n\\t\\nSGDClassifier\\n\\tmakes\\tits\\tclassification\\tdecisions.\\tFor\\neach\\tinstance,\\tit\\tcomputes\\ta\\tscore\\tbased\\ton\\ta\\t\\ndecision\\tfunction\\n,\\t\\nand\\tif\\tthat\\tscore\\tis\\tgreater\\tthan\\ta\\nthreshold,\\tit\\tassigns\\tthe\\tinstance\\tto\\tthe\\tpositive\\tclass,\\tor\\telse\\tit\\tassigns\\tit\\tto\\tthe\\tnegative\\tclass.\\t\\nFigure\\t3-3\\nshows\\ta\\tfew\\tdigits\\tpositioned\\tfrom\\tthe\\tlowest\\tscore\\ton\\tthe\\tleft\\tto\\tthe\\thighest\\tscore\\ton\\tthe\\tright.\\tSuppose\\nthe\\t\\ndecision\\tthreshold\\n\\t\\nis\\tpositioned\\tat\\tthe\\tcentral\\tarrow\\t(between\\tthe\\ttwo\\t5s):\\tyou\\twill\\tfind\\t4\\ttrue\\npositives\\t(actual\\t5s)\\ton\\tthe\\tright\\tof\\tthat\\tthreshold,\\tand\\tone\\tfalse\\tpositive\\t(actually\\ta\\t6).\\tTherefore,\\twith\\nthat\\tthreshold,\\tthe\\tprecision\\tis\\t80%\\t(4\\tout\\tof\\t5).\\tBut\\tout\\tof\\t6\\tactual\\t5s,\\tthe\\tclassifier\\tonly\\tdetects\\t4,\\tso\\tthe\\nrecall\\tis\\t67%\\t(4\\tout\\tof\\t6).\\tNow\\tif\\tyou\\traise\\tthe\\tthreshold\\t(move\\tit\\tto\\tthe\\tarrow\\ton\\tthe\\tright),\\tthe\\tfalse\\npositive\\t(the\\t6)\\tbecomes\\ta\\ttrue\\tnegative,\\tthereby\\tincreasing\\tprecision\\t(up\\tto\\t100%\\tin\\tthis\\tcase),\\tbut\\tone', 'true\\tpositive\\tbecomes\\ta\\tfalse\\tnegative,\\tdecreasing\\trecall\\tdown\\tto\\t50%.\\tConversely,\\tlowering\\tthe\\nthreshold\\tincreases\\trecall\\tand\\treduces\\tprecision.\\nFigure\\t3-3.\\t\\nDecision\\tthreshold\\tand\\tprecision/recall\\ttradeoff\\nScikit-Learn\\tdoes\\tnot\\tlet\\tyou\\tset\\tthe\\tthreshold\\tdirectly,\\tbut\\tit\\tdoes\\tgive\\tyou\\taccess\\tto\\tthe\\tdecision\\tscores\\nthat\\tit\\tuses\\tto\\tmake\\tpredictions.\\tInstead\\tof\\tcalling\\tthe\\tclassifier’s\\t\\npredict()\\n\\tmethod,\\tyou\\tcan\\tcall\\tits\\ndecision_function()\\n\\tmethod,\\twhich\\treturns\\ta\\tscore\\tfor\\teach\\tinstance,\\tand\\tthen\\tmake\\tpredictions\\tbased\\non\\tthose\\tscores\\tusing\\tany\\tthreshold\\tyou\\twant:\\n>>>\\t\\ny_scores\\n\\t\\n=\\n\\t\\nsgd_clf\\n.\\ndecision_function\\n([\\nsome_digit\\n])\\n>>>\\t\\ny_scores\\narray([\\t161855.74572176])\\n>>>\\t\\nthreshold\\n\\t\\n=\\n\\t\\n0\\n>>>\\t\\ny_some_digit_pred\\n\\t\\n=\\n\\t\\n(\\ny_scores\\n\\t\\n>\\n\\t\\nthreshold\\n)\\narray([\\tTrue],\\tdtype=bool)\\nThe\\t\\nSGDClassifier\\n\\tuses\\ta\\tthreshold\\tequal\\tto\\t0,\\tso\\tthe\\tprevious\\tcode\\treturns\\tthe\\tsame\\tresult\\tas\\tthe\\npredict()\\n\\tmethod\\t(i.e.,\\t\\nTrue\\n).\\tLet’s\\traise\\tthe\\tthreshold:\\n>>>\\t\\nthreshold\\n\\t\\n=\\n\\t\\n200000\\n>>>\\t\\ny_some_digit_pred', '=\\n\\t\\n(\\ny_scores\\n\\t\\n>\\n\\t\\nthreshold\\n)\\n>>>\\t\\ny_some_digit_pred\\narray([False],\\tdtype=bool)\\nThis\\tconfirms\\tthat\\traising\\tthe\\tthreshold\\tdecreases\\trecall.\\tThe\\timage\\tactually\\trepresents\\ta\\t5,\\tand\\tthe\\nclassifier\\tdetects\\tit\\twhen\\tthe\\tthreshold\\tis\\t0,\\tbut\\tit\\tmisses\\tit\\twhen\\tthe\\tthreshold\\tis\\t\\nincreased\\tto\\t200,000.', 'So\\thow\\tcan\\tyou\\tdecide\\twhich\\tthreshold\\tto\\tuse?\\tFor\\tthis\\tyou\\twill\\tfirst\\tneed\\tto\\tget\\tthe\\tscores\\tof\\tall\\ninstances\\tin\\tthe\\ttraining\\tset\\tusing\\tthe\\t\\ncross_val_predict()\\n\\tfunction\\tagain,\\tbut\\tthis\\ttime\\tspecifying\\tthat\\nyou\\twant\\tit\\tto\\treturn\\tdecision\\tscores\\t\\ninstead\\tof\\tpredictions:\\ny_scores\\n\\t\\n=\\n\\t\\ncross_val_predict\\n(\\nsgd_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train_5\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmethod\\n=\\n\"decision_function\"\\n)\\nNow\\twith\\tthese\\tscores\\tyou\\tcan\\tcompute\\tprecision\\tand\\trecall\\tfor\\tall\\tpossible\\tthresholds\\tusing\\tthe\\nprecision_recall_curve()\\n\\t\\nfunction:\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nprecision_recall_curve\\nprecisions\\n,\\n\\t\\nrecalls\\n,\\n\\t\\nthresholds\\n\\t\\n=\\n\\t\\nprecision_recall_curve\\n(\\ny_train_5\\n,\\n\\t\\ny_scores\\n)\\nFinally,\\tyou\\tcan\\tplot\\tprecision\\tand\\trecall\\tas\\tfunctions\\tof\\tthe\\tthreshold\\tvalue\\tusing\\tMatplotlib\\t(\\nFigure\\t3-\\n4\\n):\\ndef\\n\\t\\nplot_precision_recall_vs_threshold\\n(\\nprecisions\\n,\\n\\t\\nrecalls\\n,\\n\\t\\nthresholds\\n):\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n(\\nthresholds\\n,\\n\\t\\nprecisions\\n[:\\n-\\n1\\n],\\n\\t\\n\"b--\"\\n,\\n\\t\\nlabel\\n=\\n\"Precision\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nplot', '.\\nplot\\n(\\nthresholds\\n,\\n\\t\\nrecalls\\n[:\\n-\\n1\\n],\\n\\t\\n\"g-\"\\n,\\n\\t\\nlabel\\n=\\n\"Recall\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nxlabel\\n(\\n\"Threshold\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nlegend\\n(\\nloc\\n=\\n\"upper\\tleft\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nylim\\n([\\n0\\n,\\n\\t\\n1\\n])\\nplot_precision_recall_vs_threshold\\n(\\nprecisions\\n,\\n\\t\\nrecalls\\n,\\n\\t\\nthresholds\\n)\\nplt\\n.\\nshow\\n()\\nFigure\\t3-4.\\t\\nPrecision\\tand\\trecall\\tversus\\tthe\\tdecision\\tthreshold\\nNOTE\\nYou\\tmay\\twonder\\twhy\\tthe\\tprecision\\tcurve\\tis\\tbumpier\\tthan\\tthe\\trecall\\tcurve\\tin\\t\\nFigure\\t3-4\\n.\\tThe\\treason\\tis\\tthat\\tprecision\\tmay\\nsometimes\\tgo\\tdown\\twhen\\tyou\\traise\\tthe\\tthreshold\\t(although\\tin\\tgeneral\\tit\\twill\\tgo\\tup).\\tTo\\tunderstand\\twhy,\\tlook\\tback\\tat\\t\\nFigure\\t3-3\\nand\\tnotice\\twhat\\thappens\\twhen\\tyou\\tstart\\tfrom\\tthe\\tcentral\\tthreshold\\tand\\tmove\\tit\\tjust\\tone\\tdigit\\tto\\tthe\\tright:\\tprecision\\tgoes\\tfrom\\t4/5\\n(80%)\\tdown\\tto\\t3/4\\t(75%).\\tOn\\tthe\\tother\\thand,\\trecall\\tcan\\tonly\\tgo\\tdown\\twhen\\tthe\\tthreshold\\tis\\tincreased,\\twhich\\texplains\\twhy\\tits\\ncurve\\tlooks\\tsmooth.', 'Now\\tyou\\tcan\\tsimply\\tselect\\tthe\\tthreshold\\tvalue\\tthat\\tgives\\tyou\\tthe\\tbest\\tprecision/recall\\ttradeoff\\tfor\\tyour\\ntask.\\tAnother\\tway\\tto\\tselect\\ta\\tgood\\tprecision/recall\\ttradeoff\\tis\\tto\\tplot\\tprecision\\tdirectly\\tagainst\\trecall,\\tas\\nshown\\tin\\t\\nFigure\\t3-5\\n.\\nFigure\\t3-5.\\t\\nPrecision\\tversus\\trecall\\nYou\\tcan\\tsee\\tthat\\tprecision\\treally\\tstarts\\tto\\tfall\\tsharply\\taround\\t80%\\trecall.\\tYou\\twill\\tprobably\\twant\\tto\\nselect\\ta\\tprecision/recall\\ttradeoff\\tjust\\tbefore\\tthat\\tdrop\\t—\\tfor\\texample,\\tat\\taround\\t60%\\trecall.\\tBut\\tof\\ncourse\\tthe\\tchoice\\tdepends\\ton\\tyour\\tproject.\\nSo\\tlet’s\\tsuppose\\tyou\\tdecide\\tto\\taim\\tfor\\t90%\\tprecision.\\tYou\\tlook\\tup\\tthe\\tfirst\\tplot\\t(zooming\\tin\\ta\\tbit)\\tand\\nfind\\tthat\\tyou\\tneed\\tto\\tuse\\ta\\tthreshold\\tof\\tabout\\t70,000.\\tTo\\tmake\\tpredictions\\t(on\\tthe\\ttraining\\tset\\tfor\\tnow),\\ninstead\\tof\\tcalling\\tthe\\tclassifier’s\\t\\npredict()\\n\\tmethod,\\tyou\\tcan\\tjust\\trun\\tthis\\tcode:\\ny_train_pred_90\\n\\t\\n=\\n\\t\\n(\\ny_scores\\n\\t\\n>\\n\\t\\n70000\\n)\\nLet’s\\tcheck\\tthese\\tpredictions’\\tprecision\\tand\\trecall:\\n>>>\\t\\nprecision_score\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred_90\\n)\\n0.86592051164915484\\n>>>', '>>>\\t\\nrecall_score\\n(\\ny_train_5\\n,\\n\\t\\ny_train_pred_90\\n)\\n0.69931746910164172\\nGreat,\\tyou\\thave\\ta\\t90%\\tprecision\\tclassifier\\t\\n(or\\tclose\\tenough)!\\tAs\\tyou\\tcan\\tsee,\\tit\\tis\\tfairly\\teasy\\tto\\tcreate\\ta\\nclassifier\\twith\\tvirtually\\tany\\tprecision\\tyou\\twant:\\tjust\\tset\\ta\\thigh\\tenough\\tthreshold,\\tand\\tyou’re\\tdone.\\tHmm,\\nnot\\tso\\tfast.\\tA\\thigh-precision\\tclassifier\\tis\\tnot\\tvery\\tuseful\\tif\\tits\\t\\nrecall\\tis\\ttoo\\tlow!', 'TIP\\nIf\\tsomeone\\tsays\\t“let’s\\treach\\t99%\\tprecision,”\\tyou\\tshould\\task,\\t“at\\twhat\\trecall?”', 'The\\tROC\\tCurve\\nThe\\n\\t\\nreceiver\\toperating\\tcharacteristic\\n\\t(ROC)\\tcurve\\tis\\tanother\\tcommon\\ttool\\tused\\twith\\tbinary\\tclassifiers.\\nIt\\tis\\tvery\\tsimilar\\tto\\tthe\\tprecision/recall\\tcurve,\\tbut\\tinstead\\tof\\tplotting\\tprecision\\tversus\\trecall,\\tthe\\tROC\\ncurve\\tplots\\tthe\\t\\ntrue\\tpositive\\trate\\n\\t\\n(another\\tname\\tfor\\trecall)\\tagainst\\tthe\\t\\nfalse\\tpositive\\trate\\n.\\t\\nThe\\tFPR\\tis\\tthe\\nratio\\tof\\tnegative\\tinstances\\tthat\\tare\\tincorrectly\\tclassified\\tas\\tpositive.\\tIt\\tis\\tequal\\tto\\tone\\tminus\\tthe\\t\\ntrue\\nnegative\\trate\\n,\\t\\nwhich\\tis\\tthe\\tratio\\tof\\tnegative\\tinstances\\tthat\\tare\\tcorrectly\\tclassified\\tas\\tnegative.\\tThe\\tTNR\\nis\\talso\\tcalled\\t\\nspecificity\\n.\\t\\nHence\\tthe\\tROC\\tcurve\\tplots\\t\\nsensitivity\\n\\t(recall)\\tversus\\t\\n1\\t–\\t\\nspecificity\\n.\\nTo\\tplot\\tthe\\tROC\\tcurve,\\t\\nyou\\tfirst\\tneed\\tto\\tcompute\\tthe\\tTPR\\tand\\tFPR\\tfor\\tvarious\\tthreshold\\tvalues,\\tusing\\tthe\\nroc_curve()\\n\\tfunction:\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nroc_curve\\nfpr\\n,\\n\\t\\ntpr\\n,\\n\\t\\nthresholds\\n\\t\\n=\\n\\t\\nroc_curve\\n(\\ny_train_5\\n,\\n\\t\\ny_scores\\n)\\nThen\\tyou\\tcan\\tplot\\tthe\\tFPR\\tagainst\\tthe\\tTPR\\tusing', \"Matplotlib.\\tThis\\tcode\\tproduces\\tthe\\tplot\\tin\\t\\nFigure\\t3-6\\n:\\ndef\\n\\t\\nplot_roc_curve\\n(\\nfpr\\n,\\n\\t\\ntpr\\n,\\n\\t\\nlabel\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n(\\nfpr\\n,\\n\\t\\ntpr\\n,\\n\\t\\nlinewidth\\n=\\n2\\n,\\n\\t\\nlabel\\n=\\nlabel\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n([\\n0\\n,\\n\\t\\n1\\n],\\n\\t\\n[\\n0\\n,\\n\\t\\n1\\n],\\n\\t\\n'k--'\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\naxis\\n([\\n0\\n,\\n\\t\\n1\\n,\\n\\t\\n0\\n,\\n\\t\\n1\\n])\\n\\t\\t\\t\\t\\nplt\\n.\\nxlabel\\n(\\n'False\\tPositive\\tRate'\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nylabel\\n(\\n'True\\tPositive\\tRate'\\n)\\nplot_roc_curve\\n(\\nfpr\\n,\\n\\t\\ntpr\\n)\\nplt\\n.\\nshow\\n()\\nFigure\\t3-6.\\t\\nROC\\tcurve\\nOnce\\tagain\\tthere\\tis\\ta\\ttradeoff:\\tthe\\thigher\\tthe\\trecall\\t(TPR),\\tthe\\tmore\\tfalse\\tpositives\\t(FPR)\\tthe\\tclassifier\", 'produces.\\tThe\\tdotted\\tline\\trepresents\\tthe\\tROC\\tcurve\\tof\\ta\\tpurely\\trandom\\tclassifier;\\ta\\tgood\\tclassifier\\tstays\\nas\\tfar\\taway\\tfrom\\tthat\\tline\\tas\\tpossible\\t(toward\\tthe\\ttop-left\\tcorner).\\nOne\\tway\\tto\\tcompare\\tclassifiers\\tis\\tto\\tmeasure\\tthe\\t\\narea\\tunder\\tthe\\tcurve\\n\\t(AUC).\\t\\nA\\tperfect\\tclassifier\\twill\\nhave\\ta\\t\\nROC\\tAUC\\n\\tequal\\tto\\t1,\\twhereas\\ta\\tpurely\\trandom\\tclassifier\\twill\\thave\\ta\\tROC\\tAUC\\tequal\\tto\\t0.5.\\nScikit-Learn\\tprovides\\ta\\tfunction\\t\\nto\\tcompute\\tthe\\tROC\\tAUC:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nroc_auc_score\\n>>>\\t\\nroc_auc_score\\n(\\ny_train_5\\n,\\n\\t\\ny_scores\\n)\\n0.96244965559671547\\nTIP\\nSince\\tthe\\tROC\\tcurve\\tis\\tso\\tsimilar\\tto\\tthe\\t\\nprecision/recall\\t(or\\tPR)\\tcurve,\\tyou\\tmay\\twonder\\thow\\tto\\tdecide\\twhich\\tone\\tto\\tuse.\\tAs\\ta\\nrule\\tof\\tthumb,\\tyou\\tshould\\tprefer\\tthe\\tPR\\tcurve\\twhenever\\tthe\\tpositive\\tclass\\tis\\trare\\tor\\twhen\\tyou\\tcare\\tmore\\tabout\\tthe\\tfalse\\npositives\\tthan\\tthe\\tfalse\\tnegatives,\\tand\\tthe\\tROC\\tcurve\\totherwise.\\tFor\\texample,\\tlooking\\tat\\tthe\\tprevious\\tROC\\tcurve\\t(and\\tthe\\tROC', 'AUC\\tscore),\\tyou\\tmay\\tthink\\tthat\\tthe\\tclassifier\\tis\\treally\\tgood.\\tBut\\tthis\\tis\\tmostly\\tbecause\\tthere\\tare\\tfew\\tpositives\\t(5s)\\tcompared\\tto\\nthe\\tnegatives\\t(non-5s).\\tIn\\tcontrast,\\tthe\\tPR\\tcurve\\tmakes\\tit\\tclear\\tthat\\tthe\\tclassifier\\thas\\troom\\tfor\\timprovement\\t(the\\tcurve\\tcould\\tbe\\ncloser\\tto\\tthe\\ttop-right\\tcorner).\\nLet’s\\ttrain\\ta\\t\\nRandomForestClassifier\\n\\t\\nand\\tcompare\\tits\\tROC\\tcurve\\tand\\tROC\\tAUC\\tscore\\tto\\tthe\\nSGDClassifier\\n.\\tFirst,\\tyou\\tneed\\tto\\tget\\tscores\\tfor\\teach\\tinstance\\tin\\tthe\\ttraining\\tset.\\tBut\\tdue\\tto\\tthe\\tway\\tit\\nworks\\t(see\\t\\nChapter\\t7\\n),\\tthe\\t\\nRandomForestClassifier\\n\\tclass\\tdoes\\tnot\\thave\\ta\\t\\ndecision_function()\\nmethod.\\t\\nInstead\\tit\\thas\\ta\\t\\npredict_proba()\\n\\tmethod.\\tScikit-Learn\\tclassifiers\\tgenerally\\thave\\tone\\tor\\tthe\\nother.\\tThe\\t\\npredict_proba()\\n\\tmethod\\treturns\\tan\\tarray\\tcontaining\\ta\\trow\\tper\\tinstance\\tand\\ta\\tcolumn\\tper\\nclass,\\teach\\tcontaining\\tthe\\tprobability\\tthat\\tthe\\tgiven\\tinstance\\tbelongs\\tto\\tthe\\tgiven\\tclass\\t(e.g.,\\t70%\\tchance\\nthat\\tthe\\timage\\trepresents\\ta\\t5):\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nRandomForestClassifier\\nforest_clf', '=\\n\\t\\nRandomForestClassifier\\n(\\nrandom_state\\n=\\n42\\n)\\ny_probas_forest\\n\\t\\n=\\n\\t\\ncross_val_predict\\n(\\nforest_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train_5\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmethod\\n=\\n\"predict_proba\"\\n)\\nBut\\tto\\tplot\\ta\\tROC\\tcurve,\\tyou\\tneed\\tscores,\\tnot\\tprobabilities.\\tA\\tsimple\\tsolution\\tis\\tto\\tuse\\tthe\\tpositive\\nclass’s\\t\\nprobability\\tas\\tthe\\tscore:\\ny_scores_forest\\n\\t\\n=\\n\\t\\ny_probas_forest\\n[:,\\n\\t\\n1\\n]\\n\\t\\t\\t\\n#\\tscore\\t=\\tproba\\tof\\tpositive\\tclass\\nfpr_forest\\n,\\n\\t\\ntpr_forest\\n,\\n\\t\\nthresholds_forest\\n\\t\\n=\\n\\t\\nroc_curve\\n(\\ny_train_5\\n,\\ny_scores_forest\\n)\\nNow\\tyou\\tare\\tready\\tto\\tplot\\tthe\\tROC\\tcurve.\\tIt\\tis\\tuseful\\tto\\tplot\\tthe\\tfirst\\tROC\\tcurve\\tas\\twell\\tto\\tsee\\thow\\tthey\\ncompare\\t(\\nFigure\\t3-7\\n):\\nplt\\n.\\nplot\\n(\\nfpr\\n,\\n\\t\\ntpr\\n,\\n\\t\\n\"b:\"\\n,\\n\\t\\nlabel\\n=\\n\"SGD\"\\n)\\nplot_roc_curve\\n(\\nfpr_forest\\n,\\n\\t\\ntpr_forest\\n,\\n\\t\\n\"Random\\tForest\"\\n)\\nplt\\n.\\nlegend\\n(\\nloc\\n=\\n\"lower\\tright\"\\n)\\nplt\\n.\\nshow\\n()', 'Figure\\t3-7.\\t\\nComparing\\tROC\\tcurves\\nAs\\t\\nyou\\tcan\\tsee\\tin\\t\\nFigure\\t3-7\\n,\\tthe\\t\\nRandomForestClassifier\\n’s\\tROC\\tcurve\\tlooks\\tmuch\\tbetter\\tthan\\tthe\\nSGDClassifier\\n’s:\\tit\\tcomes\\tmuch\\tcloser\\tto\\tthe\\ttop-left\\tcorner.\\tAs\\ta\\tresult,\\tits\\tROC\\tAUC\\tscore\\tis\\talso\\nsignificantly\\t\\nbetter:\\n>>>\\t\\nroc_auc_score\\n(\\ny_train_5\\n,\\n\\t\\ny_scores_forest\\n)\\n0.99312433660038291\\nTry\\tmeasuring\\tthe\\tprecision\\tand\\trecall\\tscores:\\tyou\\tshould\\tfind\\t98.5%\\tprecision\\tand\\t82.8%\\trecall.\\tNot\\ntoo\\tbad!\\nHopefully\\tyou\\tnow\\tknow\\thow\\tto\\ttrain\\tbinary\\tclassifiers,\\tchoose\\tthe\\tappropriate\\tmetric\\tfor\\tyour\\ttask,\\nevaluate\\tyour\\tclassifiers\\tusing\\tcross-validation,\\tselect\\tthe\\tprecision/recall\\ttradeoff\\tthat\\tfits\\tyour\\tneeds,\\nand\\tcompare\\tvarious\\tmodels\\tusing\\tROC\\tcurves\\tand\\tROC\\tAUC\\tscores.\\tNow\\tlet’s\\ttry\\tto\\tdetect\\tmore\\tthan\\njust\\tthe\\t5s.', 'Multiclass\\tClassification\\nWhereas\\t\\nbinary\\tclassifiers\\tdistinguish\\tbetween\\ttwo\\tclasses,\\t\\nmulticlass\\tclassifiers\\n\\t(also\\tcalled\\nmultinomial\\tclassifiers\\n)\\tcan\\tdistinguish\\tbetween\\tmore\\tthan\\ttwo\\tclasses.\\nSome\\talgorithms\\t(such\\tas\\t\\nRandom\\tForest\\tclassifiers\\tor\\t\\nnaive\\tBayes\\tclassifiers)\\tare\\tcapable\\tof\\thandling\\nmultiple\\tclasses\\tdirectly.\\tOthers\\t(such\\tas\\tSupport\\tVector\\tMachine\\tclassifiers\\tor\\tLinear\\tclassifiers)\\tare\\nstrictly\\tbinary\\tclassifiers.\\tHowever,\\tthere\\tare\\tvarious\\tstrategies\\tthat\\tyou\\tcan\\tuse\\tto\\tperform\\tmulticlass\\nclassification\\tusing\\tmultiple\\tbinary\\tclassifiers.\\nFor\\texample,\\tone\\tway\\tto\\tcreate\\ta\\tsystem\\tthat\\tcan\\tclassify\\tthe\\tdigit\\timages\\tinto\\t10\\tclasses\\t(from\\t0\\tto\\t9)\\tis\\nto\\ttrain\\t10\\tbinary\\tclassifiers,\\tone\\tfor\\teach\\tdigit\\t(a\\t0-detector,\\ta\\t1-detector,\\ta\\t2-detector,\\tand\\tso\\ton).\\tThen\\nwhen\\tyou\\twant\\tto\\tclassify\\tan\\timage,\\tyou\\tget\\tthe\\tdecision\\tscore\\tfrom\\teach\\tclassifier\\tfor\\tthat\\timage\\tand\\tyou\\nselect\\tthe\\tclass\\twhose\\tclassifier\\toutputs\\tthe\\thighest\\tscore.\\tThis\\tis\\tcalled\\tthe\\t\\none-versus-all\\n\\t(OvA)', '(OvA)\\nstrategy\\t\\n(also\\tcalled\\t\\none-versus-the-rest\\n).\\nAnother\\tstrategy\\tis\\tto\\ttrain\\ta\\tbinary\\tclassifier\\tfor\\tevery\\tpair\\tof\\tdigits:\\tone\\tto\\tdistinguish\\t0s\\tand\\t1s,\\nanother\\tto\\tdistinguish\\t0s\\tand\\t2s,\\tanother\\tfor\\t1s\\tand\\t2s,\\tand\\tso\\ton.\\tThis\\tis\\tcalled\\t\\nthe\\t\\none-versus-one\\n(OvO)\\tstrategy.\\tIf\\tthere\\tare\\t\\nN\\n\\tclasses,\\tyou\\tneed\\tto\\ttrain\\t\\nN\\n\\t×\\t(\\nN\\n\\t–\\t1)\\t/\\t2\\tclassifiers.\\tFor\\tthe\\tMNIST\\nproblem,\\tthis\\tmeans\\ttraining\\t45\\tbinary\\tclassifiers!\\tWhen\\tyou\\twant\\tto\\tclassify\\tan\\timage,\\tyou\\thave\\tto\\trun\\nthe\\timage\\tthrough\\tall\\t45\\tclassifiers\\tand\\tsee\\twhich\\tclass\\twins\\tthe\\tmost\\tduels.\\tThe\\tmain\\tadvantage\\tof\\tOvO\\nis\\tthat\\teach\\tclassifier\\tonly\\tneeds\\tto\\tbe\\ttrained\\ton\\tthe\\tpart\\tof\\tthe\\ttraining\\tset\\tfor\\tthe\\ttwo\\tclasses\\tthat\\tit\\tmust\\ndistinguish.\\nSome\\talgorithms\\t(such\\tas\\t\\nSupport\\tVector\\tMachine\\tclassifiers)\\tscale\\tpoorly\\twith\\tthe\\tsize\\tof\\tthe\\ttraining\\nset,\\tso\\tfor\\tthese\\talgorithms\\tOvO\\tis\\tpreferred\\tsince\\tit\\tis\\tfaster\\tto\\ttrain\\tmany\\tclassifiers\\ton\\tsmall\\ttraining', 'sets\\tthan\\ttraining\\tfew\\tclassifiers\\ton\\tlarge\\ttraining\\tsets.\\tFor\\tmost\\tbinary\\tclassification\\talgorithms,\\nhowever,\\tOvA\\tis\\tpreferred.\\nScikit-Learn\\tdetects\\twhen\\tyou\\ttry\\tto\\tuse\\ta\\tbinary\\tclassification\\talgorithm\\tfor\\ta\\tmulticlass\\tclassification\\ntask,\\tand\\tit\\tautomatically\\truns\\tOvA\\t(except\\tfor\\tSVM\\tclassifiers\\tfor\\twhich\\tit\\tuses\\tOvO).\\tLet’s\\ttry\\tthis\\twith\\nthe\\t\\nSGDClassifier\\n:\\n>>>\\t\\nsgd_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n\\t\\t\\n#\\ty_train,\\tnot\\ty_train_5\\n>>>\\t\\nsgd_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([\\t5.])\\nThat\\twas\\teasy!\\tThis\\tcode\\ttrains\\tthe\\t\\nSGDClassifier\\n\\t\\non\\tthe\\ttraining\\tset\\tusing\\tthe\\toriginal\\ttarget\\tclasses\\nfrom\\t0\\tto\\t9\\t(\\ny_train\\n),\\tinstead\\tof\\tthe\\t5-versus-all\\ttarget\\tclasses\\t(\\ny_train_5\\n).\\tThen\\tit\\tmakes\\ta\\tprediction\\n(a\\tcorrect\\tone\\tin\\tthis\\tcase).\\tUnder\\tthe\\thood,\\tScikit-Learn\\tactually\\ttrained\\t10\\tbinary\\tclassifiers,\\tgot\\ttheir\\ndecision\\tscores\\tfor\\tthe\\timage,\\tand\\tselected\\tthe\\tclass\\twith\\tthe\\thighest\\tscore.\\nTo\\tsee\\tthat\\tthis\\tis\\tindeed\\tthe\\tcase,\\tyou\\tcan\\tcall\\tthe\\t\\ndecision_function()\\n\\tmethod.\\tInstead\\tof\\treturning', 'just\\tone\\tscore\\tper\\tinstance,\\tit\\tnow\\treturns\\t10\\tscores,\\tone\\tper\\tclass:\\n>>>\\t\\nsome_digit_scores\\n\\t\\n=\\n\\t\\nsgd_clf\\n.\\ndecision_function\\n([\\nsome_digit\\n])\\n>>>\\t\\nsome_digit_scores\\narray([[-311402.62954431,\\t-363517.28355739,\\t-446449.5306454\\t,', '-183226.61023518,\\t-414337.15339485,\\t\\t161855.74572176,\\n\\t\\t\\t\\t\\t\\t\\t\\t-452576.39616343,\\t-471957.14962573,\\t-518542.33997148,\\n\\t\\t\\t\\t\\t\\t\\t\\t-536774.63961222]])\\nThe\\thighest\\tscore\\tis\\tindeed\\tthe\\tone\\tcorresponding\\tto\\tclass\\t5:\\n>>>\\t\\nnp\\n.\\nargmax\\n(\\nsome_digit_scores\\n)\\n5\\n>>>\\t\\nsgd_clf\\n.\\nclasses_\\narray([\\t0.,\\t\\t1.,\\t\\t2.,\\t\\t3.,\\t\\t4.,\\t\\t5.,\\t\\t6.,\\t\\t7.,\\t\\t8.,\\t\\t9.])\\n>>>\\t\\nsgd_clf\\n.\\nclasses_\\n[\\n5\\n]\\n5.0\\nWARNING\\nWhen\\ta\\tclassifier\\tis\\ttrained,\\tit\\tstores\\tthe\\tlist\\tof\\ttarget\\tclasses\\tin\\tits\\t\\nclasses_\\n\\tattribute,\\tordered\\tby\\tvalue.\\tIn\\tthis\\tcase,\\tthe\\tindex\\tof\\neach\\tclass\\tin\\tthe\\t\\nclasses_\\n\\tarray\\tconveniently\\tmatches\\tthe\\tclass\\titself\\t(e.g.,\\tthe\\tclass\\tat\\tindex\\t5\\thappens\\tto\\tbe\\tclass\\t5),\\tbut\\tin\\ngeneral\\tyou\\twon’t\\tbe\\tso\\tlucky.\\nIf\\tyou\\twant\\tto\\tforce\\tScikitLearn\\tto\\t\\nuse\\tone-versus-one\\tor\\tone-versus-all,\\tyou\\tcan\\tuse\\tthe\\nOneVsOneClassifier\\n\\tor\\t\\nOneVsRestClassifier\\n\\tclasses.\\tSimply\\tcreate\\tan\\tinstance\\tand\\tpass\\ta\\tbinary\\nclassifier\\tto\\tits\\tconstructor.\\tFor\\texample,\\tthis\\tcode\\tcreates\\ta\\tmulticlass\\tclassifier\\tusing\\tthe\\tOvO\\tstrategy,\\nbased\\ton\\ta', 'SGDClassifier\\n:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.multiclass\\n\\t\\nimport\\n\\t\\nOneVsOneClassifier\\n>>>\\t\\novo_clf\\n\\t\\n=\\n\\t\\nOneVsOneClassifier\\n(\\nSGDClassifier\\n(\\nrandom_state\\n=\\n42\\n))\\n>>>\\t\\novo_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n>>>\\t\\novo_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([\\t5.])\\n>>>\\t\\nlen\\n(\\novo_clf\\n.\\nestimators_\\n)\\n45\\nTraining\\ta\\t\\nRandomForestClassifier\\n\\tis\\t\\njust\\tas\\teasy:\\n>>>\\t\\nforest_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n>>>\\t\\nforest_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([\\t5.])\\nThis\\ttime\\tScikit-Learn\\tdid\\tnot\\thave\\tto\\trun\\tOvA\\tor\\tOvO\\tbecause\\tRandom\\tForest\\t\\nclassifiers\\n\\tcan\\tdirectly\\nclassify\\tinstances\\tinto\\tmultiple\\tclasses.\\tYou\\tcan\\tcall\\t\\npredict_proba()\\n\\tto\\tget\\tthe\\tlist\\tof\\tprobabilities\\tthat\\nthe\\tclassifier\\tassigned\\tto\\teach\\tinstance\\tfor\\teach\\tclass:\\n>>>\\t\\nforest_clf\\n.\\npredict_proba\\n([\\nsome_digit\\n])\\narray([[\\t0.1,\\t\\t0.\\t,\\t\\t0.\\t,\\t\\t0.1,\\t\\t0.\\t,\\t\\t0.8,\\t\\t0.\\t,\\t\\t0.\\t,\\t\\t0.\\t,\\t\\t0.\\t]])\\nYou\\tcan\\tsee\\tthat\\tthe\\tclassifier\\tis\\tfairly\\tconfident\\tabout\\tits\\tprediction:\\tthe\\t0.8\\tat\\tthe\\t5\\nth\\n\\tindex\\tin\\tthe\\tarray', 'means\\tthat\\tthe\\tmodel\\testimates\\tan\\t80%\\tprobability\\tthat\\tthe\\timage\\trepresents\\ta\\t5.\\tIt\\talso\\tthinks\\tthat\\tthe\\nimage\\tcould\\tinstead\\tbe\\ta\\t0\\tor\\ta\\t3\\t(10%\\tchance\\teach).\\nNow\\tof\\t\\ncourse\\tyou\\twant\\tto\\tevaluate\\tthese\\tclassifiers.\\tAs\\tusual,\\tyou\\twant\\tto\\tuse\\tcross-validation.\\tLet’s\\nevaluate\\tthe\\t\\nSGDClassifier\\n’s\\taccuracy\\tusing\\tthe\\t\\ncross_val_score()\\n\\tfunction:', '>>>\\t\\ncross_val_score\\n(\\nsgd_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\nscoring\\n=\\n\"accuracy\"\\n)\\narray([\\t0.84063187,\\t\\t0.84899245,\\t\\t0.86652998])\\nIt\\tgets\\tover\\t84%\\ton\\tall\\ttest\\tfolds.\\tIf\\tyou\\tused\\ta\\trandom\\tclassifier,\\tyou\\twould\\tget\\t10%\\taccuracy,\\tso\\tthis\\tis\\nnot\\tsuch\\ta\\tbad\\tscore,\\tbut\\tyou\\tcan\\tstill\\tdo\\tmuch\\t\\nbetter.\\tFor\\texample,\\tsimply\\tscaling\\tthe\\tinputs\\t(as\\ndiscussed\\tin\\t\\nChapter\\t2\\n)\\tincreases\\t\\naccuracy\\tabove\\t90%:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nStandardScaler\\n>>>\\t\\nscaler\\n\\t\\n=\\n\\t\\nStandardScaler\\n()\\n>>>\\t\\nX_train_scaled\\n\\t\\n=\\n\\t\\nscaler\\n.\\nfit_transform\\n(\\nX_train\\n.\\nastype\\n(\\nnp\\n.\\nfloat64\\n))\\n>>>\\t\\ncross_val_score\\n(\\nsgd_clf\\n,\\n\\t\\nX_train_scaled\\n,\\n\\t\\ny_train\\n,\\n\\t\\ncv\\n=\\n3\\n,\\n\\t\\nscoring\\n=\\n\"accuracy\"\\n)\\narray([\\t0.91011798,\\t\\t0.90874544,\\t\\t0.906636\\t\\t])', 'Error\\tAnalysis\\nOf\\t\\ncourse,\\tif\\tthis\\twere\\ta\\treal\\tproject,\\tyou\\twould\\tfollow\\tthe\\tsteps\\tin\\tyour\\tMachine\\tLearning\\tproject\\nchecklist\\t(see\\t\\nAppendix\\tB\\n):\\texploring\\tdata\\tpreparation\\toptions,\\ttrying\\tout\\tmultiple\\tmodels,\\tshortlisting\\nthe\\tbest\\tones\\tand\\tfine-tuning\\ttheir\\thyperparameters\\tusing\\t\\nGridSearchCV\\n,\\t\\nand\\tautomating\\tas\\tmuch\\tas\\npossible,\\tas\\tyou\\tdid\\tin\\tthe\\tprevious\\tchapter.\\tHere,\\twe\\twill\\tassume\\tthat\\tyou\\thave\\tfound\\ta\\tpromising\\tmodel\\nand\\tyou\\twant\\tto\\tfind\\tways\\tto\\timprove\\tit.\\tOne\\tway\\tto\\tdo\\tthis\\tis\\tto\\tanalyze\\tthe\\ttypes\\tof\\terrors\\tit\\tmakes.\\nFirst,\\tyou\\tcan\\tlook\\tat\\tthe\\t\\nconfusion\\tmatrix.\\tYou\\tneed\\tto\\tmake\\tpredictions\\tusing\\tthe\\ncross_val_predict()\\n\\tfunction,\\t\\nthen\\tcall\\tthe\\t\\nconfusion_matrix()\\n\\tfunction,\\tjust\\tlike\\tyou\\tdid\\tearlier:\\n>>>\\t\\ny_train_pred\\n\\t\\n=\\n\\t\\ncross_val_predict\\n(\\nsgd_clf\\n,\\n\\t\\nX_train_scaled\\n,\\n\\t\\ny_train\\n,\\n\\t\\ncv\\n=\\n3\\n)\\n>>>\\t\\nconf_mx\\n\\t\\n=\\n\\t\\nconfusion_matrix\\n(\\ny_train\\n,\\n\\t\\ny_train_pred\\n)\\n>>>\\t\\nconf_mx\\narray([[5725,\\t\\t\\t\\t3,\\t\\t\\t24,\\t\\t\\t\\t9,\\t\\t\\t10,\\t\\t\\t49,\\t\\t\\t50,\\t\\t\\t10,\\t\\t\\t39,\\t\\t\\t\\t4],', '[\\t\\t\\t2,\\t6493,\\t\\t\\t43,\\t\\t\\t25,\\t\\t\\t\\t7,\\t\\t\\t40,\\t\\t\\t\\t5,\\t\\t\\t10,\\t\\t109,\\t\\t\\t\\t8],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t51,\\t\\t\\t41,\\t5321,\\t\\t104,\\t\\t\\t89,\\t\\t\\t26,\\t\\t\\t87,\\t\\t\\t60,\\t\\t166,\\t\\t\\t13],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t47,\\t\\t\\t46,\\t\\t141,\\t5342,\\t\\t\\t\\t1,\\t\\t231,\\t\\t\\t40,\\t\\t\\t50,\\t\\t141,\\t\\t\\t92],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t19,\\t\\t\\t29,\\t\\t\\t41,\\t\\t\\t10,\\t5366,\\t\\t\\t\\t9,\\t\\t\\t56,\\t\\t\\t37,\\t\\t\\t86,\\t\\t189],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t73,\\t\\t\\t45,\\t\\t\\t36,\\t\\t193,\\t\\t\\t64,\\t4582,\\t\\t111,\\t\\t\\t30,\\t\\t193,\\t\\t\\t94],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t29,\\t\\t\\t34,\\t\\t\\t44,\\t\\t\\t\\t2,\\t\\t\\t42,\\t\\t\\t85,\\t5627,\\t\\t\\t10,\\t\\t\\t45,\\t\\t\\t\\t0],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t25,\\t\\t\\t24,\\t\\t\\t74,\\t\\t\\t32,\\t\\t\\t54,\\t\\t\\t12,\\t\\t\\t\\t6,\\t5787,\\t\\t\\t15,\\t\\t236],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t52,\\t\\t161,\\t\\t\\t73,\\t\\t156,\\t\\t\\t10,\\t\\t163,\\t\\t\\t61,\\t\\t\\t25,\\t5027,\\t\\t123],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t43,\\t\\t\\t35,\\t\\t\\t26,\\t\\t\\t92,\\t\\t178,\\t\\t\\t28,\\t\\t\\t\\t2,\\t\\t223,\\t\\t\\t82,\\t5240]])\\nThat’s\\ta\\tlot\\tof\\tnumbers.\\tIt’s\\toften\\tmore\\tconvenient\\tto\\tlook\\tat\\tan\\timage\\trepresentation\\tof\\tthe\\tconfusion\\nmatrix,\\tusing\\t\\nMatplotlib’s\\t\\nmatshow()\\n\\tfunction:\\nplt\\n.\\nmatshow\\n(\\nconf_mx\\n,\\n\\t\\ncmap\\n=\\nplt\\n.\\ncm\\n.\\ngray\\n)\\nplt\\n.\\nshow\\n()', 'This\\tconfusion\\tmatrix\\tlooks\\tfairly\\tgood,\\tsince\\tmost\\timages\\tare\\ton\\tthe\\tmain\\tdiagonal,\\twhich\\tmeans\\tthat\\nthey\\twere\\tclassified\\tcorrectly.\\tThe\\t5s\\tlook\\tslightly\\tdarker\\tthan\\tthe\\tother\\tdigits,\\twhich\\tcould\\tmean\\tthat\\nthere\\tare\\tfewer\\timages\\tof\\t5s\\tin\\tthe\\tdataset\\tor\\tthat\\tthe\\tclassifier\\tdoes\\tnot\\tperform\\tas\\twell\\ton\\t5s\\tas\\ton\\tother\\ndigits.\\tIn\\tfact,\\tyou\\tcan\\tverify\\tthat\\tboth\\tare\\tthe\\tcase.\\nLet’s\\tfocus\\tthe\\tplot\\ton\\tthe\\terrors.\\tFirst,\\tyou\\tneed\\tto\\tdivide\\teach\\tvalue\\tin\\tthe\\tconfusion\\tmatrix\\tby\\tthe\\nnumber\\tof\\timages\\tin\\tthe\\tcorresponding\\tclass,\\tso\\tyou\\tcan\\tcompare\\terror\\trates\\tinstead\\tof\\tabsolute\\tnumber\\nof\\terrors\\t(which\\twould\\tmake\\tabundant\\tclasses\\tlook\\tunfairly\\tbad):\\nrow_sums\\n\\t\\n=\\n\\t\\nconf_mx\\n.\\nsum\\n(\\naxis\\n=\\n1\\n,\\n\\t\\nkeepdims\\n=\\nTrue\\n)\\nnorm_conf_mx\\n\\t\\n=\\n\\t\\nconf_mx\\n\\t\\n/\\n\\t\\nrow_sums\\nNow\\tlet’s\\tfill\\tthe\\tdiagonal\\twith\\tzeros\\tto\\tkeep\\tonly\\tthe\\terrors,\\tand\\tlet’s\\tplot\\tthe\\tresult:\\nnp\\n.\\nfill_diagonal\\n(\\nnorm_conf_mx\\n,\\n\\t\\n0\\n)\\nplt\\n.\\nmatshow\\n(\\nnorm_conf_mx\\n,\\n\\t\\ncmap\\n=\\nplt\\n.\\ncm\\n.\\ngray\\n)\\nplt\\n.\\nshow\\n()', 'Now\\tyou\\tcan\\tclearly\\tsee\\tthe\\tkinds\\tof\\terrors\\tthe\\tclassifier\\tmakes.\\tRemember\\tthat\\trows\\trepresent\\tactual\\nclasses,\\twhile\\tcolumns\\trepresent\\tpredicted\\tclasses.\\tThe\\tcolumns\\tfor\\tclasses\\t8\\tand\\t9\\tare\\tquite\\tbright,\\nwhich\\ttells\\tyou\\tthat\\tmany\\timages\\tget\\tmisclassified\\tas\\t8s\\tor\\t9s.\\tSimilarly,\\tthe\\trows\\tfor\\tclasses\\t8\\tand\\t9\\tare\\nalso\\tquite\\tbright,\\ttelling\\tyou\\tthat\\t8s\\tand\\t9s\\tare\\toften\\tconfused\\twith\\tother\\tdigits.\\tConversely,\\tsome\\trows\\nare\\tpretty\\tdark,\\tsuch\\tas\\trow\\t1:\\tthis\\tmeans\\tthat\\tmost\\t1s\\tare\\tclassified\\tcorrectly\\t(a\\tfew\\tare\\tconfused\\twith\\n8s,\\tbut\\tthat’s\\tabout\\tit).\\tNotice\\tthat\\tthe\\terrors\\tare\\tnot\\tperfectly\\tsymmetrical;\\tfor\\texample,\\tthere\\tare\\tmore\\t5s\\nmisclassified\\tas\\t8s\\tthan\\tthe\\treverse.\\nAnalyzing\\tthe\\tconfusion\\tmatrix\\tcan\\toften\\tgive\\tyou\\tinsights\\ton\\tways\\tto\\timprove\\tyour\\tclassifier.\\tLooking\\tat\\nthis\\tplot,\\tit\\tseems\\tthat\\tyour\\tefforts\\tshould\\tbe\\tspent\\ton\\timproving\\tclassification\\tof\\t8s\\tand\\t9s,\\tas\\twell\\tas\\nfixing\\tthe\\tspecific\\t3/5\\tconfusion.\\tFor\\texample,\\tyou\\tcould\\ttry\\tto\\tgather\\tmore\\ttraining\\tdata\\tfor\\tthese\\tdigits.', 'Or\\tyou\\tcould\\tengineer\\tnew\\tfeatures\\tthat\\twould\\thelp\\tthe\\tclassifier\\t—\\tfor\\texample,\\twriting\\tan\\talgorithm\\tto\\ncount\\tthe\\tnumber\\tof\\tclosed\\tloops\\t(e.g.,\\t8\\thas\\ttwo,\\t6\\thas\\tone,\\t5\\thas\\tnone).\\tOr\\tyou\\tcould\\tpreprocess\\tthe\\nimages\\t(e.g.,\\tusing\\tScikit-Image,\\tPillow,\\tor\\tOpenCV)\\tto\\tmake\\tsome\\tpatterns\\tstand\\tout\\tmore,\\tsuch\\tas\\nclosed\\tloops.\\nAnalyzing\\tindividual\\terrors\\tcan\\talso\\tbe\\ta\\tgood\\tway\\tto\\tgain\\tinsights\\ton\\twhat\\tyour\\tclassifier\\tis\\tdoing\\tand\\nwhy\\tit\\tis\\tfailing,\\tbut\\tit\\tis\\tmore\\tdifficult\\tand\\ttime-consuming.\\tFor\\texample,\\tlet’s\\tplot\\texamples\\tof\\t3s\\tand\\t5s\\n(the\\t\\nplot_digits()\\n\\tfunction\\tjust\\tuses\\tMatplotlib’s\\t\\nimshow()\\n\\tfunction;\\tsee\\tthis\\tchapter’s\\tJupyter', 'notebook\\tfor\\tdetails):\\ncl_a\\n,\\n\\t\\ncl_b\\n\\t\\n=\\n\\t\\n3\\n,\\n\\t\\n5\\nX_aa\\n\\t\\n=\\n\\t\\nX_train\\n[(\\ny_train\\n\\t\\n==\\n\\t\\ncl_a\\n)\\n\\t\\n&\\n\\t\\n(\\ny_train_pred\\n\\t\\n==\\n\\t\\ncl_a\\n)]\\nX_ab\\n\\t\\n=\\n\\t\\nX_train\\n[(\\ny_train\\n\\t\\n==\\n\\t\\ncl_a\\n)\\n\\t\\n&\\n\\t\\n(\\ny_train_pred\\n\\t\\n==\\n\\t\\ncl_b\\n)]\\nX_ba\\n\\t\\n=\\n\\t\\nX_train\\n[(\\ny_train\\n\\t\\n==\\n\\t\\ncl_b\\n)\\n\\t\\n&\\n\\t\\n(\\ny_train_pred\\n\\t\\n==\\n\\t\\ncl_a\\n)]\\nX_bb\\n\\t\\n=\\n\\t\\nX_train\\n[(\\ny_train\\n\\t\\n==\\n\\t\\ncl_b\\n)\\n\\t\\n&\\n\\t\\n(\\ny_train_pred\\n\\t\\n==\\n\\t\\ncl_b\\n)]\\nplt\\n.\\nfigure\\n(\\nfigsize\\n=\\n(\\n8\\n,\\n8\\n))\\nplt\\n.\\nsubplot\\n(\\n221\\n);\\n\\t\\nplot_digits\\n(\\nX_aa\\n[:\\n25\\n],\\n\\t\\nimages_per_row\\n=\\n5\\n)\\nplt\\n.\\nsubplot\\n(\\n222\\n);\\n\\t\\nplot_digits\\n(\\nX_ab\\n[:\\n25\\n],\\n\\t\\nimages_per_row\\n=\\n5\\n)\\nplt\\n.\\nsubplot\\n(\\n223\\n);\\n\\t\\nplot_digits\\n(\\nX_ba\\n[:\\n25\\n],\\n\\t\\nimages_per_row\\n=\\n5\\n)\\nplt\\n.\\nsubplot\\n(\\n224\\n);\\n\\t\\nplot_digits\\n(\\nX_bb\\n[:\\n25\\n],\\n\\t\\nimages_per_row\\n=\\n5\\n)\\nplt\\n.\\nshow\\n()\\nThe\\ttwo\\t5×5\\tblocks\\ton\\tthe\\tleft\\tshow\\tdigits\\tclassified\\tas\\t3s,\\tand\\tthe\\ttwo\\t5×5\\tblocks\\ton\\tthe\\tright\\tshow\\nimages\\tclassified\\tas\\t5s.\\tSome\\tof\\tthe\\tdigits\\tthat\\tthe\\tclassifier\\tgets\\twrong\\t(i.e.,\\tin\\tthe\\tbottom-left\\tand\\ttop-', 'right\\tblocks)\\tare\\tso\\tbadly\\twritten\\tthat\\teven\\ta\\thuman\\twould\\thave\\ttrouble\\tclassifying\\tthem\\t(e.g.,\\tthe\\t5\\ton\\nthe\\t8\\nth\\n\\trow\\tand\\t1\\nst\\n\\tcolumn\\ttruly\\tlooks\\tlike\\ta\\t3).\\tHowever,\\tmost\\tmisclassified\\timages\\tseem\\tlike\\tobvious\\nerrors\\tto\\tus,\\tand\\tit’s\\thard\\tto\\tunderstand\\twhy\\tthe\\tclassifier\\tmade\\tthe\\tmistakes\\tit\\tdid.\\n3\\n\\tThe\\treason\\tis\\tthat\\twe\\nused\\ta\\tsimple\\t\\nSGDClassifier\\n,\\twhich\\tis\\ta\\tlinear\\tmodel.\\tAll\\tit\\tdoes\\tis\\tassign\\ta\\tweight\\tper\\tclass\\tto\\teach\\npixel,\\tand\\twhen\\tit\\tsees\\ta\\tnew\\timage\\tit\\tjust\\tsums\\tup\\tthe\\tweighted\\tpixel\\tintensities\\tto\\tget\\ta\\tscore\\tfor\\teach\\nclass.\\tSo\\tsince\\t3s\\tand\\t5s\\tdiffer\\tonly\\tby\\ta\\tfew\\tpixels,\\tthis\\tmodel\\twill\\teasily\\tconfuse\\tthem.', 'The\\tmain\\tdifference\\tbetween\\t3s\\tand\\t5s\\tis\\tthe\\tposition\\tof\\tthe\\tsmall\\tline\\tthat\\tjoins\\tthe\\ttop\\tline\\tto\\tthe\\tbottom\\narc.\\tIf\\tyou\\tdraw\\ta\\t3\\twith\\tthe\\tjunction\\tslightly\\tshifted\\tto\\tthe\\tleft,\\tthe\\tclassifier\\tmight\\tclassify\\tit\\tas\\ta\\t5,\\tand\\nvice\\tversa.\\tIn\\tother\\twords,\\tthis\\tclassifier\\tis\\tquite\\tsensitive\\tto\\timage\\tshifting\\tand\\trotation.\\tSo\\tone\\tway\\tto\\nreduce\\tthe\\t3/5\\tconfusion\\twould\\tbe\\tto\\tpreprocess\\tthe\\timages\\tto\\tensure\\tthat\\tthey\\tare\\twell\\tcentered\\tand\\tnot\\ntoo\\trotated.\\tThis\\twill\\tprobably\\thelp\\treduce\\tother\\terrors\\tas\\t\\nwell.', 'Multilabel\\tClassification\\nUntil\\t\\nnow\\teach\\tinstance\\thas\\talways\\tbeen\\tassigned\\tto\\tjust\\tone\\tclass.\\tIn\\tsome\\tcases\\tyou\\tmay\\twant\\tyour\\nclassifier\\tto\\toutput\\tmultiple\\tclasses\\tfor\\teach\\tinstance.\\tFor\\texample,\\tconsider\\ta\\tface-recognition\\nclassifier:\\twhat\\tshould\\tit\\tdo\\tif\\tit\\trecognizes\\tseveral\\tpeople\\ton\\tthe\\tsame\\tpicture?\\tOf\\tcourse\\tit\\tshould\\nattach\\tone\\tlabel\\tper\\tperson\\tit\\trecognizes.\\tSay\\tthe\\tclassifier\\thas\\tbeen\\ttrained\\tto\\trecognize\\tthree\\tfaces,\\nAlice,\\tBob,\\tand\\tCharlie;\\tthen\\twhen\\tit\\tis\\tshown\\ta\\tpicture\\tof\\tAlice\\tand\\tCharlie,\\tit\\tshould\\toutput\\t[1,\\t0,\\t1]\\n(meaning\\t“Alice\\tyes,\\tBob\\tno,\\tCharlie\\tyes”).\\tSuch\\ta\\tclassification\\tsystem\\tthat\\toutputs\\tmultiple\\tbinary\\nlabels\\tis\\tcalled\\ta\\t\\nmultilabel\\tclassification\\n\\tsystem.\\nWe\\twon’t\\tgo\\tinto\\tface\\trecognition\\tjust\\tyet,\\tbut\\tlet’s\\tlook\\tat\\ta\\tsimpler\\texample,\\tjust\\tfor\\tillustration\\npurposes:\\nfrom\\n\\t\\nsklearn.neighbors\\n\\t\\nimport\\n\\t\\nKNeighborsClassifier\\ny_train_large\\n\\t\\n=\\n\\t\\n(\\ny_train\\n\\t\\n>=\\n\\t\\n7\\n)\\ny_train_odd\\n\\t\\n=\\n\\t\\n(\\ny_train\\n\\t\\n%\\n\\t\\n2\\n\\t\\n==\\n\\t\\n1\\n)\\ny_multilabel\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\ny_train_large\\n,', ',\\n\\t\\ny_train_odd\\n]\\nknn_clf\\n\\t\\n=\\n\\t\\nKNeighborsClassifier\\n()\\nknn_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_multilabel\\n)\\nThis\\tcode\\tcreates\\ta\\t\\ny_multilabel\\n\\tarray\\tcontaining\\ttwo\\ttarget\\tlabels\\tfor\\teach\\tdigit\\timage:\\tthe\\tfirst\\nindicates\\twhether\\tor\\tnot\\tthe\\tdigit\\tis\\tlarge\\t(7,\\t8,\\tor\\t9)\\tand\\tthe\\tsecond\\tindicates\\twhether\\tor\\tnot\\tit\\tis\\todd.\\nThe\\tnext\\tlines\\tcreate\\ta\\t\\nKNeighborsClassifier\\n\\t\\ninstance\\t(which\\tsupports\\tmultilabel\\tclassification,\\tbut\\nnot\\tall\\tclassifiers\\tdo)\\tand\\twe\\ttrain\\tit\\tusing\\tthe\\tmultiple\\ttargets\\tarray.\\tNow\\tyou\\tcan\\tmake\\ta\\tprediction,\\tand\\nnotice\\tthat\\tit\\toutputs\\ttwo\\tlabels:\\n>>>\\t\\nknn_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([[False,\\t\\tTrue]],\\tdtype=bool)\\nAnd\\tit\\tgets\\tit\\tright!\\tThe\\tdigit\\t5\\tis\\tindeed\\tnot\\tlarge\\t(\\nFalse\\n)\\tand\\todd\\t(\\nTrue\\n).\\nThere\\tare\\tmany\\tways\\tto\\tevaluate\\ta\\tmultilabel\\tclassifier,\\tand\\tselecting\\tthe\\tright\\tmetric\\treally\\tdepends\\ton\\nyour\\tproject.\\tFor\\texample,\\tone\\tapproach\\tis\\tto\\tmeasure\\tthe\\tF\\n1\\n\\tscore\\tfor\\teach\\tindividual\\tlabel\\t(or\\tany\\tother', 'binary\\tclassifier\\tmetric\\tdiscussed\\tearlier),\\tthen\\tsimply\\tcompute\\tthe\\taverage\\tscore.\\tThis\\tcode\\tcomputes\\nthe\\taverage\\tF\\n1\\n\\tscore\\tacross\\t\\nall\\tlabels:\\n>>>\\t\\ny_train_knn_pred\\n\\t\\n=\\n\\t\\ncross_val_predict\\n(\\nknn_clf\\n,\\n\\t\\nX_train\\n,\\n\\t\\ny_train\\n,\\n\\t\\ncv\\n=\\n3\\n)\\n>>>\\t\\nf1_score\\n(\\ny_train\\n,\\n\\t\\ny_train_knn_pred\\n,\\n\\t\\naverage\\n=\\n\"macro\"\\n)\\n0.96845540180280221\\nThis\\tassumes\\tthat\\tall\\tlabels\\tare\\tequally\\timportant,\\twhich\\tmay\\tnot\\tbe\\tthe\\tcase.\\tIn\\tparticular,\\tif\\tyou\\thave\\nmany\\tmore\\tpictures\\tof\\tAlice\\tthan\\tof\\tBob\\tor\\tCharlie,\\tyou\\tmay\\twant\\tto\\tgive\\tmore\\tweight\\tto\\tthe\\tclassifier’s\\nscore\\ton\\tpictures\\tof\\tAlice.\\tOne\\tsimple\\toption\\tis\\tto\\tgive\\teach\\tlabel\\ta\\tweight\\tequal\\tto\\tits\\t\\nsupport\\n\\t(i.e.,\\tthe\\nnumber\\tof\\tinstances\\twith\\tthat\\ttarget\\tlabel).\\t\\nTo\\tdo\\tthis,\\tsimply\\tset\\t\\naverage=\"weighted\"\\n\\tin\\tthe\\tpreceding\\ncode.\\n4', 'Multioutput\\tClassification\\nThe\\t\\nlast\\ttype\\tof\\tclassification\\ttask\\twe\\tare\\tgoing\\tto\\tdiscuss\\there\\tis\\tcalled\\t\\nmultioutput-multiclass\\nclassification\\n\\t(or\\tsimply\\t\\nmultioutput\\tclassification\\n).\\tIt\\tis\\tsimply\\ta\\tgeneralization\\tof\\tmultilabel\\nclassification\\twhere\\teach\\tlabel\\tcan\\tbe\\tmulticlass\\t(i.e.,\\tit\\tcan\\thave\\tmore\\tthan\\ttwo\\tpossible\\tvalues).\\nTo\\tillustrate\\tthis,\\tlet’s\\tbuild\\ta\\tsystem\\tthat\\tremoves\\tnoise\\tfrom\\timages.\\tIt\\twill\\ttake\\tas\\tinput\\ta\\tnoisy\\tdigit\\nimage,\\tand\\tit\\twill\\t(hopefully)\\toutput\\ta\\tclean\\tdigit\\timage,\\trepresented\\tas\\tan\\tarray\\tof\\tpixel\\tintensities,\\tjust\\nlike\\tthe\\tMNIST\\timages.\\tNotice\\tthat\\tthe\\tclassifier’s\\toutput\\tis\\tmultilabel\\t(one\\tlabel\\tper\\tpixel)\\tand\\teach\\nlabel\\tcan\\thave\\tmultiple\\tvalues\\t(pixel\\tintensity\\tranges\\tfrom\\t0\\tto\\t255).\\tIt\\tis\\tthus\\tan\\texample\\tof\\ta\\tmultioutput\\nclassification\\tsystem.\\nNOTE\\nThe\\tline\\tbetween\\t\\nclassification\\tand\\tregression\\tis\\tsometimes\\tblurry,\\tsuch\\tas\\tin\\tthis\\texample.\\tArguably,\\tpredicting\\tpixel\\tintensity\\tis', 'more\\takin\\tto\\tregression\\tthan\\tto\\tclassification.\\tMoreover,\\tmultioutput\\tsystems\\tare\\tnot\\tlimited\\tto\\tclassification\\ttasks;\\tyou\\tcould\\teven\\nhave\\ta\\tsystem\\tthat\\toutputs\\tmultiple\\tlabels\\tper\\tinstance,\\tincluding\\tboth\\tclass\\tlabels\\tand\\tvalue\\tlabels.\\nLet’s\\tstart\\tby\\tcreating\\tthe\\ttraining\\tand\\ttest\\tsets\\tby\\ttaking\\tthe\\tMNIST\\timages\\tand\\tadding\\tnoise\\tto\\ttheir\\tpixel\\nintensities\\tusing\\tNumPy’s\\t\\nrandint()\\n\\tfunction.\\tThe\\ttarget\\timages\\twill\\tbe\\tthe\\toriginal\\timages:\\nnoise\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandint\\n(\\n0\\n,\\n\\t\\n100\\n,\\n\\t\\n(\\nlen\\n(\\nX_train\\n),\\n\\t\\n784\\n))\\nX_train_mod\\n\\t\\n=\\n\\t\\nX_train\\n\\t\\n+\\n\\t\\nnoise\\nnoise\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandint\\n(\\n0\\n,\\n\\t\\n100\\n,\\n\\t\\n(\\nlen\\n(\\nX_test\\n),\\n\\t\\n784\\n))\\nX_test_mod\\n\\t\\n=\\n\\t\\nX_test\\n\\t\\n+\\n\\t\\nnoise\\ny_train_mod\\n\\t\\n=\\n\\t\\nX_train\\ny_test_mod\\n\\t\\n=\\n\\t\\nX_test\\nLet’s\\ttake\\ta\\tpeek\\tat\\tan\\timage\\tfrom\\tthe\\ttest\\tset\\t(yes,\\twe’re\\tsnooping\\ton\\tthe\\ttest\\tdata,\\tso\\tyou\\tshould\\tbe\\nfrowning\\tright\\tnow):\\nOn\\tthe\\tleft\\tis\\tthe\\tnoisy\\tinput\\timage,\\tand\\ton\\tthe\\tright\\tis\\tthe\\tclean\\ttarget\\timage.\\tNow\\tlet’s\\ttrain\\tthe\\tclassifier', 'and\\tmake\\tit\\tclean\\tthis\\timage:\\nknn_clf\\n.\\nfit\\n(\\nX_train_mod\\n,\\n\\t\\ny_train_mod\\n)\\nclean_digit\\n\\t\\n=\\n\\t\\nknn_clf\\n.\\npredict\\n([\\nX_test_mod\\n[\\nsome_index\\n]])\\nplot_digit\\n(\\nclean_digit\\n)\\nLooks\\tclose\\tenough\\tto\\tthe\\ttarget!\\tThis\\tconcludes\\tour\\ttour\\tof\\tclassification.\\tHopefully\\tyou\\tshould\\tnow\\nknow\\thow\\tto\\tselect\\tgood\\tmetrics\\tfor\\tclassification\\ttasks,\\tpick\\tthe\\tappropriate\\tprecision/recall\\ttradeoff,\\ncompare\\tclassifiers,\\tand\\tmore\\tgenerally\\tbuild\\tgood\\tclassification\\tsystems\\tfor\\ta\\tvariety\\tof\\t\\ntasks.', 'Exercises\\n1\\n.\\t\\nTry\\tto\\tbuild\\ta\\tclassifier\\tfor\\tthe\\tMNIST\\tdataset\\tthat\\tachieves\\tover\\t97%\\taccuracy\\ton\\tthe\\ttest\\tset.\\tHint:\\nthe\\t\\nKNeighborsClassifier\\n\\t\\nworks\\tquite\\twell\\tfor\\tthis\\ttask;\\tyou\\tjust\\tneed\\tto\\tfind\\tgood\\nhyperparameter\\tvalues\\t(try\\ta\\tgrid\\tsearch\\ton\\tthe\\t\\nweights\\n\\tand\\t\\nn_neighbors\\n\\thyperparameters).\\n2\\n.\\t\\nWrite\\ta\\tfunction\\tthat\\tcan\\tshift\\tan\\tMNIST\\timage\\tin\\tany\\tdirection\\t(left,\\tright,\\tup,\\tor\\tdown)\\tby\\tone\\npixel.\\n5\\n\\tThen,\\tfor\\teach\\timage\\tin\\tthe\\ttraining\\tset,\\tcreate\\tfour\\tshifted\\tcopies\\t(one\\tper\\tdirection)\\tand\\tadd\\nthem\\tto\\tthe\\ttraining\\tset.\\tFinally,\\ttrain\\tyour\\tbest\\tmodel\\ton\\tthis\\texpanded\\ttraining\\tset\\tand\\tmeasure\\tits\\naccuracy\\ton\\tthe\\ttest\\tset.\\tYou\\tshould\\tobserve\\tthat\\tyour\\tmodel\\tperforms\\teven\\tbetter\\tnow!\\tThis\\ntechnique\\tof\\tartificially\\tgrowing\\tthe\\ttraining\\tset\\tis\\tcalled\\t\\ndata\\taugmentation\\n\\tor\\t\\ntraining\\tset\\nexpansion\\n.\\n3\\n.\\t\\nTackle\\tthe\\t\\nTitanic\\n\\tdataset.\\tA\\tgreat\\tplace\\tto\\tstart\\tis\\ton\\t\\nKaggle\\n.\\n4\\n.\\t\\nBuild\\ta\\tspam\\tclassifier\\t(a\\tmore\\tchallenging\\texercise):\\nDownload\\texamples\\tof\\tspam\\tand\\tham\\tfrom', 'Apache\\tSpamAssassin’s\\tpublic\\tdatasets\\n.\\nUnzip\\tthe\\tdatasets\\tand\\tfamiliarize\\tyourself\\twith\\tthe\\tdata\\tformat.\\nSplit\\tthe\\tdatasets\\tinto\\ta\\ttraining\\tset\\tand\\ta\\ttest\\tset.\\nWrite\\ta\\tdata\\tpreparation\\tpipeline\\tto\\tconvert\\teach\\temail\\tinto\\ta\\tfeature\\tvector.\\tYour\\tpreparation\\npipeline\\tshould\\ttransform\\tan\\temail\\tinto\\ta\\t(sparse)\\tvector\\tindicating\\tthe\\tpresence\\tor\\tabsence\\tof\\neach\\tpossible\\tword.\\tFor\\texample,\\tif\\tall\\temails\\tonly\\tever\\tcontain\\tfour\\twords,\\t“Hello,”\\t“how,”\\n“are,”\\t“you,”\\tthen\\tthe\\temail\\t“Hello\\tyou\\tHello\\tHello\\tyou”\\twould\\tbe\\tconverted\\tinto\\ta\\tvector\\t[1,\\n0,\\t0,\\t1]\\t(meaning\\t[“Hello”\\tis\\tpresent,\\t“how”\\tis\\tabsent,\\t“are”\\tis\\tabsent,\\t“you”\\tis\\tpresent]),\\tor\\n[3,\\t0,\\t0,\\t2]\\tif\\tyou\\tprefer\\tto\\tcount\\tthe\\tnumber\\tof\\toccurrences\\tof\\teach\\tword.\\nYou\\tmay\\twant\\tto\\tadd\\thyperparameters\\tto\\tyour\\tpreparation\\tpipeline\\tto\\tcontrol\\twhether\\tor\\tnot\\tto\\nstrip\\toff\\temail\\theaders,\\tconvert\\teach\\temail\\tto\\tlowercase,\\tremove\\tpunctuation,\\treplace\\tall\\tURLs\\nwith\\t“URL,”\\treplace\\tall\\tnumbers\\twith\\t“NUMBER,”\\tor\\teven\\t\\nperform\\t\\nstemming\\n\\t(i.e.,\\ttrim\\toff', 'word\\tendings;\\tthere\\tare\\tPython\\tlibraries\\tavailable\\tto\\tdo\\tthis).\\nThen\\ttry\\tout\\tseveral\\tclassifiers\\tand\\tsee\\tif\\tyou\\tcan\\tbuild\\ta\\tgreat\\tspam\\tclassifier,\\twith\\tboth\\thigh\\nrecall\\tand\\thigh\\tprecision.\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\tthe\\tonline\\tJupyter\\tnotebooks\\tat\\nhttps://github.com/ageron/handson-ml\\n.\\nBy\\tdefault\\tScikit-Learn\\tcaches\\tdownloaded\\tdatasets\\tin\\ta\\t\\ndirectory\\tcalled\\t\\n$HOME/scikit_learn_data\\n.\\nShuffling\\tmay\\tbe\\ta\\tbad\\tidea\\tin\\tsome\\tcontexts\\t—\\tfor\\texample,\\tif\\tyou\\tare\\tworking\\ton\\ttime\\tseries\\tdata\\t(such\\tas\\tstock\\tmarket\\tprices\\tor\\nweather\\tconditions).\\tWe\\twill\\texplore\\tthis\\tin\\tthe\\tnext\\tchapters.\\nBut\\tremember\\tthat\\tour\\tbrain\\tis\\ta\\tfantastic\\tpattern\\trecognition\\tsystem,\\tand\\tour\\tvisual\\tsystem\\tdoes\\ta\\tlot\\tof\\tcomplex\\tpreprocessing\\tbefore\\nany\\tinformation\\treaches\\tour\\tconsciousness,\\tso\\tthe\\tfact\\tthat\\tit\\tfeels\\tsimple\\tdoes\\tnot\\tmean\\tthat\\tit\\tis.\\n1\\n2\\n3\\n4', 'Scikit-Learn\\toffers\\ta\\tfew\\tother\\taveraging\\toptions\\tand\\tmultilabel\\tclassifier\\tmetrics;\\tsee\\tthe\\tdocumentation\\tfor\\tmore\\tdetails.\\nYou\\tcan\\tuse\\tthe\\t\\nshift()\\n\\tfunction\\tfrom\\tthe\\t\\nscipy.ndimage.interpolation\\n\\tmodule.\\tFor\\texample,\\t\\nshift(image,\\t[2,\\t1],\\tcval=0)\\n\\tshifts\\nthe\\timage\\t2\\tpixels\\tdown\\tand\\t1\\tpixel\\tto\\tthe\\tright.\\n4\\n5', 'Chapter\\t4.\\t\\nTraining\\tModels\\nSo\\t\\nfar\\twe\\thave\\ttreated\\tMachine\\tLearning\\tmodels\\tand\\ttheir\\ttraining\\talgorithms\\tmostly\\tlike\\tblack\\tboxes.\\tIf\\nyou\\twent\\tthrough\\tsome\\tof\\tthe\\texercises\\tin\\tthe\\tprevious\\tchapters,\\tyou\\tmay\\thave\\tbeen\\tsurprised\\tby\\thow\\nmuch\\tyou\\tcan\\tget\\tdone\\twithout\\tknowing\\tanything\\tabout\\twhat’s\\tunder\\tthe\\thood:\\tyou\\toptimized\\ta\\tregression\\nsystem,\\tyou\\timproved\\ta\\tdigit\\timage\\tclassifier,\\tand\\tyou\\teven\\tbuilt\\ta\\tspam\\tclassifier\\tfrom\\tscratch\\t—\\tall\\nthis\\twithout\\tknowing\\thow\\tthey\\tactually\\twork.\\tIndeed,\\tin\\tmany\\tsituations\\tyou\\tdon’t\\treally\\tneed\\tto\\tknow\\tthe\\nimplementation\\tdetails.\\nHowever,\\thaving\\ta\\tgood\\tunderstanding\\tof\\thow\\tthings\\twork\\tcan\\thelp\\tyou\\tquickly\\thome\\tin\\ton\\tthe\\nappropriate\\tmodel,\\tthe\\tright\\ttraining\\talgorithm\\tto\\tuse,\\tand\\ta\\tgood\\tset\\tof\\thyperparameters\\tfor\\tyour\\ttask.\\nUnderstanding\\twhat’s\\tunder\\tthe\\thood\\twill\\talso\\thelp\\tyou\\tdebug\\tissues\\tand\\tperform\\terror\\tanalysis\\tmore\\nefficiently.\\tLastly,\\tmost\\tof\\tthe\\ttopics\\tdiscussed\\tin\\tthis\\tchapter\\twill\\tbe\\tessential\\tin\\tunderstanding,\\tbuilding,', 'and\\ttraining\\tneural\\tnetworks\\t(discussed\\tin\\t\\nPart\\tII\\n\\tof\\tthis\\tbook).\\nIn\\tthis\\tchapter,\\twe\\twill\\tstart\\tby\\tlooking\\tat\\tthe\\t\\nLinear\\tRegression\\tmodel,\\tone\\tof\\tthe\\tsimplest\\tmodels\\tthere\\nis.\\tWe\\twill\\tdiscuss\\ttwo\\tvery\\tdifferent\\tways\\tto\\ttrain\\tit:\\nUsing\\ta\\tdirect\\t“closed-form”\\t\\nequation\\tthat\\tdirectly\\tcomputes\\tthe\\tmodel\\tparameters\\tthat\\tbest\\tfit\\tthe\\nmodel\\tto\\tthe\\ttraining\\tset\\t(i.e.,\\tthe\\tmodel\\tparameters\\tthat\\tminimize\\tthe\\tcost\\tfunction\\tover\\tthe\\ttraining\\nset).\\nUsing\\tan\\titerative\\toptimization\\tapproach,\\tcalled\\t\\nGradient\\tDescent\\t(GD),\\tthat\\tgradually\\ttweaks\\tthe\\nmodel\\tparameters\\tto\\tminimize\\tthe\\tcost\\tfunction\\t\\nover\\tthe\\ttraining\\tset,\\teventually\\tconverging\\tto\\tthe\\nsame\\tset\\tof\\tparameters\\tas\\tthe\\tfirst\\tmethod.\\tWe\\twill\\tlook\\tat\\ta\\tfew\\tvariants\\tof\\tGradient\\tDescent\\tthat\\nwe\\twill\\tuse\\tagain\\tand\\tagain\\twhen\\twe\\tstudy\\tneural\\tnetworks\\tin\\t\\nPart\\tII\\n:\\tBatch\\tGD,\\tMini-batch\\tGD,\\nand\\tStochastic\\tGD.\\nNext\\twe\\twill\\tlook\\tat\\t\\nPolynomial\\tRegression,\\ta\\tmore\\tcomplex\\tmodel\\tthat\\tcan\\tfit\\tnonlinear\\tdatasets.\\tSince', 'this\\tmodel\\thas\\tmore\\tparameters\\tthan\\tLinear\\tRegression,\\tit\\tis\\tmore\\tprone\\tto\\toverfitting\\tthe\\ttraining\\tdata,\\nso\\twe\\twill\\tlook\\tat\\thow\\tto\\tdetect\\twhether\\tor\\tnot\\tthis\\tis\\tthe\\tcase,\\tusing\\tlearning\\tcurves,\\tand\\tthen\\twe\\twill\\nlook\\tat\\tseveral\\tregularization\\ttechniques\\tthat\\tcan\\treduce\\tthe\\trisk\\tof\\toverfitting\\tthe\\ttraining\\tset.\\nFinally,\\twe\\twill\\tlook\\tat\\ttwo\\tmore\\tmodels\\tthat\\tare\\tcommonly\\tused\\tfor\\tclassification\\ttasks:\\tLogistic\\nRegression\\tand\\tSoftmax\\tRegression.\\nWARNING\\nThere\\twill\\tbe\\tquite\\ta\\tfew\\tmath\\tequations\\tin\\tthis\\tchapter,\\tusing\\tbasic\\tnotions\\tof\\tlinear\\talgebra\\tand\\tcalculus.\\tTo\\tunderstand\\tthese\\nequations,\\tyou\\twill\\tneed\\tto\\tknow\\twhat\\tvectors\\tand\\tmatrices\\tare,\\thow\\tto\\ttranspose\\tthem,\\twhat\\tthe\\tdot\\tproduct\\tis,\\twhat\\tmatrix\\ninverse\\tis,\\tand\\twhat\\tpartial\\tderivatives\\tare.\\tIf\\tyou\\tare\\tunfamiliar\\twith\\tthese\\tconcepts,\\tplease\\tgo\\tthrough\\tthe\\tlinear\\talgebra\\tand\\ncalculus\\tintroductory\\ttutorials\\tavailable\\tas\\tJupyter\\tnotebooks\\tin\\tthe\\tonline\\tsupplemental\\tmaterial.\\tFor\\tthose\\twho\\tare\\ttruly\\tallergic', 'to\\tmathematics,\\tyou\\tshould\\tstill\\tgo\\tthrough\\tthis\\tchapter\\tand\\tsimply\\tskip\\tthe\\tequations;\\thopefully,\\tthe\\ttext\\twill\\tbe\\tsufficient\\tto\\thelp\\nyou\\tunderstand\\tmost\\tof\\tthe\\t\\nconcepts.', 'Linear\\tRegression\\nIn\\t\\nChapter\\t1\\n,\\t\\nwe\\tlooked\\tat\\ta\\tsimple\\tregression\\tmodel\\tof\\tlife\\tsatisfaction:\\t\\nlife_satisfaction\\n\\t=\\t\\nθ\\n0\\n\\t+\\t\\nθ\\n1\\n\\t×\\nGDP_per_capita\\n.\\nThis\\tmodel\\tis\\tjust\\ta\\tlinear\\tfunction\\tof\\tthe\\tinput\\tfeature\\t\\nGDP_per_capita\\n.\\t\\nθ\\n0\\n\\tand\\t\\nθ\\n1\\n\\tare\\tthe\\tmodel’s\\nparameters.\\nMore\\tgenerally,\\ta\\tlinear\\tmodel\\tmakes\\ta\\tprediction\\tby\\tsimply\\tcomputing\\ta\\tweighted\\tsum\\tof\\tthe\\tinput\\nfeatures,\\tplus\\ta\\tconstant\\tcalled\\t\\nthe\\t\\nbias\\tterm\\n\\t(also\\tcalled\\tthe\\t\\nintercept\\tterm\\n),\\tas\\tshown\\tin\\t\\nEquation\\t4-1\\n.\\nEquation\\t4-1.\\t\\nLinear\\tRegression\\tmodel\\tprediction\\nŷ\\n\\tis\\tthe\\tpredicted\\tvalue.\\nn\\n\\tis\\tthe\\tnumber\\tof\\tfeatures.\\nx\\ni\\n\\tis\\tthe\\ti\\nth\\n\\tfeature\\tvalue.\\nθ\\nj\\n\\tis\\tthe\\tj\\nth\\n\\tmodel\\tparameter\\t(including\\tthe\\tbias\\tterm\\t\\nθ\\n0\\n\\tand\\tthe\\tfeature\\tweights\\t\\nθ\\n1\\n,\\t\\nθ\\n2\\n,\\t,\\t\\nθ\\nn\\n).\\nThis\\tcan\\tbe\\twritten\\tmuch\\tmore\\tconcisely\\tusing\\ta\\tvectorized\\tform,\\tas\\tshown\\tin\\t\\nEquation\\t4-2\\n.\\nEquation\\t4-2.\\t\\nLinear\\tRegression\\tmodel\\tprediction\\t(vectorized\\tform)\\nθ\\n\\tis\\tthe\\tmodel’s\\t\\nparameter\\tvector\\n,\\t\\ncontaining\\tthe\\tbias\\tterm\\t\\nθ\\n0\\n\\tand\\tthe\\tfeature\\tweights\\t\\nθ\\n1\\n\\tto', 'θ\\n1\\n\\tto\\t\\nθ\\nn\\n.\\nθ\\nT\\n\\tis\\tthe\\ttranspose\\tof\\t\\nθ\\n\\t(a\\trow\\tvector\\tinstead\\tof\\ta\\tcolumn\\tvector).\\nx\\n\\tis\\tthe\\tinstance’s\\t\\nfeature\\tvector\\n,\\t\\ncontaining\\t\\nx\\n0\\n\\tto\\t\\nx\\nn\\n,\\twith\\t\\nx\\n0\\n\\talways\\tequal\\tto\\t1.\\nθ\\nT\\n\\t·\\t\\nx\\n\\tis\\tthe\\tdot\\tproduct\\tof\\t\\nθ\\nT\\n\\tand\\t\\nx\\n.\\nh\\nθ\\n\\tis\\tthe\\t\\nhypothesis\\tfunction,\\tusing\\tthe\\tmodel\\tparameters\\t\\nθ\\n.\\nOkay,\\tthat’s\\tthe\\tLinear\\tRegression\\tmodel,\\tso\\tnow\\thow\\tdo\\twe\\ttrain\\tit?\\tWell,\\trecall\\tthat\\ttraining\\ta\\tmodel\\nmeans\\tsetting\\tits\\tparameters\\tso\\tthat\\tthe\\tmodel\\tbest\\tfits\\tthe\\ttraining\\tset.\\tFor\\tthis\\tpurpose,\\twe\\tfirst\\tneed\\ta\\nmeasure\\tof\\thow\\twell\\t(or\\tpoorly)\\tthe\\tmodel\\tfits\\tthe\\ttraining\\tdata.\\tIn\\t\\nChapter\\t2\\n\\twe\\tsaw\\tthat\\tthe\\tmost\\ncommon\\tperformance\\tmeasure\\tof\\ta\\tregression\\tmodel\\tis\\tthe\\t\\nRoot\\tMean\\tSquare\\tError\\t(RMSE)\\t(\\nEquation\\n2-1\\n).\\tTherefore,\\tto\\ttrain\\ta\\tLinear\\tRegression\\tmodel,\\tyou\\tneed\\tto\\tfind\\tthe\\tvalue\\tof\\t\\nθ\\n\\tthat\\tminimizes\\tthe', 'RMSE.\\tIn\\tpractice,\\tit\\tis\\tsimpler\\tto\\tminimize\\tthe\\t\\nMean\\tSquare\\tError\\t(MSE)\\tthan\\tthe\\tRMSE,\\tand\\tit\\tleads\\nto\\tthe\\tsame\\tresult\\t(because\\tthe\\tvalue\\tthat\\tminimizes\\ta\\tfunction\\talso\\tminimizes\\tits\\tsquare\\troot).\\n1\\nThe\\tMSE\\tof\\ta\\tLinear\\tRegression\\thypothesis\\t\\nh\\nθ\\n\\ton\\ta\\ttraining\\tset\\t\\nX\\n\\tis\\tcalculated\\tusing\\t\\nEquation\\t4-3\\n.\\nEquation\\t4-3.\\t\\nMSE\\tcost\\tfunction\\tfor\\ta\\tLinear\\tRegression\\tmodel\\nMost\\tof\\tthese\\tnotations\\twere\\tpresented\\tin\\t\\nChapter\\t2\\n\\t(see\\t\\n“Notations”\\n).\\tThe\\tonly\\tdifference\\tis\\tthat\\twe\\nwrite\\t\\nh\\nθ\\n\\tinstead\\tof\\tjust\\t\\nh\\n\\tin\\torder\\tto\\tmake\\tit\\tclear\\tthat\\tthe\\tmodel\\tis\\tparametrized\\tby\\tthe\\tvector\\t\\nθ\\n.\\tTo\\nsimplify\\tnotations,\\twe\\twill\\tjust\\twrite\\tMSE(\\nθ\\n)\\tinstead\\tof\\tMSE(\\nX\\n,\\t\\nh\\nθ\\n).', 'The\\tNormal\\tEquation\\nTo\\t\\nfind\\tthe\\tvalue\\tof\\t\\nθ\\n\\tthat\\tminimizes\\tthe\\t\\ncost\\tfunction,\\tthere\\tis\\ta\\t\\nclosed-form\\tsolution\\n\\t—\\tin\\tother\\twords,\\na\\tmathematical\\tequation\\tthat\\tgives\\tthe\\tresult\\tdirectly.\\tThis\\tis\\tcalled\\tthe\\t\\nNormal\\tEquation\\n\\t(\\nEquation\\t4-4\\n).\\n2\\nEquation\\t4-4.\\t\\nNormal\\tEquation\\n\\tis\\tthe\\tvalue\\tof\\t\\n\\tthat\\tminimizes\\tthe\\tcost\\tfunction.\\ny\\n\\tis\\tthe\\tvector\\tof\\ttarget\\tvalues\\tcontaining\\t\\ny\\n(1)\\n\\tto\\t\\ny\\n(\\nm\\n)\\n.\\nLet’s\\tgenerate\\tsome\\tlinear-looking\\tdata\\tto\\ttest\\tthis\\tequation\\ton\\t(\\nFigure\\t4-1\\n):\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nX\\n\\t\\n=\\n\\t\\n2\\n\\t\\n*\\n\\t\\nnp\\n.\\nrandom\\n.\\nrand\\n(\\n100\\n,\\n\\t\\n1\\n)\\ny\\n\\t\\n=\\n\\t\\n4\\n\\t\\n+\\n\\t\\n3\\n\\t\\n*\\n\\t\\nX\\n\\t\\n+\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n100\\n,\\n\\t\\n1\\n)\\nFigure\\t4-1.\\t\\nRandomly\\tgenerated\\tlinear\\tdataset\\nNow\\tlet’s\\tcompute\\t\\n\\tusing\\tthe\\tNormal\\tEquation.\\tWe\\twill\\tuse\\tthe\\t\\ninv()\\n\\tfunction\\tfrom\\tNumPy’s\\tLinear\\nAlgebra\\tmodule\\t(\\nnp.linalg\\n)\\tto\\tcompute\\tthe\\tinverse\\tof\\ta\\tmatrix,\\tand\\tthe\\t\\ndot()\\n\\tmethod\\tfor\\tmatrix\\nmultiplication:\\nX_b\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\nnp\\n.\\nones\\n((\\n100\\n,\\n\\t\\n1\\n)),\\n\\t\\nX\\n]\\n\\t\\t\\n#\\tadd\\tx0\\t=\\t1\\tto\\teach\\tinstance\\ntheta_best\\n\\t\\n=\\n\\t\\nnp\\n.\\nlinalg\\n.\\ninv', '.\\ninv\\n(\\nX_b\\n.\\nT\\n.\\ndot\\n(\\nX_b\\n))\\n.\\ndot\\n(\\nX_b\\n.\\nT\\n)\\n.\\ndot\\n(\\ny\\n)', 'The\\tactual\\tfunction\\tthat\\twe\\tused\\tto\\tgenerate\\tthe\\tdata\\tis\\t\\ny\\n\\t=\\t4\\t+\\t3\\nx\\n0\\n\\t+\\tGaussian\\tnoise.\\tLet’s\\tsee\\twhat\\tthe\\nequation\\tfound:\\n>>>\\t\\ntheta_best\\narray([[\\t4.21509616],\\n\\t\\t\\t\\t\\t\\t\\t[\\t2.77011339]])\\nWe\\twould\\thave\\thoped\\tfor\\t\\nθ\\n0\\n\\t=\\t4\\tand\\t\\nθ\\n1\\n\\t=\\t3\\tinstead\\tof\\t\\nθ\\n0\\n\\t=\\t4.215\\tand\\t\\nθ\\n1\\n\\t=\\t2.770.\\tClose\\tenough,\\tbut\\tthe\\nnoise\\tmade\\tit\\timpossible\\tto\\trecover\\tthe\\texact\\tparameters\\tof\\tthe\\toriginal\\tfunction.\\nNow\\tyou\\tcan\\tmake\\tpredictions\\tusing\\t\\n:\\n>>>\\t\\nX_new\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([[\\n0\\n],\\n\\t\\n[\\n2\\n]])\\n>>>\\t\\nX_new_b\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\nnp\\n.\\nones\\n((\\n2\\n,\\n\\t\\n1\\n)),\\n\\t\\nX_new\\n]\\n\\t\\n#\\tadd\\tx0\\t=\\t1\\tto\\teach\\tinstance\\n>>>\\t\\ny_predict\\n\\t\\n=\\n\\t\\nX_new_b\\n.\\ndot\\n(\\ntheta_best\\n)\\n>>>\\t\\ny_predict\\narray([[\\t4.21509616],\\n\\t\\t\\t\\t\\t\\t\\t[\\t9.75532293]])\\nLet’s\\tplot\\tthis\\tmodel’s\\tpredictions\\t(\\nFigure\\t4-2\\n):\\nplt\\n.\\nplot\\n(\\nX_new\\n,\\n\\t\\ny_predict\\n,\\n\\t\\n\"r-\"\\n)\\nplt\\n.\\nplot\\n(\\nX\\n,\\n\\t\\ny\\n,\\n\\t\\n\"b.\"\\n)\\nplt\\n.\\naxis\\n([\\n0\\n,\\n\\t\\n2\\n,\\n\\t\\n0\\n,\\n\\t\\n15\\n])\\nplt\\n.\\nshow\\n()\\nFigure\\t4-2.\\t\\nLinear\\tRegression\\tmodel\\tpredictions\\nThe\\tequivalent\\tcode\\t\\nusing\\tScikit-Learn\\tlooks\\tlike\\tthis:\\n3\\n>>>\\t\\nfrom\\n\\t\\nsklearn.linear_model', 'import\\n\\t\\nLinearRegression\\n>>>\\t\\nlin_reg\\n\\t\\n=\\n\\t\\nLinearRegression\\n()\\n>>>\\t\\nlin_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\n>>>\\t\\nlin_reg\\n.\\nintercept_\\n,\\n\\t\\nlin_reg\\n.\\ncoef_\\n(array([\\t4.21509616]),\\tarray([[\\t2.77011339]]))\\n>>>\\t\\nlin_reg\\n.\\npredict\\n(\\nX_new\\n)', 'array([[\\t4.21509616],\\n\\t\\t\\t\\t\\t\\t\\t[\\t9.75532293]])', 'Computational\\tComplexity\\nThe\\t\\nNormal\\tEquation\\tcomputes\\tthe\\tinverse\\tof\\t\\nX\\nT\\n\\t·\\t\\nX\\n,\\twhich\\tis\\tan\\t\\nn\\n\\t×\\t\\nn\\n\\tmatrix\\t(where\\t\\nn\\n\\tis\\tthe\\tnumber\\tof\\nfeatures).\\tThe\\t\\ncomputational\\tcomplexity\\n\\tof\\tinverting\\tsuch\\ta\\tmatrix\\tis\\ttypically\\tabout\\t\\nO\\n(\\nn\\n2.4\\n)\\tto\\t\\nO\\n(\\nn\\n3\\n)\\n(depending\\ton\\tthe\\timplementation).\\tIn\\tother\\twords,\\tif\\tyou\\tdouble\\tthe\\tnumber\\tof\\tfeatures,\\tyou\\tmultiply\\tthe\\ncomputation\\ttime\\tby\\troughly\\t2\\n2.4\\n\\t=\\t5.3\\tto\\t2\\n3\\n\\t=\\t8.\\nWARNING\\nThe\\tNormal\\tEquation\\tgets\\tvery\\tslow\\twhen\\tthe\\tnumber\\tof\\tfeatures\\tgrows\\tlarge\\t(e.g.,\\t100,000).\\nOn\\tthe\\tpositive\\tside,\\tthis\\tequation\\tis\\tlinear\\twith\\tregards\\tto\\tthe\\tnumber\\tof\\tinstances\\tin\\tthe\\ttraining\\tset\\t(it\\tis\\nO\\n(\\nm\\n)),\\tso\\tit\\thandles\\tlarge\\ttraining\\tsets\\tefficiently,\\tprovided\\tthey\\tcan\\tfit\\tin\\tmemory.\\nAlso,\\tonce\\tyou\\thave\\ttrained\\tyour\\tLinear\\tRegression\\tmodel\\t(using\\tthe\\tNormal\\tEquation\\tor\\tany\\tother\\nalgorithm),\\tpredictions\\tare\\tvery\\tfast:\\tthe\\tcomputational\\tcomplexity\\tis\\tlinear\\twith\\tregards\\tto\\tboth\\tthe', 'number\\tof\\tinstances\\tyou\\twant\\tto\\tmake\\tpredictions\\ton\\tand\\tthe\\tnumber\\tof\\tfeatures.\\tIn\\tother\\twords,\\tmaking\\npredictions\\ton\\ttwice\\tas\\tmany\\tinstances\\t(or\\ttwice\\tas\\tmany\\tfeatures)\\twill\\tjust\\ttake\\troughly\\ttwice\\tas\\tmuch\\ntime.\\nNow\\twe\\twill\\tlook\\tat\\tvery\\tdifferent\\tways\\tto\\ttrain\\ta\\tLinear\\tRegression\\tmodel,\\tbetter\\tsuited\\tfor\\tcases\\nwhere\\tthere\\tare\\ta\\tlarge\\tnumber\\tof\\tfeatures,\\tor\\ttoo\\tmany\\ttraining\\tinstances\\tto\\tfit\\tin\\tmemory.', 'Gradient\\tDescent\\nGradient\\tDescent\\n\\tis\\ta\\t\\nvery\\tgeneric\\toptimization\\talgorithm\\t\\ncapable\\tof\\tfinding\\toptimal\\tsolutions\\tto\\ta\\twide\\nrange\\tof\\tproblems.\\tThe\\tgeneral\\tidea\\tof\\tGradient\\tDescent\\tis\\tto\\ttweak\\tparameters\\titeratively\\tin\\torder\\tto\\nminimize\\ta\\t\\ncost\\tfunction.\\nSuppose\\tyou\\tare\\tlost\\tin\\tthe\\tmountains\\tin\\ta\\tdense\\tfog;\\tyou\\tcan\\tonly\\tfeel\\tthe\\tslope\\tof\\tthe\\tground\\tbelow\\tyour\\nfeet.\\tA\\tgood\\tstrategy\\tto\\tget\\tto\\tthe\\tbottom\\tof\\tthe\\tvalley\\tquickly\\tis\\tto\\tgo\\tdownhill\\tin\\tthe\\tdirection\\tof\\tthe\\nsteepest\\tslope.\\tThis\\tis\\texactly\\twhat\\tGradient\\tDescent\\tdoes:\\tit\\tmeasures\\tthe\\tlocal\\tgradient\\tof\\tthe\\terror\\nfunction\\twith\\tregards\\tto\\tthe\\t\\nparameter\\tvector\\t\\nθ\\n,\\tand\\tit\\tgoes\\tin\\tthe\\tdirection\\tof\\tdescending\\tgradient.\\tOnce\\nthe\\tgradient\\tis\\tzero,\\tyou\\thave\\treached\\ta\\tminimum!\\nConcretely,\\tyou\\tstart\\tby\\tfilling\\t\\nθ\\n\\twith\\trandom\\tvalues\\t(this\\tis\\tcalled\\t\\nrandom\\tinitialization\\n),\\t\\nand\\tthen\\tyou\\nimprove\\tit\\tgradually,\\ttaking\\tone\\tbaby\\tstep\\tat\\ta\\ttime,\\teach\\tstep\\tattempting\\tto\\tdecrease\\tthe\\tcost\\tfunction\\n(e.g.,\\tthe\\tMSE),\\tuntil\\tthe\\talgorithm\\t\\nconverges', 'converges\\n\\tto\\ta\\tminimum\\t(see\\t\\nFigure\\t4-3\\n).\\nFigure\\t4-3.\\t\\nGradient\\tDescent\\nAn\\timportant\\tparameter\\tin\\tGradient\\tDescent\\tis\\tthe\\tsize\\tof\\tthe\\tsteps,\\tdetermined\\tby\\t\\nthe\\t\\nlearning\\trate\\nhyperparameter.\\tIf\\tthe\\tlearning\\trate\\tis\\ttoo\\tsmall,\\tthen\\tthe\\talgorithm\\twill\\thave\\tto\\tgo\\tthrough\\tmany\\titerations\\nto\\tconverge,\\twhich\\twill\\ttake\\ta\\tlong\\ttime\\t(see\\t\\nFigure\\t4-4\\n).', 'Figure\\t4-4.\\t\\nLearning\\trate\\ttoo\\tsmall\\nOn\\tthe\\tother\\thand,\\tif\\tthe\\tlearning\\trate\\tis\\ttoo\\thigh,\\tyou\\tmight\\tjump\\tacross\\tthe\\tvalley\\tand\\tend\\tup\\ton\\tthe\\tother\\nside,\\tpossibly\\teven\\thigher\\tup\\tthan\\tyou\\twere\\tbefore.\\tThis\\tmight\\tmake\\tthe\\talgorithm\\tdiverge,\\twith\\tlarger\\nand\\tlarger\\tvalues,\\tfailing\\tto\\tfind\\ta\\tgood\\tsolution\\t(see\\t\\nFigure\\t4-5\\n).\\nFigure\\t4-5.\\t\\nLearning\\trate\\ttoo\\tlarge\\nFinally,\\tnot\\tall\\tcost\\tfunctions\\tlook\\tlike\\tnice\\tregular\\tbowls.\\tThere\\tmay\\tbe\\tholes,\\tridges,\\tplateaus,\\tand\\tall\\nsorts\\tof\\tirregular\\tterrains,\\tmaking\\tconvergence\\tto\\tthe\\tminimum\\tvery\\tdifficult.\\t\\nFigure\\t4-6\\n\\tshows\\tthe\\ttwo\\nmain\\tchallenges\\twith\\tGradient\\tDescent:\\tif\\tthe\\trandom\\tinitialization\\tstarts\\tthe\\talgorithm\\ton\\tthe\\tleft,\\tthen\\tit\\nwill\\tconverge\\tto\\ta\\t\\nlocal\\tminimum\\n,\\twhich\\t\\nis\\tnot\\tas\\tgood\\tas\\tthe\\t\\nglobal\\tminimum\\n.\\tIf\\tit\\tstarts\\ton\\tthe\\tright,\\nthen\\tit\\twill\\ttake\\ta\\tvery\\tlong\\ttime\\tto\\tcross\\tthe\\tplateau,\\tand\\tif\\tyou\\tstop\\ttoo\\tearly\\tyou\\twill\\tnever\\treach\\tthe\\nglobal\\t\\nminimum.', 'Figure\\t4-6.\\t\\nGradient\\tDescent\\tpitfalls\\nFortunately,\\tthe\\tMSE\\tcost\\tfunction\\tfor\\ta\\t\\nLinear\\tRegression\\tmodel\\thappens\\tto\\tbe\\ta\\t\\nconvex\\tfunction\\n,\\t\\nwhich\\nmeans\\tthat\\tif\\tyou\\tpick\\tany\\ttwo\\tpoints\\ton\\tthe\\tcurve,\\tthe\\tline\\tsegment\\tjoining\\tthem\\tnever\\tcrosses\\tthe\\tcurve.\\nThis\\timplies\\tthat\\tthere\\tare\\tno\\tlocal\\tminima,\\tjust\\tone\\tglobal\\tminimum.\\tIt\\tis\\talso\\ta\\tcontinuous\\tfunction\\twith\\na\\tslope\\tthat\\tnever\\tchanges\\tabruptly.\\n4\\n\\t\\nThese\\ttwo\\tfacts\\thave\\ta\\tgreat\\tconsequence:\\tGradient\\tDescent\\tis\\nguaranteed\\tto\\tapproach\\tarbitrarily\\tclose\\tthe\\tglobal\\tminimum\\t(if\\tyou\\twait\\tlong\\tenough\\tand\\tif\\tthe\\tlearning\\nrate\\tis\\tnot\\ttoo\\thigh).\\nIn\\tfact,\\tthe\\tcost\\tfunction\\thas\\tthe\\tshape\\tof\\ta\\tbowl,\\tbut\\tit\\tcan\\tbe\\tan\\telongated\\tbowl\\tif\\tthe\\tfeatures\\thave\\tvery\\ndifferent\\tscales.\\t\\nFigure\\t4-7\\n\\tshows\\tGradient\\tDescent\\ton\\ta\\ttraining\\tset\\twhere\\tfeatures\\t1\\tand\\t2\\thave\\tthe\\nsame\\tscale\\t(on\\tthe\\tleft),\\tand\\ton\\ta\\ttraining\\tset\\twhere\\tfeature\\t1\\thas\\tmuch\\tsmaller\\tvalues\\tthan\\tfeature\\t2\\t(on\\nthe\\tright).\\n5\\nFigure\\t4-7.\\t\\nGradient\\tDescent\\twith\\tand\\twithout\\tfeature\\tscaling', 'As\\tyou\\tcan\\tsee,\\ton\\tthe\\tleft\\tthe\\tGradient\\tDescent\\talgorithm\\tgoes\\tstraight\\ttoward\\tthe\\tminimum,\\tthereby\\nreaching\\tit\\tquickly,\\twhereas\\ton\\tthe\\tright\\tit\\tfirst\\tgoes\\tin\\ta\\tdirection\\talmost\\torthogonal\\tto\\tthe\\tdirection\\tof\\nthe\\tglobal\\tminimum,\\tand\\tit\\tends\\twith\\ta\\tlong\\tmarch\\tdown\\tan\\talmost\\tflat\\tvalley.\\tIt\\twill\\teventually\\treach\\tthe\\nminimum,\\tbut\\tit\\twill\\ttake\\ta\\tlong\\ttime.', 'WARNING\\nWhen\\tusing\\tGradient\\tDescent,\\tyou\\tshould\\tensure\\tthat\\tall\\tfeatures\\thave\\ta\\tsimilar\\tscale\\t(e.g.,\\tusing\\tScikit-Learn’s\\t\\nStandardScaler\\nclass),\\tor\\t\\nelse\\tit\\twill\\ttake\\tmuch\\tlonger\\tto\\tconverge.\\nThis\\tdiagram\\talso\\tillustrates\\tthe\\tfact\\tthat\\ttraining\\ta\\tmodel\\tmeans\\tsearching\\tfor\\ta\\tcombination\\tof\\tmodel\\nparameters\\t\\nthat\\tminimizes\\ta\\tcost\\tfunction\\t(over\\tthe\\ttraining\\tset).\\tIt\\tis\\ta\\tsearch\\tin\\tthe\\tmodel’s\\t\\nparameter\\nspace\\n:\\tthe\\tmore\\t\\nparameters\\ta\\tmodel\\thas,\\tthe\\tmore\\tdimensions\\tthis\\tspace\\thas,\\tand\\tthe\\tharder\\tthe\\tsearch\\tis:\\nsearching\\tfor\\ta\\tneedle\\tin\\ta\\t300-dimensional\\thaystack\\tis\\tmuch\\ttrickier\\tthan\\tin\\tthree\\tdimensions.\\nFortunately,\\tsince\\tthe\\tcost\\tfunction\\tis\\tconvex\\tin\\tthe\\tcase\\tof\\tLinear\\tRegression,\\tthe\\tneedle\\tis\\tsimply\\tat\\tthe\\nbottom\\tof\\tthe\\tbowl.', 'Batch\\tGradient\\tDescent\\nTo\\t\\nimplement\\tGradient\\tDescent,\\tyou\\tneed\\tto\\tcompute\\tthe\\tgradient\\tof\\tthe\\tcost\\tfunction\\t\\nwith\\tregards\\tto\\neach\\tmodel\\tparameter\\t\\nθ\\nj\\n.\\tIn\\tother\\twords,\\tyou\\tneed\\tto\\tcalculate\\thow\\tmuch\\tthe\\tcost\\tfunction\\twill\\tchange\\tif\\nyou\\tchange\\t\\nθ\\nj\\n\\tjust\\ta\\tlittle\\tbit.\\tThis\\tis\\tcalled\\t\\na\\t\\npartial\\tderivative\\n.\\tIt\\tis\\tlike\\tasking\\t“what\\tis\\tthe\\tslope\\tof\\tthe\\nmountain\\tunder\\tmy\\tfeet\\tif\\tI\\tface\\teast?”\\tand\\tthen\\tasking\\tthe\\tsame\\tquestion\\tfacing\\tnorth\\t(and\\tso\\ton\\tfor\\tall\\nother\\tdimensions,\\tif\\tyou\\tcan\\timagine\\ta\\tuniverse\\twith\\tmore\\tthan\\tthree\\tdimensions).\\t\\nEquation\\t4-5\\n\\tcomputes\\nthe\\tpartial\\tderivative\\tof\\tthe\\tcost\\tfunction\\twith\\tregards\\tto\\tparameter\\t\\nθ\\nj\\n,\\tnoted\\t\\n.\\nEquation\\t4-5.\\t\\nPartial\\tderivatives\\tof\\tthe\\tcost\\tfunction\\nInstead\\tof\\tcomputing\\tthese\\tpartial\\tderivatives\\tindividually,\\tyou\\tcan\\tuse\\t\\nEquation\\t4-6\\n\\tto\\tcompute\\tthem\\tall\\nin\\tone\\tgo.\\tThe\\tgradient\\tvector,\\tnoted\\t\\nθ\\nMSE(\\nθ\\n),\\tcontains\\tall\\tthe\\tpartial\\tderivatives\\tof\\tthe\\tcost\\tfunction\\n(one\\tfor\\teach\\tmodel\\tparameter).\\nEquation\\t4-6.', 'Gradient\\tvector\\tof\\tthe\\tcost\\tfunction\\nWARNING\\nNotice\\tthat\\tthis\\tformula\\tinvolves\\tcalculations\\tover\\tthe\\tfull\\ttraining\\tset\\t\\nX\\n,\\tat\\teach\\tGradient\\tDescent\\tstep!\\tThis\\tis\\twhy\\tthe\\talgorithm\\nis\\tcalled\\t\\nBatch\\tGradient\\tDescent\\n:\\tit\\tuses\\tthe\\twhole\\tbatch\\tof\\ttraining\\tdata\\tat\\tevery\\tstep.\\tAs\\ta\\tresult\\tit\\tis\\tterribly\\tslow\\ton\\tvery\\nlarge\\ttraining\\tsets\\t(but\\twe\\twill\\tsee\\tmuch\\tfaster\\tGradient\\tDescent\\talgorithms\\tshortly).\\tHowever,\\tGradient\\tDescent\\tscales\\twell\\nwith\\tthe\\tnumber\\tof\\tfeatures;\\ttraining\\ta\\tLinear\\tRegression\\tmodel\\twhen\\tthere\\tare\\thundreds\\tof\\tthousands\\tof\\tfeatures\\tis\\tmuch\\tfaster\\nusing\\tGradient\\tDescent\\tthan\\tusing\\tthe\\tNormal\\tEquation.\\nOnce\\tyou\\thave\\tthe\\tgradient\\tvector,\\twhich\\tpoints\\tuphill,\\tjust\\tgo\\tin\\tthe\\topposite\\tdirection\\tto\\tgo\\tdownhill.\\nThis\\tmeans\\tsubtracting\\t\\nθ\\nMSE(\\nθ\\n)\\tfrom\\t\\nθ\\n.\\tThis\\tis\\twhere\\tthe\\t\\nlearning\\trate\\t\\nη\\n\\tcomes\\tinto\\tplay:\\n6\\n\\tmultiply\\tthe\\ngradient\\tvector\\tby\\t\\nη\\n\\tto\\tdetermine\\tthe\\tsize\\tof\\tthe\\tdownhill\\tstep\\t(\\nEquation\\t4-7\\n).\\nEquation\\t4-7.\\t\\nGradient\\tDescent\\tstep', 'Let’s\\tlook\\tat\\ta\\tquick\\timplementation\\tof\\tthis\\talgorithm:\\neta\\n\\t\\n=\\n\\t\\n0.1\\n\\t\\t\\n#\\tlearning\\trate\\nn_iterations\\n\\t\\n=\\n\\t\\n1000\\nm\\n\\t\\n=\\n\\t\\n100\\ntheta\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n2\\n,\\n1\\n)\\n\\t\\t\\n#\\trandom\\tinitialization\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\ngradients\\n\\t\\n=\\n\\t\\n2\\n/\\nm\\n\\t\\n*\\n\\t\\nX_b\\n.\\nT\\n.\\ndot\\n(\\nX_b\\n.\\ndot\\n(\\ntheta\\n)\\n\\t\\n-\\n\\t\\ny\\n)\\n\\t\\t\\t\\t\\ntheta\\n\\t\\n=\\n\\t\\ntheta\\n\\t\\n-\\n\\t\\neta\\n\\t\\n*\\n\\t\\ngradients\\nThat\\twasn’t\\ttoo\\thard!\\tLet’s\\t\\nlook\\tat\\tthe\\tresulting\\t\\ntheta\\n:\\n>>>\\t\\ntheta\\narray([[\\t4.21509616],\\n\\t\\t\\t\\t\\t\\t\\t[\\t2.77011339]])\\nHey,\\tthat’s\\texactly\\twhat\\tthe\\tNormal\\tEquation\\tfound!\\tGradient\\tDescent\\tworked\\tperfectly.\\tBut\\twhat\\tif\\tyou\\nhad\\tused\\ta\\tdifferent\\tlearning\\trate\\t\\neta\\n?\\t\\nFigure\\t4-8\\n\\tshows\\tthe\\tfirst\\t10\\tsteps\\tof\\tGradient\\tDescent\\tusing\\tthree\\ndifferent\\tlearning\\trates\\t(the\\tdashed\\tline\\trepresents\\tthe\\tstarting\\tpoint).\\nFigure\\t4-8.\\t\\nGradient\\tDescent\\twith\\tvarious\\tlearning\\trates\\nOn\\tthe\\tleft,\\tthe\\tlearning\\trate\\tis\\ttoo\\tlow:\\tthe\\talgorithm\\twill\\teventually\\treach\\tthe\\tsolution,\\tbut\\tit\\twill\\ttake\\ta', 'long\\ttime.\\tIn\\tthe\\tmiddle,\\tthe\\tlearning\\trate\\tlooks\\tpretty\\tgood:\\tin\\tjust\\ta\\tfew\\titerations,\\tit\\thas\\talready\\nconverged\\tto\\tthe\\tsolution.\\tOn\\tthe\\tright,\\tthe\\tlearning\\trate\\tis\\ttoo\\thigh:\\tthe\\talgorithm\\tdiverges,\\tjumping\\tall\\nover\\tthe\\tplace\\tand\\tactually\\tgetting\\tfurther\\tand\\tfurther\\taway\\tfrom\\tthe\\tsolution\\tat\\tevery\\tstep.\\nTo\\tfind\\ta\\tgood\\tlearning\\trate,\\tyou\\tcan\\tuse\\tgrid\\tsearch\\t(see\\t\\nChapter\\t2\\n).\\tHowever,\\tyou\\tmay\\twant\\tto\\tlimit\\tthe\\nnumber\\tof\\titerations\\tso\\tthat\\tgrid\\tsearch\\tcan\\teliminate\\tmodels\\tthat\\ttake\\ttoo\\tlong\\tto\\tconverge.\\nYou\\tmay\\twonder\\thow\\tto\\tset\\tthe\\tnumber\\tof\\titerations.\\tIf\\tit\\tis\\ttoo\\tlow,\\tyou\\twill\\tstill\\tbe\\tfar\\taway\\tfrom\\tthe\\noptimal\\tsolution\\twhen\\tthe\\talgorithm\\tstops,\\tbut\\tif\\tit\\tis\\ttoo\\thigh,\\tyou\\twill\\twaste\\ttime\\twhile\\tthe\\tmodel\\nparameters\\t\\ndo\\tnot\\tchange\\tanymore.\\tA\\tsimple\\tsolution\\tis\\tto\\tset\\ta\\tvery\\tlarge\\tnumber\\tof\\titerations\\tbut\\tto\\ninterrupt\\tthe\\talgorithm\\twhen\\tthe\\tgradient\\tvector\\tbecomes\\ttiny\\t—\\tthat\\tis,\\twhen\\tits\\tnorm\\tbecomes\\tsmaller\\nthan\\ta\\ttiny\\tnumber\\t\\nϵ\\n\\t(called\\tthe\\t\\ntolerance', 'tolerance\\n)\\t—\\tbecause\\tthis\\thappens\\twhen\\tGradient\\tDescent\\thas\\t(almost)', 'reached\\tthe\\tminimum.\\nCONVERGENCE\\tRATE\\nWhen\\tthe\\t\\ncost\\tfunction\\tis\\tconvex\\tand\\tits\\tslope\\tdoes\\tnot\\tchange\\tabruptly\\t(as\\tis\\tthe\\tcase\\tfor\\tthe\\tMSE\\tcost\\tfunction),\\t\\nit\\tcan\\tbe\\tshown\\tthat\\nBatch\\tGradient\\tDescent\\twith\\ta\\tfixed\\tlearning\\trate\\thas\\ta\\t\\nconvergence\\trate\\n\\tof\\t\\n.\\tIn\\tother\\twords,\\tif\\tyou\\tdivide\\tthe\\ntolerance\\t\\nϵ\\n\\tby\\t10\\t(to\\thave\\ta\\tmore\\tprecise\\tsolution),\\tthen\\tthe\\talgorithm\\twill\\thave\\tto\\trun\\tabout\\t10\\ttimes\\tmore\\t\\niterations.', 'Stochastic\\tGradient\\tDescent\\nThe\\t\\nmain\\tproblem\\twith\\tBatch\\tGradient\\tDescent\\tis\\tthe\\tfact\\tthat\\tit\\tuses\\tthe\\twhole\\ttraining\\tset\\tto\\tcompute\\nthe\\tgradients\\tat\\tevery\\tstep,\\twhich\\tmakes\\tit\\tvery\\tslow\\twhen\\tthe\\ttraining\\tset\\tis\\tlarge.\\tAt\\tthe\\topposite\\nextreme,\\t\\nStochastic\\tGradient\\tDescent\\n\\tjust\\tpicks\\ta\\trandom\\tinstance\\tin\\tthe\\ttraining\\tset\\tat\\tevery\\tstep\\tand\\ncomputes\\tthe\\tgradients\\tbased\\tonly\\ton\\tthat\\tsingle\\tinstance.\\tObviously\\tthis\\tmakes\\tthe\\talgorithm\\tmuch\\tfaster\\nsince\\tit\\thas\\tvery\\tlittle\\tdata\\tto\\tmanipulate\\tat\\tevery\\titeration.\\tIt\\talso\\tmakes\\tit\\tpossible\\tto\\ttrain\\ton\\thuge\\ntraining\\tsets,\\tsince\\tonly\\tone\\tinstance\\tneeds\\tto\\tbe\\tin\\tmemory\\tat\\teach\\titeration\\t(SGD\\tcan\\tbe\\timplemented\\tas\\nan\\tout-of-core\\talgorithm.\\n7\\n)\\nOn\\tthe\\tother\\thand,\\tdue\\tto\\tits\\tstochastic\\t(i.e.,\\trandom)\\tnature,\\tthis\\talgorithm\\tis\\tmuch\\tless\\tregular\\tthan\\tBatch\\nGradient\\tDescent:\\tinstead\\tof\\tgently\\tdecreasing\\tuntil\\tit\\treaches\\tthe\\tminimum,\\tthe\\tcost\\tfunction\\twill\\tbounce', 'up\\tand\\tdown,\\tdecreasing\\tonly\\ton\\taverage.\\tOver\\ttime\\tit\\twill\\tend\\tup\\tvery\\tclose\\tto\\tthe\\tminimum,\\tbut\\tonce\\tit\\ngets\\tthere\\tit\\twill\\tcontinue\\tto\\tbounce\\taround,\\tnever\\tsettling\\tdown\\t(see\\t\\nFigure\\t4-9\\n).\\tSo\\tonce\\tthe\\talgorithm\\nstops,\\tthe\\tfinal\\tparameter\\tvalues\\tare\\tgood,\\tbut\\tnot\\toptimal.\\nFigure\\t4-9.\\t\\nStochastic\\tGradient\\tDescent\\nWhen\\tthe\\tcost\\tfunction\\tis\\tvery\\tirregular\\t(as\\tin\\t\\nFigure\\t4-6\\n),\\tthis\\tcan\\tactually\\thelp\\tthe\\talgorithm\\tjump\\tout\\tof\\nlocal\\tminima,\\tso\\tStochastic\\tGradient\\tDescent\\thas\\ta\\tbetter\\tchance\\tof\\tfinding\\tthe\\tglobal\\tminimum\\tthan\\nBatch\\tGradient\\tDescent\\tdoes.\\nTherefore\\trandomness\\tis\\tgood\\tto\\tescape\\tfrom\\tlocal\\toptima,\\tbut\\tbad\\tbecause\\tit\\tmeans\\tthat\\tthe\\talgorithm\\ncan\\tnever\\tsettle\\tat\\tthe\\tminimum.\\tOne\\tsolution\\tto\\tthis\\tdilemma\\tis\\tto\\tgradually\\treduce\\tthe\\t\\nlearning\\trate.\\tThe\\nsteps\\tstart\\tout\\tlarge\\t(which\\thelps\\tmake\\tquick\\tprogress\\tand\\tescape\\tlocal\\tminima),\\tthen\\tget\\tsmaller\\tand\\nsmaller,\\tallowing\\tthe\\talgorithm\\tto\\tsettle\\tat\\tthe\\tglobal\\tminimum.\\tThis\\tprocess\\tis\\tcalled\\n\\t\\nsimulated', 'annealing\\n,\\tbecause\\tit\\tresembles\\tthe\\tprocess\\tof\\tannealing\\tin\\tmetallurgy\\twhere\\tmolten\\tmetal\\tis\\tslowly\\ncooled\\tdown.\\tThe\\tfunction\\tthat\\tdetermines\\tthe\\tlearning\\trate\\tat\\teach\\titeration\\tis\\tcalled\\tthe\\t\\nlearning\\nschedule\\n.\\t\\nIf\\tthe\\tlearning\\trate\\tis\\treduced\\ttoo\\tquickly,\\tyou\\tmay\\tget\\tstuck\\tin\\ta\\tlocal\\tminimum,\\tor\\teven\\tend\\tup\\nfrozen\\thalfway\\tto\\tthe\\tminimum.\\tIf\\tthe\\tlearning\\trate\\tis\\treduced\\ttoo\\tslowly,\\tyou\\tmay\\tjump\\taround\\tthe\\nminimum\\tfor\\ta\\tlong\\ttime\\tand\\tend\\tup\\twith\\ta\\tsuboptimal\\tsolution\\tif\\tyou\\thalt\\ttraining\\ttoo\\tearly.\\nThis\\tcode\\timplements\\tStochastic\\tGradient\\tDescent\\tusing\\ta\\t\\nsimple\\tlearning\\tschedule:\\nn_epochs\\n\\t\\n=\\n\\t\\n50\\nt0\\n,\\n\\t\\nt1\\n\\t\\n=\\n\\t\\n5\\n,\\n\\t\\n50\\n\\t\\t\\n#\\tlearning\\tschedule\\thyperparameters\\ndef\\n\\t\\nlearning_schedule\\n(\\nt\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nt0\\n\\t\\n/\\n\\t\\n(\\nt\\n\\t\\n+\\n\\t\\nt1\\n)\\ntheta\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n2\\n,\\n1\\n)\\n\\t\\t\\n#\\trandom\\tinitialization\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\nm\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nrandom_index\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandint\\n(\\nm\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nxi\\n\\t\\n=\\n\\t\\nX_b\\n[\\nrandom_index\\n:\\nrandom_index\\n+\\n1\\n]', '+\\n1\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nyi\\n\\t\\n=\\n\\t\\ny\\n[\\nrandom_index\\n:\\nrandom_index\\n+\\n1\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ngradients\\n\\t\\n=\\n\\t\\n2\\n\\t\\n*\\n\\t\\nxi\\n.\\nT\\n.\\ndot\\n(\\nxi\\n.\\ndot\\n(\\ntheta\\n)\\n\\t\\n-\\n\\t\\nyi\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\neta\\n\\t\\n=\\n\\t\\nlearning_schedule\\n(\\nepoch\\n\\t\\n*\\n\\t\\nm\\n\\t\\n+\\n\\t\\ni\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntheta\\n\\t\\n=\\n\\t\\ntheta\\n\\t\\n-\\n\\t\\neta\\n\\t\\n*\\n\\t\\ngradients\\nBy\\tconvention\\twe\\titerate\\tby\\trounds\\tof\\t\\nm\\n\\titerations;\\teach\\tround\\tis\\tcalled\\tan\\t\\nepoch\\n.\\t\\nWhile\\tthe\\tBatch\\nGradient\\tDescent\\tcode\\titerated\\t1,000\\ttimes\\tthrough\\tthe\\twhole\\ttraining\\tset,\\tthis\\tcode\\tgoes\\tthrough\\tthe\\ntraining\\tset\\tonly\\t50\\ttimes\\tand\\treaches\\ta\\tfairly\\tgood\\tsolution:\\n>>>\\t\\ntheta\\narray([[\\t4.21076011],\\n\\t\\t\\t\\t\\t\\t[\\t2.74856079]])\\nFigure\\t4-10\\n\\tshows\\tthe\\tfirst\\t10\\tsteps\\tof\\ttraining\\t(notice\\thow\\tirregular\\tthe\\tsteps\\tare).\\nFigure\\t4-10.\\t\\nStochastic\\tGradient\\tDescent\\tfirst\\t10\\tsteps', 'Note\\tthat\\tsince\\tinstances\\tare\\tpicked\\trandomly,\\tsome\\tinstances\\tmay\\tbe\\tpicked\\tseveral\\ttimes\\tper\\tepoch\\nwhile\\tothers\\tmay\\tnot\\tbe\\tpicked\\tat\\tall.\\tIf\\tyou\\twant\\tto\\tbe\\tsure\\tthat\\tthe\\talgorithm\\tgoes\\tthrough\\tevery\\tinstance\\nat\\teach\\tepoch,\\tanother\\tapproach\\tis\\tto\\tshuffle\\tthe\\ttraining\\tset,\\tthen\\tgo\\tthrough\\tit\\tinstance\\tby\\tinstance,\\tthen\\nshuffle\\tit\\tagain,\\tand\\tso\\ton.\\tHowever,\\tthis\\tgenerally\\tconverges\\tmore\\tslowly.\\nTo\\tperform\\t\\nLinear\\tRegression\\tusing\\tSGD\\twith\\t\\nScikit-Learn,\\tyou\\tcan\\tuse\\tthe\\t\\nSGDRegressor\\n\\tclass,\\twhich\\ndefaults\\tto\\toptimizing\\tthe\\tsquared\\terror\\tcost\\tfunction.\\tThe\\tfollowing\\tcode\\truns\\t50\\tepochs,\\tstarting\\twith\\ta\\nlearning\\trate\\tof\\t0.1\\t(\\neta0=0.1\\n),\\tusing\\tthe\\tdefault\\tlearning\\tschedule\\t(different\\tfrom\\tthe\\tpreceding\\tone),\\nand\\tit\\tdoes\\tnot\\tuse\\tany\\t\\nregularization\\t(\\npenalty=None\\n;\\tmore\\tdetails\\ton\\tthis\\tshortly):\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nSGDRegressor\\nsgd_reg\\n\\t\\n=\\n\\t\\nSGDRegressor\\n(\\nn_iter\\n=\\n50\\n,\\n\\t\\npenalty\\n=\\nNone\\n,\\n\\t\\neta0\\n=\\n0.1\\n)\\nsgd_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n.\\nravel\\n())', 'ravel\\n())\\nOnce\\tagain,\\tyou\\tfind\\ta\\tsolution\\tvery\\tclose\\tto\\tthe\\tone\\treturned\\tby\\tthe\\t\\nNormal\\tEquation:\\n>>>\\t\\nsgd_reg\\n.\\nintercept_\\n,\\n\\t\\nsgd_reg\\n.\\ncoef_\\n(array([\\t4.16782089]),\\tarray([\\t2.72603052]))', 'Mini-batch\\tGradient\\tDescent\\nThe\\t\\nlast\\tGradient\\tDescent\\talgorithm\\twe\\twill\\tlook\\tat\\tis\\tcalled\\t\\nMini-batch\\tGradient\\tDescent\\n.\\tIt\\tis\\tquite\\nsimple\\tto\\tunderstand\\tonce\\tyou\\tknow\\tBatch\\tand\\tStochastic\\tGradient\\tDescent:\\tat\\teach\\tstep,\\tinstead\\tof\\ncomputing\\tthe\\tgradients\\tbased\\ton\\tthe\\tfull\\ttraining\\tset\\t(as\\tin\\tBatch\\tGD)\\tor\\tbased\\ton\\tjust\\tone\\tinstance\\t(as\\tin\\nStochastic\\tGD),\\tMini-batch\\tGD\\tcomputes\\tthe\\tgradients\\ton\\tsmall\\trandom\\tsets\\tof\\tinstances\\tcalled\\t\\nmini-\\nbatches\\n.\\tThe\\tmain\\tadvantage\\tof\\tMini-batch\\tGD\\tover\\tStochastic\\tGD\\tis\\tthat\\tyou\\tcan\\tget\\ta\\tperformance\\nboost\\tfrom\\thardware\\toptimization\\tof\\tmatrix\\toperations,\\tespecially\\twhen\\tusing\\tGPUs.\\nThe\\talgorithm’s\\tprogress\\tin\\tparameter\\tspace\\tis\\tless\\terratic\\tthan\\twith\\tSGD,\\tespecially\\twith\\tfairly\\tlarge\\nmini-batches.\\tAs\\ta\\tresult,\\tMini-batch\\tGD\\twill\\tend\\tup\\twalking\\taround\\ta\\tbit\\tcloser\\tto\\tthe\\tminimum\\tthan\\nSGD.\\tBut,\\ton\\tthe\\tother\\thand,\\tit\\tmay\\tbe\\tharder\\tfor\\tit\\tto\\tescape\\tfrom\\tlocal\\tminima\\t(in\\tthe\\tcase\\tof\\tproblems', 'that\\tsuffer\\tfrom\\tlocal\\tminima,\\tunlike\\tLinear\\tRegression\\tas\\twe\\tsaw\\tearlier).\\t\\nFigure\\t4-11\\n\\tshows\\tthe\\tpaths\\ntaken\\tby\\tthe\\tthree\\tGradient\\tDescent\\talgorithms\\tin\\tparameter\\tspace\\tduring\\ttraining.\\tThey\\tall\\tend\\tup\\tnear\\nthe\\tminimum,\\tbut\\tBatch\\tGD’s\\tpath\\tactually\\tstops\\tat\\tthe\\tminimum,\\twhile\\tboth\\tStochastic\\tGD\\tand\\tMini-\\nbatch\\tGD\\tcontinue\\tto\\twalk\\taround.\\tHowever,\\tdon’t\\tforget\\tthat\\tBatch\\tGD\\ttakes\\ta\\tlot\\tof\\ttime\\tto\\ttake\\teach\\nstep,\\tand\\tStochastic\\tGD\\tand\\tMini-batch\\tGD\\twould\\talso\\treach\\tthe\\tminimum\\tif\\tyou\\tused\\ta\\tgood\\tlearning\\nschedule.\\nFigure\\t4-11.\\t\\nGradient\\tDescent\\tpaths\\tin\\tparameter\\tspace\\nLet’s\\tcompare\\tthe\\talgorithms\\twe’ve\\tdiscussed\\tso\\tfar\\tfor\\tLinear\\tRegression\\n8\\n\\t(recall\\tthat\\t\\nm\\n\\tis\\tthe\\tnumber\\nof\\ttraining\\tinstances\\tand\\t\\nn\\n\\tis\\tthe\\tnumber\\tof\\tfeatures);\\tsee\\t\\nTable\\t4-1\\n.\\nTable\\t4-1.\\t\\nComparison\\tof\\talgorithms\\tfor\\tLinear\\tRegression\\nAlgorithm\\nLarge\\t\\nm\\nOut-of-core\\tsupport\\nLarge\\t\\nn\\nHyperparams\\nScaling\\trequired\\nScikit-Learn\\nNormal\\tEquation\\nFast\\nNo\\nSlow\\n0\\nNo\\nLinearRegression\\nBatch\\tGD\\nSlow\\nNo\\nFast\\n2\\nYes\\nn/a', 'Stochastic\\tGD\\nFast\\nYes\\nFast\\n≥2\\nYes\\nSGDRegressor\\nMini-batch\\tGD\\nFast\\nYes\\nFast\\n≥2\\nYes\\nn/a\\nNOTE\\nThere\\tis\\talmost\\tno\\tdifference\\tafter\\ttraining:\\tall\\tthese\\talgorithms\\tend\\tup\\twith\\tvery\\tsimilar\\tmodels\\tand\\tmake\\tpredictions\\tin\\texactly\\nthe\\tsame\\t\\nway.', 'Polynomial\\tRegression\\nWhat\\t\\nif\\tyour\\tdata\\tis\\tactually\\tmore\\tcomplex\\tthan\\ta\\tsimple\\tstraight\\tline?\\tSurprisingly,\\tyou\\tcan\\tactually\\tuse\\na\\tlinear\\tmodel\\tto\\tfit\\tnonlinear\\tdata.\\tA\\tsimple\\tway\\tto\\tdo\\tthis\\tis\\tto\\tadd\\tpowers\\tof\\teach\\tfeature\\tas\\tnew\\nfeatures,\\tthen\\ttrain\\ta\\tlinear\\tmodel\\ton\\tthis\\textended\\tset\\tof\\tfeatures.\\tThis\\ttechnique\\tis\\tcalled\\t\\nPolynomial\\nRegression\\n.\\nLet’s\\tlook\\tat\\tan\\texample.\\tFirst,\\tlet’s\\tgenerate\\tsome\\tnonlinear\\tdata,\\tbased\\ton\\ta\\tsimple\\t\\nquadratic\\tequation\\n9\\n(plus\\tsome\\tnoise;\\tsee\\t\\nFigure\\t4-12\\n):\\nm\\n\\t\\n=\\n\\t\\n100\\nX\\n\\t\\n=\\n\\t\\n6\\n\\t\\n*\\n\\t\\nnp\\n.\\nrandom\\n.\\nrand\\n(\\nm\\n,\\n\\t\\n1\\n)\\n\\t\\n-\\n\\t\\n3\\ny\\n\\t\\n=\\n\\t\\n0.5\\n\\t\\n*\\n\\t\\nX\\n**\\n2\\n\\t\\n+\\n\\t\\nX\\n\\t\\n+\\n\\t\\n2\\n\\t\\n+\\n\\t\\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\nm\\n,\\n\\t\\n1\\n)\\nFigure\\t4-12.\\t\\nGenerated\\tnonlinear\\tand\\tnoisy\\tdataset\\nClearly,\\ta\\tstraight\\tline\\twill\\tnever\\tfit\\tthis\\tdata\\tproperly.\\tSo\\tlet’s\\tuse\\tScikit-Learn’s\\t\\nPolynomialFeatures\\nclass\\t\\nto\\ttransform\\tour\\ttraining\\tdata,\\tadding\\tthe\\tsquare\\t(2\\nnd\\n-degree\\tpolynomial)\\tof\\teach\\tfeature\\tin\\tthe\\ntraining\\tset\\tas\\tnew\\tfeatures\\t(in\\tthis\\tcase\\tthere\\tis\\tjust\\tone\\tfeature):\\n>>>\\t\\nfrom', 'from\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nPolynomialFeatures\\n>>>\\t\\npoly_features\\n\\t\\n=\\n\\t\\nPolynomialFeatures\\n(\\ndegree\\n=\\n2\\n,\\n\\t\\ninclude_bias\\n=\\nFalse\\n)\\n>>>\\t\\nX_poly\\n\\t\\n=\\n\\t\\npoly_features\\n.\\nfit_transform\\n(\\nX\\n)\\n>>>\\t\\nX\\n[\\n0\\n]\\narray([-0.75275929])\\n>>>\\t\\nX_poly\\n[\\n0\\n]\\narray([-0.75275929,\\t\\t0.56664654])\\nX_poly\\n\\tnow\\tcontains\\tthe\\toriginal\\tfeature\\tof\\t\\nX\\n\\tplus\\tthe\\tsquare\\tof\\tthis\\tfeature.\\tNow\\tyou\\tcan\\tfit\\ta\\nLinearRegression\\n\\t\\nmodel\\tto\\tthis\\textended\\ttraining\\tdata\\t(\\nFigure\\t4-13\\n):', '>>>\\t\\nlin_reg\\n\\t\\n=\\n\\t\\nLinearRegression\\n()\\n>>>\\t\\nlin_reg\\n.\\nfit\\n(\\nX_poly\\n,\\n\\t\\ny\\n)\\n>>>\\t\\nlin_reg\\n.\\nintercept_\\n,\\n\\t\\nlin_reg\\n.\\ncoef_\\n(array([\\t1.78134581]),\\tarray([[\\t0.93366893,\\t\\t0.56456263]]))\\nFigure\\t4-13.\\t\\nPolynomial\\tRegression\\tmodel\\tpredictions\\nNot\\tbad:\\tthe\\tmodel\\testimates\\t\\n\\twhen\\tin\\tfact\\tthe\\toriginal\\tfunction\\twas\\t\\n.\\nNote\\tthat\\twhen\\tthere\\tare\\tmultiple\\tfeatures,\\tPolynomial\\tRegression\\tis\\tcapable\\tof\\tfinding\\trelationships\\nbetween\\tfeatures\\t(which\\tis\\tsomething\\ta\\tplain\\tLinear\\tRegression\\tmodel\\tcannot\\tdo).\\tThis\\tis\\tmade\\tpossible\\nby\\tthe\\tfact\\tthat\\t\\nPolynomialFeatures\\n\\talso\\tadds\\tall\\tcombinations\\tof\\tfeatures\\tup\\tto\\tthe\\tgiven\\tdegree.\\tFor\\nexample,\\tif\\tthere\\twere\\ttwo\\tfeatures\\t\\na\\n\\tand\\t\\nb\\n,\\t\\nPolynomialFeatures\\n\\t\\nwith\\t\\ndegree=3\\n\\twould\\tnot\\tonly\\tadd\\nthe\\tfeatures\\t\\na\\n2\\n,\\t\\na\\n3\\n,\\t\\nb\\n2\\n,\\tand\\t\\nb\\n3\\n,\\tbut\\talso\\tthe\\tcombinations\\t\\nab\\n,\\t\\na\\n2\\nb\\n,\\tand\\t\\nab\\n2\\n.\\nWARNING\\nPolynomialFeatures(degree=d)\\n\\ttransforms\\tan\\tarray\\tcontaining\\t\\nn\\n\\tfeatures\\tinto\\tan\\tarray\\tcontaining\\t\\n\\tfeatures,\\twhere\\t\\nn\\n!\\nis\\tthe\\t\\nfactorial\\n\\tof\\t\\nn', 'of\\t\\nn\\n,\\tequal\\tto\\t1\\t×\\t2\\t×\\t3\\t×\\t\\t×\\t\\nn\\n.\\tBeware\\tof\\tthe\\tcombinatorial\\texplosion\\tof\\tthe\\tnumber\\tof\\t\\nfeatures!', 'Learning\\tCurves\\nIf\\tyou\\t\\nperform\\thigh-degree\\t\\nPolynomial\\tRegression,\\tyou\\twill\\tlikely\\tfit\\tthe\\ttraining\\tdata\\tmuch\\tbetter\\tthan\\nwith\\tplain\\tLinear\\tRegression.\\tFor\\texample,\\t\\nFigure\\t4-14\\n\\tapplies\\ta\\t300-degree\\tpolynomial\\tmodel\\tto\\tthe\\npreceding\\ttraining\\tdata,\\tand\\tcompares\\tthe\\tresult\\twith\\ta\\tpure\\tlinear\\tmodel\\tand\\ta\\tquadratic\\tmodel\\t(2\\nnd\\n-\\ndegree\\tpolynomial).\\tNotice\\thow\\tthe\\t300-degree\\tpolynomial\\tmodel\\twiggles\\taround\\tto\\tget\\tas\\tclose\\tas\\npossible\\tto\\tthe\\ttraining\\tinstances.\\nFigure\\t4-14.\\t\\nHigh-degree\\tPolynomial\\tRegression\\nOf\\tcourse,\\tthis\\thigh-degree\\tPolynomial\\tRegression\\tmodel\\tis\\tseverely\\toverfitting\\tthe\\ttraining\\tdata,\\twhile\\nthe\\tlinear\\tmodel\\tis\\tunderfitting\\tit.\\tThe\\tmodel\\tthat\\twill\\tgeneralize\\tbest\\tin\\tthis\\tcase\\tis\\tthe\\tquadratic\\tmodel.\\nIt\\tmakes\\tsense\\tsince\\tthe\\tdata\\twas\\tgenerated\\tusing\\ta\\tquadratic\\tmodel,\\tbut\\tin\\tgeneral\\tyou\\twon’t\\tknow\\twhat\\nfunction\\tgenerated\\tthe\\tdata,\\tso\\thow\\tcan\\tyou\\tdecide\\thow\\tcomplex\\tyour\\tmodel\\tshould\\tbe?\\tHow\\tcan\\tyou\\ttell\\nthat\\tyour\\tmodel\\tis\\toverfitting\\tor\\tunderfitting\\tthe\\tdata?\\nIn', 'In\\t\\nChapter\\t2\\n\\tyou\\tused\\tcross-validation\\tto\\tget\\tan\\testimate\\tof\\ta\\tmodel’s\\tgeneralization\\tperformance.\\tIf\\ta\\nmodel\\tperforms\\twell\\ton\\tthe\\ttraining\\tdata\\tbut\\tgeneralizes\\tpoorly\\taccording\\tto\\tthe\\tcross-validation\\tmetrics,\\nthen\\tyour\\tmodel\\tis\\toverfitting.\\tIf\\tit\\tperforms\\tpoorly\\ton\\tboth,\\tthen\\tit\\tis\\tunderfitting.\\tThis\\tis\\tone\\tway\\tto\\ttell\\nwhen\\ta\\tmodel\\tis\\ttoo\\tsimple\\tor\\ttoo\\tcomplex.\\nAnother\\tway\\tis\\tto\\tlook\\tat\\tthe\\t\\nlearning\\tcurves\\n:\\tthese\\tare\\tplots\\tof\\tthe\\tmodel’s\\tperformance\\ton\\tthe\\ttraining\\nset\\tand\\tthe\\tvalidation\\tset\\tas\\ta\\tfunction\\tof\\tthe\\ttraining\\tset\\tsize.\\tTo\\tgenerate\\tthe\\tplots,\\tsimply\\ttrain\\tthe\\tmodel\\nseveral\\ttimes\\ton\\tdifferent\\tsized\\tsubsets\\tof\\tthe\\ttraining\\tset.\\tThe\\tfollowing\\tcode\\tdefines\\ta\\tfunction\\tthat\\tplots\\nthe\\tlearning\\tcurves\\tof\\ta\\tmodel\\tgiven\\tsome\\t\\ntraining\\tdata:\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nmean_squared_error\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\ntrain_test_split\\ndef\\n\\t\\nplot_learning_curves\\n(\\nmodel\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n):', 'X_train\\n,\\n\\t\\nX_val\\n,\\n\\t\\ny_train\\n,\\n\\t\\ny_val\\n\\t\\n=\\n\\t\\ntrain_test_split\\n(\\nX\\n,\\n\\t\\ny\\n,\\n\\t\\ntest_size\\n=\\n0.2\\n)\\n\\t\\t\\t\\t\\ntrain_errors\\n,\\n\\t\\nval_errors\\n\\t\\n=\\n\\t\\n[],\\n\\t\\n[]\\n\\t\\t\\t\\t\\nfor\\n\\t\\nm\\n\\t\\nin\\n\\t\\nrange\\n(\\n1\\n,\\n\\t\\nlen\\n(\\nX_train\\n)):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nmodel\\n.\\nfit\\n(\\nX_train\\n[:\\nm\\n],\\n\\t\\ny_train\\n[:\\nm\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ny_train_predict\\n\\t\\n=\\n\\t\\nmodel\\n.\\npredict\\n(\\nX_train\\n[:\\nm\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ny_val_predict\\n\\t\\n=\\n\\t\\nmodel\\n.\\npredict\\n(\\nX_val\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntrain_errors\\n.\\nappend\\n(\\nmean_squared_error\\n(\\ny_train_predict\\n,\\n\\t\\ny_train\\n[:\\nm\\n]))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nval_errors\\n.\\nappend\\n(\\nmean_squared_error\\n(\\ny_val_predict\\n,\\n\\t\\ny_val\\n))\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n(\\nnp\\n.\\nsqrt\\n(\\ntrain_errors\\n),\\n\\t\\n\"r-+\"\\n,\\n\\t\\nlinewidth\\n=\\n2\\n,\\n\\t\\nlabel\\n=\\n\"train\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\nplot\\n(\\nnp\\n.\\nsqrt\\n(\\nval_errors\\n),\\n\\t\\n\"b-\"\\n,\\n\\t\\nlinewidth\\n=\\n3\\n,\\n\\t\\nlabel\\n=\\n\"val\"\\n)\\nLet’s\\tlook\\tat\\tthe\\tlearning\\tcurves\\tof\\tthe\\tplain\\tLinear\\tRegression\\tmodel\\t\\n(a\\tstraight\\tline;\\t\\nFigure\\t4-15\\n):\\nlin_reg\\n\\t\\n=\\n\\t\\nLinearRegression\\n()\\nplot_learning_curves\\n(\\nlin_reg\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n)\\nFigure\\t4-15.\\t\\nLearning\\tcurves', 'This\\tdeserves\\ta\\tbit\\tof\\texplanation.\\tFirst,\\tlet’s\\tlook\\tat\\tthe\\tperformance\\ton\\tthe\\ttraining\\tdata:\\twhen\\tthere\\tare\\njust\\tone\\tor\\ttwo\\tinstances\\tin\\tthe\\ttraining\\tset,\\tthe\\tmodel\\tcan\\tfit\\tthem\\tperfectly,\\twhich\\tis\\twhy\\tthe\\tcurve\\tstarts\\nat\\tzero.\\tBut\\tas\\tnew\\tinstances\\tare\\tadded\\tto\\tthe\\ttraining\\tset,\\tit\\tbecomes\\timpossible\\tfor\\tthe\\tmodel\\tto\\tfit\\tthe\\ntraining\\tdata\\tperfectly,\\tboth\\tbecause\\tthe\\tdata\\tis\\tnoisy\\tand\\tbecause\\tit\\tis\\tnot\\tlinear\\tat\\tall.\\tSo\\tthe\\terror\\ton\\tthe\\ntraining\\tdata\\tgoes\\tup\\tuntil\\tit\\treaches\\ta\\tplateau,\\tat\\twhich\\tpoint\\tadding\\tnew\\tinstances\\tto\\tthe\\ttraining\\tset\\ndoesn’t\\tmake\\tthe\\taverage\\terror\\tmuch\\tbetter\\tor\\tworse.\\tNow\\tlet’s\\tlook\\tat\\tthe\\tperformance\\tof\\tthe\\tmodel\\ton\\nthe\\tvalidation\\tdata.\\tWhen\\tthe\\tmodel\\tis\\ttrained\\ton\\tvery\\tfew\\ttraining\\tinstances,\\tit\\tis\\tincapable\\tof\\ngeneralizing\\tproperly,\\twhich\\tis\\twhy\\tthe\\tvalidation\\terror\\tis\\tinitially\\tquite\\tbig.\\tThen\\tas\\tthe\\tmodel\\tis\\tshown\\nmore\\ttraining\\texamples,\\tit\\tlearns\\tand\\tthus\\tthe\\tvalidation\\terror\\tslowly\\tgoes\\tdown.\\tHowever,\\tonce\\tagain\\ta', 'straight\\tline\\tcannot\\tdo\\ta\\tgood\\tjob\\tmodeling\\tthe\\tdata,\\tso\\tthe\\terror\\tends\\tup\\tat\\ta\\tplateau,\\tvery\\tclose\\tto\\tthe\\nother\\tcurve.\\nThese\\tlearning\\tcurves\\tare\\ttypical\\tof\\tan\\tunderfitting\\tmodel.\\tBoth\\tcurves\\thave\\treached\\ta\\tplateau;\\tthey\\tare\\nclose\\tand\\tfairly\\thigh.', 'TIP\\nIf\\tyour\\tmodel\\tis\\tunderfitting\\tthe\\ttraining\\tdata,\\tadding\\tmore\\ttraining\\texamples\\twill\\tnot\\thelp.\\tYou\\tneed\\tto\\tuse\\ta\\tmore\\tcomplex\\nmodel\\tor\\tcome\\tup\\twith\\tbetter\\tfeatures.\\nNow\\tlet’s\\tlook\\tat\\tthe\\tlearning\\tcurves\\tof\\ta\\t10\\nth\\n-degree\\tpolynomial\\tmodel\\ton\\tthe\\tsame\\tdata\\t\\n(\\nFigure\\t4-16\\n):\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nPipeline\\npolynomial_regression\\n\\t\\n=\\n\\t\\nPipeline\\n((\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"poly_features\"\\n,\\n\\t\\nPolynomialFeatures\\n(\\ndegree\\n=\\n10\\n,\\n\\t\\ninclude_bias\\n=\\nFalse\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"lin_reg\"\\n,\\n\\t\\nLinearRegression\\n()),\\n\\t\\t\\t\\t\\n))\\nplot_learning_curves\\n(\\npolynomial_regression\\n,\\n\\t\\nX\\n,\\n\\t\\ny\\n)\\nThese\\tlearning\\tcurves\\tlook\\ta\\tbit\\tlike\\tthe\\tprevious\\tones,\\tbut\\tthere\\tare\\ttwo\\tvery\\timportant\\tdifferences:\\nThe\\terror\\ton\\tthe\\ttraining\\tdata\\tis\\tmuch\\tlower\\tthan\\twith\\tthe\\tLinear\\tRegression\\tmodel.\\nThere\\tis\\ta\\tgap\\tbetween\\tthe\\tcurves.\\tThis\\tmeans\\tthat\\tthe\\tmodel\\tperforms\\tsignificantly\\tbetter\\ton\\tthe\\ntraining\\tdata\\tthan\\ton\\tthe\\tvalidation\\tdata,\\twhich\\tis\\tthe\\thallmark\\tof\\tan\\toverfitting\\tmodel.\\tHowever,\\tif', 'you\\tused\\ta\\tmuch\\tlarger\\ttraining\\tset,\\tthe\\ttwo\\tcurves\\twould\\tcontinue\\tto\\tget\\tcloser.\\nFigure\\t4-16.\\t\\nLearning\\tcurves\\tfor\\tthe\\tpolynomial\\tmodel\\nTIP\\nOne\\tway\\tto\\timprove\\tan\\toverfitting\\tmodel\\tis\\tto\\tfeed\\tit\\tmore\\ttraining\\tdata\\tuntil\\tthe\\tvalidation\\terror\\treaches\\tthe\\ttraining\\terror.', 'THE\\tBIAS/VARIANCE\\tTRADEOFF\\nAn\\timportant\\t\\ntheoretical\\tresult\\tof\\tstatistics\\tand\\tMachine\\tLearning\\tis\\tthe\\tfact\\tthat\\ta\\tmodel’s\\tgeneralization\\terror\\tcan\\tbe\\texpressed\\tas\\tthe\\nsum\\tof\\tthree\\tvery\\tdifferent\\terrors:\\nBias\\nThis\\tpart\\tof\\tthe\\tgeneralization\\terror\\tis\\tdue\\tto\\twrong\\tassumptions,\\tsuch\\tas\\tassuming\\tthat\\tthe\\tdata\\tis\\tlinear\\twhen\\tit\\tis\\tactually\\nquadratic.\\tA\\thigh-bias\\tmodel\\tis\\tmost\\tlikely\\tto\\tunderfit\\tthe\\ttraining\\tdata.\\n10\\nVariance\\nThis\\tpart\\tis\\tdue\\tto\\tthe\\tmodel’s\\texcessive\\tsensitivity\\tto\\tsmall\\tvariations\\tin\\tthe\\ttraining\\tdata.\\tA\\tmodel\\twith\\tmany\\tdegrees\\tof\\tfreedom\\n(such\\tas\\ta\\thigh-degree\\tpolynomial\\tmodel)\\tis\\tlikely\\tto\\thave\\thigh\\tvariance,\\tand\\tthus\\tto\\toverfit\\tthe\\ttraining\\tdata.\\nIrreducible\\terror\\nThis\\t\\npart\\tis\\tdue\\tto\\tthe\\tnoisiness\\tof\\tthe\\tdata\\titself.\\tThe\\tonly\\tway\\tto\\treduce\\tthis\\tpart\\tof\\tthe\\terror\\tis\\tto\\tclean\\tup\\tthe\\tdata\\t(e.g.,\\tfix\\tthe\\ndata\\tsources,\\tsuch\\tas\\tbroken\\tsensors,\\tor\\tdetect\\tand\\tremove\\toutliers).', 'Increasing\\ta\\tmodel’s\\tcomplexity\\twill\\ttypically\\tincrease\\tits\\tvariance\\tand\\treduce\\tits\\tbias.\\tConversely,\\treducing\\ta\\tmodel’s\\tcomplexity\\nincreases\\tits\\tbias\\tand\\treduces\\tits\\tvariance.\\t\\nThis\\tis\\twhy\\tit\\tis\\tcalled\\ta\\ttradeoff.', 'Regularized\\tLinear\\tModels\\nAs\\twe\\t\\nsaw\\tin\\tChapters\\t\\n1\\n\\tand\\t\\n2\\n,\\ta\\tgood\\tway\\tto\\treduce\\toverfitting\\tis\\tto\\tregularize\\tthe\\tmodel\\t(i.e.,\\tto\\nconstrain\\tit):\\tthe\\tfewer\\tdegrees\\tof\\tfreedom\\tit\\thas,\\tthe\\tharder\\tit\\twill\\tbe\\tfor\\tit\\tto\\toverfit\\tthe\\tdata.\\tFor\\nexample,\\ta\\tsimple\\tway\\tto\\tregularize\\ta\\tpolynomial\\tmodel\\tis\\tto\\treduce\\tthe\\tnumber\\tof\\tpolynomial\\tdegrees.\\nFor\\ta\\tlinear\\tmodel,\\tregularization\\tis\\ttypically\\tachieved\\tby\\tconstraining\\tthe\\tweights\\tof\\tthe\\tmodel.\\tWe\\twill\\nnow\\tlook\\tat\\tRidge\\tRegression,\\tLasso\\tRegression,\\tand\\tElastic\\tNet,\\twhich\\timplement\\tthree\\tdifferent\\tways\\nto\\tconstrain\\tthe\\tweights.', 'Ridge\\tRegression\\nRidge\\tRegression\\n\\t\\n(also\\tcalled\\t\\nTikhonov\\tregularization\\n)\\tis\\ta\\tregularized\\tversion\\tof\\tLinear\\tRegression:\\ta\\nregularization\\tterm\\n\\tequal\\tto\\t\\n\\tis\\tadded\\tto\\tthe\\tcost\\tfunction.\\t\\nThis\\tforces\\tthe\\tlearning\\talgorithm\\tto\\nnot\\tonly\\tfit\\tthe\\tdata\\tbut\\talso\\tkeep\\tthe\\tmodel\\tweights\\tas\\tsmall\\tas\\tpossible.\\tNote\\tthat\\tthe\\tregularization\\tterm\\nshould\\tonly\\tbe\\tadded\\tto\\tthe\\tcost\\tfunction\\tduring\\ttraining.\\tOnce\\tthe\\tmodel\\tis\\ttrained,\\tyou\\twant\\tto\\tevaluate\\nthe\\tmodel’s\\tperformance\\tusing\\tthe\\tunregularized\\tperformance\\tmeasure.\\nNOTE\\nIt\\tis\\tquite\\tcommon\\tfor\\tthe\\tcost\\tfunction\\tused\\tduring\\ttraining\\tto\\tbe\\tdifferent\\tfrom\\tthe\\tperformance\\tmeasure\\tused\\tfor\\ttesting.\\tApart\\nfrom\\tregularization,\\tanother\\treason\\twhy\\tthey\\tmight\\tbe\\tdifferent\\tis\\tthat\\ta\\tgood\\ttraining\\tcost\\tfunction\\tshould\\thave\\toptimization-\\nfriendly\\tderivatives,\\twhile\\tthe\\tperformance\\tmeasure\\tused\\tfor\\ttesting\\tshould\\tbe\\tas\\tclose\\tas\\tpossible\\tto\\tthe\\tfinal\\tobjective.\\tA\\tgood', 'example\\tof\\tthis\\tis\\ta\\tclassifier\\ttrained\\tusing\\ta\\tcost\\tfunction\\tsuch\\tas\\tthe\\tlog\\tloss\\t(discussed\\tin\\ta\\tmoment)\\tbut\\tevaluated\\tusing\\nprecision/recall.\\nThe\\thyperparameter\\t\\nα\\n\\tcontrols\\thow\\tmuch\\tyou\\twant\\tto\\tregularize\\tthe\\tmodel.\\tIf\\t\\nα\\n\\t=\\t0\\tthen\\tRidge\\nRegression\\tis\\tjust\\tLinear\\tRegression.\\tIf\\t\\nα\\n\\tis\\tvery\\tlarge,\\tthen\\tall\\tweights\\tend\\tup\\tvery\\tclose\\tto\\tzero\\tand\\tthe\\nresult\\tis\\ta\\tflat\\tline\\tgoing\\tthrough\\tthe\\tdata’s\\tmean.\\t\\nEquation\\t4-8\\n\\tpresents\\tthe\\tRidge\\tRegression\\tcost\\nfunction.\\n11\\nEquation\\t4-8.\\t\\nRidge\\tRegression\\tcost\\tfunction\\nNote\\tthat\\tthe\\tbias\\tterm\\t\\nθ\\n0\\n\\tis\\tnot\\tregularized\\t(the\\tsum\\tstarts\\tat\\t\\ni\\n\\t=\\t1,\\tnot\\t0).\\tIf\\twe\\tdefine\\t\\nw\\n\\tas\\tthe\\tvector\\tof\\nfeature\\tweights\\t(\\nθ\\n1\\n\\tto\\t\\nθ\\nn\\n),\\tthen\\tthe\\tregularization\\tterm\\tis\\tsimply\\tequal\\tto\\t½(\\t\\nw\\n\\t\\n2\\n)\\n2\\n,\\twhere\\t\\t·\\t\\n2\\nrepresents\\tthe\\tℓ\\n2\\n\\t\\nnorm\\tof\\tthe\\tweight\\tvector.\\n12\\n\\tFor\\tGradient\\tDescent,\\tjust\\tadd\\t\\nα\\nw\\n\\tto\\tthe\\tMSE\\tgradient\\nvector\\t(\\nEquation\\t4-6\\n).\\nWARNING\\nIt\\tis\\timportant\\tto\\tscale\\tthe\\tdata\\t(e.g.,\\tusing\\ta\\t\\nStandardScaler\\n)', ')\\t\\nbefore\\tperforming\\tRidge\\tRegression,\\tas\\tit\\tis\\tsensitive\\tto\\tthe\\tscale\\nof\\tthe\\tinput\\tfeatures.\\tThis\\tis\\ttrue\\tof\\tmost\\tregularized\\tmodels.\\nFigure\\t4-17\\n\\tshows\\tseveral\\tRidge\\tmodels\\ttrained\\ton\\tsome\\tlinear\\tdata\\tusing\\tdifferent\\t\\nα\\n\\tvalue.\\tOn\\tthe\\tleft,\\nplain\\tRidge\\tmodels\\tare\\tused,\\tleading\\tto\\tlinear\\tpredictions.\\tOn\\tthe\\tright,\\tthe\\tdata\\tis\\tfirst\\texpanded\\tusing\\nPolynomialFeatures(degree=10)\\n,\\tthen\\tit\\tis\\tscaled\\tusing\\ta\\t\\nStandardScaler\\n,\\tand\\tfinally\\tthe\\tRidge', 'models\\tare\\tapplied\\tto\\tthe\\tresulting\\tfeatures:\\tthis\\tis\\tPolynomial\\tRegression\\twith\\tRidge\\tregularization.\\nNote\\thow\\tincreasing\\t\\nα\\n\\tleads\\tto\\tflatter\\t(i.e.,\\tless\\textreme,\\tmore\\treasonable)\\tpredictions;\\tthis\\treduces\\tthe\\nmodel’s\\tvariance\\tbut\\tincreases\\tits\\tbias.\\nAs\\twith\\tLinear\\tRegression,\\twe\\tcan\\tperform\\tRidge\\tRegression\\teither\\tby\\tcomputing\\ta\\t\\nclosed-form\\nequation\\tor\\tby\\tperforming\\tGradient\\tDescent.\\tThe\\tpros\\tand\\tcons\\tare\\tthe\\tsame.\\t\\nEquation\\t4-9\\n\\tshows\\tthe\\nclosed-form\\tsolution\\t(where\\t\\nA\\n\\tis\\tthe\\t\\nn\\n\\t×\\t\\nn\\n\\t\\nidentity\\tmatrix\\n13\\n\\texcept\\twith\\ta\\t0\\tin\\tthe\\ttop-left\\tcell,\\ncorresponding\\tto\\tthe\\tbias\\tterm).\\nFigure\\t4-17.\\t\\nRidge\\tRegression\\nEquation\\t4-9.\\t\\nRidge\\tRegression\\tclosed-form\\tsolution\\nHere\\tis\\thow\\tto\\tperform\\tRidge\\tRegression\\twith\\t\\nScikit-Learn\\tusing\\ta\\tclosed-form\\tsolution\\t(a\\tvariant\\tof\\nEquation\\t4-9\\n\\tusing\\ta\\tmatrix\\tfactorization\\ttechnique\\tby\\tAndré-Louis\\tCholesky):\\n>>>\\t\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nRidge\\n>>>\\t\\nridge_reg\\n\\t\\n=\\n\\t\\nRidge\\n(\\nalpha\\n=\\n1\\n,\\n\\t\\nsolver\\n=\\n\"cholesky\"\\n)\\n>>>\\t\\nridge_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny', '(\\nX\\n,\\n\\t\\ny\\n)\\n>>>\\t\\nridge_reg\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([[\\t1.55071465]])\\nAnd\\tusing\\t\\nStochastic\\tGradient\\tDescent:\\n14\\n>>>\\t\\nsgd_reg\\n\\t\\n=\\n\\t\\nSGDRegressor\\n(\\npenalty\\n=\\n\"l2\"\\n)\\n>>>\\t\\nsgd_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n.\\nravel\\n())\\n>>>\\t\\nsgd_reg\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([\\t1.13500145])\\nThe\\t\\npenalty\\n\\thyperparameter\\tsets\\tthe\\ttype\\tof\\tregularization\\tterm\\tto\\tuse.\\tSpecifying\\t\\n\"l2\"\\n\\tindicates\\tthat\\nyou\\twant\\tSGD\\tto\\tadd\\ta\\tregularization\\tterm\\tto\\tthe\\tcost\\tfunction\\t\\nequal\\tto\\thalf\\tthe\\tsquare\\tof\\tthe\\tℓ\\n2\\n\\tnorm\\tof\\nthe\\tweight\\tvector:\\tthis\\tis\\tsimply\\t\\nRidge\\tRegression.', 'Lasso\\tRegression\\nLeast\\tAbsolute\\tShrinkage\\tand\\tSelection\\tOperator\\tRegression\\n\\t(simply\\tcalled\\t\\nLasso\\tRegression\\n)\\tis\\nanother\\tregularized\\tversion\\tof\\tLinear\\tRegression:\\tjust\\tlike\\tRidge\\tRegression,\\tit\\tadds\\ta\\tregularization\\tterm\\nto\\tthe\\t\\ncost\\tfunction,\\tbut\\tit\\tuses\\tthe\\tℓ\\n1\\n\\t\\nnorm\\tof\\tthe\\tweight\\tvector\\tinstead\\tof\\thalf\\tthe\\tsquare\\tof\\tthe\\tℓ\\n2\\n\\t\\nnorm\\n(see\\t\\nEquation\\t4-10\\n).\\nEquation\\t4-10.\\t\\nLasso\\tRegression\\tcost\\tfunction\\nFigure\\t4-18\\n\\tshows\\tthe\\tsame\\tthing\\tas\\t\\nFigure\\t4-17\\n\\tbut\\treplaces\\tRidge\\tmodels\\twith\\tLasso\\tmodels\\tand\\tuses\\nsmaller\\t\\nα\\n\\tvalues.\\nFigure\\t4-18.\\t\\nLasso\\tRegression\\nAn\\timportant\\tcharacteristic\\tof\\tLasso\\tRegression\\tis\\tthat\\tit\\ttends\\tto\\tcompletely\\teliminate\\tthe\\tweights\\tof\\tthe\\nleast\\timportant\\tfeatures\\t(i.e.,\\tset\\tthem\\tto\\tzero).\\tFor\\texample,\\tthe\\tdashed\\tline\\tin\\tthe\\tright\\tplot\\ton\\t\\nFigure\\t4-\\n18\\n\\t(with\\t\\nα\\n\\t=\\t10\\n-7\\n)\\tlooks\\tquadratic,\\talmost\\tlinear:\\tall\\tthe\\tweights\\tfor\\tthe\\thigh-degree\\tpolynomial\\tfeatures\\nare\\tequal\\tto\\tzero.\\tIn\\tother\\twords,\\tLasso\\tRegression\\tautomatically\\tperforms', 'feature\\tselection\\tand\\toutputs\\t\\na\\nsparse\\tmodel\\n\\t(i.e.,\\twith\\tfew\\tnonzero\\tfeature\\tweights).\\nYou\\tcan\\tget\\ta\\tsense\\tof\\twhy\\tthis\\tis\\tthe\\tcase\\tby\\tlooking\\tat\\t\\nFigure\\t4-19\\n:\\ton\\tthe\\ttop-left\\tplot,\\tthe\\tbackground\\ncontours\\t(ellipses)\\trepresent\\tan\\tunregularized\\tMSE\\tcost\\tfunction\\t(\\nα\\n\\t=\\t0),\\tand\\tthe\\twhite\\tcircles\\tshow\\tthe\\nBatch\\tGradient\\tDescent\\tpath\\twith\\tthat\\tcost\\tfunction.\\tThe\\tforeground\\tcontours\\t(diamonds)\\trepresent\\tthe\\tℓ\\n1\\npenalty,\\tand\\tthe\\ttriangles\\tshow\\tthe\\tBGD\\tpath\\tfor\\tthis\\tpenalty\\tonly\\t(\\nα\\n\\t→\\t∞).\\tNotice\\thow\\tthe\\tpath\\tfirst\\nreaches\\t\\nθ\\n1\\n\\t=\\t0,\\tthen\\trolls\\tdown\\ta\\tgutter\\tuntil\\tit\\treaches\\t\\nθ\\n2\\n\\t=\\t0.\\tOn\\tthe\\ttop-right\\tplot,\\tthe\\tcontours\\trepresent\\nthe\\tsame\\tcost\\tfunction\\tplus\\tan\\tℓ\\n1\\n\\tpenalty\\twith\\t\\nα\\n\\t=\\t0.5.\\tThe\\tglobal\\tminimum\\tis\\ton\\tthe\\t\\nθ\\n2\\n\\t=\\t0\\taxis.\\tBGD', 'first\\treaches\\t\\nθ\\n2\\n\\t=\\t0,\\tthen\\trolls\\tdown\\tthe\\tgutter\\tuntil\\tit\\treaches\\tthe\\tglobal\\tminimum.\\tThe\\ttwo\\tbottom\\tplots\\nshow\\tthe\\tsame\\tthing\\tbut\\tuses\\tan\\tℓ\\n2\\n\\tpenalty\\tinstead.\\tThe\\tregularized\\tminimum\\tis\\tcloser\\tto\\t\\nθ\\n\\t=\\t0\\tthan\\tthe\\nunregularized\\tminimum,\\tbut\\tthe\\tweights\\tdo\\tnot\\tget\\tfully\\teliminated.\\nFigure\\t4-19.\\t\\nLasso\\tversus\\tRidge\\tregularization\\nTIP\\nOn\\tthe\\tLasso\\tcost\\tfunction,\\tthe\\tBGD\\tpath\\ttends\\tto\\tbounce\\tacross\\tthe\\tgutter\\ttoward\\tthe\\tend.\\tThis\\tis\\tbecause\\tthe\\tslope\\tchanges\\nabruptly\\tat\\t\\nθ\\n2\\n\\t=\\t0.\\tYou\\tneed\\tto\\tgradually\\treduce\\tthe\\tlearning\\trate\\tin\\torder\\tto\\tactually\\tconverge\\tto\\tthe\\tglobal\\tminimum.\\nThe\\tLasso\\tcost\\tfunction\\tis\\tnot\\tdifferentiable\\tat\\t\\nθ\\ni\\n\\t=\\t0\\t(for\\t\\ni\\n\\t=\\t1,\\t2,\\t,\\t\\nn\\n),\\tbut\\tGradient\\tDescent\\tstill\\tworks\\nfine\\tif\\tyou\\tuse\\t\\na\\t\\nsubgradient\\tvector\\n\\t\\ng\\n15\\n\\tinstead\\twhen\\tany\\t\\nθ\\ni\\n\\t=\\t0.\\t\\nEquation\\t4-11\\n\\tshows\\ta\\tsubgradient\\nvector\\tequation\\tyou\\tcan\\tuse\\tfor\\tGradient\\tDescent\\twith\\tthe\\tLasso\\t\\ncost\\tfunction.\\nEquation\\t4-11.\\t\\nLasso\\tRegression\\tsubgradient\\tvector\\nHere\\tis\\ta\\tsmall\\tScikit-Learn\\texample\\tusing\\tthe\\t\\nLasso', 'Lasso\\n\\tclass.\\tNote\\tthat\\tyou\\tcould\\tinstead\\tuse\\t\\nan\\nSGDRegressor(penalty=\"l1\")\\n.\\n>>>\\t\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLasso\\n>>>\\t\\nlasso_reg\\n\\t\\n=\\n\\t\\nLasso\\n(\\nalpha\\n=\\n0.1\\n)\\n>>>\\t\\nlasso_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\n>>>\\t\\nlasso_reg\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([\\t1.53788174])', 'Elastic\\tNet\\nElastic\\tNet\\t\\nis\\ta\\tmiddle\\tground\\tbetween\\tRidge\\tRegression\\tand\\tLasso\\tRegression.\\tThe\\tregularization\\tterm\\nis\\ta\\tsimple\\tmix\\tof\\tboth\\tRidge\\tand\\tLasso’s\\tregularization\\tterms,\\tand\\tyou\\tcan\\tcontrol\\tthe\\tmix\\tratio\\t\\nr\\n.\\tWhen\\nr\\n\\t=\\t0,\\tElastic\\tNet\\tis\\tequivalent\\tto\\tRidge\\tRegression,\\tand\\twhen\\t\\nr\\n\\t=\\t1,\\tit\\tis\\tequivalent\\tto\\t\\nLasso\\tRegression\\n(see\\t\\nEquation\\t4-12\\n).\\nEquation\\t4-12.\\t\\nElastic\\tNet\\tcost\\tfunction\\nSo\\twhen\\tshould\\tyou\\tuse\\t\\nplain\\tLinear\\tRegression\\t(i.e.,\\twithout\\tany\\tregularization),\\tRidge,\\tLasso,\\tor\\nElastic\\tNet?\\tIt\\tis\\talmost\\talways\\tpreferable\\tto\\thave\\tat\\tleast\\ta\\tlittle\\tbit\\tof\\tregularization,\\tso\\tgenerally\\tyou\\nshould\\tavoid\\tplain\\tLinear\\tRegression.\\tRidge\\tis\\ta\\tgood\\tdefault,\\tbut\\tif\\tyou\\tsuspect\\tthat\\tonly\\ta\\tfew\\tfeatures\\nare\\tactually\\tuseful,\\tyou\\tshould\\tprefer\\tLasso\\tor\\tElastic\\tNet\\tsince\\tthey\\ttend\\tto\\treduce\\tthe\\tuseless\\tfeatures’\\nweights\\tdown\\tto\\tzero\\tas\\twe\\thave\\tdiscussed.\\tIn\\tgeneral,\\t\\nElastic\\tNet\\tis\\tpreferred\\tover\\tLasso\\tsince\\tLasso', 'may\\tbehave\\terratically\\twhen\\tthe\\tnumber\\tof\\tfeatures\\tis\\tgreater\\tthan\\tthe\\tnumber\\tof\\ttraining\\tinstances\\tor\\nwhen\\tseveral\\tfeatures\\tare\\tstrongly\\tcorrelated.\\nHere\\tis\\ta\\tshort\\texample\\tusing\\tScikit-Learn’s\\t\\nElasticNet\\n\\t(\\nl1_ratio\\n\\tcorresponds\\tto\\tthe\\tmix\\tratio\\t\\nr\\n):\\n>>>\\t\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nElasticNet\\n>>>\\t\\nelastic_net\\n\\t\\n=\\n\\t\\nElasticNet\\n(\\nalpha\\n=\\n0.1\\n,\\n\\t\\nl1_ratio\\n=\\n0.5\\n)\\n>>>\\t\\nelastic_net\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\n>>>\\t\\nelastic_net\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([\\t1.54333232])', 'Early\\tStopping\\nA\\t\\nvery\\tdifferent\\tway\\tto\\tregularize\\titerative\\tlearning\\talgorithms\\tsuch\\tas\\tGradient\\tDescent\\tis\\tto\\tstop\\ntraining\\tas\\tsoon\\tas\\tthe\\tvalidation\\terror\\treaches\\ta\\tminimum.\\tThis\\tis\\tcalled\\t\\nearly\\tstopping\\n.\\t\\nFigure\\t4-20\\nshows\\ta\\tcomplex\\tmodel\\t(in\\tthis\\tcase\\ta\\thigh-degree\\tPolynomial\\tRegression\\tmodel)\\tbeing\\ttrained\\tusing\\nBatch\\tGradient\\tDescent.\\tAs\\tthe\\tepochs\\tgo\\tby,\\tthe\\talgorithm\\tlearns\\tand\\tits\\tprediction\\terror\\t(RMSE)\\ton\\tthe\\ntraining\\tset\\tnaturally\\tgoes\\tdown,\\tand\\tso\\tdoes\\tits\\tprediction\\terror\\ton\\tthe\\tvalidation\\tset.\\tHowever,\\tafter\\ta\\nwhile\\tthe\\tvalidation\\terror\\tstops\\tdecreasing\\tand\\tactually\\tstarts\\tto\\tgo\\tback\\tup.\\tThis\\tindicates\\tthat\\tthe\\tmodel\\nhas\\tstarted\\tto\\toverfit\\tthe\\ttraining\\tdata.\\tWith\\tearly\\tstopping\\tyou\\tjust\\tstop\\ttraining\\tas\\tsoon\\tas\\tthe\\tvalidation\\nerror\\treaches\\tthe\\tminimum.\\tIt\\tis\\tsuch\\ta\\tsimple\\tand\\tefficient\\tregularization\\ttechnique\\tthat\\tGeoffrey\\tHinton\\ncalled\\tit\\ta\\t“beautiful\\tfree\\tlunch.”\\nFigure\\t4-20.\\t\\nEarly\\tstopping\\tregularization\\nTIP', 'TIP\\nWith\\tStochastic\\tand\\tMini-batch\\tGradient\\tDescent,\\tthe\\tcurves\\tare\\tnot\\tso\\tsmooth,\\tand\\tit\\tmay\\tbe\\thard\\tto\\tknow\\twhether\\tyou\\thave\\nreached\\tthe\\tminimum\\tor\\tnot.\\tOne\\tsolution\\tis\\tto\\tstop\\tonly\\tafter\\tthe\\tvalidation\\terror\\thas\\tbeen\\tabove\\tthe\\tminimum\\tfor\\tsome\\ttime\\n(when\\tyou\\tare\\tconfident\\tthat\\tthe\\tmodel\\twill\\tnot\\tdo\\tany\\tbetter),\\tthen\\troll\\tback\\tthe\\t\\nmodel\\tparameters\\tto\\tthe\\tpoint\\twhere\\tthe\\nvalidation\\terror\\twas\\tat\\ta\\tminimum.\\nHere\\tis\\ta\\tbasic\\timplementation\\tof\\tearly\\t\\nstopping:\\nfrom\\n\\t\\nsklearn.base\\n\\t\\nimport\\n\\t\\nclone\\nsgd_reg\\n\\t\\n=\\n\\t\\nSGDRegressor\\n(\\nn_iter\\n=\\n1\\n,\\n\\t\\nwarm_start\\n=\\nTrue\\n,\\n\\t\\npenalty\\n=\\nNone\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlearning_rate\\n=\\n\"constant\"\\n,\\n\\t\\neta0\\n=\\n0.0005\\n)\\nminimum_val_error\\n\\t\\n=\\n\\t\\nfloat\\n(\\n\"inf\"\\n)\\nbest_epoch\\n\\t\\n=\\n\\t\\nNone', 'best_model\\n\\t\\n=\\n\\t\\nNone\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\n1000\\n):\\n\\t\\t\\t\\t\\nsgd_reg\\n.\\nfit\\n(\\nX_train_poly_scaled\\n,\\n\\t\\ny_train\\n)\\n\\t\\t\\n#\\tcontinues\\twhere\\tit\\tleft\\toff\\n\\t\\t\\t\\t\\ny_val_predict\\n\\t\\n=\\n\\t\\nsgd_reg\\n.\\npredict\\n(\\nX_val_poly_scaled\\n)\\n\\t\\t\\t\\t\\nval_error\\n\\t\\n=\\n\\t\\nmean_squared_error\\n(\\ny_val_predict\\n,\\n\\t\\ny_val\\n)\\n\\t\\t\\t\\t\\nif\\n\\t\\nval_error\\n\\t\\n<\\n\\t\\nminimum_val_error\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nminimum_val_error\\n\\t\\n=\\n\\t\\nval_error\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nbest_epoch\\n\\t\\n=\\n\\t\\nepoch\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nbest_model\\n\\t\\n=\\n\\t\\nclone\\n(\\nsgd_reg\\n)\\nNote\\tthat\\twith\\t\\nwarm_start=True\\n,\\twhen\\tthe\\t\\nfit()\\n\\tmethod\\tis\\tcalled,\\tit\\tjust\\tcontinues\\ttraining\\twhere\\tit\\tleft\\noff\\tinstead\\tof\\trestarting\\t\\nfrom\\tscratch.', 'Logistic\\tRegression\\nAs\\t\\nwe\\tdiscussed\\tin\\t\\nChapter\\t1\\n,\\tsome\\tregression\\talgorithms\\tcan\\tbe\\tused\\tfor\\tclassification\\tas\\twell\\t(and\\nvice\\tversa).\\t\\nLogistic\\tRegression\\n\\t(also\\tcalled\\t\\nLogit\\tRegression\\n)\\tis\\tcommonly\\tused\\tto\\testimate\\tthe\\nprobability\\tthat\\tan\\tinstance\\tbelongs\\tto\\ta\\tparticular\\tclass\\t(e.g.,\\twhat\\tis\\tthe\\tprobability\\tthat\\tthis\\temail\\tis\\nspam?).\\tIf\\tthe\\testimated\\tprobability\\tis\\tgreater\\tthan\\t50%,\\tthen\\tthe\\tmodel\\tpredicts\\tthat\\tthe\\tinstance\\tbelongs\\nto\\tthat\\tclass\\t(called\\tthe\\tpositive\\tclass,\\tlabeled\\t“1”),\\tor\\telse\\tit\\tpredicts\\tthat\\tit\\tdoes\\tnot\\t(i.e.,\\tit\\tbelongs\\tto\\nthe\\tnegative\\tclass,\\tlabeled\\t“0”).\\tThis\\tmakes\\tit\\ta\\t\\nbinary\\tclassifier.', 'Estimating\\tProbabilities\\nSo\\t\\nhow\\tdoes\\tit\\twork?\\tJust\\tlike\\ta\\tLinear\\tRegression\\tmodel,\\ta\\tLogistic\\tRegression\\tmodel\\tcomputes\\ta\\nweighted\\tsum\\tof\\tthe\\tinput\\tfeatures\\t(plus\\ta\\tbias\\tterm),\\tbut\\tinstead\\tof\\toutputting\\tthe\\tresult\\tdirectly\\tlike\\tthe\\nLinear\\tRegression\\tmodel\\tdoes,\\tit\\toutputs\\tthe\\t\\nlogistic\\n\\tof\\tthis\\tresult\\t(see\\t\\nEquation\\t4-13\\n).\\nEquation\\t4-13.\\t\\nLogistic\\tRegression\\tmodel\\testimated\\tprobability\\t(vectorized\\tform)\\nThe\\tlogistic\\t—\\talso\\tcalled\\tthe\\t\\nlogit\\n,\\tnoted\\t\\nσ\\n(·)\\t—\\tis\\ta\\t\\nsigmoid\\tfunction\\n\\t(i.e.,\\t\\nS\\n-shaped)\\tthat\\toutputs\\t\\na\\nnumber\\tbetween\\t0\\tand\\t1.\\tIt\\tis\\tdefined\\tas\\tshown\\tin\\t\\nEquation\\t4-14\\n\\tand\\t\\nFigure\\t4-21\\n.\\nEquation\\t4-14.\\t\\nLogistic\\tfunction\\nFigure\\t4-21.\\t\\nLogistic\\tfunction\\nOnce\\tthe\\tLogistic\\tRegression\\tmodel\\thas\\testimated\\tthe\\tprobability\\t\\n\\t=\\t\\nh\\nθ\\n(\\nx\\n)\\t\\nthat\\tan\\tinstance\\t\\nx\\n\\tbelongs\\tto\\nthe\\tpositive\\tclass,\\tit\\tcan\\tmake\\tits\\tprediction\\t\\nŷ\\n\\teasily\\t(see\\t\\nEquation\\t4-15\\n).\\nEquation\\t4-15.\\t\\nLogistic\\tRegression\\tmodel\\tprediction\\nNotice\\tthat\\t\\nσ\\n(\\nt\\n)\\t<\\t0.5\\twhen\\t\\nt\\n\\t<\\t0,\\tand\\t\\nσ\\n(\\nt\\n)\\t≥\\t0.5\\twhen\\t\\nt', 't\\n\\t≥\\t0,\\tso\\ta\\tLogistic\\tRegression\\tmodel\\tpredicts\\t1\\tif', 'θ\\nT\\n\\t·\\t\\nx\\n\\tis\\tpositive,\\tand\\t0\\tif\\tit\\tis\\t\\nnegative.', 'Training\\tand\\tCost\\tFunction\\nGood,\\t\\nnow\\tyou\\tknow\\thow\\ta\\tLogistic\\tRegression\\tmodel\\testimates\\tprobabilities\\tand\\tmakes\\tpredictions.\\nBut\\thow\\tis\\tit\\ttrained?\\tThe\\tobjective\\tof\\ttraining\\tis\\tto\\tset\\tthe\\t\\nparameter\\tvector\\t\\nθ\\n\\tso\\tthat\\tthe\\tmodel\\testimates\\nhigh\\tprobabilities\\tfor\\tpositive\\tinstances\\t(\\ny\\n\\t=\\t1)\\tand\\tlow\\tprobabilities\\tfor\\tnegative\\tinstances\\t(\\ny\\n\\t=\\t0).\\tThis\\nidea\\tis\\tcaptured\\tby\\tthe\\tcost\\tfunction\\tshown\\tin\\t\\nEquation\\t4-16\\n\\tfor\\ta\\tsingle\\ttraining\\tinstance\\t\\nx\\n.\\nEquation\\t4-16.\\t\\nCost\\tfunction\\tof\\ta\\tsingle\\ttraining\\tinstance\\nThis\\tcost\\tfunction\\tmakes\\tsense\\tbecause\\t\\n–\\tlog(\\nt\\n)\\tgrows\\tvery\\tlarge\\twhen\\t\\nt\\n\\tapproaches\\t0,\\tso\\tthe\\tcost\\twill\\nbe\\tlarge\\tif\\tthe\\tmodel\\testimates\\ta\\tprobability\\tclose\\tto\\t0\\tfor\\ta\\tpositive\\tinstance,\\tand\\tit\\twill\\talso\\tbe\\tvery\\nlarge\\tif\\tthe\\tmodel\\testimates\\ta\\tprobability\\tclose\\tto\\t1\\tfor\\ta\\tnegative\\tinstance.\\tOn\\tthe\\tother\\thand,\\t–\\tlog(\\nt\\n)\\tis\\nclose\\tto\\t0\\twhen\\t\\nt\\n\\tis\\tclose\\tto\\t1,\\tso\\tthe\\tcost\\twill\\tbe\\tclose\\tto\\t0\\tif\\tthe\\testimated\\tprobability\\tis\\tclose\\tto\\t0\\tfor\\ta', 'negative\\tinstance\\tor\\tclose\\tto\\t1\\tfor\\ta\\tpositive\\tinstance,\\twhich\\tis\\tprecisely\\twhat\\twe\\twant.\\nThe\\tcost\\tfunction\\tover\\tthe\\twhole\\ttraining\\tset\\tis\\tsimply\\tthe\\taverage\\tcost\\tover\\tall\\ttraining\\tinstances.\\tIt\\tcan\\nbe\\twritten\\tin\\ta\\tsingle\\texpression\\t(as\\tyou\\tcan\\tverify\\teasily),\\tcalled\\t\\nthe\\t\\nlog\\tloss\\n,\\tshown\\tin\\t\\nEquation\\t4-17\\n.\\nEquation\\t4-17.\\t\\nLogistic\\tRegression\\tcost\\tfunction\\t(log\\tloss)\\nThe\\tbad\\tnews\\tis\\tthat\\tthere\\tis\\tno\\tknown\\t\\nclosed-form\\tequation\\tto\\tcompute\\tthe\\tvalue\\tof\\t\\nθ\\n\\tthat\\tminimizes\\tthis\\ncost\\tfunction\\t(there\\tis\\tno\\tequivalent\\tof\\tthe\\tNormal\\tEquation).\\tBut\\tthe\\tgood\\tnews\\tis\\tthat\\tthis\\tcost\\tfunction\\nis\\tconvex,\\tso\\tGradient\\tDescent\\t(or\\tany\\tother\\toptimization\\talgorithm)\\tis\\tguaranteed\\tto\\tfind\\tthe\\tglobal\\nminimum\\t(if\\tthe\\tlearning\\trate\\tis\\tnot\\ttoo\\tlarge\\tand\\tyou\\twait\\tlong\\tenough).\\tThe\\tpartial\\tderivatives\\tof\\tthe\\ncost\\tfunction\\twith\\tregards\\tto\\tthe\\tj\\nth\\n\\tmodel\\tparameter\\t\\nθ\\nj\\n\\tis\\tgiven\\tby\\t\\nEquation\\t4-18\\n.\\nEquation\\t4-18.\\t\\nLogistic\\tcost\\tfunction\\tpartial\\tderivatives\\nThis\\tequation\\tlooks\\tvery\\tmuch\\tlike\\t\\nEquation\\t4-5', ':\\tfor\\teach\\tinstance\\tit\\tcomputes\\tthe\\tprediction\\terror\\tand\\nmultiplies\\tit\\tby\\tthe\\tj\\nth\\n\\tfeature\\tvalue,\\tand\\tthen\\tit\\tcomputes\\tthe\\taverage\\tover\\tall\\ttraining\\tinstances.\\tOnce\\tyou\\nhave\\tthe\\tgradient\\tvector\\tcontaining\\tall\\tthe\\tpartial\\tderivatives\\tyou\\tcan\\tuse\\tit\\tin\\tthe\\tBatch\\tGradient\\tDescent\\nalgorithm.\\tThat’s\\tit:\\tyou\\tnow\\tknow\\thow\\tto\\ttrain\\ta\\tLogistic\\tRegression\\tmodel.\\tFor\\t\\nStochastic\\tGD\\tyou', 'would\\tof\\tcourse\\tjust\\ttake\\tone\\tinstance\\tat\\ta\\ttime,\\tand\\tfor\\t\\nMini-batch\\tGD\\tyou\\twould\\tuse\\ta\\t\\nmini-batch\\tat\\ta\\ntime.', 'Decision\\tBoundaries\\nLet’s\\t\\nuse\\tthe\\tiris\\tdataset\\tto\\tillustrate\\tLogistic\\tRegression.\\tThis\\tis\\ta\\tfamous\\tdataset\\tthat\\tcontains\\tthe\\tsepal\\nand\\tpetal\\tlength\\tand\\twidth\\tof\\t150\\tiris\\tflowers\\tof\\tthree\\tdifferent\\tspecies:\\tIris-Setosa,\\tIris-Versicolor,\\tand\\nIris-Virginica\\t(see\\t\\nFigure\\t4-22\\n).\\nFigure\\t4-22.\\t\\nFlowers\\tof\\tthree\\tiris\\tplant\\tspecies\\n16\\nLet’s\\ttry\\tto\\tbuild\\ta\\tclassifier\\tto\\tdetect\\tthe\\tIris-Virginica\\ttype\\tbased\\tonly\\ton\\tthe\\tpetal\\twidth\\tfeature.\\t\\nFirst\\nlet’s\\tload\\tthe\\tdata:\\n>>>\\t\\nfrom\\n\\t\\nsklearn\\n\\t\\nimport\\n\\t\\ndatasets\\n>>>\\t\\niris\\n\\t\\n=\\n\\t\\ndatasets\\n.\\nload_iris\\n()\\n>>>\\t\\nlist\\n(\\niris\\n.\\nkeys\\n())\\n[\\'data\\',\\t\\'target_names\\',\\t\\'feature_names\\',\\t\\'target\\',\\t\\'DESCR\\']\\n>>>\\t\\nX\\n\\t\\n=\\n\\t\\niris\\n[\\n\"data\"\\n][:,\\n\\t\\n3\\n:]\\n\\t\\t\\n#\\tpetal\\twidth\\n>>>\\t\\ny\\n\\t\\n=\\n\\t\\n(\\niris\\n[\\n\"target\"\\n]\\n\\t\\n==\\n\\t\\n2\\n)\\n.\\nastype\\n(\\nnp\\n.\\nint\\n)\\n\\t\\t\\n#\\t1\\tif\\tIris-Virginica,\\telse\\t0\\nNow\\tlet’s\\ttrain\\ta\\t\\nLogistic\\tRegression\\tmodel:\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLogisticRegression\\nlog_reg\\n\\t\\n=\\n\\t\\nLogisticRegression\\n()\\nlog_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)', 'X\\n,\\n\\t\\ny\\n)\\nLet’s\\tlook\\tat\\tthe\\tmodel’s\\testimated\\tprobabilities\\tfor\\tflowers\\twith\\tpetal\\twidths\\tvarying\\tfrom\\t0\\tto\\t3\\tcm\\n(\\nFigure\\t4-23\\n):\\nX_new\\n\\t\\n=\\n\\t\\nnp\\n.\\nlinspace\\n(\\n0\\n,\\n\\t\\n3\\n,\\n\\t\\n1000\\n)\\n.\\nreshape\\n(\\n-\\n1\\n,\\n\\t\\n1\\n)\\ny_proba\\n\\t\\n=\\n\\t\\nlog_reg\\n.\\npredict_proba\\n(\\nX_new\\n)\\nplt\\n.\\nplot\\n(\\nX_new\\n,\\n\\t\\ny_proba\\n[:,\\n\\t\\n1\\n],\\n\\t\\n\"g-\"\\n,\\n\\t\\nlabel\\n=\\n\"Iris-Virginica\"\\n)\\nplt\\n.\\nplot\\n(\\nX_new\\n,\\n\\t\\ny_proba\\n[:,\\n\\t\\n0\\n],\\n\\t\\n\"b--\"\\n,\\n\\t\\nlabel\\n=\\n\"Not\\tIris-Virginica\"\\n)\\n#\\t+\\tmore\\tMatplotlib\\tcode\\tto\\tmake\\tthe\\timage\\tlook\\tpretty', 'Figure\\t4-23.\\t\\nEstimated\\tprobabilities\\tand\\tdecision\\tboundary\\nThe\\tpetal\\twidth\\tof\\tIris-Virginica\\tflowers\\t(represented\\tby\\ttriangles)\\tranges\\tfrom\\t1.4\\tcm\\tto\\t2.5\\tcm,\\twhile\\nthe\\tother\\tiris\\tflowers\\t(represented\\tby\\tsquares)\\tgenerally\\thave\\ta\\tsmaller\\tpetal\\twidth,\\tranging\\tfrom\\t0.1\\tcm\\nto\\t1.8\\tcm.\\tNotice\\tthat\\tthere\\tis\\ta\\tbit\\tof\\toverlap.\\tAbove\\tabout\\t2\\tcm\\tthe\\tclassifier\\tis\\thighly\\tconfident\\tthat\\tthe\\nflower\\tis\\tan\\tIris-Virginica\\t(it\\toutputs\\ta\\thigh\\tprobability\\tto\\tthat\\tclass),\\twhile\\tbelow\\t1\\tcm\\tit\\tis\\thighly\\nconfident\\tthat\\tit\\tis\\tnot\\tan\\tIris-Virginica\\t(high\\tprobability\\tfor\\tthe\\t“Not\\tIris-Virginica”\\tclass).\\tIn\\tbetween\\nthese\\textremes,\\tthe\\tclassifier\\tis\\tunsure.\\tHowever,\\tif\\tyou\\task\\tit\\tto\\tpredict\\tthe\\tclass\\t(using\\tthe\\t\\npredict()\\nmethod\\trather\\tthan\\tthe\\t\\npredict_proba()\\n\\tmethod),\\tit\\twill\\treturn\\twhichever\\tclass\\tis\\tthe\\tmost\\tlikely.\\nTherefore,\\tthere\\tis\\ta\\t\\ndecision\\tboundary\\n\\tat\\taround\\t1.6\\tcm\\twhere\\tboth\\tprobabilities\\tare\\tequal\\tto\\t50%:\\tif', 'the\\tpetal\\twidth\\tis\\thigher\\tthan\\t1.6\\tcm,\\tthe\\tclassifier\\twill\\tpredict\\tthat\\tthe\\tflower\\tis\\tan\\tIris-Virginica,\\tor\\telse\\nit\\twill\\tpredict\\tthat\\tit\\tis\\tnot\\t(even\\tif\\tit\\tis\\tnot\\tvery\\tconfident):\\n>>>\\t\\nlog_reg\\n.\\npredict\\n([[\\n1.7\\n],\\n\\t\\n[\\n1.5\\n]])\\narray([1,\\t0])\\nFigure\\t4-24\\n\\tshows\\tthe\\tsame\\tdataset\\tbut\\tthis\\ttime\\tdisplaying\\ttwo\\tfeatures:\\tpetal\\twidth\\tand\\tlength.\\tOnce\\ntrained,\\tthe\\tLogistic\\tRegression\\tclassifier\\tcan\\testimate\\tthe\\tprobability\\tthat\\ta\\tnew\\tflower\\tis\\tan\\tIris-\\nVirginica\\tbased\\ton\\tthese\\ttwo\\tfeatures.\\tThe\\tdashed\\tline\\trepresents\\tthe\\tpoints\\twhere\\tthe\\tmodel\\testimates\\ta\\n50%\\tprobability:\\tthis\\tis\\tthe\\tmodel’s\\tdecision\\tboundary.\\tNote\\tthat\\tit\\tis\\ta\\tlinear\\tboundary.\\n17\\n\\tEach\\tparallel\\nline\\trepresents\\tthe\\tpoints\\twhere\\tthe\\tmodel\\toutputs\\ta\\tspecific\\tprobability,\\tfrom\\t15%\\t(bottom\\tleft)\\tto\\t90%\\n(top\\tright).\\tAll\\tthe\\tflowers\\tbeyond\\tthe\\ttop-right\\tline\\thave\\tan\\tover\\t90%\\tchance\\tof\\tbeing\\tIris-Virginica\\naccording\\tto\\tthe\\tmodel.\\nFigure\\t4-24.\\t\\nLinear\\tdecision\\tboundary', 'Just\\tlike\\tthe\\tother\\tlinear\\tmodels,\\tLogistic\\tRegression\\tmodels\\tcan\\tbe\\tregularized\\tusing\\t\\nℓ\\n1\\n\\tor\\tℓ\\n2\\n\\tpenalties.\\nScitkit-Learn\\tactually\\tadds\\tan\\tℓ\\n2\\n\\tpenalty\\tby\\tdefault.\\nNOTE\\nThe\\thyperparameter\\tcontrolling\\tthe\\tregularization\\tstrength\\tof\\ta\\tScikit-Learn\\t\\nLogisticRegression\\n\\tmodel\\tis\\tnot\\t\\nalpha\\n\\t(as\\tin\\tother\\nlinear\\tmodels),\\tbut\\tits\\tinverse:\\t\\nC\\n.\\tThe\\thigher\\tthe\\tvalue\\tof\\t\\nC\\n,\\tthe\\t\\nless\\n\\tthe\\tmodel\\tis\\t\\nregularized.', 'Softmax\\tRegression\\nThe\\t\\nLogistic\\tRegression\\tmodel\\tcan\\tbe\\tgeneralized\\tto\\tsupport\\tmultiple\\tclasses\\tdirectly,\\twithout\\thaving\\tto\\ntrain\\tand\\tcombine\\tmultiple\\tbinary\\tclassifiers\\t(as\\tdiscussed\\tin\\t\\nChapter\\t3\\n).\\tThis\\tis\\tcalled\\t\\nSoftmax\\nRegression\\n,\\tor\\t\\nMultinomial\\tLogistic\\tRegression\\n.\\nThe\\tidea\\tis\\tquite\\tsimple:\\twhen\\tgiven\\tan\\tinstance\\t\\nx\\n,\\tthe\\tSoftmax\\tRegression\\tmodel\\tfirst\\tcomputes\\ta\\tscore\\ns\\nk\\n(\\nx\\n)\\tfor\\teach\\tclass\\t\\nk\\n,\\tthen\\testimates\\tthe\\tprobability\\tof\\teach\\tclass\\tby\\tapplying\\t\\nthe\\t\\nsoftmax\\tfunction\\n\\t(also\\ncalled\\tthe\\t\\nnormalized\\texponential\\n)\\tto\\tthe\\tscores.\\tThe\\tequation\\tto\\tcompute\\t\\ns\\nk\\n(\\nx\\n)\\tshould\\tlook\\tfamiliar,\\tas\\nit\\tis\\tjust\\tlike\\tthe\\tequation\\tfor\\tLinear\\tRegression\\tprediction\\t(see\\t\\nEquation\\t4-19\\n).\\nEquation\\t4-19.\\t\\nSoftmax\\tscore\\tfor\\tclass\\tk\\nNote\\tthat\\teach\\tclass\\thas\\tits\\town\\tdedicated\\t\\nparameter\\tvector\\t\\nθ\\n(k)\\n.\\tAll\\tthese\\tvectors\\tare\\ttypically\\tstored\\tas\\nrows\\t\\nin\\ta\\t\\nparameter\\tmatrix\\n\\t\\nΘ\\n.\\nOnce\\tyou\\thave\\tcomputed\\tthe\\tscore\\tof\\tevery\\tclass\\tfor\\tthe\\tinstance\\t\\nx\\n,\\tyou\\tcan\\testimate\\tthe\\tprobability\\t\\nk', 'k\\nthat\\tthe\\tinstance\\tbelongs\\tto\\tclass\\t\\nk\\n\\tby\\trunning\\tthe\\tscores\\tthrough\\tthe\\tsoftmax\\tfunction\\t(\\nEquation\\t4-20\\n):\\tit\\ncomputes\\tthe\\texponential\\tof\\tevery\\tscore,\\tthen\\tnormalizes\\tthem\\t(dividing\\tby\\tthe\\tsum\\tof\\tall\\tthe\\nexponentials).\\nEquation\\t4-20.\\t\\nSoftmax\\tfunction\\nK\\n\\tis\\tthe\\tnumber\\tof\\tclasses.\\ns\\n(\\nx\\n)\\tis\\ta\\tvector\\tcontaining\\tthe\\tscores\\tof\\teach\\tclass\\tfor\\tthe\\tinstance\\t\\nx\\n.\\nσ\\n(\\ns\\n(\\nx\\n))\\nk\\n\\tis\\tthe\\testimated\\tprobability\\tthat\\tthe\\tinstance\\t\\nx\\n\\tbelongs\\tto\\tclass\\t\\nk\\n\\tgiven\\tthe\\tscores\\tof\\teach\\nclass\\tfor\\tthat\\tinstance.\\nJust\\tlike\\tthe\\tLogistic\\tRegression\\tclassifier,\\tthe\\tSoftmax\\tRegression\\tclassifier\\tpredicts\\tthe\\tclass\\twith\\tthe\\nhighest\\testimated\\tprobability\\t(which\\tis\\tsimply\\tthe\\tclass\\twith\\tthe\\thighest\\tscore),\\tas\\tshown\\tin\\t\\nEquation\\t4-\\n21\\n.\\nEquation\\t4-21.\\t\\nSoftmax\\tRegression\\tclassifier\\tprediction', 'The\\t\\nargmax\\n\\toperator\\treturns\\tthe\\tvalue\\tof\\ta\\tvariable\\tthat\\tmaximizes\\ta\\tfunction.\\tIn\\tthis\\tequation,\\tit\\nreturns\\tthe\\tvalue\\tof\\t\\nk\\n\\tthat\\tmaximizes\\tthe\\testimated\\tprobability\\t\\nσ\\n(\\ns\\n(\\nx\\n))\\nk\\n.\\nTIP\\nThe\\tSoftmax\\tRegression\\tclassifier\\tpredicts\\tonly\\tone\\tclass\\tat\\ta\\ttime\\t(i.e.,\\tit\\tis\\tmulticlass,\\tnot\\tmultioutput)\\tso\\tit\\tshould\\tbe\\tused\\tonly\\nwith\\tmutually\\texclusive\\tclasses\\tsuch\\tas\\tdifferent\\ttypes\\tof\\tplants.\\tYou\\tcannot\\tuse\\tit\\tto\\trecognize\\tmultiple\\tpeople\\tin\\tone\\tpicture.\\nNow\\tthat\\tyou\\tknow\\thow\\tthe\\tmodel\\testimates\\tprobabilities\\tand\\tmakes\\tpredictions,\\tlet’s\\ttake\\ta\\tlook\\tat\\ntraining.\\tThe\\tobjective\\tis\\tto\\thave\\ta\\tmodel\\tthat\\testimates\\ta\\thigh\\tprobability\\tfor\\tthe\\ttarget\\tclass\\t(and\\nconsequently\\ta\\tlow\\tprobability\\tfor\\tthe\\tother\\tclasses).\\tMinimizing\\tthe\\tcost\\tfunction\\tshown\\tin\\t\\nEquation\\t4-\\n22\\n,\\tcalled\\t\\nthe\\t\\ncross\\tentropy\\n,\\tshould\\tlead\\tto\\tthis\\tobjective\\tbecause\\tit\\tpenalizes\\tthe\\tmodel\\twhen\\tit\\nestimates\\ta\\tlow\\tprobability\\tfor\\ta\\ttarget\\tclass.\\tCross\\tentropy\\tis\\tfrequently\\tused\\tto\\tmeasure\\thow\\twell\\ta\\tset', 'of\\testimated\\tclass\\tprobabilities\\tmatch\\tthe\\ttarget\\tclasses\\t(we\\twill\\tuse\\tit\\tagain\\tseveral\\ttimes\\tin\\tthe\\nfollowing\\tchapters).\\nEquation\\t4-22.\\t\\nCross\\tentropy\\tcost\\tfunction\\n\\tis\\tequal\\tto\\t1\\tif\\tthe\\ttarget\\tclass\\tfor\\tthe\\ti\\nth\\n\\tinstance\\tis\\t\\nk\\n;\\totherwise,\\tit\\tis\\tequal\\tto\\t0.\\nNotice\\tthat\\twhen\\tthere\\tare\\tjust\\ttwo\\tclasses\\t(\\nK\\n\\t=\\t2),\\tthis\\tcost\\tfunction\\tis\\tequivalent\\tto\\tthe\\tLogistic\\nRegression’s\\tcost\\tfunction\\t(log\\tloss;\\tsee\\t\\nEquation\\t4-17\\n).\\nCROSS\\tENTROPY\\nCross\\tentropy\\toriginated\\tfrom\\tinformation\\ttheory.\\tSuppose\\tyou\\twant\\tto\\tefficiently\\ttransmit\\tinformation\\tabout\\tthe\\tweather\\tevery\\tday.\\tIf\\nthere\\tare\\teight\\toptions\\t(sunny,\\trainy,\\tetc.),\\tyou\\tcould\\tencode\\teach\\toption\\tusing\\t3\\tbits\\tsince\\t2\\n3\\n\\t=\\t8.\\tHowever,\\tif\\tyou\\tthink\\tit\\twill\\tbe\\tsunny\\nalmost\\tevery\\tday,\\tit\\twould\\tbe\\tmuch\\tmore\\tefficient\\tto\\tcode\\t“sunny”\\ton\\tjust\\tone\\tbit\\t(0)\\tand\\tthe\\tother\\tseven\\toptions\\ton\\t4\\tbits\\t(starting\\twith\\na\\t1).\\tCross\\tentropy\\tmeasures\\tthe\\taverage\\tnumber\\tof\\tbits\\tyou\\tactually\\tsend\\tper\\toption.\\tIf\\tyour\\tassumption\\tabout\\tthe\\tweather\\tis\\tperfect,', 'cross\\tentropy\\twill\\tjust\\tbe\\tequal\\tto\\tthe\\tentropy\\tof\\tthe\\tweather\\titself\\t(i.e.,\\tits\\tintrinsic\\tunpredictability).\\tBut\\tif\\tyour\\tassumptions\\tare\\twrong\\n(e.g.,\\tif\\tit\\trains\\toften),\\tcross\\tentropy\\twill\\tbe\\tgreater\\tby\\tan\\tamount\\t\\ncalled\\tthe\\t\\nKullback–Leibler\\tdivergence\\n.\\nThe\\tcross\\tentropy\\tbetween\\ttwo\\tprobability\\tdistributions\\t\\np\\n\\tand\\t\\nq\\n\\tis\\tdefined\\tas\\t\\n\\t(at\\tleast\\nwhen\\tthe\\tdistributions\\tare\\tdiscrete).\\nThe\\t\\ngradient\\tvector\\tof\\tthis\\tcost\\tfunction\\twith\\tregards\\tto\\t\\nθ\\n(k)\\n\\tis\\tgiven\\tby\\t\\nEquation\\t4-23\\n:\\nEquation\\t4-23.\\t\\nCross\\tentropy\\tgradient\\tvector\\tfor\\tclass\\tk', 'Now\\tyou\\tcan\\tcompute\\tthe\\tgradient\\tvector\\tfor\\tevery\\tclass,\\tthen\\tuse\\tGradient\\tDescent\\t(or\\tany\\tother\\noptimization\\talgorithm)\\tto\\tfind\\tthe\\tparameter\\tmatrix\\t\\nΘ\\n\\tthat\\tminimizes\\tthe\\tcost\\tfunction.\\nLet’s\\tuse\\tSoftmax\\tRegression\\tto\\tclassify\\tthe\\tiris\\tflowers\\tinto\\tall\\tthree\\tclasses.\\tScikit-Learn’s\\nLogisticRegression\\n\\t\\nuses\\t\\none-versus-all\\tby\\tdefault\\twhen\\tyou\\ttrain\\tit\\ton\\tmore\\tthan\\ttwo\\tclasses,\\tbut\\tyou\\ncan\\tset\\tthe\\t\\nmulti_class\\n\\thyperparameter\\tto\\t\\n\"multinomial\"\\n\\tto\\tswitch\\tit\\tto\\tSoftmax\\tRegression\\tinstead.\\nYou\\tmust\\talso\\tspecify\\ta\\tsolver\\tthat\\tsupports\\tSoftmax\\tRegression,\\tsuch\\tas\\tthe\\t\\n\"lbfgs\"\\n\\tsolver\\t(see\\tScikit-\\nLearn’s\\tdocumentation\\tfor\\tmore\\tdetails).\\tIt\\talso\\tapplies\\tℓ\\n2\\n\\t\\nregularization\\tby\\tdefault,\\twhich\\tyou\\tcan\\ncontrol\\tusing\\tthe\\thyperparameter\\t\\nC\\n.\\nX\\n\\t\\n=\\n\\t\\niris\\n[\\n\"data\"\\n][:,\\n\\t\\n(\\n2\\n,\\n\\t\\n3\\n)]\\n\\t\\t\\n#\\tpetal\\tlength,\\tpetal\\twidth\\ny\\n\\t\\n=\\n\\t\\niris\\n[\\n\"target\"\\n]\\nsoftmax_reg\\n\\t\\n=\\n\\t\\nLogisticRegression\\n(\\nmulti_class\\n=\\n\"multinomial\"\\n,\\nsolver\\n=\\n\"lbfgs\"\\n,\\n\\t\\nC\\n=\\n10\\n)\\nsoftmax_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)', 'X\\n,\\n\\t\\ny\\n)\\nSo\\tthe\\tnext\\ttime\\tyou\\tfind\\tan\\tiris\\twith\\t5\\tcm\\tlong\\tand\\t2\\tcm\\twide\\tpetals,\\tyou\\tcan\\task\\tyour\\tmodel\\tto\\ttell\\tyou\\nwhat\\ttype\\tof\\tiris\\tit\\tis,\\tand\\tit\\twill\\tanswer\\tIris-Virginica\\t(class\\t2)\\twith\\t94.2%\\tprobability\\t(or\\tIris-\\nVersicolor\\twith\\t5.8%\\tprobability):\\n>>>\\t\\nsoftmax_reg\\n.\\npredict\\n([[\\n5\\n,\\n\\t\\n2\\n]])\\narray([2])\\n>>>\\t\\nsoftmax_reg\\n.\\npredict_proba\\n([[\\n5\\n,\\n\\t\\n2\\n]])\\narray([[\\t\\t6.33134078e-07,\\t\\t\\t5.75276067e-02,\\t\\t\\t9.42471760e-01]])\\nFigure\\t4-25\\n\\tshows\\tthe\\tresulting\\t\\ndecision\\tboundaries,\\trepresented\\tby\\tthe\\tbackground\\tcolors.\\tNotice\\tthat\\nthe\\tdecision\\tboundaries\\tbetween\\tany\\ttwo\\tclasses\\tare\\tlinear.\\tThe\\tfigure\\talso\\tshows\\tthe\\tprobabilities\\tfor\\nthe\\tIris-Versicolor\\tclass,\\trepresented\\tby\\tthe\\tcurved\\tlines\\t(e.g.,\\tthe\\tline\\tlabeled\\twith\\t0.450\\trepresents\\tthe\\n45%\\tprobability\\tboundary).\\tNotice\\tthat\\tthe\\tmodel\\tcan\\tpredict\\ta\\tclass\\tthat\\thas\\tan\\testimated\\tprobability\\nbelow\\t50%.\\tFor\\texample,\\tat\\tthe\\tpoint\\twhere\\tall\\tdecision\\tboundaries\\tmeet,\\tall\\tclasses\\thave\\tan\\tequal\\nestimated\\tprobability\\t\\nof\\t33%.', 'Figure\\t4-25.\\t\\nSoftmax\\tRegression\\tdecision\\tboundaries', 'Exercises\\n1\\n.\\t\\nWhat\\tLinear\\tRegression\\ttraining\\talgorithm\\tcan\\tyou\\tuse\\tif\\tyou\\thave\\ta\\ttraining\\tset\\twith\\tmillions\\tof\\nfeatures?\\n2\\n.\\t\\nSuppose\\tthe\\tfeatures\\tin\\tyour\\ttraining\\tset\\thave\\tvery\\tdifferent\\tscales.\\tWhat\\talgorithms\\tmight\\tsuffer\\nfrom\\tthis,\\tand\\thow?\\tWhat\\tcan\\tyou\\tdo\\tabout\\tit?\\n3\\n.\\t\\nCan\\tGradient\\tDescent\\tget\\tstuck\\tin\\ta\\tlocal\\tminimum\\twhen\\ttraining\\ta\\tLogistic\\tRegression\\tmodel?\\n4\\n.\\t\\nDo\\tall\\tGradient\\tDescent\\talgorithms\\tlead\\tto\\tthe\\tsame\\tmodel\\tprovided\\tyou\\tlet\\tthem\\trun\\tlong\\tenough?\\n5\\n.\\t\\nSuppose\\tyou\\tuse\\tBatch\\tGradient\\tDescent\\tand\\tyou\\tplot\\tthe\\tvalidation\\terror\\tat\\tevery\\tepoch.\\tIf\\tyou\\nnotice\\tthat\\tthe\\tvalidation\\terror\\tconsistently\\tgoes\\tup,\\twhat\\tis\\tlikely\\tgoing\\ton?\\tHow\\tcan\\tyou\\tfix\\tthis?\\n6\\n.\\t\\nIs\\tit\\ta\\tgood\\tidea\\tto\\tstop\\tMini-batch\\tGradient\\tDescent\\timmediately\\twhen\\tthe\\tvalidation\\terror\\tgoes\\nup?\\n7\\n.\\t\\nWhich\\tGradient\\tDescent\\talgorithm\\t(among\\tthose\\twe\\tdiscussed)\\twill\\treach\\tthe\\tvicinity\\tof\\tthe\\toptimal\\nsolution\\tthe\\tfastest?\\tWhich\\twill\\tactually\\tconverge?\\tHow\\tcan\\tyou\\tmake\\tthe\\tothers\\tconverge\\tas\\twell?\\n8\\n.', '8\\n.\\t\\nSuppose\\tyou\\tare\\tusing\\tPolynomial\\tRegression.\\tYou\\tplot\\tthe\\tlearning\\tcurves\\tand\\tyou\\tnotice\\tthat\\tthere\\nis\\ta\\tlarge\\tgap\\tbetween\\tthe\\ttraining\\terror\\tand\\tthe\\tvalidation\\terror.\\tWhat\\tis\\thappening?\\tWhat\\tare\\tthree\\nways\\tto\\tsolve\\tthis?\\n9\\n.\\t\\nSuppose\\tyou\\tare\\tusing\\tRidge\\tRegression\\tand\\tyou\\tnotice\\tthat\\tthe\\ttraining\\terror\\tand\\tthe\\tvalidation\\nerror\\tare\\talmost\\tequal\\tand\\tfairly\\thigh.\\tWould\\tyou\\tsay\\tthat\\tthe\\tmodel\\tsuffers\\tfrom\\thigh\\tbias\\tor\\thigh\\nvariance?\\tShould\\tyou\\tincrease\\tthe\\tregularization\\thyperparameter\\t\\nα\\n\\tor\\treduce\\tit?\\n10\\n.\\t\\nWhy\\twould\\tyou\\twant\\tto\\tuse:\\nRidge\\tRegression\\tinstead\\tof\\tplain\\tLinear\\tRegression\\t(i.e.,\\twithout\\tany\\tregularization)?\\nLasso\\tinstead\\tof\\tRidge\\tRegression?\\nElastic\\tNet\\tinstead\\tof\\tLasso?\\n11\\n.\\t\\nSuppose\\tyou\\twant\\tto\\tclassify\\tpictures\\tas\\toutdoor/indoor\\tand\\tdaytime/nighttime.\\tShould\\tyou\\nimplement\\ttwo\\tLogistic\\tRegression\\tclassifiers\\tor\\tone\\tSoftmax\\tRegression\\tclassifier?\\n12\\n.\\t\\nImplement\\tBatch\\tGradient\\tDescent\\twith\\tearly\\tstopping\\tfor\\tSoftmax\\tRegression\\t\\n(without\\tusing\\tScikit-\\nLearn).', 'Learn).\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\nIt\\tis\\toften\\tthe\\tcase\\tthat\\ta\\tlearning\\talgorithm\\twill\\ttry\\tto\\toptimize\\ta\\tdifferent\\tfunction\\tthan\\tthe\\tperformance\\tmeasure\\tused\\tto\\tevaluate\\tthe\\nfinal\\tmodel.\\tThis\\tis\\tgenerally\\tbecause\\tthat\\tfunction\\tis\\teasier\\tto\\tcompute,\\tbecause\\tit\\thas\\tuseful\\tdifferentiation\\tproperties\\tthat\\tthe\\nperformance\\tmeasure\\tlacks,\\tor\\tbecause\\twe\\twant\\tto\\tconstrain\\tthe\\tmodel\\tduring\\ttraining,\\tas\\twe\\twill\\tsee\\twhen\\twe\\tdiscuss\\tregularization.\\n1', 'The\\tdemonstration\\tthat\\tthis\\treturns\\tthe\\tvalue\\tof\\t\\nθ\\n\\tthat\\tminimizes\\tthe\\tcost\\tfunction\\tis\\toutside\\tthe\\tscope\\tof\\tthis\\tbook.\\nNote\\tthat\\tScikit-Learn\\tseparates\\tthe\\tbias\\tterm\\t(\\nintercept_\\n)\\tfrom\\tthe\\tfeature\\tweights\\t(\\ncoef_\\n).\\nTechnically\\tspeaking,\\tits\\tderivative\\tis\\t\\nLipschitz\\tcontinuous\\n.\\nSince\\tfeature\\t1\\tis\\tsmaller,\\tit\\ttakes\\ta\\tlarger\\tchange\\tin\\t\\nθ\\n1\\n\\tto\\taffect\\tthe\\tcost\\tfunction,\\twhich\\tis\\twhy\\tthe\\tbowl\\tis\\telongated\\talong\\tthe\\t\\nθ\\n1\\n\\taxis.\\nEta\\t(\\nη\\n)\\tis\\tthe\\t7\\nletter\\tof\\tthe\\tGreek\\talphabet.\\nOut-of-core\\talgorithms\\tare\\tdiscussed\\tin\\t\\nChapter\\t1\\n.\\nWhile\\tthe\\tNormal\\tEquation\\tcan\\tonly\\tperform\\tLinear\\tRegression,\\tthe\\tGradient\\tDescent\\talgorithms\\tcan\\tbe\\tused\\tto\\ttrain\\tmany\\tother\\tmodels,\\nas\\twe\\twill\\tsee.\\nA\\tquadratic\\tequation\\tis\\tof\\tthe\\tform\\t\\ny\\n\\t=\\t\\nax\\n+\\t\\nbx\\n\\t+\\t\\nc\\n.\\nThis\\tnotion\\tof\\tbias\\tis\\tnot\\tto\\tbe\\tconfused\\twith\\tthe\\tbias\\tterm\\tof\\tlinear\\tmodels.\\nIt\\tis\\tcommon\\tto\\tuse\\tthe\\tnotation\\t\\nJ\\n(\\nθ\\n)\\tfor\\tcost\\tfunctions\\tthat\\tdon’t\\thave\\ta\\tshort\\tname;\\twe\\twill\\toften\\tuse\\tthis\\tnotation\\tthroughout\\tthe\\trest\\tof', 'this\\tbook.\\tThe\\tcontext\\twill\\tmake\\tit\\tclear\\twhich\\tcost\\tfunction\\tis\\tbeing\\tdiscussed.\\nNorms\\tare\\tdiscussed\\tin\\t\\nChapter\\t2\\n.\\nA\\tsquare\\tmatrix\\tfull\\tof\\t0s\\texcept\\tfor\\t1s\\ton\\tthe\\tmain\\tdiagonal\\t(top-left\\tto\\tbottom-right).\\nAlternatively\\tyou\\tcan\\tuse\\tthe\\t\\nRidge\\n\\tclass\\twith\\tthe\\t\\n\"sag\"\\n\\tsolver.\\tStochastic\\tAverage\\tGD\\t\\nis\\ta\\tvariant\\tof\\tSGD.\\tFor\\tmore\\tdetails,\\tsee\\tthe\\npresentation\\t\\n“Minimizing\\tFinite\\tSums\\twith\\tthe\\tStochastic\\tAverage\\tGradient\\tAlgorithm”\\n\\tby\\tMark\\tSchmidt\\tet\\tal.\\tfrom\\tthe\\tUniversity\\tof\\nBritish\\tColumbia.\\nYou\\tcan\\tthink\\tof\\ta\\tsubgradient\\tvector\\tat\\ta\\tnondifferentiable\\tpoint\\tas\\tan\\tintermediate\\tvector\\tbetween\\tthe\\tgradient\\tvectors\\taround\\tthat\\tpoint.\\nPhotos\\treproduced\\tfrom\\tthe\\tcorresponding\\tWikipedia\\tpages.\\tIris-Virginica\\tphoto\\tby\\tFrank\\tMayfield\\t(\\nCreative\\tCommons\\tBY-SA\\t2.0\\n),\\tIris-\\nVersicolor\\tphoto\\tby\\tD.\\tGordon\\tE.\\tRobertson\\t(\\nCreative\\tCommons\\tBY-SA\\t3.0\\n),\\tand\\tIris-Setosa\\tphoto\\tis\\tpublic\\tdomain.\\nIt\\tis\\tthe\\tthe\\tset\\tof\\tpoints\\t\\nx\\n\\tsuch\\tthat\\t\\nθ\\n0\\n\\t+\\t\\nθ\\n1\\nx\\n1\\n\\t+\\t\\nθ\\n2\\nx\\n2\\n\\t=\\t0,\\twhich\\tdefines\\ta\\tstraight\\tline.\\n2', '2\\n3\\n4\\n5\\n6\\nth\\n7\\n8\\n9\\n2\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17', 'Chapter\\t5.\\t\\nSupport\\tVector\\tMachines\\nA\\t\\nSupport\\tVector\\tMachine\\n\\t(SVM)\\tis\\t\\na\\tvery\\tpowerful\\tand\\tversatile\\tMachine\\tLearning\\tmodel,\\tcapable\\tof\\nperforming\\tlinear\\tor\\tnonlinear\\tclassification,\\tregression,\\tand\\teven\\toutlier\\tdetection.\\tIt\\tis\\tone\\tof\\tthe\\tmost\\npopular\\tmodels\\tin\\tMachine\\tLearning,\\tand\\tanyone\\tinterested\\tin\\tMachine\\tLearning\\tshould\\thave\\tit\\tin\\ttheir\\ntoolbox.\\tSVMs\\tare\\tparticularly\\twell\\tsuited\\tfor\\tclassification\\tof\\tcomplex\\tbut\\tsmall-\\tor\\tmedium-sized\\ndatasets.\\nThis\\tchapter\\twill\\texplain\\tthe\\tcore\\tconcepts\\tof\\tSVMs,\\thow\\tto\\tuse\\tthem,\\tand\\thow\\tthey\\twork.', 'Linear\\tSVM\\tClassification\\nThe\\t\\nfundamental\\tidea\\tbehind\\tSVMs\\tis\\tbest\\texplained\\twith\\tsome\\tpictures.\\t\\nFigure\\t5-1\\n\\tshows\\tpart\\tof\\tthe\\niris\\tdataset\\tthat\\twas\\tintroduced\\tat\\tthe\\tend\\tof\\t\\nChapter\\t4\\n.\\tThe\\ttwo\\tclasses\\tcan\\tclearly\\tbe\\tseparated\\teasily\\nwith\\ta\\tstraight\\tline\\t(they\\tare\\t\\nlinearly\\tseparable\\n).\\tThe\\tleft\\tplot\\tshows\\tthe\\tdecision\\tboundaries\\tof\\tthree\\npossible\\tlinear\\tclassifiers.\\tThe\\tmodel\\twhose\\tdecision\\tboundary\\tis\\trepresented\\tby\\tthe\\tdashed\\tline\\tis\\tso\\nbad\\tthat\\tit\\tdoes\\tnot\\teven\\tseparate\\tthe\\tclasses\\tproperly.\\tThe\\tother\\ttwo\\tmodels\\twork\\tperfectly\\ton\\tthis\\ntraining\\tset,\\tbut\\ttheir\\tdecision\\tboundaries\\tcome\\tso\\tclose\\tto\\tthe\\tinstances\\tthat\\tthese\\tmodels\\twill\\tprobably\\nnot\\tperform\\tas\\twell\\ton\\tnew\\tinstances.\\tIn\\tcontrast,\\tthe\\tsolid\\tline\\tin\\tthe\\tplot\\ton\\tthe\\tright\\trepresents\\tthe\\ndecision\\tboundary\\tof\\tan\\tSVM\\tclassifier;\\tthis\\tline\\tnot\\tonly\\tseparates\\tthe\\ttwo\\tclasses\\tbut\\talso\\tstays\\tas\\tfar\\naway\\tfrom\\tthe\\tclosest\\ttraining\\tinstances\\tas\\tpossible.\\tYou\\tcan\\tthink\\tof\\tan\\tSVM\\tclassifier\\tas\\tfitting\\tthe', 'widest\\tpossible\\tstreet\\t(represented\\tby\\tthe\\tparallel\\tdashed\\tlines)\\tbetween\\tthe\\tclasses.\\tThis\\tis\\t\\ncalled\\t\\nlarge\\nmargin\\tclassification\\n.\\nFigure\\t5-1.\\t\\nLarge\\tmargin\\tclassification\\nNotice\\tthat\\tadding\\tmore\\ttraining\\tinstances\\t“off\\tthe\\tstreet”\\twill\\tnot\\taffect\\tthe\\tdecision\\tboundary\\tat\\tall:\\tit\\tis\\nfully\\tdetermined\\t(or\\t“supported”)\\tby\\tthe\\tinstances\\tlocated\\ton\\tthe\\tedge\\tof\\tthe\\tstreet.\\tThese\\tinstances\\tare\\ncalled\\t\\nthe\\t\\nsupport\\tvectors\\n\\t(they\\tare\\tcircled\\tin\\t\\nFigure\\t5-1\\n).\\nWARNING\\nSVMs\\tare\\t\\nsensitive\\tto\\tthe\\tfeature\\tscales,\\tas\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t5-2\\n:\\ton\\tthe\\tleft\\tplot,\\tthe\\tvertical\\tscale\\tis\\tmuch\\tlarger\\tthan\\tthe\\nhorizontal\\tscale,\\tso\\tthe\\twidest\\tpossible\\tstreet\\tis\\tclose\\tto\\thorizontal.\\tAfter\\tfeature\\tscaling\\t(e.g.,\\tusing\\tScikit-Learn’s\\nStandardScaler\\n),\\t\\nthe\\tdecision\\tboundary\\tlooks\\tmuch\\tbetter\\t(on\\tthe\\tright\\tplot).\\nFigure\\t5-2.\\t\\nSensitivity\\tto\\tfeature\\tscales', 'Soft\\tMargin\\tClassification\\nIf\\twe\\t\\nstrictly\\timpose\\tthat\\tall\\tinstances\\tbe\\toff\\tthe\\tstreet\\tand\\ton\\tthe\\tright\\tside,\\tthis\\tis\\tcalled\\t\\nhard\\tmargin\\nclassification\\n.\\t\\nThere\\tare\\ttwo\\tmain\\tissues\\twith\\thard\\tmargin\\tclassification.\\tFirst,\\tit\\tonly\\tworks\\tif\\tthe\\tdata\\nis\\tlinearly\\tseparable,\\tand\\tsecond\\tit\\tis\\tquite\\tsensitive\\tto\\toutliers.\\t\\nFigure\\t5-3\\n\\tshows\\tthe\\tiris\\tdataset\\twith\\njust\\tone\\tadditional\\toutlier:\\ton\\tthe\\tleft,\\tit\\tis\\timpossible\\tto\\tfind\\ta\\thard\\tmargin,\\tand\\ton\\tthe\\tright\\tthe\\tdecision\\nboundary\\tends\\tup\\tvery\\tdifferent\\tfrom\\tthe\\tone\\twe\\tsaw\\tin\\t\\nFigure\\t5-1\\n\\twithout\\tthe\\toutlier,\\tand\\tit\\twill\\nprobably\\tnot\\tgeneralize\\tas\\twell.\\nFigure\\t5-3.\\t\\nHard\\tmargin\\tsensitivity\\tto\\toutliers\\nTo\\tavoid\\tthese\\tissues\\t\\nit\\tis\\tpreferable\\tto\\tuse\\ta\\tmore\\tflexible\\tmodel.\\tThe\\tobjective\\tis\\tto\\tfind\\ta\\tgood\\nbalance\\tbetween\\tkeeping\\tthe\\tstreet\\tas\\tlarge\\tas\\tpossible\\tand\\tlimiting\\tthe\\t\\nmargin\\tviolations\\n\\t\\n(i.e.,\\tinstances\\nthat\\tend\\tup\\tin\\tthe\\tmiddle\\tof\\tthe\\tstreet\\tor\\teven\\ton\\tthe\\twrong\\tside).\\tThis\\tis\\tcalled\\t\\nsoft\\tmargin\\nclassification\\n.', '.\\nIn\\tScikit-Learn’s\\tSVM\\tclasses,\\tyou\\tcan\\tcontrol\\tthis\\tbalance\\tusing\\tthe\\t\\nC\\n\\thyperparameter:\\ta\\tsmaller\\t\\nC\\n\\tvalue\\nleads\\tto\\ta\\twider\\tstreet\\tbut\\tmore\\tmargin\\tviolations.\\t\\nFigure\\t5-4\\n\\tshows\\tthe\\tdecision\\tboundaries\\tand\\tmargins\\nof\\ttwo\\tsoft\\tmargin\\tSVM\\tclassifiers\\ton\\ta\\tnonlinearly\\tseparable\\tdataset.\\tOn\\tthe\\tleft,\\tusing\\ta\\thigh\\t\\nC\\n\\tvalue\\nthe\\tclassifier\\tmakes\\tfewer\\tmargin\\tviolations\\tbut\\tends\\tup\\twith\\ta\\tsmaller\\tmargin.\\tOn\\tthe\\tright,\\tusing\\ta\\tlow\\nC\\n\\tvalue\\tthe\\tmargin\\tis\\tmuch\\tlarger,\\tbut\\tmany\\tinstances\\tend\\tup\\ton\\tthe\\tstreet.\\tHowever,\\tit\\tseems\\tlikely\\tthat\\nthe\\tsecond\\tclassifier\\twill\\tgeneralize\\tbetter:\\tin\\tfact\\teven\\ton\\tthis\\ttraining\\tset\\tit\\tmakes\\tfewer\\tprediction\\nerrors,\\tsince\\tmost\\tof\\tthe\\tmargin\\tviolations\\tare\\tactually\\ton\\tthe\\tcorrect\\tside\\tof\\tthe\\tdecision\\tboundary.\\nFigure\\t5-4.\\t\\nFewer\\tmargin\\tviolations\\tversus\\tlarge\\tmargin\\nTIP\\nIf\\tyour\\tSVM\\tmodel\\tis\\t\\noverfitting,\\tyou\\tcan\\ttry\\tregularizing\\tit\\tby\\treducing\\t\\nC\\n.', 'The\\tfollowing\\tScikit-Learn\\tcode\\tloads\\tthe\\tiris\\tdataset,\\tscales\\tthe\\tfeatures,\\tand\\tthen\\ttrains\\ta\\tlinear\\tSVM\\nmodel\\t\\n(using\\tthe\\t\\nLinearSVC\\n\\tclass\\twith\\t\\nC\\n\\t=\\t0.1\\tand\\tthe\\t\\nhinge\\tloss\\n\\tfunction,\\tdescribed\\tshortly)\\tto\\tdetect\\nIris-Virginica\\tflowers.\\tThe\\tresulting\\tmodel\\tis\\trepresented\\ton\\t\\nthe\\tright\\tof\\t\\nFigure\\t5-4\\n.\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nfrom\\n\\t\\nsklearn\\n\\t\\nimport\\n\\t\\ndatasets\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nPipeline\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nStandardScaler\\nfrom\\n\\t\\nsklearn.svm\\n\\t\\nimport\\n\\t\\nLinearSVC\\niris\\n\\t\\n=\\n\\t\\ndatasets\\n.\\nload_iris\\n()\\nX\\n\\t\\n=\\n\\t\\niris\\n[\\n\"data\"\\n][:,\\n\\t\\n(\\n2\\n,\\n\\t\\n3\\n)]\\n\\t\\t\\n#\\tpetal\\tlength,\\tpetal\\twidth\\ny\\n\\t\\n=\\n\\t\\n(\\niris\\n[\\n\"target\"\\n]\\n\\t\\n==\\n\\t\\n2\\n)\\n.\\nastype\\n(\\nnp\\n.\\nfloat64\\n)\\n\\t\\t\\n#\\tIris-Virginica\\nsvm_clf\\n\\t\\n=\\n\\t\\nPipeline\\n((\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"scaler\"\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"linear_svc\"\\n,\\n\\t\\nLinearSVC\\n(\\nC\\n=\\n1\\n,\\n\\t\\nloss\\n=\\n\"hinge\"\\n)),\\n\\t\\t\\t\\t\\n))\\nsvm_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nThen,\\tas\\tusual,\\tyou\\tcan\\tuse\\tthe\\tmodel\\tto\\tmake\\tpredictions:\\n>>>\\t\\nsvm_clf\\n.\\npredict\\n([[\\n5.5\\n,\\n\\t\\n1.7\\n]])\\narray([\\t1.])\\nNOTE', 'NOTE\\nUnlike\\tLogistic\\tRegression\\tclassifiers,\\tSVM\\tclassifiers\\tdo\\tnot\\toutput\\tprobabilities\\tfor\\teach\\tclass.\\nAlternatively,\\t\\nyou\\tcould\\tuse\\tthe\\t\\nSVC\\n\\tclass,\\tusing\\t\\nSVC(kernel=\"linear\",\\tC=1)\\n,\\tbut\\tit\\tis\\tmuch\\tslower,\\nespecially\\twith\\tlarge\\ttraining\\tsets,\\tso\\tit\\tis\\tnot\\trecommended.\\tAnother\\toption\\tis\\tto\\tuse\\tthe\\t\\nSGDClassifier\\nclass,\\twith\\t\\nSGDClassifier(loss=\"hinge\",\\talpha=1/(m*C))\\n.\\tThis\\tapplies\\tregular\\t\\nStochastic\\nGradient\\tDescent\\t(see\\t\\nChapter\\t4\\n)\\tto\\ttrain\\ta\\tlinear\\tSVM\\tclassifier.\\tIt\\tdoes\\tnot\\tconverge\\tas\\tfast\\tas\\tthe\\nLinearSVC\\n\\tclass,\\tbut\\tit\\tcan\\tbe\\tuseful\\tto\\thandle\\thuge\\tdatasets\\tthat\\tdo\\tnot\\tfit\\tin\\tmemory\\t(out-of-core\\ntraining),\\tor\\tto\\thandle\\tonline\\tclassification\\ttasks.\\nTIP\\nThe\\t\\nLinearSVC\\n\\tclass\\tregularizes\\tthe\\tbias\\tterm,\\tso\\tyou\\tshould\\tcenter\\tthe\\ttraining\\tset\\tfirst\\tby\\tsubtracting\\tits\\tmean.\\tThis\\tis\\nautomatic\\tif\\tyou\\tscale\\tthe\\tdata\\tusing\\tthe\\t\\nStandardScaler\\n.\\tMoreover,\\tmake\\tsure\\tyou\\tset\\tthe\\t\\nloss\\n\\thyperparameter\\tto\\t\\n\"hinge\"\\n,\\tas', ',\\tas\\nit\\tis\\tnot\\tthe\\tdefault\\tvalue.\\tFinally,\\tfor\\tbetter\\tperformance\\tyou\\tshould\\tset\\tthe\\t\\ndual\\n\\thyperparameter\\tto\\t\\nFalse\\n,\\tunless\\tthere\\tare\\tmore\\nfeatures\\tthan\\ttraining\\tinstances\\t(we\\twill\\tdiscuss\\tduality\\tlater\\tin\\tthe\\t\\nchapter).', 'Nonlinear\\tSVM\\tClassification\\nAlthough\\t\\nlinear\\tSVM\\tclassifiers\\tare\\tefficient\\tand\\twork\\tsurprisingly\\twell\\tin\\tmany\\tcases,\\tmany\\tdatasets\\nare\\tnot\\teven\\tclose\\tto\\tbeing\\tlinearly\\tseparable.\\tOne\\tapproach\\tto\\thandling\\tnonlinear\\tdatasets\\tis\\tto\\tadd\\tmore\\nfeatures,\\tsuch\\tas\\t\\npolynomial\\tfeatures\\t(as\\tyou\\tdid\\tin\\t\\nChapter\\t4\\n);\\tin\\tsome\\tcases\\tthis\\tcan\\tresult\\tin\\ta\\tlinearly\\nseparable\\tdataset.\\tConsider\\tthe\\tleft\\tplot\\tin\\t\\nFigure\\t5-5\\n:\\tit\\trepresents\\ta\\tsimple\\tdataset\\twith\\tjust\\tone\\tfeature\\nx\\n1\\n.\\tThis\\tdataset\\tis\\tnot\\tlinearly\\tseparable,\\tas\\tyou\\tcan\\tsee.\\tBut\\tif\\tyou\\tadd\\ta\\tsecond\\tfeature\\t\\nx\\n2\\n\\t=\\t(\\nx\\n1\\n)\\n2\\n,\\tthe\\nresulting\\t2D\\tdataset\\tis\\tperfectly\\tlinearly\\tseparable.\\nFigure\\t5-5.\\t\\nAdding\\tfeatures\\tto\\tmake\\ta\\tdataset\\tlinearly\\tseparable\\nTo\\timplement\\tthis\\tidea\\tusing\\t\\nScikit-Learn,\\tyou\\tcan\\tcreate\\ta\\t\\nPipeline\\n\\tcontaining\\ta\\t\\nPolynomialFeatures\\ntransformer\\t(discussed\\tin\\t\\n“Polynomial\\tRegression”\\n),\\tfollowed\\tby\\ta\\t\\nStandardScaler\\n\\tand\\ta\\t\\nLinearSVC\\n.\\nLet’s\\ttest\\tthis\\ton\\tthe\\tmoons\\tdataset\\t(see\\t\\nFigure\\t5-6\\n):\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport', 'import\\n\\t\\nmake_moons\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nPipeline\\nfrom\\n\\t\\nsklearn.preprocessing\\n\\t\\nimport\\n\\t\\nPolynomialFeatures\\npolynomial_svm_clf\\n\\t\\n=\\n\\t\\nPipeline\\n((\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"poly_features\"\\n,\\n\\t\\nPolynomialFeatures\\n(\\ndegree\\n=\\n3\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"scaler\"\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"svm_clf\"\\n,\\n\\t\\nLinearSVC\\n(\\nC\\n=\\n10\\n,\\n\\t\\nloss\\n=\\n\"hinge\"\\n))\\n\\t\\t\\t\\t\\n))\\npolynomial_svm_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)', 'Figure\\t5-6.\\t\\nLinear\\tSVM\\tclassifier\\tusing\\tpolynomial\\tfeatures', 'Polynomial\\tKernel\\nAdding\\t\\npolynomial\\tfeatures\\tis\\tsimple\\tto\\timplement\\tand\\tcan\\twork\\tgreat\\twith\\tall\\tsorts\\tof\\tMachine\\tLearning\\nalgorithms\\t(not\\tjust\\tSVMs),\\tbut\\tat\\ta\\tlow\\tpolynomial\\tdegree\\tit\\tcannot\\tdeal\\twith\\tvery\\tcomplex\\tdatasets,\\nand\\twith\\ta\\thigh\\tpolynomial\\tdegree\\tit\\tcreates\\ta\\thuge\\tnumber\\tof\\tfeatures,\\tmaking\\tthe\\tmodel\\ttoo\\tslow.\\nFortunately,\\twhen\\tusing\\tSVMs\\tyou\\tcan\\tapply\\tan\\talmost\\tmiraculous\\tmathematical\\ttechnique\\tcalled\\tthe\\nkernel\\ttrick\\n\\t(it\\tis\\texplained\\tin\\ta\\tmoment).\\tIt\\tmakes\\tit\\tpossible\\tto\\tget\\tthe\\tsame\\tresult\\tas\\tif\\tyou\\tadded\\tmany\\npolynomial\\tfeatures,\\teven\\twith\\tvery\\thigh-degree\\tpolynomials,\\twithout\\tactually\\thaving\\tto\\tadd\\tthem.\\tSo\\nthere\\tis\\tno\\tcombinatorial\\texplosion\\tof\\tthe\\tnumber\\tof\\tfeatures\\tsince\\tyou\\tdon’t\\tactually\\tadd\\tany\\tfeatures.\\nThis\\ttrick\\tis\\timplemented\\tby\\tthe\\t\\nSVC\\n\\tclass.\\t\\nLet’s\\ttest\\tit\\ton\\tthe\\tmoons\\tdataset:\\nfrom\\n\\t\\nsklearn.svm\\n\\t\\nimport\\n\\t\\nSVC\\npoly_kernel_svm_clf\\n\\t\\n=\\n\\t\\nPipeline\\n((\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"scaler\"\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"svm_clf\"\\n,\\n\\t\\nSVC\\n(\\nkernel\\n=\\n\"poly\"\\n,\\n\\t\\ndegree\\n=\\n3\\n,', '=\\n3\\n,\\n\\t\\ncoef0\\n=\\n1\\n,\\n\\t\\nC\\n=\\n5\\n))\\n\\t\\t\\t\\t\\n))\\npoly_kernel_svm_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nThis\\tcode\\t\\ntrains\\tan\\tSVM\\tclassifier\\tusing\\ta\\t3\\nrd\\n-degree\\tpolynomial\\tkernel.\\tIt\\tis\\trepresented\\ton\\tthe\\tleft\\tof\\nFigure\\t5-7\\n.\\tOn\\tthe\\tright\\tis\\tanother\\tSVM\\tclassifier\\tusing\\ta\\t10\\nth\\n-degree\\tpolynomial\\tkernel.\\tObviously,\\tif\\nyour\\tmodel\\tis\\toverfitting,\\tyou\\tmight\\twant\\tto\\treduce\\tthe\\tpolynomial\\tdegree.\\tConversely,\\tif\\tit\\tis\\nunderfitting,\\tyou\\tcan\\ttry\\tincreasing\\tit.\\tThe\\thyperparameter\\t\\ncoef0\\n\\tcontrols\\thow\\tmuch\\tthe\\tmodel\\tis\\ninfluenced\\tby\\thigh-degree\\tpolynomials\\tversus\\tlow-degree\\tpolynomials.\\nFigure\\t5-7.\\t\\nSVM\\tclassifiers\\twith\\ta\\tpolynomial\\tkernel\\nTIP\\nA\\tcommon\\tapproach\\tto\\tfind\\tthe\\tright\\t\\nhyperparameter\\tvalues\\tis\\tto\\tuse\\tgrid\\t\\nsearch\\t(see\\t\\nChapter\\t2\\n).\\tIt\\tis\\toften\\tfaster\\tto\\tfirst\\tdo\\ta\\nvery\\tcoarse\\tgrid\\tsearch,\\tthen\\ta\\tfiner\\tgrid\\tsearch\\taround\\tthe\\tbest\\tvalues\\tfound.\\tHaving\\ta\\tgood\\tsense\\tof\\twhat\\teach\\nhyperparameter\\tactually\\tdoes\\tcan\\talso\\thelp\\tyou\\tsearch\\tin\\tthe\\tright\\tpart\\tof\\tthe\\t\\nhyperparameter\\tspace.', 'Adding\\tSimilarity\\tFeatures\\nAnother\\t\\ntechnique\\tto\\ttackle\\tnonlinear\\tproblems\\tis\\tto\\tadd\\tfeatures\\tcomputed\\tusing\\ta\\t\\nsimilarity\\tfunction\\nthat\\tmeasures\\thow\\tmuch\\teach\\tinstance\\tresembles\\ta\\tparticular\\t\\nlandmark\\n.\\t\\nFor\\texample,\\tlet’s\\ttake\\tthe\\tone-\\ndimensional\\tdataset\\tdiscussed\\tearlier\\tand\\tadd\\ttwo\\tlandmarks\\tto\\tit\\tat\\t\\nx\\n1\\n\\t=\\t–2\\tand\\t\\nx\\n1\\n\\t=\\t1\\t(see\\tthe\\tleft\\tplot\\nin\\t\\nFigure\\t5-8\\n).\\tNext,\\tlet’s\\tdefine\\tthe\\tsimilarity\\tfunction\\tto\\tbe\\tthe\\t\\nGaussian\\t\\nRadial\\tBasis\\tFunction\\n\\t(\\nRBF\\n)\\nwith\\t\\nγ\\n\\t=\\t0.3\\t(see\\t\\nEquation\\t5-1\\n).\\nEquation\\t5-1.\\t\\nGaussian\\tRBF\\nIt\\tis\\ta\\tbell-shaped\\tfunction\\tvarying\\tfrom\\t0\\t(very\\tfar\\taway\\tfrom\\tthe\\tlandmark)\\tto\\t1\\t(at\\tthe\\tlandmark).\\tNow\\nwe\\tare\\tready\\tto\\tcompute\\tthe\\tnew\\tfeatures.\\tFor\\texample,\\tlet’s\\tlook\\tat\\tthe\\tinstance\\t\\nx\\n1\\n\\t=\\t–1:\\tit\\tis\\tlocated\\tat\\ta\\ndistance\\tof\\t1\\tfrom\\tthe\\tfirst\\tlandmark,\\tand\\t2\\tfrom\\tthe\\tsecond\\tlandmark.\\tTherefore\\tits\\tnew\\tfeatures\\tare\\t\\nx\\n2\\n\\t=\\nexp\\t(–0.3\\t×\\t1\\n2\\n)\\t≈\\t0.74\\tand\\t\\nx\\n3\\n\\t=\\texp\\t(–0.3\\t×\\t2\\n2\\n)\\t≈\\t0.30.\\tThe\\tplot\\ton\\tthe\\tright\\tof\\t\\nFigure\\t5-8\\n\\tshows\\tthe', 'transformed\\tdataset\\t(dropping\\tthe\\toriginal\\tfeatures).\\tAs\\tyou\\tcan\\tsee,\\tit\\tis\\tnow\\tlinearly\\t\\nseparable\\n.\\nFigure\\t5-8.\\t\\nSimilarity\\tfeatures\\tusing\\tthe\\tGaussian\\tRBF\\nYou\\tmay\\twonder\\thow\\tto\\tselect\\tthe\\tlandmarks.\\tThe\\tsimplest\\tapproach\\tis\\tto\\tcreate\\ta\\tlandmark\\tat\\tthe\\nlocation\\tof\\teach\\tand\\tevery\\tinstance\\tin\\tthe\\tdataset.\\tThis\\tcreates\\tmany\\tdimensions\\tand\\tthus\\tincreases\\tthe\\nchances\\tthat\\tthe\\ttransformed\\ttraining\\tset\\twill\\tbe\\tlinearly\\tseparable.\\tThe\\tdownside\\tis\\tthat\\ta\\ttraining\\tset\\nwith\\t\\nm\\n\\tinstances\\tand\\t\\nn\\n\\tfeatures\\tgets\\ttransformed\\tinto\\ta\\ttraining\\tset\\twith\\t\\nm\\n\\tinstances\\tand\\t\\nm\\n\\tfeatures\\n(assuming\\tyou\\tdrop\\tthe\\toriginal\\tfeatures).\\tIf\\tyour\\ttraining\\tset\\tis\\tvery\\tlarge,\\tyou\\tend\\tup\\twith\\tan\\tequally\\nlarge\\tnumber\\tof\\t\\nfeatures.', 'Gaussian\\tRBF\\tKernel\\nJust\\t\\nlike\\tthe\\tpolynomial\\tfeatures\\tmethod,\\tthe\\tsimilarity\\tfeatures\\tmethod\\tcan\\tbe\\tuseful\\twith\\tany\\tMachine\\nLearning\\talgorithm,\\tbut\\tit\\tmay\\tbe\\tcomputationally\\texpensive\\tto\\tcompute\\tall\\tthe\\tadditional\\tfeatures,\\nespecially\\ton\\tlarge\\ttraining\\tsets.\\tHowever,\\tonce\\tagain\\tthe\\t\\nkernel\\ttrick\\tdoes\\tits\\tSVM\\tmagic:\\tit\\tmakes\\tit\\npossible\\tto\\tobtain\\ta\\tsimilar\\tresult\\tas\\tif\\tyou\\thad\\tadded\\tmany\\tsimilarity\\tfeatures,\\twithout\\tactually\\thaving\\tto\\nadd\\tthem.\\tLet’s\\ttry\\tthe\\tGaussian\\tRBF\\tkernel\\tusing\\t\\nthe\\t\\nSVC\\n\\tclass:\\nrbf_kernel_svm_clf\\n\\t\\n=\\n\\t\\nPipeline\\n((\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"scaler\"\\n,\\n\\t\\nStandardScaler\\n()),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"svm_clf\"\\n,\\n\\t\\nSVC\\n(\\nkernel\\n=\\n\"rbf\"\\n,\\n\\t\\ngamma\\n=\\n5\\n,\\n\\t\\nC\\n=\\n0.001\\n))\\n\\t\\t\\t\\t\\n))\\nrbf_kernel_svm_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nThis\\tmodel\\tis\\trepresented\\ton\\tthe\\tbottom\\tleft\\tof\\t\\nFigure\\t5-9\\n.\\tThe\\tother\\tplots\\tshow\\tmodels\\ttrained\\twith\\ndifferent\\tvalues\\tof\\thyperparameters\\t\\ngamma\\n\\t(\\nγ\\n)\\tand\\t\\nC\\n.\\tIncreasing\\t\\ngamma\\n\\tmakes\\tthe\\tbell-shape\\tcurve\\nnarrower\\t(see\\tthe\\tleft\\tplot\\tof\\t\\nFigure\\t5-8', '),\\tand\\tas\\ta\\tresult\\teach\\tinstance’s\\trange\\tof\\tinfluence\\tis\\tsmaller:\\tthe\\ndecision\\tboundary\\tends\\tup\\tbeing\\tmore\\tirregular,\\twiggling\\taround\\tindividual\\tinstances.\\tConversely,\\ta\\nsmall\\t\\ngamma\\n\\t\\nvalue\\tmakes\\tthe\\tbell-shaped\\tcurve\\twider,\\tso\\tinstances\\thave\\ta\\tlarger\\trange\\tof\\tinfluence,\\tand\\nthe\\tdecision\\tboundary\\tends\\tup\\tsmoother.\\tSo\\t\\nγ\\n\\tacts\\tlike\\ta\\tregularization\\thyperparameter:\\tif\\tyour\\tmodel\\tis\\noverfitting,\\tyou\\tshould\\treduce\\tit,\\tand\\tif\\tit\\tis\\t\\nunderfitting,\\tyou\\tshould\\tincrease\\tit\\t(similar\\tto\\tthe\\t\\nC\\nhyperparameter).\\nFigure\\t5-9.\\t\\nSVM\\tclassifiers\\tusing\\tan\\tRBF\\tkernel\\nOther\\tkernels\\texist\\tbut\\tare\\tused\\tmuch\\tmore\\trarely.\\tFor\\texample,\\tsome\\tkernels\\tare\\tspecialized\\tfor\\tspecific\\ndata\\tstructures.\\t\\nString\\tkernels\\n\\t\\nare\\tsometimes\\tused\\twhen\\tclassifying\\ttext\\tdocuments\\tor\\tDNA\\tsequences\\n(e.g.,\\tusing\\tthe\\t\\nstring\\tsubsequence\\tkernel\\n\\tor\\tkernels\\tbased\\ton\\t\\nthe\\t\\nLevenshtein\\tdistance\\n).', 'TIP\\nWith\\tso\\tmany\\tkernels\\tto\\tchoose\\tfrom,\\thow\\tcan\\tyou\\tdecide\\twhich\\tone\\tto\\tuse?\\tAs\\ta\\trule\\tof\\tthumb,\\tyou\\tshould\\talways\\ttry\\tthe\\tlinear\\nkernel\\tfirst\\t(remember\\tthat\\t\\nLinearSVC\\n\\t\\nis\\tmuch\\tfaster\\tthan\\t\\nSVC(kernel=\"linear\")\\n),\\tespecially\\tif\\tthe\\ttraining\\tset\\tis\\tvery\\tlarge\\tor\\nif\\tit\\thas\\tplenty\\tof\\tfeatures.\\tIf\\tthe\\ttraining\\tset\\tis\\tnot\\ttoo\\tlarge,\\tyou\\tshould\\ttry\\tthe\\tGaussian\\tRBF\\tkernel\\tas\\twell;\\tit\\tworks\\twell\\tin\\nmost\\tcases.\\tThen\\tif\\tyou\\thave\\tspare\\ttime\\tand\\tcomputing\\tpower,\\tyou\\tcan\\talso\\texperiment\\twith\\ta\\tfew\\tother\\tkernels\\tusing\\tcross-\\nvalidation\\tand\\tgrid\\tsearch,\\tespecially\\tif\\tthere\\tare\\tkernels\\tspecialized\\tfor\\tyour\\ttraining\\tset’s\\tdata\\t\\nstructure.', 'Computational\\tComplexity\\nThe\\n\\t\\nLinearSVC\\n\\tclass\\tis\\tbased\\ton\\tthe\\t\\nliblinear\\n\\tlibrary,\\t\\nwhich\\timplements\\tan\\t\\noptimized\\talgorithm\\n\\tfor\\nlinear\\tSVMs.\\n1\\n\\tIt\\tdoes\\tnot\\tsupport\\tthe\\tkernel\\ttrick,\\tbut\\tit\\tscales\\talmost\\tlinearly\\twith\\tthe\\tnumber\\tof\\ttraining\\ninstances\\tand\\tthe\\tnumber\\tof\\tfeatures:\\tits\\ttraining\\ttime\\tcomplexity\\tis\\troughly\\t\\nO\\n(\\nm\\n\\t×\\t\\nn\\n).\\nThe\\talgorithm\\ttakes\\tlonger\\tif\\tyou\\trequire\\ta\\tvery\\thigh\\tprecision.\\tThis\\tis\\tcontrolled\\tby\\tthe\\t\\ntolerance\\nhyperparameter\\t\\nϵ\\n\\t(called\\t\\ntol\\n\\tin\\tScikit-Learn).\\tIn\\tmost\\tclassification\\ttasks,\\tthe\\tdefault\\ttolerance\\tis\\tfine.\\nThe\\t\\nSVC\\n\\tclass\\tis\\tbased\\ton\\t\\nthe\\t\\nlibsvm\\n\\tlibrary,\\twhich\\timplements\\t\\nan\\talgorithm\\n\\tthat\\tsupports\\tthe\\tkernel\\ntrick.\\n2\\n\\tThe\\ttraining\\ttime\\tcomplexity\\tis\\tusually\\tbetween\\t\\nO\\n(\\nm\\n2\\n\\t×\\t\\nn\\n)\\tand\\t\\nO\\n(\\nm\\n3\\n\\t×\\t\\nn\\n).\\tUnfortunately,\\tthis\\nmeans\\tthat\\tit\\tgets\\tdreadfully\\tslow\\twhen\\tthe\\tnumber\\tof\\ttraining\\tinstances\\tgets\\tlarge\\t(e.g.,\\thundreds\\tof\\nthousands\\tof\\tinstances).\\tThis\\talgorithm\\tis\\tperfect\\tfor\\tcomplex\\tbut\\tsmall\\tor\\tmedium\\ttraining\\tsets.', 'However,\\tit\\tscales\\twell\\twith\\tthe\\tnumber\\tof\\tfeatures,\\tespecially\\twith\\t\\nsparse\\tfeatures\\n\\t(i.e.,\\twhen\\teach\\ninstance\\thas\\tfew\\tnonzero\\tfeatures).\\tIn\\tthis\\tcase,\\tthe\\talgorithm\\tscales\\troughly\\twith\\tthe\\taverage\\tnumber\\tof\\nnonzero\\tfeatures\\tper\\tinstance.\\t\\nTable\\t5-1\\n\\tcompares\\t\\nScikit-Learn’s\\tSVM\\tclassification\\t\\nclasses.\\nTable\\t5-1.\\t\\nComparison\\tof\\tScikit-Learn\\tclasses\\tfor\\tSVM\\t\\nclassification\\nClass\\nTime\\tcomplexity\\nOut-of-core\\tsupport\\nScaling\\trequired\\nKernel\\ttrick\\nLinearSVC\\nO(\\nm\\n\\t×\\t\\nn\\n)\\nNo\\nYes\\nNo\\nSGDClassifier\\nO(\\nm\\n\\t×\\t\\nn\\n)\\nYes\\nYes\\nNo\\nSVC\\nO(\\nm\\n²\\t×\\t\\nn\\n)\\tto\\tO(\\nm\\n³\\t×\\t\\nn\\n)\\nNo\\nYes\\nYes', 'SVM\\tRegression\\nAs\\t\\nwe\\tmentioned\\tearlier,\\tthe\\tSVM\\talgorithm\\tis\\tquite\\tversatile:\\tnot\\tonly\\tdoes\\tit\\tsupport\\tlinear\\tand\\nnonlinear\\tclassification,\\tbut\\tit\\talso\\tsupports\\tlinear\\tand\\tnonlinear\\tregression.\\tThe\\ttrick\\tis\\tto\\treverse\\tthe\\nobjective:\\tinstead\\tof\\ttrying\\tto\\tfit\\tthe\\tlargest\\tpossible\\tstreet\\tbetween\\ttwo\\tclasses\\twhile\\tlimiting\\tmargin\\nviolations,\\tSVM\\tRegression\\ttries\\tto\\tfit\\tas\\tmany\\tinstances\\tas\\tpossible\\t\\non\\n\\tthe\\tstreet\\twhile\\tlimiting\\tmargin\\nviolations\\t(i.e.,\\tinstances\\t\\noff\\n\\tthe\\tstreet).\\tThe\\twidth\\tof\\tthe\\tstreet\\tis\\tcontrolled\\tby\\ta\\thyperparameter\\t\\nϵ\\n.\\nFigure\\t5-10\\n\\tshows\\ttwo\\tlinear\\tSVM\\tRegression\\tmodels\\ttrained\\ton\\tsome\\trandom\\tlinear\\tdata,\\tone\\twith\\ta\\nlarge\\tmargin\\t(\\nϵ\\n\\t=\\t1.5)\\tand\\tthe\\tother\\twith\\ta\\tsmall\\tmargin\\t(\\nϵ\\n\\t=\\t0.5).\\nFigure\\t5-10.\\t\\nSVM\\tRegression\\nAdding\\tmore\\ttraining\\tinstances\\twithin\\tthe\\tmargin\\tdoes\\tnot\\taffect\\tthe\\tmodel’s\\tpredictions;\\tthus,\\tthe\\tmodel\\nis\\tsaid\\tto\\t\\nbe\\t\\nϵ-insensitive\\n.\\nYou\\tcan\\tuse\\tScikit-Learn’s\\t\\nLinearSVR\\n\\tclass\\t\\nto\\tperform\\tlinear\\tSVM\\tRegression.\\tThe\\tfollowing\\tcode', 'produces\\tthe\\tmodel\\trepresented\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t5-10\\n\\t(the\\ttraining\\tdata\\tshould\\tbe\\tscaled\\tand\\tcentered\\nfirst):\\nfrom\\n\\t\\nsklearn.svm\\n\\t\\nimport\\n\\t\\nLinearSVR\\nsvm_reg\\n\\t\\n=\\n\\t\\nLinearSVR\\n(\\nepsilon\\n=\\n1.5\\n)\\nsvm_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nTo\\ttackle\\tnonlinear\\tregression\\ttasks,\\tyou\\tcan\\tuse\\ta\\tkernelized\\tSVM\\tmodel.\\tFor\\texample,\\t\\nFigure\\t5-11\\nshows\\tSVM\\tRegression\\ton\\ta\\trandom\\tquadratic\\ttraining\\tset,\\tusing\\ta\\t2\\nnd\\n-degree\\tpolynomial\\tkernel.\\tThere\\nis\\tlittle\\tregularization\\ton\\tthe\\tleft\\tplot\\t(i.e.,\\ta\\tlarge\\t\\nC\\n\\tvalue),\\tand\\tmuch\\tmore\\tregularization\\ton\\tthe\\tright\\tplot\\n(i.e.,\\ta\\tsmall\\t\\nC\\n\\tvalue).', 'Figure\\t5-11.\\t\\nSVM\\tregression\\tusing\\ta\\t2\\nnd\\n-degree\\tpolynomial\\tkernel\\nThe\\tfollowing\\tcode\\tproduces\\tthe\\tmodel\\trepresented\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t5-11\\n\\tusing\\tScikit-Learn’s\\t\\nSVR\\nclass\\t(which\\tsupports\\tthe\\tkernel\\ttrick).\\tThe\\t\\nSVR\\n\\tclass\\tis\\tthe\\tregression\\tequivalent\\tof\\tthe\\t\\nSVC\\n\\tclass,\\tand\\nthe\\t\\nLinearSVR\\n\\tclass\\tis\\tthe\\tregression\\tequivalent\\tof\\tthe\\t\\nLinearSVC\\n\\tclass.\\tThe\\t\\nLinearSVR\\n\\tclass\\tscales\\nlinearly\\twith\\tthe\\tsize\\tof\\tthe\\ttraining\\tset\\t(just\\tlike\\tthe\\t\\nLinearSVC\\n\\tclass),\\twhile\\tthe\\t\\nSVR\\n\\tclass\\tgets\\tmuch\\ttoo\\nslow\\twhen\\tthe\\ttraining\\tset\\tgrows\\t\\nlarge\\t\\n(just\\tlike\\tthe\\t\\nSVC\\n\\tclass).\\nfrom\\n\\t\\nsklearn.svm\\n\\t\\nimport\\n\\t\\nSVR\\nsvm_poly_reg\\n\\t\\n=\\n\\t\\nSVR\\n(\\nkernel\\n=\\n\"poly\"\\n,\\n\\t\\ndegree\\n=\\n2\\n,\\n\\t\\nC\\n=\\n100\\n,\\n\\t\\nepsilon\\n=\\n0.1\\n)\\nsvm_poly_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nNOTE\\nSVMs\\tcan\\talso\\tbe\\tused\\tfor\\toutlier\\tdetection;\\tsee\\tScikit-Learn’s\\tdocumentation\\tfor\\tmore\\tdetails.', 'Under\\tthe\\tHood\\nThis\\t\\nsection\\texplains\\thow\\tSVMs\\tmake\\tpredictions\\tand\\thow\\ttheir\\ttraining\\talgorithms\\twork,\\tstarting\\twith\\nlinear\\tSVM\\tclassifiers.\\tYou\\tcan\\tsafely\\tskip\\tit\\tand\\tgo\\tstraight\\tto\\tthe\\texercises\\tat\\tthe\\tend\\tof\\tthis\\tchapter\\tif\\nyou\\tare\\tjust\\tgetting\\tstarted\\twith\\tMachine\\tLearning,\\tand\\tcome\\tback\\tlater\\twhen\\tyou\\twant\\tto\\tget\\ta\\tdeeper\\nunderstanding\\tof\\tSVMs.\\nFirst,\\ta\\tword\\tabout\\tnotations:\\tin\\t\\nChapter\\t4\\n\\twe\\tused\\tthe\\tconvention\\tof\\tputting\\tall\\tthe\\t\\nmodel\\tparameters\\tin\\none\\tvector\\t\\nθ\\n,\\tincluding\\tthe\\tbias\\tterm\\t\\nθ\\n0\\n\\tand\\tthe\\tinput\\tfeature\\tweights\\t\\nθ\\n1\\n\\tto\\t\\nθ\\nn\\n,\\tand\\tadding\\ta\\tbias\\tinput\\t\\nx\\n0\\n\\t=\\n1\\tto\\tall\\tinstances.\\tIn\\tthis\\tchapter,\\twe\\twill\\tuse\\ta\\tdifferent\\tconvention,\\twhich\\tis\\tmore\\tconvenient\\t(and\\tmore\\ncommon)\\twhen\\tyou\\tare\\tdealing\\twith\\tSVMs:\\tthe\\tbias\\tterm\\twill\\tbe\\tcalled\\t\\nb\\n\\tand\\tthe\\tfeature\\tweights\\tvector\\nwill\\tbe\\tcalled\\t\\nw\\n.\\tNo\\tbias\\tfeature\\twill\\tbe\\tadded\\tto\\tthe\\tinput\\t\\nfeature\\tvectors.', 'Decision\\tFunction\\tand\\tPredictions\\nThe\\t\\nlinear\\tSVM\\tclassifier\\tmodel\\tpredicts\\tthe\\tclass\\tof\\ta\\tnew\\tinstance\\t\\nx\\n\\tby\\tsimply\\tcomputing\\tthe\\tdecision\\nfunction\\t\\nw\\nT\\n\\t·\\t\\nx\\n\\t+\\t\\nb\\n\\t=\\t\\nw\\n1\\n\\t\\nx\\n1\\n\\t+\\t\\t+\\t\\nw\\nn\\n\\t\\nx\\nn\\n\\t+\\t\\nb\\n:\\tif\\tthe\\tresult\\tis\\tpositive,\\tthe\\tpredicted\\tclass\\t\\nŷ\\n\\tis\\tthe\\tpositive\\nclass\\t(1),\\tor\\telse\\tit\\tis\\tthe\\tnegative\\tclass\\t(0);\\tsee\\t\\nEquation\\t5-2\\n.\\nEquation\\t5-2.\\t\\nLinear\\tSVM\\tclassifier\\tprediction\\nFigure\\t5-12\\n\\tshows\\tthe\\tdecision\\tfunction\\tthat\\tcorresponds\\tto\\tthe\\tmodel\\ton\\tthe\\tright\\tof\\t\\nFigure\\t5-4\\n:\\tit\\tis\\ta\\ntwo-dimensional\\tplane\\tsince\\tthis\\tdataset\\thas\\ttwo\\tfeatures\\t(petal\\twidth\\tand\\tpetal\\tlength).\\tThe\\tdecision\\nboundary\\tis\\tthe\\tset\\tof\\tpoints\\twhere\\tthe\\tdecision\\tfunction\\tis\\tequal\\tto\\t0:\\tit\\tis\\tthe\\tintersection\\tof\\ttwo\\tplanes,\\nwhich\\tis\\ta\\tstraight\\tline\\t(represented\\tby\\tthe\\tthick\\tsolid\\tline).\\n3\\nFigure\\t5-12.\\t\\nDecision\\tfunction\\tfor\\tthe\\tiris\\tdataset\\nThe\\tdashed\\tlines\\trepresent\\tthe\\tpoints\\twhere\\tthe\\tdecision\\tfunction\\tis\\tequal\\tto\\t1\\tor\\t–1:\\tthey\\tare\\tparallel\\tand', 'at\\tequal\\tdistance\\tto\\tthe\\tdecision\\tboundary,\\tforming\\ta\\tmargin\\taround\\tit.\\tTraining\\ta\\tlinear\\tSVM\\tclassifier\\nmeans\\tfinding\\tthe\\tvalue\\tof\\t\\nw\\n\\tand\\t\\nb\\n\\tthat\\tmake\\tthis\\tmargin\\tas\\twide\\tas\\tpossible\\twhile\\tavoiding\\tmargin\\nviolations\\t(hard\\tmargin)\\tor\\t\\nlimiting\\tthem\\t(soft\\tmargin).', 'Training\\tObjective\\nConsider\\t\\nthe\\tslope\\tof\\tthe\\tdecision\\tfunction:\\tit\\tis\\tequal\\tto\\tthe\\tnorm\\tof\\tthe\\tweight\\tvector,\\t\\t\\nw\\n\\t.\\tIf\\twe\\ndivide\\tthis\\tslope\\tby\\t2,\\tthe\\tpoints\\twhere\\tthe\\tdecision\\tfunction\\tis\\tequal\\tto\\t±1\\tare\\tgoing\\tto\\tbe\\ttwice\\tas\\tfar\\naway\\tfrom\\tthe\\tdecision\\tboundary.\\tIn\\tother\\twords,\\tdividing\\tthe\\tslope\\tby\\t2\\twill\\tmultiply\\tthe\\tmargin\\tby\\t2.\\nPerhaps\\tthis\\tis\\teasier\\tto\\tvisualize\\tin\\t2D\\tin\\t\\nFigure\\t5-13\\n.\\tThe\\tsmaller\\tthe\\tweight\\tvector\\t\\nw\\n,\\tthe\\tlarger\\tthe\\nmargin.\\nFigure\\t5-13.\\t\\nA\\tsmaller\\tweight\\tvector\\tresults\\tin\\ta\\tlarger\\tmargin\\nSo\\twe\\twant\\tto\\tminimize\\t\\t\\nw\\n\\t\\tto\\tget\\ta\\tlarge\\tmargin.\\tHowever,\\tif\\twe\\talso\\twant\\tto\\tavoid\\tany\\tmargin\\nviolation\\t(hard\\tmargin),\\tthen\\twe\\tneed\\tthe\\tdecision\\tfunction\\tto\\tbe\\tgreater\\tthan\\t1\\tfor\\tall\\tpositive\\ttraining\\ninstances,\\tand\\tlower\\tthan\\t–1\\tfor\\tnegative\\ttraining\\tinstances.\\tIf\\twe\\tdefine\\t\\nt\\n(i)\\n\\t=\\t\\n–1\\tfor\\tnegative\\tinstances\\t(if\\ny\\n(i)\\n\\t=\\t0)\\tand\\t\\nt\\n(i)\\n\\t=\\t1\\tfor\\tpositive\\tinstances\\t(if\\t\\ny\\n(i)\\n\\t=\\t1),\\tthen\\twe\\tcan\\texpress\\tthis\\tconstraint\\tas\\t\\nt\\n(i)\\n(\\nw\\nT\\n\\t·\\t\\nx\\n(i)\\n\\t+\\nb\\n)\\t≥\\t1\\tfor\\tall\\tinstances.', 'We\\tcan\\ttherefore\\texpress\\tthe\\thard\\tmargin\\tlinear\\tSVM\\tclassifier\\tobjective\\tas\\tthe\\t\\nconstrained\\noptimization\\n\\t\\nproblem\\tin\\t\\nEquation\\t5-3\\n.\\nEquation\\t5-3.\\t\\nHard\\tmargin\\tlinear\\tSVM\\tclassifier\\tobjective\\nNOTE\\nWe\\tare\\tminimizing\\t\\nw\\nT\\n\\t·\\t\\nw\\n,\\twhich\\tis\\tequal\\tto\\t\\n\\t\\nw\\n\\t\\n2\\n,\\trather\\tthan\\tminimizing\\t\\t\\nw\\n\\t.\\tThis\\tis\\tbecause\\tit\\twill\\tgive\\tthe\\tsame\\nresult\\t(since\\tthe\\tvalues\\tof\\t\\nw\\n\\tand\\t\\nb\\n\\tthat\\tminimize\\ta\\tvalue\\talso\\tminimize\\thalf\\tof\\tits\\tsquare),\\tbut\\t\\n\\t\\nw\\n\\t\\n2\\n\\thas\\ta\\tnice\\tand\\tsimple\\nderivative\\t(it\\tis\\tjust\\t\\nw\\n)\\twhile\\t\\t\\nw\\n\\t\\tis\\tnot\\tdifferentiable\\tat\\t\\nw\\n\\t=\\t\\n0\\n.\\tOptimization\\talgorithms\\twork\\tmuch\\tbetter\\ton\\tdifferentiable\\nfunctions.\\nTo\\tget\\tthe\\tsoft\\tmargin\\tobjective,\\twe\\tneed\\tto\\tintroduce\\t\\na\\t\\nslack\\tvariable\\n\\t\\nζ\\n(i)\\n\\t≥\\t0\\tfor\\teach\\tinstance:\\n4\\n\\t\\nζ\\n(i)\\nmeasures\\thow\\tmuch\\tthe\\ti\\nth\\n\\tinstance\\tis\\tallowed\\tto\\tviolate\\tthe\\tmargin.\\tWe\\tnow\\thave\\ttwo\\tconflicting\\nobjectives:\\tmaking\\tthe\\tslack\\tvariables\\tas\\tsmall\\tas\\tpossible\\tto\\treduce\\tthe\\tmargin\\tviolations,\\tand\\tmaking\\t\\nw\\nT\\n\\t·\\t\\nw', 'w\\nT\\n\\t·\\t\\nw\\n\\tas\\tsmall\\tas\\tpossible\\tto\\tincrease\\tthe\\tmargin.\\tThis\\tis\\twhere\\tthe\\t\\nC\\n\\thyperparameter\\tcomes\\tin:\\tit', 'allows\\tus\\tto\\tdefine\\tthe\\ttradeoff\\tbetween\\tthese\\ttwo\\tobjectives.\\tThis\\tgives\\tus\\tthe\\tconstrained\\t\\noptimization\\nproblem\\tin\\t\\nEquation\\t5-4\\n.\\nEquation\\t5-4.\\t\\nSoft\\tmargin\\tlinear\\tSVM\\tclassifier\\tobjective', 'Quadratic\\tProgramming\\nThe\\t\\nhard\\tmargin\\tand\\tsoft\\tmargin\\tproblems\\tare\\tboth\\tconvex\\tquadratic\\toptimization\\tproblems\\twith\\tlinear\\nconstraints.\\tSuch\\tproblems\\tare\\tknown\\tas\\t\\nQuadratic\\tProgramming\\n\\t(QP)\\tproblems.\\tMany\\toff-the-shelf\\nsolvers\\tare\\tavailable\\tto\\tsolve\\tQP\\tproblems\\tusing\\ta\\tvariety\\tof\\ttechniques\\tthat\\tare\\toutside\\tthe\\tscope\\tof\\tthis\\nbook.\\n5\\n\\tThe\\tgeneral\\tproblem\\tformulation\\tis\\tgiven\\tby\\t\\nEquation\\t5-5\\n.\\nEquation\\t5-5.\\t\\nQuadratic\\tProgramming\\tproblem\\nNote\\tthat\\tthe\\texpression\\t\\nA\\n\\t·\\t\\np\\n\\t≤\\t\\nb\\n\\tactually\\tdefines\\t\\nn\\nc\\n\\tconstraints:\\t\\np\\nT\\n\\t·\\t\\na\\n(i)\\n\\t≤\\t\\nb\\n(i)\\n\\tfor\\t\\ni\\n\\t=\\t1,\\t2,\\t,\\t\\nn\\nc\\n,\\twhere\\na\\n(i)\\n\\tis\\tthe\\tvector\\tcontaining\\tthe\\telements\\tof\\tthe\\ti\\nth\\n\\trow\\tof\\t\\nA\\n\\tand\\t\\nb\\n(i)\\n\\tis\\tthe\\ti\\nth\\n\\telement\\tof\\t\\nb\\n.\\nYou\\tcan\\teasily\\tverify\\tthat\\tif\\tyou\\tset\\tthe\\tQP\\t\\nparameters\\tin\\tthe\\tfollowing\\tway,\\tyou\\tget\\tthe\\thard\\tmargin\\nlinear\\tSVM\\tclassifier\\tobjective:\\nn\\np\\n\\t=\\t\\nn\\n\\t+\\t1,\\twhere\\t\\nn\\n\\tis\\tthe\\tnumber\\tof\\tfeatures\\t(the\\t+1\\tis\\tfor\\tthe\\tbias\\tterm).\\nn\\nc\\n\\t=\\t\\nm\\n,\\twhere\\t\\nm\\n\\tis\\tthe\\tnumber\\tof\\ttraining\\tinstances.\\nH\\n\\tis\\tthe\\t\\nn\\np\\n\\t×\\t\\nn\\np', '×\\t\\nn\\np\\n\\t\\nidentity\\tmatrix,\\texcept\\twith\\ta\\tzero\\tin\\tthe\\ttop-left\\tcell\\t(to\\tignore\\tthe\\tbias\\tterm).\\nf\\n\\t=\\t\\n0\\n,\\tan\\t\\nn\\np\\n-dimensional\\tvector\\tfull\\tof\\t0s.\\nb\\n\\t=\\t\\n1\\n,\\tan\\t\\nn\\nc\\n-dimensional\\tvector\\tfull\\tof\\t1s.\\na\\n(i)\\n\\t=\\t–\\nt\\n(i)\\n\\t\\n\\t\\n(i)\\n,\\twhere\\t\\n\\t\\n(i)\\n\\tis\\tequal\\tto\\t\\nx\\n(i)\\n\\twith\\tan\\textra\\tbias\\tfeature\\t\\n\\t\\n0\\n\\t=\\t1.\\nSo\\tone\\tway\\tto\\ttrain\\ta\\thard\\tmargin\\tlinear\\tSVM\\tclassifier\\tis\\tjust\\tto\\tuse\\tan\\toff-the-shelf\\tQP\\tsolver\\tby\\npassing\\tit\\tthe\\tpreceding\\tparameters.\\tThe\\tresulting\\tvector\\t\\np\\n\\twill\\tcontain\\tthe\\tbias\\tterm\\t\\nb\\n\\t=\\t\\np\\n0\\n\\tand\\tthe\\nfeature\\tweights\\t\\nw\\ni\\n\\t=\\t\\np\\ni\\n\\tfor\\t\\ni\\n\\t=\\t1,\\t2,\\t,\\t\\nm\\n.\\tSimilarly,\\tyou\\tcan\\tuse\\ta\\tQP\\tsolver\\tto\\tsolve\\tthe\\tsoft\\tmargin\\nproblem\\t(see\\tthe\\texercises\\tat\\tthe\\tend\\tof\\tthe\\tchapter).\\nHowever,\\tto\\tuse\\tthe\\tkernel\\ttrick\\twe\\tare\\tgoing\\tto\\tlook\\tat\\ta\\tdifferent\\tconstrained\\toptimization\\t\\nproblem.', 'The\\tDual\\tProblem\\nGiven\\t\\na\\tconstrained\\toptimization\\tproblem,\\tknown\\tas\\t\\nthe\\t\\nprimal\\tproblem\\n,\\tit\\tis\\tpossible\\tto\\texpress\\ta\\ndifferent\\tbut\\tclosely\\trelated\\tproblem,\\tcalled\\t\\nits\\t\\ndual\\tproblem\\n.\\tThe\\tsolution\\tto\\tthe\\tdual\\tproblem\\ttypically\\ngives\\ta\\tlower\\tbound\\tto\\tthe\\tsolution\\tof\\tthe\\tprimal\\tproblem,\\tbut\\tunder\\tsome\\tconditions\\tit\\tcan\\teven\\thave\\tthe\\nsame\\tsolutions\\tas\\tthe\\tprimal\\tproblem.\\tLuckily,\\tthe\\tSVM\\tproblem\\thappens\\tto\\tmeet\\tthese\\tconditions,\\n6\\n\\tso\\nyou\\tcan\\tchoose\\tto\\tsolve\\tthe\\tprimal\\tproblem\\tor\\tthe\\tdual\\tproblem;\\tboth\\twill\\thave\\tthe\\tsame\\tsolution.\\nEquation\\t5-6\\n\\tshows\\tthe\\tdual\\tform\\tof\\tthe\\tlinear\\tSVM\\tobjective\\t(if\\tyou\\tare\\tinterested\\tin\\tknowing\\thow\\tto\\nderive\\tthe\\tdual\\tproblem\\tfrom\\tthe\\tprimal\\tproblem,\\tsee\\t\\nAppendix\\tC\\n).\\nEquation\\t5-6.\\t\\nDual\\tform\\tof\\tthe\\tlinear\\tSVM\\tobjective\\nOnce\\tyou\\tfind\\tthe\\tvector\\t\\n\\tthat\\tminimizes\\tthis\\tequation\\t(using\\ta\\tQP\\tsolver),\\tyou\\tcan\\tcompute\\t\\n\\tand\\t\\nthat\\tminimize\\tthe\\tprimal\\tproblem\\tby\\tusing\\t\\nEquation\\t5-7\\n.\\nEquation\\t5-7.\\t\\nFrom\\tthe\\tdual\\tsolution\\tto\\tthe\\tprimal\\tsolution', 'The\\tdual\\tproblem\\tis\\tfaster\\tto\\tsolve\\tthan\\tthe\\tprimal\\twhen\\tthe\\tnumber\\tof\\ttraining\\tinstances\\tis\\tsmaller\\tthan\\nthe\\tnumber\\tof\\tfeatures.\\tMore\\timportantly,\\tit\\tmakes\\tthe\\tkernel\\ttrick\\tpossible,\\twhile\\tthe\\tprimal\\tdoes\\tnot.\\tSo\\nwhat\\tis\\tthis\\t\\nkernel\\ttrick\\tanyway?', 'Kernelized\\tSVM\\nSuppose\\t\\nyou\\twant\\tto\\tapply\\ta\\t2\\nnd\\n-degree\\tpolynomial\\ttransformation\\tto\\ta\\ttwo-dimensional\\ttraining\\tset\\n(such\\tas\\tthe\\tmoons\\ttraining\\tset),\\tthen\\ttrain\\ta\\tlinear\\tSVM\\tclassifier\\ton\\tthe\\ttransformed\\ttraining\\tset.\\nEquation\\t5-8\\n\\tshows\\tthe\\t2\\nnd\\n-degree\\tpolynomial\\tmapping\\tfunction\\t\\nϕ\\n\\tthat\\tyou\\twant\\tto\\tapply.\\nEquation\\t5-8.\\t\\nSecond-degree\\tpolynomial\\tmapping\\nNotice\\tthat\\tthe\\ttransformed\\tvector\\tis\\tthree-dimensional\\tinstead\\tof\\ttwo-dimensional.\\tNow\\tlet’s\\tlook\\tat\\nwhat\\thappens\\tto\\ta\\tcouple\\tof\\ttwo-dimensional\\tvectors,\\t\\na\\n\\tand\\t\\nb\\n,\\tif\\twe\\tapply\\tthis\\t2\\nnd\\n-degree\\tpolynomial\\nmapping\\tand\\tthen\\tcompute\\tthe\\tdot\\tproduct\\tof\\tthe\\ttransformed\\tvectors\\t(See\\t\\nEquation\\t5-9\\n).\\nEquation\\t5-9.\\t\\nKernel\\ttrick\\tfor\\ta\\t2\\nnd\\n-degree\\tpolynomial\\tmapping\\nHow\\tabout\\tthat?\\tThe\\tdot\\tproduct\\tof\\tthe\\ttransformed\\tvectors\\tis\\tequal\\tto\\tthe\\tsquare\\tof\\tthe\\tdot\\tproduct\\tof\\tthe\\noriginal\\tvectors:\\t\\nϕ\\n(\\na\\n)\\nT\\n\\t·\\t\\nϕ\\n(\\nb\\n)\\t=\\t(\\na\\nT\\n\\t·\\t\\nb\\n)\\n2\\n.\\nNow\\there\\tis\\tthe\\tkey\\tinsight:\\tif\\tyou\\tapply\\tthe\\ttransformation\\t\\nϕ\\n\\tto\\tall\\ttraining\\tinstances,\\tthen\\tthe\\tdual', 'problem\\t(see\\t\\nEquation\\t5-6\\n)\\twill\\tcontain\\tthe\\tdot\\tproduct\\t\\nϕ\\n(\\nx\\n(i)\\n)\\nT\\n\\t·\\t\\nϕ\\n(\\nx\\n(j)\\n).\\tBut\\tif\\t\\nϕ\\n\\tis\\tthe\\t2\\nnd\\n-degree\\npolynomial\\ttransformation\\tdefined\\tin\\t\\nEquation\\t5-8\\n,\\tthen\\tyou\\tcan\\treplace\\tthis\\tdot\\tproduct\\tof\\ttransformed\\nvectors\\tsimply\\tby\\t\\n.\\tSo\\tyou\\tdon’t\\tactually\\tneed\\tto\\ttransform\\tthe\\ttraining\\tinstances\\tat\\tall:\\tjust\\nreplace\\tthe\\tdot\\tproduct\\tby\\tits\\tsquare\\tin\\t\\nEquation\\t5-6\\n.\\tThe\\tresult\\twill\\tbe\\tstrictly\\tthe\\tsame\\tas\\tif\\tyou\\twent\\nthrough\\tthe\\ttrouble\\tof\\tactually\\ttransforming\\tthe\\ttraining\\tset\\tthen\\tfitting\\ta\\tlinear\\tSVM\\talgorithm,\\tbut\\tthis\\ntrick\\tmakes\\tthe\\twhole\\tprocess\\tmuch\\tmore\\tcomputationally\\tefficient.\\tThis\\tis\\tthe\\tessence\\tof\\tthe\\tkernel\\ntrick.\\nThe\\tfunction\\t\\nK\\n(\\na\\n,\\t\\nb\\n)\\t=\\t(\\na\\nT\\n\\t·\\t\\nb\\n)\\n2\\n\\tis\\tcalled\\ta\\t2\\nnd\\n-degree\\t\\npolynomial\\tkernel\\n.\\t\\nIn\\tMachine\\tLearning,\\ta\\t\\nkernel\\nis\\ta\\tfunction\\tcapable\\tof\\tcomputing\\tthe\\tdot\\tproduct\\t\\nϕ\\n(\\na\\n)\\nT\\n\\t·\\t\\nϕ\\n(\\nb\\n)\\tbased\\tonly\\ton\\tthe\\toriginal\\tvectors\\t\\na\\n\\tand\\t\\nb\\n,', 'without\\thaving\\tto\\tcompute\\t(or\\teven\\tto\\tknow\\tabout)\\tthe\\ttransformation\\t\\nϕ\\n.\\t\\nEquation\\t5-10\\n\\tlists\\tsome\\tof\\tthe\\nmost\\tcommonly\\tused\\tkernels.\\nEquation\\t5-10.\\t\\nCommon\\tkernels\\nMERCER’S\\tTHEOREM\\nAccording\\t\\nto\\t\\nMercer’s\\ttheorem\\n,\\tif\\ta\\tfunction\\t\\nK\\n(\\na\\n,\\t\\nb\\n)\\trespects\\ta\\tfew\\tmathematical\\tconditions\\tcalled\\t\\nMercer’s\\tconditions\\n\\t(\\nK\\n\\tmust\\tbe\\ncontinuous,\\tsymmetric\\tin\\tits\\targuments\\tso\\t\\nK\\n(\\na\\n,\\t\\nb\\n)\\t=\\t\\nK\\n(\\nb\\n,\\t\\na\\n),\\tetc.),\\tthen\\tthere\\texists\\ta\\tfunction\\t\\nϕ\\n\\tthat\\tmaps\\t\\na\\n\\tand\\t\\nb\\n\\tinto\\tanother\\tspace\\n(possibly\\twith\\tmuch\\thigher\\tdimensions)\\tsuch\\tthat\\t\\nK\\n(\\na\\n,\\t\\nb\\n)\\t=\\t\\nϕ\\n(\\na\\n)\\nT\\n\\t·\\t\\nϕ\\n(\\nb\\n).\\tSo\\tyou\\tcan\\tuse\\t\\nK\\n\\tas\\ta\\tkernel\\tsince\\tyou\\tknow\\t\\nϕ\\n\\texists,\\teven\\tif\\nyou\\tdon’t\\tknow\\twhat\\t\\nϕ\\n\\tis.\\tIn\\tthe\\tcase\\tof\\tthe\\t\\nGaussian\\tRBF\\tkernel,\\tit\\tcan\\tbe\\tshown\\tthat\\t\\nϕ\\n\\tactually\\tmaps\\teach\\ttraining\\tinstance\\tto\\tan\\ninfinite-dimensional\\tspace,\\tso\\tit’s\\ta\\tgood\\tthing\\tyou\\tdon’t\\tneed\\tto\\tactually\\tperform\\tthe\\tmapping!', 'Note\\tthat\\tsome\\tfrequently\\tused\\tkernels\\t(such\\tas\\tthe\\tSigmoid\\tkernel)\\tdon’t\\trespect\\tall\\tof\\tMercer’s\\tconditions,\\tyet\\tthey\\tgenerally\\twork\\nwell\\tin\\tpractice.\\nThere\\tis\\tstill\\tone\\tloose\\tend\\twe\\tmust\\ttie.\\t\\nEquation\\t5-7\\n\\tshows\\thow\\tto\\tgo\\tfrom\\tthe\\tdual\\tsolution\\tto\\tthe\\nprimal\\tsolution\\tin\\tthe\\tcase\\tof\\ta\\tlinear\\tSVM\\tclassifier,\\tbut\\tif\\tyou\\tapply\\tthe\\tkernel\\ttrick\\tyou\\tend\\tup\\twith\\nequations\\tthat\\tinclude\\t\\nϕ\\n(\\nx\\n(i)\\n).\\tIn\\tfact,\\t\\n\\tmust\\thave\\tthe\\tsame\\tnumber\\tof\\tdimensions\\tas\\t\\nϕ\\n(\\nx\\n(i)\\n),\\twhich\\tmay\\nbe\\thuge\\tor\\teven\\tinfinite,\\tso\\tyou\\tcan’t\\tcompute\\tit.\\tBut\\thow\\tcan\\tyou\\tmake\\tpredictions\\twithout\\tknowing\\t\\n?\\nWell,\\tthe\\tgood\\tnews\\tis\\tthat\\tyou\\tcan\\tplug\\tin\\tthe\\tformula\\tfor\\t\\n\\tfrom\\t\\nEquation\\t5-7\\n\\tinto\\tthe\\tdecision\\tfunction\\nfor\\ta\\tnew\\tinstance\\t\\nx\\n(n)\\n,\\tand\\tyou\\tget\\tan\\tequation\\twith\\tonly\\tdot\\tproducts\\tbetween\\tinput\\tvectors.\\tThis\\tmakes\\nit\\tpossible\\tto\\tuse\\tthe\\tkernel\\ttrick,\\tonce\\tagain\\t(\\nEquation\\t5-11\\n).\\nEquation\\t5-11.\\t\\nMaking\\tpredictions\\twith\\ta\\tkernelized\\tSVM\\nNote\\tthat\\tsince\\t\\nα\\n(i)', 'α\\n(i)\\n\\t≠\\t0\\tonly\\tfor\\tsupport\\tvectors,\\tmaking\\tpredictions\\tinvolves\\tcomputing\\tthe\\tdot\\tproduct\\tof\\nthe\\tnew\\tinput\\tvector\\t\\nx\\n(n)\\n\\twith\\tonly\\tthe\\tsupport\\tvectors,\\tnot\\tall\\tthe\\ttraining\\tinstances.\\tOf\\tcourse,\\tyou\\talso\\nneed\\tto\\tcompute\\tthe\\tbias\\tterm\\t\\n,\\tusing\\tthe\\tsame\\ttrick\\t(\\nEquation\\t5-12\\n).\\nEquation\\t5-12.\\t\\nComputing\\tthe\\tbias\\tterm\\tusing\\tthe\\tkernel\\ttrick', 'If\\tyou\\tare\\tstarting\\tto\\tget\\ta\\theadache,\\tit’s\\tperfectly\\tnormal:\\tit’s\\tan\\tunfortunate\\tside\\teffects\\tof\\tthe\\t\\nkernel\\ntrick.', 'Online\\tSVMs\\nBefore\\t\\nconcluding\\tthis\\tchapter,\\tlet’s\\ttake\\ta\\tquick\\tlook\\tat\\tonline\\tSVM\\tclassifiers\\t(recall\\tthat\\tonline\\nlearning\\tmeans\\tlearning\\tincrementally,\\ttypically\\tas\\tnew\\tinstances\\tarrive).\\nFor\\tlinear\\tSVM\\tclassifiers,\\tone\\tmethod\\tis\\tto\\tuse\\t\\nGradient\\tDescent\\t(e.g.,\\tusing\\t\\nSGDClassifier\\n)\\tto\\nminimize\\tthe\\tcost\\tfunction\\tin\\t\\nEquation\\t5-13\\n,\\twhich\\tis\\tderived\\tfrom\\tthe\\tprimal\\tproblem.\\tUnfortunately\\tit\\nconverges\\tmuch\\tmore\\tslowly\\tthan\\tthe\\tmethods\\tbased\\ton\\tQP.\\nEquation\\t5-13.\\t\\nLinear\\tSVM\\tclassifier\\tcost\\tfunction\\nThe\\tfirst\\tsum\\tin\\tthe\\tcost\\tfunction\\twill\\tpush\\tthe\\tmodel\\tto\\thave\\ta\\tsmall\\tweight\\tvector\\t\\nw\\n,\\tleading\\tto\\ta\\tlarger\\nmargin.\\tThe\\tsecond\\tsum\\tcomputes\\tthe\\ttotal\\tof\\tall\\tmargin\\tviolations.\\tAn\\tinstance’s\\tmargin\\tviolation\\tis\\nequal\\tto\\t0\\tif\\tit\\tis\\tlocated\\toff\\tthe\\tstreet\\tand\\ton\\tthe\\tcorrect\\tside,\\tor\\telse\\tit\\tis\\tproportional\\tto\\tthe\\tdistance\\tto\\nthe\\tcorrect\\tside\\tof\\tthe\\tstreet.\\tMinimizing\\tthis\\tterm\\tensures\\tthat\\tthe\\tmodel\\tmakes\\tthe\\tmargin\\tviolations\\tas\\nsmall\\tand\\tas\\tfew\\tas\\tpossible\\nHINGE\\tLOSS\\nThe\\tfunction\\t\\nmax', 'max\\n(0,\\t1\\t–\\t\\nt\\n)\\tis\\tcalled\\t\\nthe\\t\\nhinge\\tloss\\n\\tfunction\\t(represented\\tbelow).\\tIt\\tis\\tequal\\tto\\t0\\twhen\\t\\nt\\n\\t≥\\t1.\\tIts\\tderivative\\t(slope)\\tis\\tequal\\nto\\t–1\\tif\\t\\nt\\n\\t<\\t1\\tand\\t0\\tif\\t\\nt\\n\\t>\\t1.\\tIt\\tis\\tnot\\tdifferentiable\\tat\\t\\nt\\n\\t=\\t1,\\tbut\\tjust\\tlike\\tfor\\tLasso\\tRegression\\t(see\\t\\n“Lasso\\tRegression”\\n)\\tyou\\tcan\\tstill\\tuse\\nGradient\\tDescent\\tusing\\t\\nany\\t\\nsubderivative\\n\\tat\\t\\nt\\n\\t=\\t1\\t(i.e.,\\tany\\tvalue\\tbetween\\t–1\\tand\\t0).\\nIt\\tis\\talso\\tpossible\\tto\\timplement\\tonline\\tkernelized\\tSVMs\\t—\\tfor\\texample,\\tusing\\t\\n“Incremental\\tand\\nDecremental\\tSVM\\tLearning”\\n7\\n\\tor\\t\\n“Fast\\tKernel\\tClassifiers\\twith\\tOnline\\tand\\tActive\\tLearning.”\\n8\\n\\tHowever,\\nthese\\tare\\timplemented\\tin\\tMatlab\\tand\\tC++.\\tFor\\tlarge-scale\\tnonlinear\\tproblems,\\tyou\\tmay\\twant\\tto\\tconsider\\nusing\\tneural\\tnetworks\\tinstead\\t\\n(see\\t\\nPart\\tII\\n).', 'Exercises\\n1\\n.\\t\\nWhat\\tis\\tthe\\tfundamental\\tidea\\tbehind\\tSupport\\tVector\\tMachines?\\n2\\n.\\t\\nWhat\\tis\\ta\\tsupport\\tvector?\\n3\\n.\\t\\nWhy\\tis\\tit\\timportant\\tto\\tscale\\tthe\\tinputs\\twhen\\tusing\\tSVMs?\\n4\\n.\\t\\nCan\\tan\\tSVM\\tclassifier\\toutput\\ta\\tconfidence\\tscore\\twhen\\tit\\tclassifies\\tan\\tinstance?\\tWhat\\tabout\\ta\\nprobability?\\n5\\n.\\t\\nShould\\tyou\\tuse\\tthe\\tprimal\\tor\\tthe\\tdual\\tform\\tof\\tthe\\tSVM\\tproblem\\tto\\ttrain\\ta\\tmodel\\ton\\ta\\ttraining\\tset\\nwith\\tmillions\\tof\\tinstances\\tand\\thundreds\\tof\\tfeatures?\\n6\\n.\\t\\nSay\\tyou\\ttrained\\tan\\tSVM\\tclassifier\\twith\\tan\\tRBF\\tkernel.\\tIt\\tseems\\tto\\tunderfit\\tthe\\ttraining\\tset:\\tshould\\nyou\\tincrease\\tor\\tdecrease\\t\\nγ\\n\\t(\\ngamma\\n)?\\tWhat\\tabout\\t\\nC\\n?\\n7\\n.\\t\\nHow\\tshould\\tyou\\tset\\tthe\\tQP\\tparameters\\t(\\nH\\n,\\t\\nf\\n,\\t\\nA\\n,\\tand\\t\\nb\\n)\\tto\\tsolve\\tthe\\tsoft\\tmargin\\tlinear\\tSVM\\nclassifier\\tproblem\\tusing\\tan\\toff-the-shelf\\tQP\\tsolver?\\n8\\n.\\t\\nTrain\\ta\\t\\nLinearSVC\\n\\ton\\ta\\tlinearly\\tseparable\\tdataset.\\tThen\\ttrain\\tan\\t\\nSVC\\n\\tand\\ta\\t\\nSGDClassifier\\n\\ton\\tthe\\nsame\\tdataset.\\t\\nSee\\tif\\tyou\\tcan\\tget\\tthem\\tto\\tproduce\\troughly\\tthe\\tsame\\tmodel.\\n9\\n.', '9\\n.\\t\\nTrain\\tan\\tSVM\\tclassifier\\ton\\tthe\\tMNIST\\tdataset.\\tSince\\tSVM\\tclassifiers\\tare\\tbinary\\tclassifiers,\\tyou\\nwill\\tneed\\tto\\tuse\\t\\none-versus-all\\tto\\tclassify\\tall\\t10\\tdigits.\\tYou\\tmay\\twant\\tto\\ttune\\tthe\\thyperparameters\\nusing\\tsmall\\tvalidation\\tsets\\tto\\tspeed\\tup\\tthe\\tprocess.\\tWhat\\taccuracy\\tcan\\tyou\\treach?\\n10\\n.\\t\\nTrain\\tan\\tSVM\\tregressor\\ton\\tthe\\tCalifornia\\thousing\\t\\ndataset.\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\n“A\\tDual\\tCoordinate\\tDescent\\tMethod\\tfor\\tLarge-scale\\tLinear\\tSVM,”\\tLin\\tet\\tal.\\t(2008).\\n“Sequential\\tMinimal\\tOptimization\\t(SMO),”\\tJ.\\tPlatt\\t(1998).\\nMore\\tgenerally,\\twhen\\tthere\\tare\\t\\nn\\n\\tfeatures,\\tthe\\tdecision\\tfunction\\tis\\tan\\t\\nn\\n-dimensional\\t\\nhyperplane\\n,\\t\\nand\\tthe\\tdecision\\tboundary\\tis\\tan\\t(\\nn\\n\\t–\\t1)-\\ndimensional\\thyperplane.\\nZeta\\t(\\nζ\\n)\\tis\\tthe\\t8\\nletter\\tof\\tthe\\tGreek\\talphabet.\\nTo\\tlearn\\tmore\\tabout\\tQuadratic\\tProgramming,\\tyou\\tcan\\tstart\\tby\\treading\\tStephen\\tBoyd\\tand\\tLieven\\tVandenberghe,\\t\\nConvex\\tOptimization\\n(Cambridge,\\tUK:\\tCambridge\\tUniversity\\tPress,\\t2004)\\tor\\twatch\\tRichard\\tBrown’s\\t\\nseries\\tof\\tvideo\\tlectures\\n.', '.\\nThe\\tobjective\\tfunction\\tis\\tconvex,\\tand\\tthe\\tinequality\\tconstraints\\tare\\tcontinuously\\tdifferentiable\\tand\\tconvex\\tfunctions.\\n“Incremental\\tand\\tDecremental\\tSupport\\tVector\\tMachine\\tLearning,”\\tG.\\tCauwenberghs,\\tT.\\tPoggio\\t(2001).\\n“Fast\\tKernel\\tClassifiers\\twith\\tOnline\\tand\\tActive\\tLearning,“\\tA.\\tBordes,\\tS.\\tErtekin,\\tJ.\\tWeston,\\tL.\\tBottou\\t(2005).\\n1\\n2\\n3\\n4\\nth\\n5\\n6\\n7\\n8', 'Chapter\\t6.\\t\\nDecision\\tTrees\\nLike\\t\\nSVMs,\\t\\nDecision\\tTrees\\n\\t\\nare\\tversatile\\tMachine\\tLearning\\talgorithms\\tthat\\tcan\\tperform\\tboth\\nclassification\\tand\\tregression\\ttasks,\\tand\\teven\\tmultioutput\\ttasks.\\tThey\\tare\\tvery\\tpowerful\\talgorithms,\\ncapable\\tof\\tfitting\\tcomplex\\tdatasets.\\tFor\\texample,\\tin\\t\\nChapter\\t2\\n\\tyou\\ttrained\\ta\\t\\nDecisionTreeRegressor\\nmodel\\ton\\tthe\\tCalifornia\\thousing\\tdataset,\\tfitting\\tit\\tperfectly\\t(actually\\toverfitting\\tit).\\nDecision\\tTrees\\tare\\talso\\tthe\\tfundamental\\tcomponents\\tof\\t\\nRandom\\tForests\\t(see\\t\\nChapter\\t7\\n),\\twhich\\tare\\namong\\tthe\\tmost\\tpowerful\\tMachine\\tLearning\\talgorithms\\tavailable\\ttoday.\\nIn\\tthis\\tchapter\\twe\\twill\\tstart\\tby\\tdiscussing\\thow\\tto\\ttrain,\\tvisualize,\\tand\\tmake\\tpredictions\\twith\\tDecision\\nTrees.\\tThen\\twe\\twill\\tgo\\tthrough\\tthe\\tCART\\ttraining\\talgorithm\\tused\\tby\\tScikit-Learn,\\tand\\twe\\twill\\tdiscuss\\nhow\\tto\\tregularize\\ttrees\\tand\\tuse\\tthem\\tfor\\tregression\\ttasks.\\tFinally,\\twe\\twill\\tdiscuss\\tsome\\tof\\tthe\\tlimitations\\nof\\tDecision\\tTrees.', 'Training\\tand\\tVisualizing\\ta\\tDecision\\tTree\\nTo\\t\\nunderstand\\tDecision\\tTrees,\\tlet’s\\tjust\\tbuild\\tone\\tand\\ttake\\ta\\tlook\\tat\\thow\\tit\\tmakes\\tpredictions.\\tThe\\nfollowing\\tcode\\ttrains\\ta\\t\\nDecisionTreeClassifier\\n\\ton\\tthe\\t\\niris\\tdataset\\t(see\\t\\nChapter\\t4\\n):\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nload_iris\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nDecisionTreeClassifier\\niris\\n\\t\\n=\\n\\t\\nload_iris\\n()\\nX\\n\\t\\n=\\n\\t\\niris\\n.\\ndata\\n[:,\\n\\t\\n2\\n:]\\n\\t\\n#\\tpetal\\tlength\\tand\\twidth\\ny\\n\\t\\n=\\n\\t\\niris\\n.\\ntarget\\ntree_clf\\n\\t\\n=\\n\\t\\nDecisionTreeClassifier\\n(\\nmax_depth\\n=\\n2\\n)\\ntree_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nYou\\tcan\\tvisualize\\tthe\\ttrained\\tDecision\\tTree\\tby\\tfirst\\tusing\\tthe\\t\\nexport_graphviz()\\n\\t\\nmethod\\t\\nto\\toutput\\ta\\ngraph\\tdefinition\\tfile\\tcalled\\t\\niris_tree.dot\\n:\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nexport_graphviz\\nexport_graphviz\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntree_clf\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nout_file\\n=\\nimage_path\\n(\\n\"iris_tree.dot\"\\n),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfeature_names\\n=\\niris\\n.\\nfeature_names\\n[\\n2\\n:],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nclass_names\\n=\\niris\\n.\\ntarget_names\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nrounded\\n=\\nTrue\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfilled\\n=\\nTrue\\n\\t\\t\\t\\t\\n)\\nThen\\tyou\\tcan\\tconvert\\tthis\\t\\n.dot', '.dot\\n\\tfile\\tto\\ta\\tvariety\\tof\\tformats\\tsuch\\tas\\tPDF\\tor\\tPNG\\tusing\\tthe\\t\\ndot\\n\\tcommand-\\nline\\ttool\\tfrom\\tthe\\t\\ngraphviz\\n\\tpackage.\\n1\\n\\tThis\\tcommand\\tline\\tconverts\\tthe\\t\\n.dot\\n\\tfile\\tto\\ta\\t\\n.png\\n\\timage\\tfile:\\n$\\tdot\\t-Tpng\\tiris_tree.dot\\t-o\\tiris_tree.png\\nYour\\tfirst\\tdecision\\ttree\\tlooks\\tlike\\t\\nFigure\\t6-1\\n.', 'Figure\\t6-1.\\t\\nIris\\tDecision\\tTree', 'Making\\tPredictions\\nLet’s\\t\\nsee\\thow\\tthe\\ttree\\trepresented\\tin\\t\\nFigure\\t6-1\\n\\tmakes\\tpredictions.\\tSuppose\\tyou\\tfind\\tan\\tiris\\tflower\\tand\\nyou\\twant\\tto\\tclassify\\tit.\\tYou\\tstart\\tat\\tthe\\t\\nroot\\tnode\\n\\t(depth\\t0,\\tat\\tthe\\ttop):\\tthis\\tnode\\tasks\\twhether\\tthe\\tflower’s\\npetal\\tlength\\tis\\tsmaller\\tthan\\t2.45\\tcm.\\tIf\\tit\\tis,\\tthen\\tyou\\tmove\\tdown\\tto\\tthe\\troot’s\\tleft\\tchild\\tnode\\t(depth\\t1,\\nleft).\\tIn\\tthis\\tcase,\\tit\\tis\\ta\\t\\nleaf\\tnode\\n\\t(i.e.,\\tit\\tdoes\\tnot\\thave\\tany\\tchildren\\tnodes),\\tso\\tit\\tdoes\\tnot\\task\\tany\\nquestions:\\tyou\\tcan\\tsimply\\tlook\\tat\\tthe\\tpredicted\\tclass\\tfor\\tthat\\tnode\\tand\\tthe\\tDecision\\tTree\\tpredicts\\tthat\\nyour\\tflower\\tis\\tan\\tIris-Setosa\\t(\\nclass=setosa\\n).\\nNow\\tsuppose\\tyou\\tfind\\tanother\\tflower,\\tbut\\tthis\\ttime\\tthe\\tpetal\\tlength\\tis\\tgreater\\tthan\\t2.45\\tcm.\\tYou\\tmust\\nmove\\tdown\\tto\\tthe\\troot’s\\tright\\tchild\\tnode\\t(depth\\t1,\\tright),\\twhich\\tis\\tnot\\ta\\tleaf\\tnode,\\tso\\tit\\tasks\\tanother\\nquestion:\\tis\\tthe\\tpetal\\twidth\\tsmaller\\tthan\\t1.75\\tcm?\\tIf\\tit\\tis,\\tthen\\tyour\\tflower\\tis\\tmost\\tlikely\\tan\\tIris-', 'Versicolor\\t(depth\\t2,\\tleft).\\tIf\\tnot,\\tit\\tis\\tlikely\\tan\\tIris-Virginica\\t(depth\\t2,\\tright).\\tIt’s\\treally\\tthat\\tsimple.\\nNOTE\\nOne\\tof\\tthe\\tmany\\tqualities\\tof\\tDecision\\tTrees\\tis\\tthat\\tthey\\trequire\\tvery\\tlittle\\tdata\\tpreparation.\\tIn\\tparticular,\\tthey\\tdon’t\\trequire\\nfeature\\tscaling\\tor\\tcentering\\tat\\tall.\\nA\\tnode’s\\t\\nsamples\\n\\tattribute\\tcounts\\thow\\tmany\\ttraining\\tinstances\\tit\\tapplies\\tto.\\tFor\\texample,\\t100\\ttraining\\ninstances\\thave\\ta\\tpetal\\tlength\\tgreater\\tthan\\t2.45\\tcm\\t(depth\\t1,\\tright),\\tamong\\twhich\\t54\\thave\\ta\\tpetal\\twidth\\nsmaller\\tthan\\t1.75\\tcm\\t(depth\\t2,\\tleft).\\tA\\tnode’s\\t\\nvalue\\n\\tattribute\\ttells\\tyou\\thow\\tmany\\ttraining\\tinstances\\tof\\neach\\tclass\\tthis\\tnode\\tapplies\\tto:\\tfor\\texample,\\tthe\\tbottom-right\\tnode\\tapplies\\tto\\t0\\tIris-Setosa,\\t1\\tIris-\\nVersicolor,\\tand\\t45\\tIris-Virginica.\\tFinally,\\ta\\tnode’s\\t\\ngini\\n\\tattribute\\tmeasures\\tits\\t\\nimpurity\\n:\\ta\\tnode\\tis\\t“pure”\\n(\\ngini=0\\n)\\tif\\tall\\ttraining\\tinstances\\tit\\tapplies\\tto\\tbelong\\tto\\tthe\\tsame\\tclass.\\tFor\\texample,\\tsince\\tthe\\tdepth-1\\nleft\\tnode\\tapplies\\tonly\\tto\\tIris-Setosa\\ttraining\\tinstances,\\tit\\tis\\tpure\\tand\\tits\\t\\ngini', 'gini\\n\\tscore\\tis\\t0.\\t\\nEquation\\t6-1\\nshows\\thow\\tthe\\ttraining\\talgorithm\\tcomputes\\tthe\\tgini\\tscore\\t\\nG\\ni\\n\\tof\\tthe\\ti\\nth\\n\\tnode.\\tFor\\texample,\\tthe\\tdepth-2\\tleft\\nnode\\thas\\ta\\t\\ngini\\n\\tscore\\tequal\\tto\\t1\\t–\\t(0/54)\\n2\\n\\t–\\t(49/54)\\n2\\n\\t–\\t(5/54)\\n2\\n\\t≈\\t0.168.\\tAnother\\t\\nimpurity\\tmeasure\\n\\tis\\ndiscussed\\tshortly.\\nEquation\\t6-1.\\t\\nGini\\timpurity\\np\\ni\\n,\\nk\\n\\tis\\tthe\\tratio\\tof\\tclass\\t\\nk\\n\\tinstances\\tamong\\tthe\\ttraining\\tinstances\\tin\\tthe\\t\\ni\\nth\\n\\tnode.', 'NOTE\\nScikit-Learn\\t\\nuses\\tthe\\t\\nCART\\talgorithm,\\twhich\\tproduces\\t\\nonly\\t\\nbinary\\ttrees\\n:\\tnonleaf\\tnodes\\talways\\thave\\ttwo\\tchildren\\t(i.e.,\\nquestions\\tonly\\thave\\tyes/no\\tanswers).\\tHowever,\\tother\\talgorithms\\tsuch\\tas\\tID3\\tcan\\tproduce\\tDecision\\tTrees\\twith\\tnodes\\tthat\\thave\\nmore\\tthan\\ttwo\\tchildren.\\nFigure\\t6-2\\n\\tshows\\tthis\\t\\nDecision\\tTree’s\\tdecision\\tboundaries.\\tThe\\tthick\\tvertical\\tline\\trepresents\\tthe\\tdecision\\nboundary\\tof\\tthe\\troot\\tnode\\t(depth\\t0):\\tpetal\\tlength\\t=\\t2.45\\tcm.\\tSince\\tthe\\tleft\\tarea\\tis\\tpure\\t(only\\tIris-Setosa),\\nit\\tcannot\\tbe\\tsplit\\tany\\tfurther.\\tHowever,\\tthe\\tright\\tarea\\tis\\timpure,\\tso\\tthe\\tdepth-1\\tright\\tnode\\tsplits\\tit\\tat\\tpetal\\nwidth\\t=\\t1.75\\tcm\\t(represented\\tby\\tthe\\tdashed\\tline).\\tSince\\t\\nmax_depth\\n\\twas\\tset\\tto\\t2,\\tthe\\tDecision\\tTree\\tstops\\nright\\tthere.\\tHowever,\\tif\\tyou\\tset\\t\\nmax_depth\\n\\tto\\t3,\\tthen\\tthe\\ttwo\\tdepth-2\\tnodes\\twould\\teach\\tadd\\tanother\\ndecision\\tboundary\\t(represented\\tby\\tthe\\tdotted\\tlines).\\nFigure\\t6-2.\\t\\nDecision\\tTree\\tdecision\\tboundaries\\nMODEL\\tINTERPRETATION:\\tWHITE\\tBOX\\tVERSUS\\tBLACK\\tBOX', 'As\\tyou\\tcan\\tsee\\tDecision\\tTrees\\tare\\tfairly\\tintuitive\\tand\\ttheir\\tdecisions\\tare\\teasy\\tto\\tinterpret.\\tSuch\\tmodels\\tare\\toften\\t\\ncalled\\t\\nwhite\\tbox\\nmodels\\n.\\tIn\\tcontrast,\\tas\\twe\\twill\\tsee,\\tRandom\\tForests\\tor\\tneural\\tnetworks\\tare\\tgenerally\\t\\nconsidered\\t\\nblack\\tbox\\tmodels\\n.\\tThey\\tmake\\tgreat\\npredictions,\\tand\\tyou\\tcan\\teasily\\tcheck\\tthe\\tcalculations\\tthat\\tthey\\tperformed\\tto\\tmake\\tthese\\tpredictions;\\tnevertheless,\\tit\\tis\\tusually\\thard\\tto\\nexplain\\tin\\tsimple\\tterms\\twhy\\tthe\\tpredictions\\twere\\tmade.\\tFor\\texample,\\tif\\ta\\tneural\\tnetwork\\tsays\\tthat\\ta\\tparticular\\tperson\\tappears\\ton\\ta\\npicture,\\tit\\tis\\thard\\tto\\tknow\\twhat\\tactually\\tcontributed\\tto\\tthis\\tprediction:\\tdid\\tthe\\tmodel\\trecognize\\tthat\\tperson’s\\teyes?\\tHer\\tmouth?\\tHer\\tnose?\\nHer\\tshoes?\\tOr\\teven\\tthe\\tcouch\\tthat\\tshe\\twas\\tsitting\\ton?\\tConversely,\\tDecision\\tTrees\\tprovide\\tnice\\tand\\tsimple\\tclassification\\trules\\tthat\\tcan\\neven\\tbe\\tapplied\\tmanually\\tif\\tneed\\tbe\\t(e.g.,\\tfor\\tflower\\tclassification).', 'Estimating\\tClass\\tProbabilities\\nA\\t\\nDecision\\tTree\\tcan\\talso\\testimate\\tthe\\tprobability\\tthat\\tan\\tinstance\\tbelongs\\tto\\ta\\tparticular\\tclass\\t\\nk\\n:\\tfirst\\tit\\ntraverses\\tthe\\ttree\\tto\\tfind\\tthe\\tleaf\\tnode\\tfor\\tthis\\tinstance,\\tand\\tthen\\tit\\treturns\\tthe\\tratio\\tof\\ttraining\\tinstances\\tof\\nclass\\t\\nk\\n\\tin\\tthis\\tnode.\\tFor\\texample,\\tsuppose\\tyou\\thave\\tfound\\ta\\tflower\\twhose\\tpetals\\tare\\t5\\tcm\\tlong\\tand\\t1.5\\ncm\\twide.\\tThe\\tcorresponding\\tleaf\\tnode\\tis\\tthe\\tdepth-2\\tleft\\tnode,\\tso\\tthe\\tDecision\\tTree\\tshould\\toutput\\tthe\\nfollowing\\tprobabilities:\\t0%\\tfor\\tIris-Setosa\\t(0/54),\\t90.7%\\tfor\\tIris-Versicolor\\t(49/54),\\tand\\t9.3%\\tfor\\tIris-\\nVirginica\\t(5/54).\\tAnd\\tof\\tcourse\\tif\\tyou\\task\\tit\\tto\\tpredict\\tthe\\tclass,\\tit\\tshould\\toutput\\tIris-Versicolor\\t(class\\t1)\\nsince\\tit\\thas\\tthe\\thighest\\tprobability.\\tLet’s\\tcheck\\tthis:\\n>>>\\t\\ntree_clf\\n.\\npredict_proba\\n([[\\n5\\n,\\n\\t\\n1.5\\n]])\\narray([[\\t0.\\t,\\t\\t0.90740741,\\t\\t0.09259259]])\\n>>>\\t\\ntree_clf\\n.\\npredict\\n([[\\n5\\n,\\n\\t\\n1.5\\n]])\\narray([1])\\nPerfect!\\tNotice\\tthat\\tthe\\testimated\\tprobabilities\\twould\\tbe\\tidentical\\tanywhere\\telse\\tin\\tthe\\tbottom-right\\nrectangle\\tof', 'Figure\\t6-2\\n\\t—\\tfor\\texample,\\tif\\tthe\\tpetals\\twere\\t6\\tcm\\tlong\\tand\\t1.5\\tcm\\twide\\t(even\\tthough\\tit\\nseems\\tobvious\\tthat\\tit\\twould\\tmost\\tlikely\\tbe\\tan\\tIris-Virginica\\tin\\tthis\\tcase).', 'The\\tCART\\tTraining\\tAlgorithm\\nScikit-Learn\\t\\nuses\\tthe\\t\\nClassification\\tAnd\\tRegression\\tTree\\n\\t(CART)\\talgorithm\\tto\\ttrain\\tDecision\\tTrees\\t(also\\ncalled\\t“growing”\\ttrees).\\tThe\\tidea\\tis\\treally\\tquite\\tsimple:\\tthe\\talgorithm\\tfirst\\tsplits\\tthe\\ttraining\\tset\\tin\\ttwo\\nsubsets\\tusing\\ta\\tsingle\\tfeature\\t\\nk\\n\\tand\\ta\\tthreshold\\t\\nt\\nk\\n\\t(e.g.,\\t“petal\\tlength\\t≤\\t2.45\\tcm”).\\tHow\\tdoes\\tit\\tchoose\\t\\nk\\nand\\t\\nt\\nk\\n?\\tIt\\tsearches\\tfor\\tthe\\tpair\\t(\\nk\\n,\\t\\nt\\nk\\n)\\tthat\\tproduces\\tthe\\tpurest\\tsubsets\\t(weighted\\tby\\ttheir\\tsize).\\tThe\\tcost\\nfunction\\tthat\\tthe\\talgorithm\\ttries\\tto\\tminimize\\tis\\tgiven\\tby\\t\\nEquation\\t6-2\\n.\\nEquation\\t6-2.\\t\\nCART\\tcost\\tfunction\\tfor\\tclassification\\nOnce\\tit\\thas\\tsuccessfully\\tsplit\\tthe\\ttraining\\tset\\tin\\ttwo,\\tit\\tsplits\\tthe\\tsubsets\\tusing\\tthe\\tsame\\tlogic,\\tthen\\tthe\\tsub-\\nsubsets\\tand\\tso\\ton,\\trecursively.\\tIt\\tstops\\trecursing\\tonce\\tit\\treaches\\tthe\\tmaximum\\tdepth\\t(defined\\tby\\tthe\\nmax_depth\\n\\thyperparameter),\\tor\\tif\\tit\\tcannot\\tfind\\ta\\tsplit\\tthat\\twill\\treduce\\timpurity.\\tA\\tfew\\tother\\nhyperparameters\\t(described\\tin\\ta\\tmoment)\\tcontrol\\tadditional\\tstopping\\tconditions\\t(', 'min_samples_split\\n,\\nmin_samples_leaf\\n,\\t\\nmin_weight_fraction_leaf\\n,\\tand\\t\\nmax_leaf_nodes\\n).\\nWARNING\\nAs\\tyou\\tcan\\tsee,\\tthe\\tCART\\talgorithm\\tis\\t\\na\\t\\ngreedy\\talgorithm\\n:\\tit\\tgreedily\\tsearches\\tfor\\tan\\toptimum\\tsplit\\tat\\tthe\\ttop\\tlevel,\\tthen\\nrepeats\\tthe\\tprocess\\tat\\teach\\tlevel.\\tIt\\tdoes\\tnot\\tcheck\\twhether\\tor\\tnot\\tthe\\tsplit\\twill\\tlead\\tto\\tthe\\tlowest\\tpossible\\timpurity\\tseveral\\tlevels\\ndown.\\tA\\tgreedy\\talgorithm\\toften\\tproduces\\ta\\treasonably\\tgood\\tsolution,\\tbut\\tit\\tis\\tnot\\tguaranteed\\tto\\tbe\\tthe\\toptimal\\tsolution.\\nUnfortunately,\\tfinding\\tthe\\toptimal\\ttree\\tis\\tknown\\tto\\tbe\\t\\nan\\t\\nNP-Complete\\n\\tproblem:\\n2\\n\\tit\\trequires\\t\\nO\\n(exp(\\nm\\n))\\ntime,\\tmaking\\tthe\\tproblem\\tintractable\\teven\\tfor\\tfairly\\tsmall\\ttraining\\tsets.\\tThis\\tis\\twhy\\twe\\tmust\\tsettle\\tfor\\ta\\n“reasonably\\tgood”\\tsolution.', 'Computational\\tComplexity\\nMaking\\t\\npredictions\\trequires\\ttraversing\\tthe\\tDecision\\tTree\\tfrom\\tthe\\troot\\tto\\ta\\tleaf.\\tDecision\\tTrees\\tare\\ngenerally\\tapproximately\\tbalanced,\\tso\\ttraversing\\tthe\\tDecision\\tTree\\trequires\\tgoing\\tthrough\\troughly\\nO\\n(\\nlog\\n2\\n(\\nm\\n))\\tnodes.\\n3\\n\\tSince\\teach\\tnode\\tonly\\trequires\\tchecking\\tthe\\tvalue\\tof\\tone\\tfeature,\\tthe\\toverall\\nprediction\\tcomplexity\\tis\\tjust\\t\\nO\\n(\\nlog\\n2\\n(\\nm\\n)),\\tindependent\\tof\\tthe\\tnumber\\tof\\tfeatures.\\tSo\\tpredictions\\tare\\tvery\\nfast,\\teven\\twhen\\tdealing\\twith\\tlarge\\ttraining\\tsets.\\nHowever,\\tthe\\ttraining\\talgorithm\\tcompares\\tall\\tfeatures\\t(or\\tless\\tif\\t\\nmax_features\\n\\tis\\tset)\\ton\\tall\\tsamples\\tat\\neach\\tnode.\\tThis\\tresults\\tin\\ta\\ttraining\\tcomplexity\\tof\\t\\nO\\n(\\nn\\n\\t×\\t\\nm\\n\\t\\nlog\\n(\\nm\\n)).\\tFor\\tsmall\\ttraining\\tsets\\t(less\\tthan\\ta\\nfew\\tthousand\\tinstances),\\tScikit-Learn\\tcan\\tspeed\\tup\\ttraining\\tby\\tpresorting\\tthe\\tdata\\t(set\\t\\npresort=True\\n),\\nbut\\tthis\\tslows\\tdown\\ttraining\\tconsiderably\\tfor\\tlarger\\ttraining\\tsets.', 'Gini\\tImpurity\\tor\\tEntropy?\\nBy\\tdefault,\\tthe\\t\\nGini\\timpurity\\tmeasure\\t\\nis\\tused,\\tbut\\tyou\\tcan\\tselect\\tthe\\t\\nentropy\\n\\t\\nimpurity\\tmeasure\\tinstead\\tby\\nsetting\\tthe\\t\\ncriterion\\n\\thyperparameter\\tto\\t\\n\"entropy\"\\n.\\tThe\\tconcept\\tof\\tentropy\\toriginated\\tin\\nthermodynamics\\tas\\ta\\tmeasure\\tof\\tmolecular\\tdisorder:\\tentropy\\tapproaches\\tzero\\twhen\\tmolecules\\tare\\tstill\\nand\\twell\\tordered.\\tIt\\tlater\\tspread\\tto\\ta\\twide\\tvariety\\tof\\tdomains,\\tincluding\\tShannon’s\\t\\ninformation\\ttheory\\n,\\nwhere\\tit\\tmeasures\\tthe\\taverage\\tinformation\\tcontent\\tof\\ta\\tmessage:\\n4\\n\\t\\nentropy\\tis\\tzero\\twhen\\tall\\tmessages\\tare\\nidentical.\\tIn\\tMachine\\tLearning,\\tit\\tis\\tfrequently\\tused\\tas\\tan\\timpurity\\tmeasure:\\ta\\tset’s\\tentropy\\tis\\tzero\\twhen\\nit\\tcontains\\tinstances\\tof\\tonly\\tone\\tclass.\\t\\nEquation\\t6-3\\n\\tshows\\tthe\\tdefinition\\tof\\tthe\\tentropy\\tof\\tthe\\ti\\nth\\n\\tnode.\\nFor\\texample,\\tthe\\tdepth-2\\tleft\\tnode\\tin\\t\\nFigure\\t6-1\\n\\thas\\tan\\tentropy\\tequal\\tto\\t\\n≈\\t0.31.\\nEquation\\t6-3.\\t\\nEntropy\\nSo\\tshould\\tyou\\tuse\\tGini\\timpurity\\tor\\tentropy?\\tThe\\ttruth\\tis,\\tmost\\tof\\tthe\\ttime\\tit\\tdoes\\tnot\\tmake\\ta\\tbig', 'difference:\\tthey\\tlead\\tto\\tsimilar\\ttrees.\\tGini\\timpurity\\tis\\tslightly\\tfaster\\tto\\tcompute,\\tso\\tit\\tis\\ta\\tgood\\tdefault.\\nHowever,\\twhen\\tthey\\tdiffer,\\tGini\\timpurity\\ttends\\tto\\tisolate\\tthe\\tmost\\tfrequent\\tclass\\tin\\tits\\town\\tbranch\\tof\\tthe\\ntree,\\twhile\\tentropy\\ttends\\tto\\tproduce\\tslightly\\tmore\\tbalanced\\ttrees.\\n5', 'Regularization\\tHyperparameters\\nDecision\\tTrees\\t\\nmake\\tvery\\tfew\\tassumptions\\tabout\\tthe\\ttraining\\tdata\\t(as\\topposed\\tto\\tlinear\\tmodels,\\twhich\\nobviously\\tassume\\tthat\\tthe\\tdata\\tis\\tlinear,\\tfor\\texample).\\tIf\\tleft\\tunconstrained,\\tthe\\ttree\\tstructure\\twill\\tadapt\\nitself\\tto\\tthe\\ttraining\\tdata,\\tfitting\\tit\\tvery\\tclosely,\\tand\\tmost\\tlikely\\toverfitting\\tit.\\tSuch\\ta\\tmodel\\tis\\toften\\tcalled\\na\\t\\nnonparametric\\tmodel\\n,\\t\\nnot\\tbecause\\tit\\tdoes\\tnot\\thave\\tany\\tparameters\\t(it\\toften\\thas\\ta\\tlot)\\tbut\\tbecause\\tthe\\nnumber\\tof\\tparameters\\tis\\tnot\\tdetermined\\tprior\\tto\\ttraining,\\tso\\tthe\\tmodel\\tstructure\\tis\\tfree\\tto\\tstick\\tclosely\\tto\\nthe\\tdata.\\tIn\\tcontrast,\\t\\na\\t\\nparametric\\tmodel\\n\\tsuch\\tas\\ta\\tlinear\\tmodel\\thas\\ta\\tpredetermined\\tnumber\\tof\\nparameters,\\tso\\tits\\tdegree\\tof\\tfreedom\\tis\\tlimited,\\treducing\\tthe\\trisk\\tof\\toverfitting\\t(but\\tincreasing\\tthe\\trisk\\tof\\nunderfitting).\\nTo\\tavoid\\t\\noverfitting\\tthe\\ttraining\\tdata,\\tyou\\tneed\\tto\\trestrict\\tthe\\tDecision\\tTree’s\\tfreedom\\tduring\\ttraining.\\tAs', 'you\\tknow\\tby\\tnow,\\tthis\\tis\\tcalled\\tregularization.\\tThe\\tregularization\\thyperparameters\\tdepend\\ton\\tthe\\nalgorithm\\tused,\\tbut\\tgenerally\\tyou\\tcan\\tat\\tleast\\trestrict\\tthe\\tmaximum\\tdepth\\tof\\tthe\\tDecision\\tTree.\\tIn\\tScikit-\\nLearn,\\tthis\\tis\\tcontrolled\\tby\\tthe\\t\\nmax_depth\\n\\thyperparameter\\t(the\\tdefault\\tvalue\\tis\\t\\nNone\\n,\\twhich\\tmeans\\nunlimited).\\tReducing\\t\\nmax_depth\\n\\twill\\t\\nregularize\\tthe\\tmodel\\tand\\tthus\\treduce\\tthe\\trisk\\tof\\toverfitting.\\nThe\\t\\nDecisionTreeClassifier\\n\\t\\nclass\\thas\\ta\\tfew\\tother\\tparameters\\tthat\\tsimilarly\\trestrict\\tthe\\tshape\\tof\\tthe\\nDecision\\tTree:\\t\\nmin_samples_split\\n\\t(the\\tminimum\\tnumber\\tof\\tsamples\\ta\\tnode\\tmust\\thave\\tbefore\\tit\\tcan\\tbe\\nsplit),\\t\\nmin_samples_leaf\\n\\t(the\\tminimum\\tnumber\\tof\\tsamples\\ta\\tleaf\\tnode\\tmust\\thave),\\nmin_weight_fraction_leaf\\n\\t(same\\tas\\t\\nmin_samples_leaf\\n\\tbut\\texpressed\\tas\\ta\\tfraction\\tof\\tthe\\ttotal\\nnumber\\tof\\tweighted\\tinstances),\\t\\nmax_leaf_nodes\\n\\t(maximum\\tnumber\\tof\\tleaf\\tnodes),\\tand\\t\\nmax_features\\n(maximum\\tnumber\\tof\\tfeatures\\tthat\\tare\\tevaluated\\tfor\\tsplitting\\tat\\teach\\tnode).\\tIncreasing\\t\\nmin_*', 'min_*\\nhyperparameters\\tor\\treducing\\t\\nmax_*\\n\\thyperparameters\\twill\\tregularize\\tthe\\tmodel.\\nNOTE\\nOther\\talgorithms\\twork\\tby\\tfirst\\ttraining\\tthe\\tDecision\\tTree\\twithout\\trestrictions,\\t\\nthen\\t\\npruning\\n\\t(deleting)\\tunnecessary\\tnodes.\\tA\\tnode\\nwhose\\tchildren\\tare\\tall\\tleaf\\tnodes\\tis\\tconsidered\\tunnecessary\\tif\\tthe\\tpurity\\timprovement\\tit\\tprovides\\tis\\t\\nnot\\t\\nstatistically\\tsignificant\\n.\\nStandard\\tstatistical\\ttests,\\tsuch\\tas\\tthe\\t\\nχ\\n2\\n\\t\\ntest\\n,\\t\\nare\\tused\\tto\\testimate\\tthe\\tprobability\\tthat\\tthe\\timprovement\\tis\\tpurely\\tthe\\tresult\\tof\\nchance\\t(which\\tis\\tcalled\\t\\nthe\\t\\nnull\\thypothesis\\n).\\tIf\\tthis\\tprobability,\\tcalled\\tthe\\t\\np-value\\n,\\t\\nis\\thigher\\tthan\\ta\\tgiven\\tthreshold\\t(typically\\t5%,\\ncontrolled\\tby\\ta\\thyperparameter),\\tthen\\tthe\\tnode\\tis\\tconsidered\\tunnecessary\\tand\\tits\\tchildren\\tare\\tdeleted.\\tThe\\tpruning\\tcontinues\\tuntil\\nall\\tunnecessary\\tnodes\\thave\\tbeen\\tpruned.\\nFigure\\t6-3\\n\\tshows\\ttwo\\tDecision\\tTrees\\ttrained\\ton\\tthe\\tmoons\\tdataset\\t(introduced\\tin\\t\\nChapter\\t5\\n).\\tOn\\tthe\\tleft,', 'the\\tDecision\\tTree\\tis\\ttrained\\twith\\tthe\\tdefault\\thyperparameters\\t(i.e.,\\tno\\trestrictions),\\tand\\ton\\tthe\\tright\\tthe\\nDecision\\tTree\\tis\\ttrained\\twith\\t\\nmin_samples_leaf=4\\n.\\tIt\\tis\\tquite\\tobvious\\tthat\\tthe\\tmodel\\ton\\tthe\\tleft\\tis\\noverfitting,\\tand\\tthe\\tmodel\\ton\\tthe\\tright\\twill\\tprobably\\tgeneralize\\t\\nbetter.', 'Figure\\t6-3.\\t\\nRegularization\\tusing\\tmin_samples_leaf', 'Regression\\nDecision\\tTrees\\t\\nare\\talso\\tcapable\\tof\\tperforming\\tregression\\ttasks.\\tLet’s\\tbuild\\ta\\tregression\\ttree\\tusing\\t\\nScikit-\\nLearn’s\\t\\nDecisionTreeRegressor\\n\\tclass,\\ttraining\\tit\\ton\\ta\\tnoisy\\tquadratic\\tdataset\\twith\\t\\nmax_depth=2\\n:\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nDecisionTreeRegressor\\ntree_reg\\n\\t\\n=\\n\\t\\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n)\\ntree_reg\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nThe\\tresulting\\ttree\\tis\\trepresented\\ton\\t\\nFigure\\t6-4\\n.\\nFigure\\t6-4.\\t\\nA\\tDecision\\tTree\\tfor\\tregression\\nThis\\ttree\\tlooks\\tvery\\tsimilar\\tto\\tthe\\tclassification\\ttree\\tyou\\tbuilt\\tearlier.\\tThe\\tmain\\tdifference\\tis\\tthat\\tinstead\\nof\\tpredicting\\ta\\tclass\\tin\\teach\\tnode,\\tit\\tpredicts\\ta\\tvalue.\\tFor\\texample,\\tsuppose\\tyou\\twant\\tto\\tmake\\ta\\nprediction\\tfor\\ta\\tnew\\tinstance\\twith\\t\\nx\\n1\\n\\t=\\t0.6.\\tYou\\ttraverse\\tthe\\ttree\\tstarting\\tat\\tthe\\troot,\\tand\\tyou\\teventually\\nreach\\tthe\\tleaf\\tnode\\tthat\\tpredicts\\t\\nvalue=0.1106\\n.\\tThis\\tprediction\\tis\\tsimply\\tthe\\taverage\\ttarget\\tvalue\\tof\\tthe\\n110\\ttraining\\tinstances\\tassociated\\tto\\tthis\\tleaf\\tnode.\\tThis\\tprediction\\tresults\\tin\\ta\\tMean\\tSquared\\tError', '(MSE)\\tequal\\tto\\t0.0151\\tover\\tthese\\t110\\tinstances.\\nThis\\tmodel’s\\tpredictions\\tare\\trepresented\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t6-5\\n.\\tIf\\tyou\\tset\\t\\nmax_depth=3\\n,\\tyou\\tget\\tthe\\npredictions\\trepresented\\ton\\tthe\\tright.\\tNotice\\thow\\tthe\\tpredicted\\tvalue\\tfor\\teach\\tregion\\tis\\talways\\tthe\\taverage\\ntarget\\tvalue\\tof\\tthe\\tinstances\\tin\\tthat\\tregion.\\tThe\\talgorithm\\tsplits\\teach\\tregion\\tin\\ta\\tway\\tthat\\tmakes\\tmost\\ntraining\\tinstances\\tas\\tclose\\tas\\tpossible\\tto\\tthat\\tpredicted\\tvalue.', 'Figure\\t6-5.\\t\\nPredictions\\tof\\ttwo\\tDecision\\tTree\\tregression\\tmodels\\nThe\\t\\nCART\\talgorithm\\tworks\\tmostly\\tthe\\tsame\\tway\\tas\\tearlier,\\texcept\\tthat\\tinstead\\tof\\ttrying\\tto\\tsplit\\tthe\\ntraining\\tset\\tin\\ta\\tway\\tthat\\tminimizes\\timpurity,\\tit\\tnow\\ttries\\tto\\tsplit\\tthe\\ttraining\\tset\\tin\\ta\\tway\\tthat\\tminimizes\\nthe\\tMSE.\\t\\nEquation\\t6-4\\n\\tshows\\tthe\\tcost\\tfunction\\tthat\\tthe\\talgorithm\\ttries\\tto\\tminimize.\\nEquation\\t6-4.\\t\\nCART\\tcost\\tfunction\\tfor\\tregression\\nJust\\tlike\\tfor\\tclassification\\ttasks,\\tDecision\\tTrees\\tare\\tprone\\tto\\toverfitting\\t\\nwhen\\tdealing\\twith\\tregression\\ntasks.\\tWithout\\tany\\tregularization\\t(i.e.,\\tusing\\tthe\\tdefault\\thyperparameters),\\tyou\\tget\\tthe\\tpredictions\\ton\\tthe\\nleft\\tof\\t\\nFigure\\t6-6\\n.\\tIt\\tis\\tobviously\\toverfitting\\tthe\\ttraining\\tset\\tvery\\tbadly.\\tJust\\tsetting\\nmin_samples_leaf=10\\n\\tresults\\tin\\ta\\tmuch\\tmore\\treasonable\\tmodel,\\trepresented\\ton\\tthe\\t\\nright\\tof\\t\\nFigure\\t6-6\\n.\\nFigure\\t6-6.\\t\\nRegularizing\\ta\\tDecision\\tTree\\tregressor', 'Instability\\nHopefully\\t\\nby\\tnow\\tyou\\tare\\tconvinced\\tthat\\tDecision\\tTrees\\thave\\ta\\tlot\\tgoing\\tfor\\tthem:\\tthey\\tare\\tsimple\\tto\\nunderstand\\tand\\tinterpret,\\teasy\\tto\\tuse,\\tversatile,\\tand\\tpowerful.\\tHowever\\tthey\\tdo\\thave\\ta\\tfew\\tlimitations.\\nFirst,\\tas\\tyou\\tmay\\thave\\tnoticed,\\tDecision\\tTrees\\tlove\\torthogonal\\tdecision\\tboundaries\\t(all\\tsplits\\tare\\nperpendicular\\tto\\tan\\taxis),\\twhich\\tmakes\\tthem\\tsensitive\\tto\\ttraining\\tset\\trotation.\\tFor\\texample,\\t\\nFigure\\t6-7\\nshows\\ta\\tsimple\\tlinearly\\tseparable\\tdataset:\\ton\\tthe\\tleft,\\ta\\tDecision\\tTree\\tcan\\tsplit\\tit\\teasily,\\twhile\\ton\\tthe\\nright,\\tafter\\tthe\\tdataset\\tis\\trotated\\tby\\t45°,\\tthe\\tdecision\\tboundary\\tlooks\\tunnecessarily\\tconvoluted.\\tAlthough\\nboth\\tDecision\\tTrees\\tfit\\tthe\\ttraining\\tset\\tperfectly,\\tit\\tis\\tvery\\tlikely\\tthat\\tthe\\tmodel\\ton\\tthe\\tright\\twill\\tnot\\ngeneralize\\twell.\\tOne\\tway\\tto\\tlimit\\tthis\\tproblem\\tis\\tto\\tuse\\tPCA\\t(see\\t\\nChapter\\t8\\n),\\twhich\\toften\\tresults\\tin\\ta\\nbetter\\torientation\\tof\\tthe\\ttraining\\tdata.\\nFigure\\t6-7.\\t\\nSensitivity\\tto\\ttraining\\tset\\trotation', 'More\\tgenerally,\\tthe\\tmain\\tissue\\twith\\tDecision\\tTrees\\tis\\tthat\\tthey\\tare\\tvery\\tsensitive\\tto\\tsmall\\tvariations\\tin\\nthe\\ttraining\\tdata.\\tFor\\texample,\\tif\\tyou\\tjust\\tremove\\tthe\\twidest\\tIris-Versicolor\\tfrom\\tthe\\tiris\\ttraining\\tset\\t(the\\none\\twith\\tpetals\\t4.8\\tcm\\tlong\\tand\\t1.8\\tcm\\twide)\\tand\\ttrain\\ta\\tnew\\tDecision\\tTree,\\tyou\\tmay\\tget\\tthe\\tmodel\\nrepresented\\tin\\t\\nFigure\\t6-8\\n.\\tAs\\tyou\\tcan\\tsee,\\tit\\tlooks\\tvery\\tdifferent\\tfrom\\tthe\\tprevious\\tDecision\\tTree\\n(\\nFigure\\t6-2\\n).\\tActually,\\tsince\\tthe\\ttraining\\talgorithm\\tused\\tby\\tScikit-Learn\\tis\\tstochastic\\n6\\n\\tyou\\tmay\\t\\nget\\tvery\\ndifferent\\tmodels\\teven\\ton\\tthe\\tsame\\ttraining\\tdata\\t(unless\\tyou\\tset\\tthe\\t\\nrandom_state\\n\\thyperparameter).\\nFigure\\t6-8.\\t\\nSensitivity\\tto\\ttraining\\tset\\tdetails', 'Random\\tForests\\t\\ncan\\tlimit\\tthis\\tinstability\\tby\\taveraging\\tpredictions\\tover\\tmany\\ttrees,\\tas\\twe\\twill\\tsee\\tin\\tthe\\nnext\\t\\nchapter.', 'Exercises\\n1\\n.\\t\\nWhat\\tis\\tthe\\tapproximate\\tdepth\\tof\\ta\\tDecision\\tTree\\ttrained\\t(without\\trestrictions)\\ton\\ta\\ttraining\\tset\\twith\\n1\\tmillion\\tinstances?\\n2\\n.\\t\\nIs\\ta\\tnode’s\\tGini\\timpurity\\tgenerally\\tlower\\tor\\tgreater\\tthan\\tits\\tparent’s?\\tIs\\tit\\t\\ngenerally\\n\\tlower/greater,\\nor\\t\\nalways\\n\\tlower/greater?\\n3\\n.\\t\\nIf\\ta\\tDecision\\tTree\\tis\\toverfitting\\tthe\\ttraining\\tset,\\tis\\tit\\ta\\tgood\\tidea\\tto\\ttry\\tdecreasing\\t\\nmax_depth\\n?\\n4\\n.\\t\\nIf\\ta\\tDecision\\tTree\\tis\\tunderfitting\\tthe\\ttraining\\tset,\\tis\\tit\\ta\\tgood\\tidea\\tto\\ttry\\tscaling\\tthe\\tinput\\tfeatures?\\n5\\n.\\t\\nIf\\tit\\ttakes\\tone\\thour\\tto\\ttrain\\ta\\tDecision\\tTree\\ton\\ta\\ttraining\\tset\\tcontaining\\t1\\tmillion\\tinstances,\\troughly\\nhow\\tmuch\\ttime\\twill\\tit\\ttake\\tto\\ttrain\\tanother\\tDecision\\tTree\\ton\\ta\\ttraining\\tset\\tcontaining\\t10\\tmillion\\ninstances?\\n6\\n.\\t\\nIf\\tyour\\ttraining\\tset\\tcontains\\t100,000\\tinstances,\\twill\\tsetting\\t\\npresort=True\\n\\tspeed\\tup\\ttraining?\\n7\\n.\\t\\nTrain\\tand\\tfine-tune\\ta\\tDecision\\tTree\\tfor\\tthe\\tmoons\\tdataset.\\na\\n.\\t\\nGenerate\\ta\\tmoons\\tdataset\\t\\nusing\\t\\nmake_moons(n_samples=10000,\\tnoise=0.4)\\n.\\nb\\n.\\t\\nSplit\\tit\\tinto\\ta\\ttraining\\tset\\tand\\ta', 'test\\tset\\tusing\\t\\ntrain_test_split()\\n.\\nc\\n.\\t\\nUse\\tgrid\\tsearch\\twith\\tcross-validation\\t(with\\tthe\\thelp\\tof\\tthe\\t\\nGridSearchCV\\n\\tclass)\\tto\\tfind\\tgood\\nhyperparameter\\tvalues\\tfor\\ta\\t\\nDecisionTreeClassifier\\n.\\t\\nHint:\\ttry\\tvarious\\tvalues\\tfor\\nmax_leaf_nodes\\n.\\nd\\n.\\t\\nTrain\\tit\\ton\\tthe\\tfull\\ttraining\\tset\\tusing\\tthese\\thyperparameters,\\tand\\tmeasure\\tyour\\tmodel’s\\nperformance\\ton\\tthe\\ttest\\tset.\\tYou\\tshould\\tget\\troughly\\t85%\\tto\\t87%\\taccuracy.\\n8\\n.\\t\\nGrow\\ta\\tforest.\\na\\n.\\t\\nContinuing\\tthe\\tprevious\\texercise,\\tgenerate\\t1,000\\tsubsets\\tof\\tthe\\ttraining\\tset,\\teach\\tcontaining\\t100\\ninstances\\tselected\\trandomly.\\tHint:\\tyou\\tcan\\tuse\\tScikit-Learn’s\\t\\nShuffleSplit\\n\\tclass\\tfor\\tthis.\\nb\\n.\\t\\nTrain\\tone\\tDecision\\tTree\\ton\\teach\\tsubset,\\tusing\\tthe\\tbest\\thyperparameter\\tvalues\\tfound\\tabove.\\nEvaluate\\tthese\\t1,000\\tDecision\\tTrees\\ton\\tthe\\ttest\\tset.\\tSince\\tthey\\twere\\ttrained\\ton\\tsmaller\\tsets,\\nthese\\tDecision\\tTrees\\twill\\tlikely\\tperform\\tworse\\tthan\\tthe\\tfirst\\tDecision\\tTree,\\tachieving\\tonly\\nabout\\t80%\\taccuracy.\\nc\\n.', 'c\\n.\\t\\nNow\\tcomes\\tthe\\tmagic.\\tFor\\teach\\ttest\\tset\\tinstance,\\tgenerate\\tthe\\tpredictions\\tof\\tthe\\t1,000\\tDecision\\nTrees,\\tand\\tkeep\\tonly\\tthe\\tmost\\tfrequent\\tprediction\\t(you\\tcan\\tuse\\tSciPy’s\\t\\nmode()\\n\\tfunction\\tfor\\nthis).\\tThis\\tgives\\tyou\\t\\nmajority-vote\\tpredictions\\n\\tover\\tthe\\ttest\\tset.\\nd\\n.\\t\\nEvaluate\\tthese\\tpredictions\\ton\\tthe\\ttest\\tset:\\tyou\\tshould\\tobtain\\ta\\tslightly\\thigher\\taccuracy\\tthan\\tyour\\nfirst\\tmodel\\t(about\\t0.5\\tto\\t1.5%\\thigher).\\tCongratulations,\\tyou\\thave\\ttrained\\ta\\tRandom\\tForest\\nclassifier!', 'Solutions\\tto\\tthese\\texercises\\tare\\t\\navailable\\tin\\t\\nAppendix\\tA\\n.\\nGraphviz\\tis\\tan\\topen\\tsource\\tgraph\\tvisualization\\tsoftware\\tpackage,\\tavailable\\tat\\t\\nhttp://www.graphviz.org/\\n.\\nP\\tis\\tthe\\tset\\tof\\tproblems\\tthat\\tcan\\tbe\\tsolved\\tin\\tpolynomial\\ttime.\\tNP\\tis\\tthe\\tset\\tof\\tproblems\\twhose\\tsolutions\\tcan\\tbe\\tverified\\tin\\tpolynomial\\ttime.\\nAn\\tNP-Hard\\tproblem\\tis\\ta\\tproblem\\tto\\twhich\\tany\\tNP\\tproblem\\tcan\\tbe\\treduced\\tin\\tpolynomial\\ttime.\\tAn\\tNP-Complete\\tproblem\\tis\\tboth\\tNP\\tand\\nNP-Hard.\\tA\\tmajor\\topen\\tmathematical\\tquestion\\tis\\twhether\\tor\\tnot\\tP\\t=\\tNP.\\tIf\\tP\\t≠\\tNP\\t(which\\tseems\\tlikely),\\tthen\\tno\\tpolynomial\\talgorithm\\twill\\never\\tbe\\tfound\\tfor\\tany\\tNP-Complete\\tproblem\\t(except\\tperhaps\\ton\\ta\\tquantum\\tcomputer).\\nlog\\n2\\n\\tis\\tthe\\tbinary\\tlogarithm.\\tIt\\tis\\tequal\\tto\\t\\nlog\\n2\\n(\\nm\\n)\\t=\\t\\nlog\\n(\\nm\\n)\\t/\\t\\nlog\\n(2).\\nA\\treduction\\tof\\tentropy\\tis\\toften\\tcalled\\tan\\t\\ninformation\\tgain\\n.\\nSee\\tSebastian\\tRaschka’s\\t\\ninteresting\\tanalysis\\tfor\\tmore\\tdetails\\n.\\nIt\\trandomly\\tselects\\tthe\\tset\\tof\\tfeatures\\tto\\tevaluate\\tat\\teach\\tnode.\\n1\\n2\\n3\\n4\\n5\\n6', 'Chapter\\t7.\\t\\nEnsemble\\tLearning\\tand\\tRandom\\nForests\\nSuppose\\t\\nyou\\task\\ta\\tcomplex\\tquestion\\tto\\tthousands\\tof\\trandom\\tpeople,\\tthen\\taggregate\\ttheir\\tanswers.\\tIn\\nmany\\tcases\\tyou\\twill\\tfind\\tthat\\tthis\\taggregated\\tanswer\\tis\\tbetter\\tthan\\tan\\texpert’s\\tanswer.\\tThis\\tis\\tcalled\\tthe\\nwisdom\\tof\\tthe\\tcrowd\\n.\\tSimilarly,\\tif\\tyou\\taggregate\\tthe\\tpredictions\\tof\\ta\\tgroup\\tof\\tpredictors\\t(such\\tas\\nclassifiers\\tor\\tregressors),\\tyou\\twill\\toften\\tget\\tbetter\\tpredictions\\tthan\\twith\\tthe\\tbest\\tindividual\\tpredictor.\\tA\\ngroup\\tof\\tpredictors\\tis\\tcalled\\tan\\t\\nensemble\\n;\\tthus,\\tthis\\ttechnique\\tis\\tcalled\\t\\nEnsemble\\tLearning\\n,\\tand\\tan\\nEnsemble\\tLearning\\talgorithm\\tis\\tcalled\\tan\\t\\nEnsemble\\tmethod\\n.\\nFor\\texample,\\tyou\\tcan\\ttrain\\ta\\tgroup\\tof\\t\\nDecision\\tTree\\tclassifiers,\\teach\\ton\\ta\\tdifferent\\trandom\\tsubset\\tof\\tthe\\ntraining\\tset.\\tTo\\tmake\\tpredictions,\\tyou\\tjust\\tobtain\\tthe\\tpredictions\\tof\\tall\\tindividual\\ttrees,\\tthen\\tpredict\\tthe\\nclass\\tthat\\tgets\\tthe\\tmost\\tvotes\\t(see\\tthe\\tlast\\texercise\\tin\\t\\nChapter\\t6\\n).\\tSuch\\tan\\tensemble\\tof\\tDecision\\tTrees\\tis\\ncalled\\ta\\t\\nRandom\\tForest\\n,', ',\\t\\nand\\tdespite\\tits\\tsimplicity,\\tthis\\tis\\tone\\tof\\tthe\\tmost\\tpowerful\\tMachine\\tLearning\\nalgorithms\\tavailable\\ttoday.\\nMoreover,\\tas\\twe\\tdiscussed\\tin\\t\\nChapter\\t2\\n,\\tyou\\twill\\toften\\tuse\\tEnsemble\\tmethods\\tnear\\tthe\\tend\\tof\\ta\\tproject,\\nonce\\tyou\\thave\\talready\\tbuilt\\ta\\tfew\\tgood\\tpredictors,\\tto\\tcombine\\tthem\\tinto\\tan\\teven\\tbetter\\tpredictor.\\tIn\\tfact,\\nthe\\twinning\\tsolutions\\tin\\tMachine\\tLearning\\tcompetitions\\toften\\tinvolve\\tseveral\\tEnsemble\\tmethods\\t(most\\nfamously\\tin\\tthe\\t\\nNetflix\\tPrize\\tcompetition\\n).\\nIn\\tthis\\tchapter\\twe\\twill\\tdiscuss\\tthe\\tmost\\tpopular\\tEnsemble\\tmethods,\\tincluding\\t\\nbagging\\n,\\t\\nboosting\\n,\\nstacking\\n,\\tand\\ta\\tfew\\tothers.\\tWe\\twill\\talso\\texplore\\tRandom\\tForests.', 'Voting\\tClassifiers\\nSuppose\\t\\nyou\\thave\\ttrained\\ta\\tfew\\tclassifiers,\\teach\\tone\\tachieving\\tabout\\t80%\\taccuracy.\\tYou\\tmay\\thave\\ta\\nLogistic\\tRegression\\tclassifier,\\tan\\tSVM\\tclassifier,\\ta\\tRandom\\tForest\\tclassifier,\\ta\\tK-Nearest\\tNeighbors\\nclassifier,\\tand\\tperhaps\\ta\\tfew\\tmore\\t(see\\t\\nFigure\\t7-1\\n).\\nFigure\\t7-1.\\t\\nTraining\\tdiverse\\tclassifiers\\nA\\tvery\\tsimple\\tway\\tto\\tcreate\\tan\\teven\\tbetter\\tclassifier\\tis\\tto\\taggregate\\tthe\\tpredictions\\tof\\teach\\tclassifier\\tand\\npredict\\tthe\\tclass\\tthat\\tgets\\tthe\\tmost\\tvotes.\\tThis\\tmajority-vote\\tclassifier\\tis\\tcalled\\ta\\t\\nhard\\tvoting\\n\\tclassifier\\n(see\\t\\nFigure\\t7-2\\n).\\nFigure\\t7-2.\\t\\nHard\\tvoting\\tclassifier\\tpredictions\\nSomewhat\\tsurprisingly,\\tthis\\tvoting\\tclassifier\\toften\\tachieves\\ta\\thigher\\taccuracy\\tthan\\tthe\\tbest\\tclassifier\\tin\\nthe\\tensemble.\\tIn\\tfact,\\teven\\tif\\teach\\tclassifier\\tis\\ta\\t\\nweak\\tlearner\\n\\t\\n(meaning\\tit\\tdoes\\tonly\\tslightly\\tbetter\\tthan\\nrandom\\tguessing),\\tthe\\tensemble\\tcan\\tstill\\tbe\\ta\\t\\nstrong\\tlearner\\n\\t(achieving\\thigh\\taccuracy),\\t\\nprovided\\tthere', 'are\\ta\\tsufficient\\tnumber\\tof\\tweak\\tlearners\\tand\\tthey\\tare\\tsufficiently\\tdiverse.\\nHow\\tis\\tthis\\tpossible?\\tThe\\tfollowing\\tanalogy\\tcan\\thelp\\tshed\\tsome\\tlight\\ton\\tthis\\tmystery.\\tSuppose\\tyou\\thave', 'a\\tslightly\\tbiased\\tcoin\\tthat\\thas\\ta\\t51%\\tchance\\tof\\tcoming\\tup\\theads,\\tand\\t49%\\tchance\\tof\\tcoming\\tup\\ttails.\\tIf\\nyou\\ttoss\\tit\\t1,000\\ttimes,\\tyou\\twill\\tgenerally\\tget\\tmore\\tor\\tless\\t510\\theads\\tand\\t490\\ttails,\\tand\\thence\\ta\\tmajority\\nof\\theads.\\tIf\\tyou\\tdo\\tthe\\tmath,\\tyou\\twill\\tfind\\tthat\\tthe\\tprobability\\tof\\tobtaining\\ta\\tmajority\\tof\\theads\\tafter\\t1,000\\ntosses\\tis\\tclose\\tto\\t75%.\\tThe\\tmore\\tyou\\ttoss\\tthe\\tcoin,\\tthe\\thigher\\tthe\\tprobability\\t(e.g.,\\twith\\t10,000\\ttosses,\\tthe\\nprobability\\tclimbs\\tover\\t97%).\\tThis\\tis\\tdue\\tto\\tthe\\t\\nlaw\\tof\\tlarge\\tnumbers\\n:\\t\\nas\\tyou\\tkeep\\ttossing\\tthe\\tcoin,\\tthe\\nratio\\tof\\theads\\tgets\\tcloser\\tand\\tcloser\\tto\\tthe\\tprobability\\tof\\theads\\t(51%).\\t\\nFigure\\t7-3\\n\\tshows\\t10\\tseries\\tof\\nbiased\\tcoin\\ttosses.\\tYou\\tcan\\tsee\\tthat\\tas\\tthe\\tnumber\\tof\\ttosses\\tincreases,\\tthe\\tratio\\tof\\theads\\tapproaches\\t51%.\\nEventually\\tall\\t10\\tseries\\tend\\tup\\tso\\tclose\\tto\\t51%\\tthat\\tthey\\tare\\tconsistently\\tabove\\t50%.\\nFigure\\t7-3.\\t\\nThe\\tlaw\\tof\\tlarge\\tnumbers\\nSimilarly,\\tsuppose\\tyou\\tbuild\\tan\\tensemble\\tcontaining\\t1,000\\tclassifiers\\tthat\\tare\\tindividually\\tcorrect\\tonly', '51%\\tof\\tthe\\ttime\\t(barely\\tbetter\\tthan\\trandom\\tguessing).\\tIf\\tyou\\tpredict\\tthe\\tmajority\\tvoted\\tclass,\\tyou\\tcan\\nhope\\tfor\\tup\\tto\\t75%\\taccuracy!\\tHowever,\\tthis\\tis\\tonly\\ttrue\\tif\\tall\\tclassifiers\\tare\\tperfectly\\tindependent,\\nmaking\\tuncorrelated\\terrors,\\twhich\\tis\\tclearly\\tnot\\tthe\\tcase\\tsince\\tthey\\tare\\ttrained\\ton\\tthe\\tsame\\tdata.\\tThey\\tare\\nlikely\\tto\\tmake\\tthe\\tsame\\ttypes\\tof\\terrors,\\tso\\tthere\\twill\\tbe\\tmany\\tmajority\\tvotes\\tfor\\tthe\\twrong\\tclass,\\treducing\\nthe\\tensemble’s\\taccuracy.\\nTIP\\nEnsemble\\tmethods\\twork\\tbest\\twhen\\tthe\\tpredictors\\tare\\tas\\tindependent\\tfrom\\tone\\tanother\\tas\\tpossible.\\tOne\\tway\\tto\\tget\\tdiverse\\nclassifiers\\tis\\tto\\ttrain\\tthem\\tusing\\tvery\\tdifferent\\talgorithms.\\tThis\\tincreases\\tthe\\tchance\\tthat\\tthey\\twill\\tmake\\tvery\\tdifferent\\ttypes\\tof\\nerrors,\\timproving\\tthe\\tensemble’s\\taccuracy.\\nThe\\tfollowing\\tcode\\tcreates\\tand\\ttrains\\ta\\tvoting\\tclassifier\\tin\\tScikit-Learn,\\tcomposed\\tof\\tthree\\tdiverse\\nclassifiers\\t(the\\ttraining\\tset\\tis\\tthe\\tmoons\\tdataset,\\t\\nintroduced\\tin\\t\\nChapter\\t5\\n):\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nRandomForestClassifier\\nfrom', \"from\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nVotingClassifier\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLogisticRegression\\nfrom\\n\\t\\nsklearn.svm\\n\\t\\nimport\\n\\t\\nSVC\\nlog_clf\\n\\t\\n=\\n\\t\\nLogisticRegression\\n()\\nrnd_clf\\n\\t\\n=\\n\\t\\nRandomForestClassifier\\n()\\nsvm_clf\\n\\t\\n=\\n\\t\\nSVC\\n()\\nvoting_clf\\n\\t\\n=\\n\\t\\nVotingClassifier\\n(\\n\\t\\t\\t\\t\\nestimators\\n=\\n[(\\n'lr'\\n,\\n\\t\\nlog_clf\\n),\\n\\t\\n(\\n'rf'\\n,\\n\\t\\nrnd_clf\\n),\\n\\t\\n(\\n'svc'\\n,\\n\\t\\nsvm_clf\\n)],\", \"voting\\n=\\n'hard'\\n)\\nvoting_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\nLet’s\\tlook\\tat\\teach\\tclassifier’s\\taccuracy\\t\\non\\tthe\\ttest\\tset:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\naccuracy_score\\n>>>\\t\\nfor\\n\\t\\nclf\\n\\t\\nin\\n\\t\\n(\\nlog_clf\\n,\\n\\t\\nrnd_clf\\n,\\n\\t\\nsvm_clf\\n,\\n\\t\\nvoting_clf\\n):\\n...\\t\\n\\t\\t\\t\\t\\nclf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n...\\t\\n\\t\\t\\t\\t\\ny_pred\\n\\t\\n=\\n\\t\\nclf\\n.\\npredict\\n(\\nX_test\\n)\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\nclf\\n.\\n__class__\\n.\\n__name__\\n,\\n\\t\\naccuracy_score\\n(\\ny_test\\n,\\n\\t\\ny_pred\\n))\\n...\\nLogisticRegression\\t0.864\\nRandomForestClassifier\\t0.872\\nSVC\\t0.888\\nVotingClassifier\\t0.896\\nThere\\tyou\\thave\\tit!\\tThe\\tvoting\\tclassifier\\tslightly\\toutperforms\\tall\\tthe\\tindividual\\tclassifiers.\\nIf\\tall\\tclassifiers\\tare\\table\\tto\\testimate\\tclass\\tprobabilities\\t(i.e.,\\tthey\\thave\\ta\\t\\npredict_proba()\\n\\tmethod),\\nthen\\tyou\\tcan\\ttell\\tScikit-Learn\\tto\\tpredict\\tthe\\tclass\\twith\\tthe\\thighest\\tclass\\tprobability,\\taveraged\\tover\\tall\\tthe\\nindividual\\tclassifiers.\\tThis\\tis\\t\\ncalled\\t\\nsoft\\tvoting\\n.\\tIt\\toften\\tachieves\\thigher\\tperformance\\tthan\\thard\\tvoting\", 'because\\tit\\tgives\\tmore\\tweight\\tto\\thighly\\tconfident\\tvotes.\\tAll\\tyou\\tneed\\tto\\tdo\\tis\\treplace\\t\\nvoting=\"hard\"\\nwith\\t\\nvoting=\"soft\"\\n\\tand\\tensure\\tthat\\tall\\tclassifiers\\tcan\\testimate\\tclass\\tprobabilities.\\tThis\\tis\\tnot\\tthe\\tcase\\nof\\tthe\\t\\nSVC\\n\\tclass\\tby\\tdefault,\\tso\\tyou\\tneed\\tto\\tset\\tits\\t\\nprobability\\n\\thyperparameter\\tto\\t\\nTrue\\n\\t(this\\twill\\tmake\\nthe\\t\\nSVC\\n\\tclass\\tuse\\tcross-validation\\tto\\testimate\\tclass\\tprobabilities,\\tslowing\\tdown\\ttraining,\\tand\\tit\\twill\\tadd\\na\\t\\npredict_proba()\\n\\tmethod).\\tIf\\tyou\\tmodify\\tthe\\tpreceding\\tcode\\tto\\tuse\\tsoft\\tvoting,\\tyou\\twill\\tfind\\tthat\\tthe\\nvoting\\tclassifier\\tachieves\\tover\\t91%\\t\\naccuracy!', 'Bagging\\tand\\tPasting\\nOne\\t\\nway\\tto\\tget\\ta\\tdiverse\\tset\\tof\\tclassifiers\\tis\\tto\\tuse\\tvery\\tdifferent\\ttraining\\talgorithms,\\tas\\tjust\\tdiscussed.\\nAnother\\tapproach\\tis\\tto\\tuse\\tthe\\tsame\\ttraining\\talgorithm\\tfor\\tevery\\tpredictor,\\tbut\\tto\\ttrain\\tthem\\ton\\tdifferent\\nrandom\\tsubsets\\tof\\tthe\\ttraining\\tset.\\tWhen\\tsampling\\tis\\tperformed\\t\\nwith\\n\\treplacement,\\tthis\\tmethod\\tis\\tcalled\\nbagging\\n1\\n\\t(short\\tfor\\t\\nbootstrap\\taggregating\\n2\\n).\\tWhen\\tsampling\\tis\\tperformed\\t\\nwithout\\n\\treplacement,\\tit\\tis\\ncalled\\t\\npasting\\n.\\n3\\nIn\\tother\\twords,\\tboth\\tbagging\\tand\\tpasting\\tallow\\ttraining\\tinstances\\tto\\tbe\\tsampled\\tseveral\\ttimes\\tacross\\nmultiple\\tpredictors,\\tbut\\tonly\\tbagging\\tallows\\ttraining\\tinstances\\tto\\tbe\\tsampled\\tseveral\\ttimes\\tfor\\tthe\\tsame\\npredictor.\\tThis\\tsampling\\tand\\ttraining\\tprocess\\tis\\trepresented\\tin\\t\\nFigure\\t7-4\\n.\\nFigure\\t7-4.\\t\\nPasting/bagging\\ttraining\\tset\\tsampling\\tand\\ttraining\\nOnce\\tall\\tpredictors\\tare\\ttrained,\\tthe\\tensemble\\tcan\\tmake\\ta\\tprediction\\tfor\\ta\\tnew\\tinstance\\tby\\tsimply\\naggregating\\tthe\\tpredictions\\tof\\tall\\tpredictors.\\tThe\\taggregation\\tfunction\\tis\\ttypically', 'the\\t\\nstatistical\\tmode\\n(i.e.,\\tthe\\tmost\\tfrequent\\tprediction,\\tjust\\tlike\\ta\\thard\\tvoting\\tclassifier)\\tfor\\tclassification,\\tor\\tthe\\taverage\\tfor\\nregression.\\tEach\\tindividual\\tpredictor\\thas\\ta\\thigher\\tbias\\tthan\\tif\\tit\\twere\\ttrained\\ton\\tthe\\toriginal\\ttraining\\tset,\\nbut\\taggregation\\treduces\\tboth\\tbias\\tand\\tvariance.\\n4\\n\\tGenerally,\\tthe\\tnet\\tresult\\tis\\tthat\\tthe\\tensemble\\thas\\ta\\nsimilar\\tbias\\tbut\\ta\\tlower\\tvariance\\tthan\\ta\\tsingle\\tpredictor\\ttrained\\ton\\tthe\\toriginal\\ttraining\\tset.\\nAs\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t7-4\\n,\\tpredictors\\tcan\\tall\\tbe\\ttrained\\tin\\tparallel,\\tvia\\tdifferent\\tCPU\\tcores\\tor\\teven\\ndifferent\\tservers.\\tSimilarly,\\tpredictions\\tcan\\tbe\\tmade\\tin\\tparallel.\\tThis\\tis\\tone\\tof\\tthe\\treasons\\twhy\\tbagging\\nand\\tpasting\\tare\\tsuch\\tpopular\\tmethods:\\tthey\\tscale\\tvery\\twell.', 'Bagging\\tand\\tPasting\\tin\\tScikit-Learn\\nScikit-Learn\\t\\noffers\\ta\\tsimple\\tAPI\\tfor\\tboth\\tbagging\\tand\\tpasting\\twith\\tthe\\t\\nBaggingClassifier\\n\\tclass\\t(or\\nBaggingRegressor\\n\\tfor\\tregression).\\tThe\\tfollowing\\tcode\\ttrains\\tan\\tensemble\\tof\\t\\n500\\tDecision\\tTree\\nclassifiers,\\n5\\n\\teach\\ttrained\\ton\\t100\\ttraining\\tinstances\\trandomly\\tsampled\\tfrom\\tthe\\ttraining\\tset\\twith\\nreplacement\\t(this\\tis\\tan\\texample\\tof\\tbagging,\\tbut\\tif\\tyou\\twant\\tto\\tuse\\tpasting\\tinstead,\\tjust\\tset\\nbootstrap=False\\n).\\tThe\\t\\nn_jobs\\n\\tparameter\\ttells\\tScikit-Learn\\tthe\\tnumber\\tof\\tCPU\\tcores\\tto\\tuse\\tfor\\ttraining\\nand\\tpredictions\\t(\\n–1\\n\\ttells\\tScikit-Learn\\tto\\tuse\\tall\\tavailable\\tcores):\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nBaggingClassifier\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nDecisionTreeClassifier\\nbag_clf\\n\\t\\n=\\n\\t\\nBaggingClassifier\\n(\\n\\t\\t\\t\\t\\nDecisionTreeClassifier\\n(),\\n\\t\\nn_estimators\\n=\\n500\\n,\\n\\t\\t\\t\\t\\nmax_samples\\n=\\n100\\n,\\n\\t\\nbootstrap\\n=\\nTrue\\n,\\n\\t\\nn_jobs\\n=-\\n1\\n)\\nbag_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\ny_pred\\n\\t\\n=\\n\\t\\nbag_clf\\n.\\npredict\\n(\\nX_test\\n)\\nNOTE\\nThe\\t\\nBaggingClassifier', 'automatically\\tperforms\\tsoft\\tvoting\\tinstead\\tof\\thard\\tvoting\\tif\\tthe\\tbase\\tclassifier\\tcan\\testimate\\tclass\\nprobabilities\\t(i.e.,\\tif\\tit\\thas\\ta\\t\\npredict_proba()\\n\\tmethod),\\twhich\\tis\\tthe\\tcase\\twith\\tDecision\\tTrees\\tclassifiers.\\nFigure\\t7-5\\n\\tcompares\\tthe\\tdecision\\tboundary\\tof\\ta\\tsingle\\tDecision\\tTree\\twith\\tthe\\tdecision\\tboundary\\tof\\ta\\nbagging\\tensemble\\tof\\t500\\ttrees\\t(from\\tthe\\tpreceding\\tcode),\\tboth\\ttrained\\ton\\tthe\\tmoons\\tdataset.\\tAs\\tyou\\tcan\\nsee,\\tthe\\tensemble’s\\tpredictions\\twill\\tlikely\\tgeneralize\\tmuch\\tbetter\\tthan\\tthe\\tsingle\\tDecision\\tTree’s\\npredictions:\\tthe\\tensemble\\thas\\ta\\tcomparable\\tbias\\tbut\\ta\\tsmaller\\tvariance\\t(it\\tmakes\\troughly\\tthe\\tsame\\nnumber\\tof\\terrors\\ton\\tthe\\ttraining\\tset,\\tbut\\tthe\\tdecision\\tboundary\\tis\\tless\\tirregular).\\nFigure\\t7-5.\\t\\nA\\tsingle\\tDecision\\tTree\\tversus\\ta\\tbagging\\tensemble\\tof\\t500\\ttrees\\nBootstrapping\\tintroduces\\ta\\tbit\\tmore\\tdiversity\\tin\\tthe\\tsubsets\\tthat\\teach\\tpredictor\\tis\\ttrained\\ton,\\tso\\tbagging\\nends\\tup\\twith\\ta\\tslightly\\thigher\\tbias\\tthan\\tpasting,\\tbut\\tthis\\talso\\tmeans\\tthat\\tpredictors\\tend\\tup\\tbeing\\tless', 'correlated\\tso\\tthe\\tensemble’s\\tvariance\\tis\\treduced.\\tOverall,\\tbagging\\toften\\tresults\\tin\\tbetter\\tmodels,\\twhich\\nexplains\\twhy\\tit\\tis\\tgenerally\\tpreferred.\\tHowever,\\tif\\tyou\\thave\\tspare\\ttime\\tand\\tCPU\\tpower\\tyou\\tcan\\tuse\\ncross-validation\\tto\\tevaluate\\tboth\\tbagging\\tand\\tpasting\\tand\\tselect\\tthe\\tone\\tthat\\t\\nworks\\tbest.', 'Out-of-Bag\\tEvaluation\\nWith\\t\\nbagging,\\tsome\\tinstances\\tmay\\tbe\\tsampled\\tseveral\\ttimes\\tfor\\tany\\tgiven\\tpredictor,\\twhile\\tothers\\tmay\\tnot\\nbe\\tsampled\\tat\\tall.\\tBy\\tdefault\\ta\\t\\nBaggingClassifier\\n\\tsamples\\t\\nm\\n\\ttraining\\tinstances\\twith\\treplacement\\n(\\nbootstrap=True\\n),\\twhere\\t\\nm\\n\\tis\\tthe\\tsize\\tof\\tthe\\ttraining\\tset.\\tThis\\tmeans\\tthat\\tonly\\tabout\\t63%\\tof\\tthe\\ttraining\\ninstances\\tare\\tsampled\\ton\\taverage\\tfor\\teach\\tpredictor.\\n6\\n\\tThe\\tremaining\\t37%\\tof\\tthe\\ttraining\\tinstances\\tthat\\tare\\nnot\\tsampled\\tare\\tcalled\\t\\nout-of-bag\\n\\t(oob)\\tinstances.\\tNote\\tthat\\tthey\\tare\\tnot\\tthe\\tsame\\t37%\\tfor\\tall\\tpredictors.\\nSince\\ta\\tpredictor\\tnever\\tsees\\tthe\\toob\\tinstances\\tduring\\ttraining,\\tit\\tcan\\tbe\\tevaluated\\ton\\tthese\\tinstances,\\nwithout\\tthe\\tneed\\tfor\\ta\\tseparate\\tvalidation\\tset\\tor\\tcross-validation.\\tYou\\tcan\\tevaluate\\tthe\\tensemble\\titself\\tby\\naveraging\\tout\\tthe\\toob\\tevaluations\\tof\\teach\\tpredictor.\\nIn\\tScikit-Learn,\\tyou\\tcan\\tset\\t\\noob_score=True\\n\\twhen\\tcreating\\ta\\t\\nBaggingClassifier\\n\\tto\\trequest\\tan', 'automatic\\toob\\tevaluation\\tafter\\ttraining.\\tThe\\tfollowing\\tcode\\tdemonstrates\\tthis.\\tThe\\tresulting\\tevaluation\\nscore\\tis\\tavailable\\t\\nthrough\\tthe\\t\\noob_score_\\n\\tvariable:\\n>>>\\t\\nbag_clf\\n\\t\\n=\\n\\t\\nBaggingClassifier\\n(\\n...\\t\\n\\t\\t\\t\\t\\nDecisionTreeClassifier\\n(),\\n\\t\\nn_estimators\\n=\\n500\\n,\\n...\\t\\n\\t\\t\\t\\t\\nbootstrap\\n=\\nTrue\\n,\\n\\t\\nn_jobs\\n=-\\n1\\n,\\n\\t\\noob_score\\n=\\nTrue\\n)\\n...\\n>>>\\t\\nbag_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n>>>\\t\\nbag_clf\\n.\\noob_score_\\n0.90133333333333332\\nAccording\\tto\\tthis\\toob\\tevaluation,\\tthis\\t\\nBaggingClassifier\\n\\tis\\tlikely\\tto\\tachieve\\tabout\\t90.1%\\taccuracy\\t\\non\\nthe\\ttest\\tset.\\tLet’s\\tverify\\tthis:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\naccuracy_score\\n>>>\\t\\ny_pred\\n\\t\\n=\\n\\t\\nbag_clf\\n.\\npredict\\n(\\nX_test\\n)\\n>>>\\t\\naccuracy_score\\n(\\ny_test\\n,\\n\\t\\ny_pred\\n)\\n0.91200000000000003\\nWe\\tget\\t91.2%\\taccuracy\\ton\\tthe\\ttest\\tset\\t—\\tclose\\tenough!\\nThe\\toob\\tdecision\\tfunction\\tfor\\teach\\ttraining\\tinstance\\tis\\talso\\tavailable\\tthrough\\tthe\\noob_decision_function_\\n\\tvariable.\\tIn\\tthis\\tcase\\t(since\\tthe\\tbase\\testimator\\thas\\ta\\t\\npredict_proba()', 'method)\\tthe\\tdecision\\tfunction\\treturns\\tthe\\tclass\\tprobabilities\\tfor\\teach\\ttraining\\tinstance.\\tFor\\texample,\\tthe\\noob\\tevaluation\\testimates\\tthat\\tthe\\tsecond\\ttraining\\tinstance\\thas\\ta\\t60.6%\\tprobability\\tof\\tbelonging\\tto\\tthe\\npositive\\t\\nclass\\t(and\\t39.4%\\tof\\tbelonging\\tto\\tthe\\tpositive\\t\\nclass):\\n>>>\\t\\nbag_clf\\n.\\noob_decision_function_\\narray([[\\t0.31746032,\\t\\t0.68253968],\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.34117647,\\t\\t0.65882353],\\n\\t\\t\\t\\t\\t\\t\\t[\\t1.\\t\\t\\t\\t\\t\\t\\t\\t,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t],\\n\\t\\t\\t\\t\\t\\t\\t...\\n\\t\\t\\t\\t\\t\\t\\t[\\t1.\\t\\t\\t\\t\\t\\t\\t\\t,\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t],\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.03108808,\\t\\t0.96891192],\\n\\t\\t\\t\\t\\t\\t\\t[\\t0.57291667,\\t\\t0.42708333]])', 'Random\\tPatches\\tand\\tRandom\\tSubspaces\\nThe\\t\\nBaggingClassifier\\n\\t\\nclass\\tsupports\\tsampling\\tthe\\tfeatures\\tas\\twell.\\tThis\\tis\\tcontrolled\\tby\\ttwo\\nhyperparameters:\\t\\nmax_features\\n\\tand\\t\\nbootstrap_features\\n.\\tThey\\twork\\tthe\\tsame\\tway\\tas\\t\\nmax_samples\\nand\\t\\nbootstrap\\n,\\tbut\\tfor\\tfeature\\tsampling\\tinstead\\tof\\tinstance\\tsampling.\\tThus,\\teach\\tpredictor\\twill\\tbe\\ntrained\\ton\\ta\\trandom\\tsubset\\tof\\tthe\\tinput\\tfeatures.\\nThis\\tis\\tparticularly\\tuseful\\twhen\\tyou\\tare\\tdealing\\twith\\thigh-dimensional\\tinputs\\t(such\\tas\\timages).\\tSampling\\nboth\\ttraining\\tinstances\\tand\\tfeatures\\tis\\tcalled\\tthe\\t\\nRandom\\tPatches\\n\\tmethod\\n.\\n7\\n\\tKeeping\\tall\\ttraining\\tinstances\\n(i.e.,\\t\\nbootstrap=False\\n\\tand\\t\\nmax_samples=1.0\\n)\\tbut\\tsampling\\tfeatures\\t(i.e.,\\t\\nbootstrap_features=True\\nand/or\\t\\nmax_features\\n\\tsmaller\\tthan\\t1.0)\\tis\\tcalled\\tthe\\t\\nRandom\\tSubspaces\\n\\tmethod\\n.\\n8\\nSampling\\tfeatures\\tresults\\tin\\teven\\tmore\\tpredictor\\tdiversity,\\ttrading\\ta\\tbit\\tmore\\tbias\\tfor\\ta\\tlower\\tvariance.', 'Random\\tForests\\nAs\\t\\nwe\\thave\\tdiscussed,\\ta\\t\\nRandom\\tForest\\n9\\n\\tis\\tan\\tensemble\\tof\\tDecision\\tTrees,\\tgenerally\\ttrained\\tvia\\tthe\\nbagging\\tmethod\\t(or\\tsometimes\\tpasting),\\ttypically\\twith\\t\\nmax_samples\\n\\tset\\tto\\tthe\\tsize\\tof\\tthe\\ttraining\\tset.\\nInstead\\tof\\tbuilding\\ta\\t\\nBaggingClassifier\\n\\tand\\tpassing\\tit\\ta\\t\\nDecisionTreeClassifier\\n,\\tyou\\tcan\\tinstead\\nuse\\tthe\\t\\nRandomForestClassifier\\n\\tclass,\\twhich\\tis\\tmore\\tconvenient\\tand\\toptimized\\tfor\\tDecision\\tTrees\\n10\\n(similarly,\\tthere\\tis\\ta\\t\\nRandomForestRegressor\\n\\tclass\\tfor\\tregression\\ttasks).\\tThe\\tfollowing\\tcode\\ttrains\\ta\\nRandom\\tForest\\tclassifier\\twith\\t500\\ttrees\\t(each\\tlimited\\tto\\tmaximum\\t16\\tnodes),\\tusing\\tall\\tavailable\\tCPU\\ncores:\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nRandomForestClassifier\\nrnd_clf\\n\\t\\n=\\n\\t\\nRandomForestClassifier\\n(\\nn_estimators\\n=\\n500\\n,\\n\\t\\nmax_leaf_nodes\\n=\\n16\\n,\\n\\t\\nn_jobs\\n=-\\n1\\n)\\nrnd_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\ny_pred_rf\\n\\t\\n=\\n\\t\\nrnd_clf\\n.\\npredict\\n(\\nX_test\\n)\\nWith\\ta\\tfew\\texceptions,\\ta\\t\\nRandomForestClassifier\\n\\thas\\tall\\tthe\\thyperparameters\\tof\\ta\\nDecisionTreeClassifier', '(to\\tcontrol\\thow\\ttrees\\tare\\tgrown),\\tplus\\tall\\tthe\\thyperparameters\\tof\\ta\\nBaggingClassifier\\n\\tto\\tcontrol\\tthe\\tensemble\\titself.\\n11\\nThe\\tRandom\\tForest\\talgorithm\\tintroduces\\textra\\trandomness\\twhen\\tgrowing\\ttrees;\\tinstead\\tof\\tsearching\\tfor\\nthe\\tvery\\tbest\\tfeature\\twhen\\tsplitting\\ta\\tnode\\t(see\\t\\nChapter\\t6\\n),\\tit\\tsearches\\tfor\\tthe\\tbest\\tfeature\\tamong\\ta\\nrandom\\tsubset\\tof\\tfeatures.\\tThis\\tresults\\tin\\ta\\tgreater\\ttree\\tdiversity,\\twhich\\t(once\\tagain)\\ttrades\\ta\\thigher\\tbias\\nfor\\ta\\tlower\\tvariance,\\tgenerally\\tyielding\\tan\\toverall\\tbetter\\tmodel.\\tThe\\tfollowing\\t\\nBaggingClassifier\\n\\tis\\nroughly\\tequivalent\\tto\\tthe\\tprevious\\t\\nRandomForestClassifier\\n:\\nbag_clf\\n\\t\\n=\\n\\t\\nBaggingClassifier\\n(\\n\\t\\t\\t\\t\\nDecisionTreeClassifier\\n(\\nsplitter\\n=\\n\"random\"\\n,\\n\\t\\nmax_leaf_nodes\\n=\\n16\\n),\\n\\t\\t\\t\\t\\nn_estimators\\n=\\n500\\n,\\n\\t\\nmax_samples\\n=\\n1.0\\n,\\n\\t\\nbootstrap\\n=\\nTrue\\n,\\n\\t\\nn_jobs\\n=-\\n1\\n)', 'Extra-Trees\\nWhen\\t\\nyou\\tare\\tgrowing\\ta\\ttree\\tin\\ta\\tRandom\\tForest,\\tat\\teach\\tnode\\tonly\\ta\\trandom\\tsubset\\tof\\tthe\\tfeatures\\tis\\nconsidered\\tfor\\tsplitting\\t(as\\tdiscussed\\tearlier).\\tIt\\tis\\tpossible\\tto\\tmake\\ttrees\\teven\\tmore\\trandom\\tby\\talso\\nusing\\trandom\\tthresholds\\tfor\\teach\\tfeature\\trather\\tthan\\tsearching\\tfor\\tthe\\tbest\\tpossible\\tthresholds\\t(like\\nregular\\tDecision\\tTrees\\tdo).\\nA\\tforest\\tof\\tsuch\\textremely\\trandom\\ttrees\\tis\\tsimply\\tcalled\\tan\\t\\nExtremely\\tRandomized\\tTrees\\n\\tensemble\\n12\\n\\t(or\\nExtra-Trees\\n\\tfor\\tshort).\\tOnce\\tagain,\\tthis\\ttrades\\tmore\\tbias\\tfor\\ta\\tlower\\tvariance.\\tIt\\talso\\tmakes\\tExtra-Trees\\nmuch\\tfaster\\tto\\ttrain\\tthan\\tregular\\tRandom\\tForests\\tsince\\tfinding\\tthe\\tbest\\tpossible\\tthreshold\\tfor\\teach\\tfeature\\nat\\tevery\\tnode\\tis\\tone\\tof\\tthe\\tmost\\ttime-consuming\\ttasks\\tof\\tgrowing\\ta\\ttree.\\nYou\\tcan\\tcreate\\tan\\tExtra-Trees\\tclassifier\\tusing\\tScikit-Learn’s\\t\\nExtraTreesClassifier\\n\\tclass.\\tIts\\tAPI\\tis\\nidentical\\tto\\t\\nthe\\t\\nRandomForestClassifier\\n\\tclass.\\tSimilarly,\\tthe\\t\\nExtraTreesRegressor\\n\\tclass\\thas\\tthe\\nsame\\tAPI\\tas\\tthe\\t\\nRandomForestRegressor\\n\\tclass.\\nTIP', 'TIP\\nIt\\tis\\thard\\tto\\ttell\\tin\\tadvance\\twhether\\ta\\t\\nRandomForestClassifier\\n\\twill\\tperform\\tbetter\\tor\\tworse\\tthan\\tan\\t\\nExtraTreesClassifier\\n.\\nGenerally,\\tthe\\tonly\\tway\\tto\\tknow\\tis\\tto\\ttry\\tboth\\tand\\tcompare\\tthem\\tusing\\tcross-validation\\t(and\\ttuning\\tthe\\thyperparameters\\tusing\\ngrid\\tsearch).', 'Feature\\tImportance\\nYet\\tanother\\tgreat\\tquality\\tof\\tRandom\\tForests\\tis\\tthat\\tthey\\tmake\\tit\\teasy\\tto\\tmeasure\\tthe\\t\\nrelative\\timportance\\tof\\neach\\tfeature.\\tScikit-Learn\\tmeasures\\ta\\tfeature’s\\timportance\\tby\\tlooking\\tat\\thow\\tmuch\\tthe\\ttree\\tnodes\\tthat\\tuse\\nthat\\tfeature\\treduce\\timpurity\\ton\\taverage\\t(across\\tall\\ttrees\\tin\\tthe\\tforest).\\tMore\\tprecisely,\\tit\\tis\\ta\\tweighted\\naverage,\\twhere\\teach\\tnode’s\\tweight\\tis\\tequal\\tto\\tthe\\tnumber\\tof\\ttraining\\tsamples\\tthat\\tare\\tassociated\\twith\\tit\\n(see\\t\\nChapter\\t6\\n).\\nScikit-Learn\\tcomputes\\tthis\\tscore\\tautomatically\\tfor\\teach\\tfeature\\tafter\\ttraining,\\tthen\\tit\\tscales\\tthe\\tresults\\tso\\nthat\\tthe\\tsum\\tof\\tall\\timportances\\tis\\tequal\\tto\\t1.\\tYou\\tcan\\taccess\\tthe\\tresult\\tusing\\tthe\\t\\nfeature_importances_\\nvariable.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\ttrains\\ta\\t\\nRandomForestClassifier\\n\\ton\\tthe\\t\\niris\\tdataset\\n(introduced\\tin\\t\\nChapter\\t4\\n)\\tand\\toutputs\\teach\\tfeature’s\\timportance.\\tIt\\tseems\\tthat\\tthe\\tmost\\timportant\\tfeatures\\nare\\tthe\\tpetal\\tlength\\t(44%)\\tand\\twidth\\t(42%),\\twhile\\tsepal\\tlength\\tand\\twidth\\tare\\trather\\tunimportant\\tin', 'comparison\\t(11%\\tand\\t2%,\\trespectively).\\n>>>\\t\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nload_iris\\n>>>\\t\\niris\\n\\t\\n=\\n\\t\\nload_iris\\n()\\n>>>\\t\\nrnd_clf\\n\\t\\n=\\n\\t\\nRandomForestClassifier\\n(\\nn_estimators\\n=\\n500\\n,\\n\\t\\nn_jobs\\n=-\\n1\\n)\\n>>>\\t\\nrnd_clf\\n.\\nfit\\n(\\niris\\n[\\n\"data\"\\n],\\n\\t\\niris\\n[\\n\"target\"\\n])\\n>>>\\t\\nfor\\n\\t\\nname\\n,\\n\\t\\nscore\\n\\t\\nin\\n\\t\\nzip\\n(\\niris\\n[\\n\"feature_names\"\\n],\\n\\t\\nrnd_clf\\n.\\nfeature_importances_\\n):\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\nname\\n,\\n\\t\\nscore\\n)\\n...\\nsepal\\tlength\\t(cm)\\t0.112492250999\\nsepal\\twidth\\t(cm)\\t0.0231192882825\\npetal\\tlength\\t(cm)\\t0.441030464364\\npetal\\twidth\\t(cm)\\t0.423357996355\\nSimilarly,\\tif\\tyou\\ttrain\\ta\\tRandom\\tForest\\tclassifier\\ton\\tthe\\tMNIST\\tdataset\\t(introduced\\tin\\t\\nChapter\\t3\\n)\\tand\\nplot\\teach\\tpixel’s\\timportance,\\tyou\\tget\\tthe\\timage\\trepresented\\tin\\t\\nFigure\\t7-6\\n.\\nFigure\\t7-6.\\t\\nMNIST\\tpixel\\timportance\\t(according\\tto\\ta\\tRandom\\tForest\\tclassifier)', 'Random\\tForests\\tare\\tvery\\thandy\\tto\\tget\\ta\\tquick\\tunderstanding\\tof\\twhat\\tfeatures\\tactually\\tmatter,\\tin\\tparticular\\nif\\tyou\\tneed\\tto\\tperform\\t\\nfeature\\tselection.', 'Boosting\\nBoosting\\n\\t\\n(originally\\tcalled\\t\\nhypothesis\\tboosting\\n)\\trefers\\tto\\tany\\tEnsemble\\tmethod\\tthat\\tcan\\tcombine\\tseveral\\nweak\\tlearners\\tinto\\ta\\tstrong\\tlearner.\\tThe\\tgeneral\\tidea\\tof\\tmost\\tboosting\\tmethods\\tis\\tto\\ttrain\\tpredictors\\nsequentially,\\teach\\ttrying\\tto\\tcorrect\\tits\\tpredecessor.\\tThere\\tare\\tmany\\tboosting\\tmethods\\tavailable,\\tbut\\tby\\tfar\\nthe\\tmost\\tpopular\\tare\\t\\nAdaBoost\\n13\\n\\t(short\\tfor\\t\\nAdaptive\\tBoosting\\n)\\tand\\t\\nGradient\\tBoosting\\n.\\tLet’s\\tstart\\twith\\nAdaBoost.', 'AdaBoost\\nOne\\t\\nway\\tfor\\ta\\tnew\\tpredictor\\tto\\tcorrect\\tits\\tpredecessor\\tis\\tto\\tpay\\ta\\tbit\\tmore\\tattention\\tto\\tthe\\ttraining\\ninstances\\tthat\\tthe\\tpredecessor\\tunderfitted.\\tThis\\tresults\\tin\\tnew\\tpredictors\\tfocusing\\tmore\\tand\\tmore\\ton\\tthe\\nhard\\tcases.\\tThis\\tis\\tthe\\ttechnique\\tused\\tby\\tAdaBoost.\\nFor\\texample,\\tto\\tbuild\\tan\\tAdaBoost\\tclassifier,\\ta\\tfirst\\tbase\\tclassifier\\t(such\\tas\\ta\\tDecision\\tTree)\\tis\\ttrained\\nand\\tused\\tto\\tmake\\tpredictions\\ton\\tthe\\ttraining\\tset.\\tThe\\trelative\\tweight\\tof\\tmisclassified\\ttraining\\tinstances\\tis\\nthen\\tincreased.\\tA\\tsecond\\tclassifier\\tis\\ttrained\\tusing\\tthe\\tupdated\\tweights\\tand\\tagain\\tit\\tmakes\\tpredictions\\ton\\nthe\\ttraining\\tset,\\tweights\\tare\\tupdated,\\tand\\tso\\ton\\t(see\\t\\nFigure\\t7-7\\n).\\nFigure\\t7-7.\\t\\nAdaBoost\\tsequential\\ttraining\\twith\\tinstance\\tweight\\tupdates\\nFigure\\t7-8\\n\\tshows\\tthe\\tdecision\\tboundaries\\tof\\tfive\\tconsecutive\\tpredictors\\ton\\tthe\\tmoons\\tdataset\\t(in\\tthis\\nexample,\\teach\\tpredictor\\tis\\ta\\thighly\\tregularized\\tSVM\\tclassifier\\twith\\tan\\tRBF\\tkernel\\n14\\n).\\tThe\\tfirst\\tclassifier', 'gets\\tmany\\tinstances\\twrong,\\tso\\ttheir\\tweights\\tget\\tboosted.\\tThe\\tsecond\\tclassifier\\ttherefore\\tdoes\\ta\\tbetter\\tjob\\non\\tthese\\tinstances,\\tand\\tso\\ton.\\tThe\\tplot\\ton\\tthe\\tright\\trepresents\\tthe\\tsame\\tsequence\\tof\\tpredictors\\texcept\\tthat\\nthe\\tlearning\\trate\\tis\\thalved\\t(i.e.,\\tthe\\tmisclassified\\tinstance\\tweights\\tare\\tboosted\\thalf\\tas\\tmuch\\tat\\tevery\\niteration).\\tAs\\tyou\\tcan\\tsee,\\tthis\\tsequential\\tlearning\\ttechnique\\thas\\tsome\\tsimilarities\\twith\\tGradient\\tDescent,\\nexcept\\tthat\\tinstead\\tof\\ttweaking\\ta\\tsingle\\tpredictor’s\\tparameters\\tto\\tminimize\\ta\\t\\ncost\\tfunction,\\tAdaBoost\\nadds\\tpredictors\\tto\\tthe\\tensemble,\\tgradually\\tmaking\\tit\\tbetter.', 'Figure\\t7-8.\\t\\nDecision\\tboundaries\\tof\\tconsecutive\\tpredictors\\nOnce\\tall\\tpredictors\\tare\\ttrained,\\tthe\\tensemble\\tmakes\\tpredictions\\tvery\\tmuch\\tlike\\tbagging\\tor\\tpasting,\\texcept\\nthat\\tpredictors\\thave\\tdifferent\\tweights\\tdepending\\ton\\ttheir\\toverall\\taccuracy\\ton\\tthe\\tweighted\\ttraining\\tset.\\nWARNING\\nThere\\tis\\tone\\timportant\\tdrawback\\tto\\tthis\\tsequential\\tlearning\\ttechnique:\\tit\\tcannot\\tbe\\tparallelized\\t(or\\tonly\\tpartially),\\tsince\\teach\\npredictor\\tcan\\tonly\\tbe\\ttrained\\tafter\\tthe\\tprevious\\tpredictor\\thas\\tbeen\\ttrained\\tand\\tevaluated.\\tAs\\ta\\tresult,\\tit\\tdoes\\tnot\\tscale\\tas\\twell\\tas\\nbagging\\tor\\tpasting.\\nLet’s\\ttake\\ta\\tcloser\\tlook\\tat\\tthe\\tAdaBoost\\talgorithm.\\tEach\\tinstance\\tweight\\t\\nw\\n(i)\\n\\tis\\tinitially\\tset\\tto\\t\\n.\\tA\\tfirst\\npredictor\\tis\\ttrained\\tand\\tits\\tweighted\\terror\\trate\\t\\nr\\n1\\n\\tis\\tcomputed\\ton\\tthe\\ttraining\\tset;\\tsee\\t\\nEquation\\t7-1\\n.\\nEquation\\t7-1.\\t\\nWeighted\\terror\\trate\\tof\\tthe\\tj\\nth\\n\\tpredictor\\nThe\\tpredictor’s\\tweight\\t\\nα\\nj\\n\\tis\\tthen\\tcomputed\\tusing\\t\\nEquation\\t7-2\\n,\\twhere\\t\\nη\\n\\tis\\tthe\\tlearning\\trate\\nhyperparameter\\t(defaults\\tto\\t1).\\n15', '15\\n\\tThe\\tmore\\taccurate\\tthe\\tpredictor\\tis,\\tthe\\thigher\\tits\\tweight\\twill\\tbe.\\tIf\\tit\\tis\\njust\\tguessing\\trandomly,\\tthen\\tits\\tweight\\twill\\tbe\\tclose\\tto\\tzero.\\tHowever,\\tif\\tit\\tis\\tmost\\toften\\twrong\\t(i.e.,\\tless\\naccurate\\tthan\\trandom\\tguessing),\\tthen\\tits\\tweight\\twill\\tbe\\tnegative.\\nEquation\\t7-2.\\t\\nPredictor\\tweight\\nNext\\tthe\\tinstance\\tweights\\tare\\tupdated\\tusing\\t\\nEquation\\t7-3\\n:\\tthe\\tmisclassified\\tinstances\\tare\\tboosted.', 'Equation\\t7-3.\\t\\nWeight\\tupdate\\trule\\nThen\\tall\\tthe\\tinstance\\tweights\\tare\\tnormalized\\t(i.e.,\\tdivided\\tby\\t\\n).\\nFinally,\\ta\\tnew\\tpredictor\\tis\\ttrained\\tusing\\tthe\\tupdated\\tweights,\\tand\\tthe\\twhole\\tprocess\\tis\\trepeated\\t(the\\tnew\\npredictor’s\\tweight\\tis\\tcomputed,\\tthe\\tinstance\\tweights\\tare\\tupdated,\\tthen\\tanother\\tpredictor\\tis\\ttrained,\\tand\\tso\\non).\\tThe\\talgorithm\\tstops\\twhen\\tthe\\tdesired\\tnumber\\tof\\tpredictors\\tis\\treached,\\tor\\twhen\\ta\\tperfect\\tpredictor\\tis\\nfound.\\nTo\\tmake\\tpredictions,\\tAdaBoost\\tsimply\\tcomputes\\tthe\\tpredictions\\tof\\tall\\tthe\\tpredictors\\tand\\tweighs\\tthem\\nusing\\tthe\\tpredictor\\tweights\\t\\nα\\nj\\n.\\tThe\\tpredicted\\tclass\\tis\\tthe\\tone\\tthat\\treceives\\tthe\\tmajority\\tof\\tweighted\\tvotes\\n(see\\t\\nEquation\\t7-4\\n).\\nEquation\\t7-4.\\t\\nAdaBoost\\tpredictions\\nScikit-Learn\\t\\nactually\\tuses\\ta\\tmulticlass\\tversion\\tof\\tAdaBoost\\tcalled\\t\\nSAMME\\n16\\n\\t(which\\tstands\\tfor\\nStagewise\\tAdditive\\tModeling\\tusing\\ta\\tMulticlass\\tExponential\\tloss\\tfunction\\n).\\tWhen\\tthere\\tare\\tjust\\ttwo\\nclasses,\\tSAMME\\tis\\tequivalent\\tto\\tAdaBoost.\\tMoreover,\\tif\\tthe\\tpredictors\\tcan\\testimate\\tclass\\tprobabilities', '(i.e.,\\tif\\tthey\\thave\\ta\\t\\npredict_proba()\\n\\tmethod),\\tScikit-Learn\\tcan\\tuse\\ta\\tvariant\\tof\\tSAMME\\tcalled\\nSAMME.R\\n\\t(the\\t\\nR\\n\\tstands\\tfor\\t“Real”),\\twhich\\trelies\\ton\\tclass\\tprobabilities\\trather\\tthan\\tpredictions\\tand\\ngenerally\\tperforms\\tbetter.\\nThe\\tfollowing\\tcode\\ttrains\\tan\\t\\nAdaBoost\\tclassifier\\tbased\\ton\\t200\\t\\nDecision\\tStumps\\n\\tusing\\tScikit-Learn’s\\nAdaBoostClassifier\\n\\tclass\\t(as\\tyou\\tmight\\texpect,\\tthere\\tis\\talso\\tan\\t\\nAdaBoostRegressor\\n\\tclass).\\tA\\nDecision\\tStump\\tis\\ta\\t\\nDecision\\tTree\\twith\\t\\nmax_depth=1\\n\\t—\\tin\\tother\\twords,\\ta\\ttree\\tcomposed\\tof\\ta\\tsingle\\ndecision\\tnode\\tplus\\ttwo\\tleaf\\tnodes.\\tThis\\tis\\tthe\\tdefault\\tbase\\testimator\\tfor\\tthe\\t\\nAdaBoostClassifier\\n\\tclass:\\nfrom\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nAdaBoostClassifier\\nada_clf\\n\\t\\n=\\n\\t\\nAdaBoostClassifier\\n(\\n\\t\\t\\t\\t\\nDecisionTreeClassifier\\n(\\nmax_depth\\n=\\n1\\n),\\n\\t\\nn_estimators\\n=\\n200\\n,\\n\\t\\t\\t\\t\\nalgorithm\\n=\\n\"SAMME.R\"\\n,\\n\\t\\nlearning_rate\\n=\\n0.5\\n)\\nada_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)', 'TIP\\nIf\\tyour\\tAdaBoost\\tensemble\\tis\\toverfitting\\tthe\\ttraining\\tset,\\tyou\\tcan\\ttry\\treducing\\tthe\\tnumber\\tof\\testimators\\tor\\tmore\\tstrongly\\nregularizing\\tthe\\tbase\\t\\nestimator.', 'Gradient\\tBoosting\\nAnother\\t\\nvery\\tpopular\\tBoosting\\talgorithm\\tis\\t\\nGradient\\tBoosting\\n.\\n17\\n\\tJust\\tlike\\tAdaBoost,\\tGradient\\tBoosting\\nworks\\tby\\tsequentially\\tadding\\tpredictors\\tto\\tan\\tensemble,\\teach\\tone\\tcorrecting\\tits\\tpredecessor.\\tHowever,\\ninstead\\tof\\ttweaking\\tthe\\tinstance\\tweights\\tat\\tevery\\titeration\\tlike\\tAdaBoost\\tdoes,\\tthis\\tmethod\\ttries\\tto\\tfit\\tthe\\nnew\\tpredictor\\tto\\tthe\\t\\nresidual\\terrors\\n\\t\\nmade\\tby\\tthe\\tprevious\\tpredictor.\\nLet’s\\tgo\\tthrough\\ta\\tsimple\\tregression\\texample\\tusing\\t\\nDecision\\tTrees\\tas\\tthe\\tbase\\tpredictors\\t(of\\tcourse\\nGradient\\tBoosting\\talso\\tworks\\tgreat\\twith\\tregression\\ttasks).\\tThis\\tis\\tcalled\\n\\t\\nGradient\\tTree\\tBoosting\\n,\\tor\\nGradient\\tBoosted\\tRegression\\tTrees\\n\\t(\\nGBRT\\n).\\tFirst,\\tlet’s\\tfit\\ta\\t\\nDecisionTreeRegressor\\n\\tto\\tthe\\ttraining\\tset\\n(for\\texample,\\ta\\tnoisy\\tquadratic\\ttraining\\tset):\\nfrom\\n\\t\\nsklearn.tree\\n\\t\\nimport\\n\\t\\nDecisionTreeRegressor\\ntree_reg1\\n\\t\\n=\\n\\t\\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n)\\ntree_reg1\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nNow\\ttrain\\ta\\tsecond\\t\\nDecisionTreeRegressor\\n\\ton\\tthe\\tresidual\\terrors\\tmade\\tby\\tthe\\tfirst\\tpredictor:', 'y2\\n\\t\\n=\\n\\t\\ny\\n\\t\\n-\\n\\t\\ntree_reg1\\n.\\npredict\\n(\\nX\\n)\\ntree_reg2\\n\\t\\n=\\n\\t\\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n)\\ntree_reg2\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny2\\n)\\nThen\\t\\nwe\\ttrain\\ta\\tthird\\tregressor\\ton\\tthe\\tresidual\\terrors\\tmade\\tby\\tthe\\tsecond\\tpredictor:\\ny3\\n\\t\\n=\\n\\t\\ny2\\n\\t\\n-\\n\\t\\ntree_reg2\\n.\\npredict\\n(\\nX\\n)\\ntree_reg3\\n\\t\\n=\\n\\t\\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n)\\ntree_reg3\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny3\\n)\\nNow\\twe\\thave\\tan\\tensemble\\tcontaining\\tthree\\ttrees.\\tIt\\tcan\\tmake\\tpredictions\\ton\\ta\\tnew\\tinstance\\tsimply\\tby\\nadding\\tup\\tthe\\tpredictions\\tof\\tall\\tthe\\ttrees:\\ny_pred\\n\\t\\n=\\n\\t\\nsum\\n(\\ntree\\n.\\npredict\\n(\\nX_new\\n)\\n\\t\\nfor\\n\\t\\ntree\\n\\t\\nin\\n\\t\\n(\\ntree_reg1\\n,\\n\\t\\ntree_reg2\\n,\\n\\t\\ntree_reg3\\n))\\nFigure\\t7-9\\n\\trepresents\\tthe\\tpredictions\\tof\\tthese\\tthree\\ttrees\\tin\\tthe\\tleft\\tcolumn,\\tand\\tthe\\tensemble’s\\npredictions\\tin\\tthe\\tright\\tcolumn.\\tIn\\tthe\\tfirst\\trow,\\tthe\\tensemble\\thas\\tjust\\tone\\ttree,\\tso\\tits\\tpredictions\\tare\\nexactly\\tthe\\tsame\\tas\\tthe\\tfirst\\ttree’s\\tpredictions.\\tIn\\tthe\\tsecond\\trow,\\ta\\tnew\\ttree\\tis\\ttrained\\ton\\tthe\\tresidual', 'errors\\tof\\tthe\\tfirst\\ttree.\\tOn\\tthe\\tright\\tyou\\tcan\\tsee\\tthat\\tthe\\tensemble’s\\tpredictions\\tare\\tequal\\tto\\tthe\\tsum\\tof\\tthe\\npredictions\\tof\\tthe\\tfirst\\ttwo\\ttrees.\\tSimilarly,\\tin\\tthe\\tthird\\trow\\tanother\\ttree\\tis\\ttrained\\ton\\tthe\\tresidual\\terrors\\nof\\tthe\\tsecond\\ttree.\\tYou\\tcan\\tsee\\tthat\\tthe\\tensemble’s\\tpredictions\\tgradually\\tget\\tbetter\\tas\\ttrees\\tare\\tadded\\tto\\nthe\\t\\nensemble.\\nA\\tsimpler\\tway\\tto\\ttrain\\tGBRT\\tensembles\\tis\\tto\\tuse\\tScikit-Learn’s\\t\\nGradientBoostingRegressor\\n\\tclass.\\nMuch\\tlike\\tthe\\t\\nRandomForestRegressor\\n\\tclass,\\tit\\thas\\thyperparameters\\tto\\tcontrol\\tthe\\tgrowth\\tof\\tDecision\\nTrees\\t(e.g.,\\t\\nmax_depth\\n,\\t\\nmin_samples_leaf\\n,\\tand\\tso\\ton),\\tas\\twell\\tas\\thyperparameters\\tto\\tcontrol\\tthe\\nensemble\\ttraining,\\tsuch\\tas\\tthe\\tnumber\\tof\\ttrees\\t(\\nn_estimators\\n).\\tThe\\tfollowing\\tcode\\tcreates\\tthe\\tsame\\nensemble\\tas\\tthe\\tprevious\\tone:', 'from\\n\\t\\nsklearn.ensemble\\n\\t\\nimport\\n\\t\\nGradientBoostingRegressor\\ngbrt\\n\\t\\n=\\n\\t\\nGradientBoostingRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n\\t\\nn_estimators\\n=\\n3\\n,\\n\\t\\nlearning_rate\\n=\\n1.0\\n)\\ngbrt\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nFigure\\t7-9.\\t\\nGradient\\tBoosting\\nThe\\t\\nlearning_rate\\n\\thyperparameter\\tscales\\tthe\\tcontribution\\tof\\teach\\ttree.\\tIf\\tyou\\tset\\tit\\tto\\ta\\tlow\\tvalue,\\tsuch\\nas\\t\\n0.1\\n,\\tyou\\twill\\tneed\\tmore\\ttrees\\tin\\tthe\\tensemble\\tto\\tfit\\tthe\\ttraining\\tset,\\tbut\\tthe\\tpredictions\\twill\\tusually\\ngeneralize\\tbetter.\\tThis\\tis\\ta\\tregularization\\ttechnique\\t\\ncalled\\t\\nshrinkage\\n.\\t\\nFigure\\t7-10\\n\\tshows\\ttwo\\tGBRT\\nensembles\\ttrained\\twith\\ta\\tlow\\tlearning\\trate:\\tthe\\tone\\ton\\tthe\\tleft\\tdoes\\tnot\\thave\\tenough\\ttrees\\tto\\tfit\\tthe\\ntraining\\tset,\\twhile\\tthe\\tone\\ton\\tthe\\tright\\thas\\ttoo\\tmany\\ttrees\\tand\\toverfits\\tthe\\ttraining\\tset.', 'Figure\\t7-10.\\t\\nGBRT\\tensembles\\twith\\tnot\\tenough\\tpredictors\\t(left)\\tand\\ttoo\\tmany\\t(right)\\nIn\\torder\\tto\\tfind\\tthe\\toptimal\\tnumber\\tof\\ttrees,\\tyou\\tcan\\tuse\\t\\nearly\\tstopping\\t(see\\t\\nChapter\\t4\\n).\\tA\\tsimple\\tway\\tto\\nimplement\\tthis\\tis\\tto\\tuse\\tthe\\t\\nstaged_predict()\\n\\tmethod:\\tit\\treturns\\tan\\titerator\\tover\\tthe\\tpredictions\\tmade\\nby\\tthe\\tensemble\\tat\\teach\\tstage\\tof\\ttraining\\t(with\\tone\\ttree,\\ttwo\\ttrees,\\tetc.).\\tThe\\tfollowing\\tcode\\ttrains\\ta\\nGBRT\\tensemble\\twith\\t120\\ttrees,\\tthen\\tmeasures\\tthe\\tvalidation\\terror\\tat\\teach\\tstage\\tof\\ttraining\\tto\\tfind\\tthe\\noptimal\\tnumber\\tof\\ttrees,\\tand\\tfinally\\ttrains\\tanother\\tGBRT\\tensemble\\t\\nusing\\tthe\\toptimal\\tnumber\\tof\\ttrees:\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\ntrain_test_split\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nmean_squared_error\\nX_train\\n,\\n\\t\\nX_val\\n,\\n\\t\\ny_train\\n,\\n\\t\\ny_val\\n\\t\\n=\\n\\t\\ntrain_test_split\\n(\\nX\\n,\\n\\t\\ny\\n)\\ngbrt\\n\\t\\n=\\n\\t\\nGradientBoostingRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n\\t\\nn_estimators\\n=\\n120\\n)\\ngbrt\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\nerrors\\n\\t\\n=\\n\\t\\n[\\nmean_squared_error\\n(\\ny_val\\n,\\n\\t\\ny_pred\\n)', 'for\\n\\t\\ny_pred\\n\\t\\nin\\n\\t\\ngbrt\\n.\\nstaged_predict\\n(\\nX_val\\n)]\\nbst_n_estimators\\n\\t\\n=\\n\\t\\nnp\\n.\\nargmin\\n(\\nerrors\\n)\\ngbrt_best\\n\\t\\n=\\n\\t\\nGradientBoostingRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\nn_estimators\\n=\\nbst_n_estimators\\n)\\ngbrt_best\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\nThe\\tvalidation\\terrors\\tare\\trepresented\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t7-11\\n,\\tand\\tthe\\tbest\\tmodel’s\\tpredictions\\tare\\nrepresented\\ton\\tthe\\tright.\\nFigure\\t7-11.\\t\\nTuning\\tthe\\tnumber\\tof\\ttrees\\tusing\\tearly\\tstopping\\nIt\\tis\\talso\\tpossible\\tto\\timplement\\tearly\\tstopping\\tby\\tactually\\tstopping\\ttraining\\tearly\\t(instead\\tof\\ttraining\\ta\\nlarge\\tnumber\\tof\\ttrees\\tfirst\\tand\\tthen\\tlooking\\tback\\tto\\tfind\\tthe\\toptimal\\tnumber).\\tYou\\tcan\\tdo\\tso\\tby\\tsetting\\nwarm_start=True\\n,\\twhich\\tmakes\\tScikit-Learn\\tkeep\\texisting\\ttrees\\twhen\\tthe\\t\\nfit()\\n\\tmethod\\tis\\tcalled,\\nallowing\\tincremental\\ttraining.\\tThe\\t\\nfollowing\\tcode\\tstops\\ttraining\\twhen\\tthe\\tvalidation\\terror\\tdoes\\tnot', 'improve\\tfor\\tfive\\titerations\\tin\\ta\\trow:\\ngbrt\\n\\t\\n=\\n\\t\\nGradientBoostingRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n\\t\\nwarm_start\\n=\\nTrue\\n)\\nmin_val_error\\n\\t\\n=\\n\\t\\nfloat\\n(\\n\"inf\"\\n)\\nerror_going_up\\n\\t\\n=\\n\\t\\n0\\nfor\\n\\t\\nn_estimators\\n\\t\\nin\\n\\t\\nrange\\n(\\n1\\n,\\n\\t\\n120\\n):\\n\\t\\t\\t\\t\\ngbrt\\n.\\nn_estimators\\n\\t\\n=\\n\\t\\nn_estimators\\n\\t\\t\\t\\t\\ngbrt\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n)\\n\\t\\t\\t\\t\\ny_pred\\n\\t\\n=\\n\\t\\ngbrt\\n.\\npredict\\n(\\nX_val\\n)\\n\\t\\t\\t\\t\\nval_error\\n\\t\\n=\\n\\t\\nmean_squared_error\\n(\\ny_val\\n,\\n\\t\\ny_pred\\n)\\n\\t\\t\\t\\t\\nif\\n\\t\\nval_error\\n\\t\\n<\\n\\t\\nmin_val_error\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nmin_val_error\\n\\t\\n=\\n\\t\\nval_error\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nerror_going_up\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nerror_going_up\\n\\t\\n+=\\n\\t\\n1\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nerror_going_up\\n\\t\\n==\\n\\t\\n5\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbreak\\n\\t\\t\\n#\\tearly\\tstopping\\nThe\\t\\nGradientBoostingRegressor\\n\\tclass\\talso\\tsupports\\t\\na\\t\\nsubsample\\n\\thyperparameter,\\twhich\\tspecifies\\nthe\\tfraction\\tof\\ttraining\\tinstances\\tto\\tbe\\tused\\tfor\\ttraining\\teach\\ttree.\\tFor\\texample,\\tif\\t\\nsubsample=0.25\\n,\\tthen\\neach\\ttree\\tis\\ttrained\\ton\\t25%\\tof\\tthe\\ttraining\\tinstances,\\tselected\\trandomly.\\tAs\\tyou\\tcan\\tprobably\\tguess\\tby', 'now,\\tthis\\ttrades\\ta\\thigher\\tbias\\tfor\\ta\\tlower\\tvariance.\\tIt\\talso\\tspeeds\\tup\\ttraining\\tconsiderably.\\tThis\\t\\ntechnique\\nis\\tcalled\\t\\nStochastic\\tGradient\\tBoosting\\n.\\nNOTE\\nIt\\tis\\tpossible\\tto\\tuse\\tGradient\\tBoosting\\twith\\tother\\t\\ncost\\tfunctions.\\tThis\\tis\\tcontrolled\\tby\\tthe\\t\\nloss\\n\\thyperparameter\\t(see\\tScikit-\\nLearn’s\\tdocumentation\\tfor\\tmore\\t\\ndetails).', 'Stacking\\nThe\\t\\nlast\\tEnsemble\\tmethod\\twe\\twill\\tdiscuss\\tin\\tthis\\tchapter\\tis\\tcalled\\t\\nstacking\\n\\t\\n(short\\tfor\\t\\nstacked\\ngeneralization\\n).\\n18\\n\\tIt\\tis\\tbased\\ton\\ta\\tsimple\\tidea:\\tinstead\\tof\\tusing\\ttrivial\\tfunctions\\t(such\\tas\\thard\\tvoting)\\tto\\naggregate\\tthe\\tpredictions\\tof\\tall\\tpredictors\\tin\\tan\\tensemble,\\twhy\\tdon’t\\twe\\ttrain\\ta\\tmodel\\tto\\tperform\\tthis\\naggregation?\\t\\nFigure\\t7-12\\n\\t\\nshows\\tsuch\\tan\\tensemble\\tperforming\\ta\\tregression\\ttask\\ton\\ta\\tnew\\tinstance.\\tEach\\nof\\tthe\\tbottom\\tthree\\tpredictors\\tpredicts\\ta\\tdifferent\\tvalue\\t(3.1,\\t2.7,\\tand\\t2.9),\\tand\\tthen\\tthe\\tfinal\\tpredictor\\n(called\\ta\\t\\nblender\\n,\\tor\\ta\\t\\nmeta\\tlearner\\n)\\ttakes\\tthese\\tpredictions\\tas\\tinputs\\tand\\tmakes\\tthe\\tfinal\\tprediction\\n(3.0).\\nFigure\\t7-12.\\t\\nAggregating\\tpredictions\\tusing\\ta\\tblending\\tpredictor\\nTo\\ttrain\\tthe\\tblender,\\ta\\tcommon\\tapproach\\tis\\tto\\tuse\\ta\\t\\nhold-out\\tset.\\n19\\n\\tLet’s\\tsee\\thow\\tit\\tworks.\\tFirst,\\tthe\\ntraining\\tset\\tis\\tsplit\\tin\\ttwo\\tsubsets.\\tThe\\tfirst\\tsubset\\tis\\tused\\tto\\ttrain\\tthe\\tpredictors\\tin\\tthe\\tfirst\\tlayer\\t(see\\nFigure\\t7-13\\n).', 'Figure\\t7-13.\\t\\nTraining\\tthe\\tfirst\\tlayer\\nNext,\\tthe\\tfirst\\tlayer\\tpredictors\\tare\\tused\\tto\\tmake\\tpredictions\\ton\\tthe\\tsecond\\t(held-out)\\tset\\t(see\\t\\nFigure\\t7-\\n14\\n).\\tThis\\tensures\\tthat\\tthe\\tpredictions\\tare\\t“clean,”\\tsince\\tthe\\tpredictors\\tnever\\tsaw\\tthese\\tinstances\\tduring\\ntraining.\\tNow\\tfor\\teach\\tinstance\\tin\\tthe\\thold-out\\tset\\tthere\\tare\\tthree\\tpredicted\\tvalues.\\tWe\\tcan\\tcreate\\ta\\tnew\\ntraining\\tset\\tusing\\tthese\\tpredicted\\tvalues\\tas\\tinput\\tfeatures\\t(which\\tmakes\\tthis\\tnew\\ttraining\\tset\\tthree-\\ndimensional),\\tand\\tkeeping\\tthe\\ttarget\\tvalues.\\tThe\\tblender\\tis\\ttrained\\ton\\tthis\\tnew\\ttraining\\tset,\\tso\\tit\\tlearns\\tto\\npredict\\tthe\\ttarget\\tvalue\\tgiven\\tthe\\tfirst\\tlayer’s\\tpredictions.', 'Figure\\t7-14.\\t\\nTraining\\tthe\\tblender\\nIt\\tis\\tactually\\tpossible\\tto\\ttrain\\tseveral\\tdifferent\\tblenders\\tthis\\tway\\t(e.g.,\\tone\\tusing\\tLinear\\tRegression,\\nanother\\tusing\\tRandom\\tForest\\tRegression,\\tand\\tso\\ton):\\twe\\tget\\ta\\twhole\\tlayer\\tof\\tblenders.\\tThe\\ttrick\\tis\\tto\\nsplit\\tthe\\ttraining\\tset\\tinto\\tthree\\tsubsets:\\tthe\\tfirst\\tone\\tis\\tused\\tto\\ttrain\\tthe\\tfirst\\tlayer,\\tthe\\tsecond\\tone\\tis\\tused\\tto\\ncreate\\tthe\\ttraining\\tset\\tused\\tto\\ttrain\\tthe\\tsecond\\tlayer\\t(using\\tpredictions\\tmade\\tby\\tthe\\tpredictors\\tof\\tthe\\tfirst\\nlayer),\\tand\\tthe\\tthird\\tone\\tis\\tused\\tto\\tcreate\\tthe\\ttraining\\tset\\tto\\ttrain\\tthe\\tthird\\tlayer\\t(using\\tpredictions\\tmade\\tby\\nthe\\tpredictors\\tof\\tthe\\tsecond\\tlayer).\\tOnce\\tthis\\tis\\tdone,\\twe\\tcan\\tmake\\ta\\tprediction\\tfor\\ta\\tnew\\tinstance\\tby\\ngoing\\tthrough\\teach\\tlayer\\tsequentially,\\tas\\tshown\\tin\\t\\nFigure\\t7-15\\n.', 'Figure\\t7-15.\\t\\nPredictions\\tin\\ta\\tmultilayer\\tstacking\\tensemble\\nUnfortunately,\\tScikit-Learn\\tdoes\\tnot\\tsupport\\tstacking\\tdirectly,\\tbut\\tit\\tis\\tnot\\ttoo\\thard\\tto\\troll\\tout\\tyour\\town\\nimplementation\\t(see\\tthe\\tfollowing\\texercises).\\tAlternatively,\\tyou\\tcan\\tuse\\tan\\topen\\tsource\\timplementation\\nsuch\\t\\nas\\t\\nbrew\\n\\t(available\\tat\\t\\nhttps://github.com/viisar/brew\\n).', 'Exercises\\n1\\n.\\t\\nIf\\tyou\\thave\\ttrained\\tfive\\tdifferent\\tmodels\\ton\\tthe\\texact\\tsame\\ttraining\\tdata,\\tand\\tthey\\tall\\tachieve\\t95%\\nprecision,\\tis\\tthere\\tany\\tchance\\tthat\\tyou\\tcan\\tcombine\\tthese\\tmodels\\tto\\tget\\tbetter\\tresults?\\tIf\\tso,\\thow?\\tIf\\nnot,\\twhy?\\n2\\n.\\t\\nWhat\\tis\\tthe\\tdifference\\tbetween\\thard\\tand\\tsoft\\tvoting\\tclassifiers?\\n3\\n.\\t\\nIs\\tit\\tpossible\\tto\\tspeed\\tup\\ttraining\\tof\\ta\\tbagging\\tensemble\\tby\\tdistributing\\tit\\tacross\\tmultiple\\tservers?\\nWhat\\tabout\\tpasting\\tensembles,\\tboosting\\tensembles,\\trandom\\tforests,\\tor\\tstacking\\tensembles?\\n4\\n.\\t\\nWhat\\tis\\tthe\\tbenefit\\tof\\tout-of-bag\\tevaluation?\\n5\\n.\\t\\nWhat\\tmakes\\tExtra-Trees\\tmore\\trandom\\tthan\\tregular\\tRandom\\tForests?\\tHow\\tcan\\tthis\\textra\\trandomness\\nhelp?\\tAre\\tExtra-Trees\\tslower\\tor\\tfaster\\tthan\\tregular\\tRandom\\tForests?\\n6\\n.\\t\\nIf\\tyour\\tAdaBoost\\tensemble\\tunderfits\\tthe\\ttraining\\tdata,\\twhat\\thyperparameters\\tshould\\tyou\\ttweak\\tand\\nhow?\\n7\\n.\\t\\nIf\\tyour\\tGradient\\tBoosting\\tensemble\\toverfits\\tthe\\ttraining\\tset,\\tshould\\tyou\\tincrease\\tor\\tdecrease\\tthe\\nlearning\\trate?\\n8\\n.\\t\\nLoad\\tthe\\tMNIST\\tdata\\t(introduced\\tin\\t\\nChapter\\t3', 'Chapter\\t3\\n),\\tand\\tsplit\\tit\\tinto\\ta\\ttraining\\tset,\\ta\\tvalidation\\tset,\\tand\\ta\\ntest\\tset\\t(e.g.,\\tuse\\t40,000\\tinstances\\tfor\\ttraining,\\t10,000\\tfor\\tvalidation,\\tand\\t10,000\\tfor\\ttesting).\\tThen\\ntrain\\tvarious\\tclassifiers,\\tsuch\\tas\\ta\\tRandom\\tForest\\tclassifier,\\tan\\tExtra-Trees\\tclassifier,\\tand\\tan\\tSVM.\\nNext,\\ttry\\tto\\tcombine\\tthem\\tinto\\tan\\tensemble\\tthat\\toutperforms\\tthem\\tall\\ton\\tthe\\tvalidation\\tset,\\tusing\\ta\\nsoft\\tor\\thard\\tvoting\\tclassifier.\\tOnce\\tyou\\thave\\tfound\\tone,\\ttry\\tit\\ton\\tthe\\ttest\\tset.\\tHow\\tmuch\\tbetter\\tdoes\\tit\\nperform\\tcompared\\tto\\tthe\\tindividual\\tclassifiers?\\n9\\n.\\t\\nRun\\tthe\\tindividual\\tclassifiers\\tfrom\\tthe\\tprevious\\texercise\\tto\\tmake\\tpredictions\\ton\\tthe\\tvalidation\\tset,\\nand\\tcreate\\ta\\tnew\\ttraining\\tset\\twith\\tthe\\tresulting\\tpredictions:\\teach\\ttraining\\tinstance\\tis\\ta\\tvector\\ncontaining\\tthe\\tset\\tof\\tpredictions\\tfrom\\tall\\tyour\\tclassifiers\\tfor\\tan\\timage,\\tand\\tthe\\ttarget\\tis\\tthe\\timage’s\\nclass.\\tCongratulations,\\tyou\\thave\\tjust\\ttrained\\ta\\tblender,\\tand\\ttogether\\twith\\tthe\\tclassifiers\\tthey\\tform\\ta\\nstacking\\tensemble!', 'Now\\tlet’s\\tevaluate\\tthe\\tensemble\\ton\\tthe\\ttest\\tset.\\tFor\\teach\\timage\\tin\\tthe\\ttest\\tset,\\nmake\\tpredictions\\twith\\tall\\tyour\\tclassifiers,\\tthen\\tfeed\\tthe\\tpredictions\\tto\\tthe\\tblender\\tto\\tget\\tthe\\nensemble’s\\tpredictions.\\tHow\\tdoes\\tit\\tcompare\\tto\\tthe\\tvoting\\tclassifier\\tyou\\ttrained\\t\\nearlier?\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\n“Bagging\\tPredictors,”\\tL.\\tBreiman\\t(1996).\\nIn\\tstatistics,\\tresampling\\twith\\treplacement\\tis\\tcalled\\t\\nbootstrapping\\n.\\n“Pasting\\tsmall\\tvotes\\tfor\\tclassification\\tin\\tlarge\\tdatabases\\tand\\ton-line,”\\tL.\\tBreiman\\t(1999).\\nBias\\tand\\tvariance\\twere\\tintroduced\\tin\\t\\nChapter\\t4\\n.\\nmax_samples\\n\\tcan\\talternatively\\tbe\\tset\\tto\\ta\\tfloat\\tbetween\\t0.0\\tand\\t1.0,\\tin\\twhich\\tcase\\tthe\\tmax\\tnumber\\tof\\tinstances\\tto\\tsample\\tis\\tequal\\tto\\tthe\\nsize\\tof\\tthe\\ttraining\\tset\\ttimes\\t\\nmax_samples\\n.\\n1\\n2\\n3\\n4\\n5\\n6', 'As\\t\\nm\\n\\tgrows,\\tthis\\tratio\\tapproaches\\t1\\t–\\texp(–1)\\t≈\\t63.212%.\\n“Ensembles\\ton\\tRandom\\tPatches,”\\tG.\\tLouppe\\tand\\tP.\\tGeurts\\t(2012).\\n“The\\trandom\\tsubspace\\tmethod\\tfor\\tconstructing\\tdecision\\tforests,”\\tTin\\tKam\\tHo\\t(1998).\\n“Random\\tDecision\\tForests,”\\tT.\\tHo\\t(1995).\\nThe\\t\\nBaggingClassifier\\n\\tclass\\tremains\\tuseful\\tif\\tyou\\twant\\ta\\tbag\\tof\\tsomething\\tother\\tthan\\tDecision\\tTrees.\\nThere\\tare\\ta\\tfew\\tnotable\\texceptions:\\t\\nsplitter\\n\\tis\\tabsent\\t(forced\\tto\\t\\n\"random\"\\n),\\t\\npresort\\n\\tis\\tabsent\\t(forced\\tto\\t\\nFalse\\n),\\t\\nmax_samples\\n\\tis\\tabsent\\n(forced\\tto\\t\\n1.0\\n),\\t\\nand\\t\\nbase_estimator\\n\\tis\\tabsent\\t(forced\\tto\\t\\nDecisionTreeClassifier\\n\\twith\\tthe\\tprovided\\thyperparameters).\\n“Extremely\\trandomized\\ttrees,”\\tP.\\tGeurts,\\tD.\\tErnst,\\tL.\\tWehenkel\\t(2005).\\n“A\\tDecision-Theoretic\\tGeneralization\\tof\\tOn-Line\\tLearning\\tand\\tan\\tApplication\\tto\\tBoosting,”\\tYoav\\tFreund,\\tRobert\\tE.\\tSchapire\\t(1997).\\nThis\\tis\\tjust\\tfor\\tillustrative\\tpurposes.\\tSVMs\\tare\\tgenerally\\tnot\\tgood\\tbase\\tpredictors\\tfor\\tAdaBoost,\\tbecause\\tthey\\tare\\tslow\\tand\\ttend\\tto\\tbe\\nunstable\\twith\\tAdaBoost.', 'The\\toriginal\\tAdaBoost\\talgorithm\\tdoes\\tnot\\tuse\\ta\\tlearning\\trate\\thyperparameter.\\nFor\\tmore\\tdetails,\\tsee\\t“Multi-Class\\tAdaBoost,”\\tJ.\\tZhu\\tet\\tal.\\t(2006).\\nFirst\\tintroduced\\tin\\t“Arcing\\tthe\\tEdge,”\\tL.\\tBreiman\\t(1997).\\n“Stacked\\tGeneralization,”\\tD.\\tWolpert\\t(1992).\\nAlternatively,\\tit\\tis\\tpossible\\tto\\tuse\\tout-of-fold\\tpredictions.\\tIn\\tsome\\tcontexts\\tthis\\tis\\tcalled\\t\\nstacking\\n,\\twhile\\tusing\\ta\\thold-out\\tset\\tis\\tcalled\\nblending\\n.\\tHowever,\\tfor\\tmany\\tpeople\\tthese\\tterms\\tare\\tsynonymous.\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19', 'Chapter\\t8.\\t\\nDimensionality\\tReduction\\nMany\\t\\nMachine\\tLearning\\tproblems\\tinvolve\\tthousands\\tor\\teven\\tmillions\\tof\\tfeatures\\tfor\\teach\\ttraining\\ninstance.\\tNot\\tonly\\tdoes\\tthis\\tmake\\ttraining\\textremely\\tslow,\\tit\\tcan\\talso\\tmake\\tit\\tmuch\\tharder\\tto\\tfind\\ta\\tgood\\nsolution,\\tas\\twe\\twill\\tsee.\\tThis\\tproblem\\tis\\toften\\treferred\\tto\\tas\\t\\nthe\\t\\ncurse\\tof\\tdimensionality\\n.\\nFortunately,\\tin\\treal-world\\tproblems,\\tit\\tis\\toften\\tpossible\\tto\\treduce\\tthe\\tnumber\\tof\\tfeatures\\tconsiderably,\\nturning\\tan\\tintractable\\tproblem\\tinto\\ta\\ttractable\\tone.\\tFor\\texample,\\tconsider\\tthe\\tMNIST\\timages\\t(introduced\\nin\\t\\nChapter\\t3\\n):\\tthe\\tpixels\\ton\\tthe\\timage\\tborders\\tare\\talmost\\talways\\twhite,\\tso\\tyou\\tcould\\tcompletely\\tdrop\\nthese\\tpixels\\tfrom\\tthe\\ttraining\\tset\\twithout\\tlosing\\tmuch\\tinformation.\\t\\nFigure\\t7-6\\n\\tconfirms\\tthat\\tthese\\tpixels\\nare\\tutterly\\tunimportant\\tfor\\tthe\\tclassification\\ttask.\\tMoreover,\\ttwo\\tneighboring\\tpixels\\tare\\toften\\thighly\\ncorrelated:\\tif\\tyou\\tmerge\\tthem\\tinto\\ta\\tsingle\\tpixel\\t(e.g.,\\tby\\ttaking\\tthe\\tmean\\tof\\tthe\\ttwo\\tpixel\\tintensities),\\tyou\\nwill\\tnot\\tlose\\tmuch\\tinformation.', 'WARNING\\nReducing\\tdimensionality\\tdoes\\tlose\\tsome\\tinformation\\t(just\\tlike\\tcompressing\\tan\\timage\\tto\\tJPEG\\tcan\\tdegrade\\tits\\tquality),\\tso\\teven\\nthough\\tit\\twill\\tspeed\\tup\\ttraining,\\tit\\tmay\\talso\\tmake\\tyour\\tsystem\\tperform\\tslightly\\tworse.\\tIt\\talso\\tmakes\\tyour\\tpipelines\\ta\\tbit\\tmore\\ncomplex\\tand\\tthus\\tharder\\tto\\tmaintain.\\tSo\\tyou\\tshould\\tfirst\\ttry\\tto\\ttrain\\tyour\\tsystem\\twith\\tthe\\toriginal\\tdata\\tbefore\\tconsidering\\tusing\\ndimensionality\\treduction\\tif\\ttraining\\tis\\ttoo\\tslow.\\tIn\\tsome\\tcases,\\thowever,\\treducing\\tthe\\tdimensionality\\tof\\tthe\\ttraining\\tdata\\tmay\\tfilter\\nout\\tsome\\tnoise\\tand\\tunnecessary\\tdetails\\tand\\tthus\\tresult\\tin\\thigher\\tperformance\\t(but\\tin\\tgeneral\\tit\\twon’t;\\tit\\twill\\tjust\\tspeed\\tup\\ntraining).\\nApart\\tfrom\\tspeeding\\tup\\ttraining,\\t\\ndimensionality\\treduction\\tis\\talso\\textremely\\tuseful\\tfor\\tdata\\tvisualization\\n(or\\t\\nDataViz\\n).\\tReducing\\tthe\\tnumber\\tof\\tdimensions\\tdown\\tto\\ttwo\\t(or\\tthree)\\tmakes\\tit\\tpossible\\tto\\tplot\\ta\\thigh-\\ndimensional\\ttraining\\tset\\ton\\ta\\tgraph\\tand\\toften\\tgain\\tsome\\timportant\\tinsights\\tby\\tvisually\\tdetecting\\tpatterns,\\nsuch\\tas\\tclusters.', 'In\\tthis\\tchapter\\twe\\twill\\tdiscuss\\tthe\\tcurse\\tof\\tdimensionality\\tand\\tget\\ta\\tsense\\tof\\twhat\\tgoes\\ton\\tin\\thigh-\\ndimensional\\tspace.\\tThen,\\twe\\twill\\tpresent\\tthe\\ttwo\\tmain\\tapproaches\\tto\\tdimensionality\\treduction\\n(projection\\tand\\tManifold\\tLearning),\\tand\\twe\\twill\\tgo\\tthrough\\tthree\\tof\\tthe\\tmost\\tpopular\\tdimensionality\\nreduction\\ttechniques:\\tPCA,\\tKernel\\tPCA,\\tand\\tLLE.', 'The\\tCurse\\tof\\tDimensionality\\nWe\\tare\\tso\\tused\\tto\\tliving\\tin\\tthree\\tdimensions\\n1\\n\\tthat\\tour\\tintuition\\tfails\\tus\\twhen\\twe\\ttry\\tto\\timagine\\ta\\thigh-\\ndimensional\\tspace.\\tEven\\ta\\tbasic\\t4D\\thypercube\\tis\\tincredibly\\thard\\tto\\tpicture\\tin\\tour\\tmind\\t(see\\t\\nFigure\\t8-1\\n),\\nlet\\talone\\ta\\t200-dimensional\\tellipsoid\\tbent\\tin\\ta\\t1,000-dimensional\\tspace.\\nFigure\\t8-1.\\t\\nPoint,\\tsegment,\\tsquare,\\tcube,\\tand\\ttesseract\\t(0D\\tto\\t4D\\thypercubes)\\n2\\nIt\\tturns\\tout\\tthat\\tmany\\tthings\\tbehave\\tvery\\tdifferently\\tin\\thigh-dimensional\\tspace.\\tFor\\texample,\\tif\\tyou\\tpick\\ta\\nrandom\\tpoint\\tin\\ta\\tunit\\tsquare\\t(a\\t1\\t×\\t1\\tsquare),\\tit\\twill\\thave\\tonly\\tabout\\ta\\t0.4%\\tchance\\tof\\tbeing\\tlocated\\tless\\nthan\\t0.001\\tfrom\\ta\\tborder\\t(in\\tother\\twords,\\tit\\tis\\tvery\\tunlikely\\tthat\\ta\\trandom\\tpoint\\twill\\tbe\\t“extreme”\\talong\\nany\\tdimension).\\tBut\\tin\\ta\\t10,000-dimensional\\tunit\\thypercube\\t(a\\t1\\t×\\t1\\t×\\t\\t×\\t1\\tcube,\\twith\\tten\\tthousand\\t1s),\\nthis\\tprobability\\tis\\tgreater\\tthan\\t99.999999%.\\tMost\\tpoints\\tin\\ta\\thigh-dimensional\\thypercube\\tare\\tvery\\tclose\\nto\\tthe\\tborder.\\n3', '3\\nHere\\tis\\ta\\tmore\\ttroublesome\\tdifference:\\tif\\tyou\\tpick\\ttwo\\tpoints\\trandomly\\tin\\ta\\tunit\\tsquare,\\tthe\\tdistance\\nbetween\\tthese\\ttwo\\tpoints\\twill\\tbe,\\ton\\taverage,\\troughly\\t0.52.\\tIf\\tyou\\tpick\\ttwo\\trandom\\tpoints\\tin\\ta\\tunit\\t3D\\ncube,\\tthe\\taverage\\tdistance\\twill\\tbe\\troughly\\t0.66.\\tBut\\twhat\\tabout\\ttwo\\tpoints\\tpicked\\trandomly\\tin\\ta\\n1,000,000-dimensional\\thypercube?\\tWell,\\tthe\\taverage\\tdistance,\\tbelieve\\tit\\tor\\tnot,\\twill\\tbe\\tabout\\t408.25\\n(roughly\\t\\n)!\\tThis\\tis\\tquite\\tcounterintuitive:\\thow\\tcan\\ttwo\\tpoints\\tbe\\tso\\tfar\\tapart\\twhen\\nthey\\tboth\\tlie\\twithin\\tthe\\tsame\\tunit\\thypercube?\\tThis\\tfact\\timplies\\tthat\\thigh-dimensional\\tdatasets\\tare\\tat\\trisk\\nof\\tbeing\\tvery\\tsparse:\\tmost\\ttraining\\tinstances\\tare\\tlikely\\tto\\tbe\\tfar\\taway\\tfrom\\teach\\tother.\\tOf\\tcourse,\\tthis\\nalso\\tmeans\\tthat\\ta\\tnew\\tinstance\\twill\\tlikely\\tbe\\tfar\\taway\\tfrom\\tany\\ttraining\\tinstance,\\tmaking\\tpredictions\\nmuch\\tless\\treliable\\tthan\\tin\\tlower\\tdimensions,\\tsince\\tthey\\twill\\tbe\\tbased\\ton\\tmuch\\tlarger\\textrapolations.\\tIn\\nshort,\\tthe\\tmore\\tdimensions\\tthe\\ttraining\\tset\\thas,\\tthe\\tgreater\\tthe\\trisk\\tof\\toverfitting\\tit.', 'In\\ttheory,\\tone\\tsolution\\tto\\tthe\\tcurse\\tof\\tdimensionality\\tcould\\tbe\\tto\\tincrease\\tthe\\tsize\\tof\\tthe\\ttraining\\tset\\tto\\nreach\\ta\\tsufficient\\tdensity\\tof\\ttraining\\tinstances.\\tUnfortunately,\\tin\\tpractice,\\tthe\\tnumber\\tof\\ttraining\\tinstances\\nrequired\\tto\\treach\\ta\\tgiven\\tdensity\\tgrows\\texponentially\\twith\\tthe\\tnumber\\tof\\tdimensions.\\tWith\\tjust\\t100\\nfeatures\\t(much\\tless\\tthan\\tin\\tthe\\tMNIST\\tproblem),\\tyou\\twould\\tneed\\tmore\\ttraining\\tinstances\\tthan\\tatoms\\tin\\nthe\\tobservable\\tuniverse\\tin\\torder\\tfor\\ttraining\\tinstances\\tto\\tbe\\twithin\\t0.1\\tof\\teach\\tother\\ton\\taverage,\\tassuming\\nthey\\twere\\tspread\\tout\\tuniformly\\t\\nacross\\tall\\tdimensions.', 'Main\\tApproaches\\tfor\\tDimensionality\\tReduction\\nBefore\\twe\\tdive\\tinto\\tspecific\\tdimensionality\\treduction\\talgorithms,\\tlet’s\\ttake\\ta\\tlook\\tat\\tthe\\ttwo\\tmain\\napproaches\\tto\\treducing\\tdimensionality:\\tprojection\\tand\\tManifold\\tLearning.', 'Projection\\nIn\\tmost\\t\\nreal-world\\tproblems,\\ttraining\\tinstances\\tare\\t\\nnot\\n\\tspread\\tout\\tuniformly\\tacross\\tall\\tdimensions.\\tMany\\nfeatures\\tare\\talmost\\tconstant,\\twhile\\tothers\\tare\\thighly\\tcorrelated\\t(as\\tdiscussed\\tearlier\\tfor\\tMNIST).\\tAs\\ta\\nresult,\\tall\\ttraining\\tinstances\\tactually\\tlie\\twithin\\t(or\\tclose\\tto)\\ta\\tmuch\\tlower-dimensional\\t\\nsubspace\\n\\tof\\tthe\\nhigh-dimensional\\tspace.\\tThis\\tsounds\\tvery\\tabstract,\\tso\\tlet’s\\tlook\\tat\\tan\\texample.\\tIn\\t\\nFigure\\t8-2\\n\\tyou\\tcan\\tsee\\na\\t3D\\tdataset\\trepresented\\tby\\tthe\\tcircles.\\nFigure\\t8-2.\\t\\nA\\t3D\\tdataset\\tlying\\tclose\\tto\\ta\\t2D\\tsubspace\\nNotice\\tthat\\tall\\ttraining\\tinstances\\tlie\\tclose\\tto\\ta\\tplane:\\tthis\\tis\\ta\\tlower-dimensional\\t(2D)\\tsubspace\\tof\\tthe\\nhigh-dimensional\\t(3D)\\tspace.\\tNow\\tif\\twe\\tproject\\tevery\\ttraining\\tinstance\\tperpendicularly\\tonto\\tthis\\nsubspace\\t(as\\trepresented\\tby\\tthe\\tshort\\tlines\\tconnecting\\tthe\\tinstances\\tto\\tthe\\tplane),\\twe\\tget\\tthe\\tnew\\t2D\\ndataset\\tshown\\tin\\t\\nFigure\\t8-3\\n.\\tTa-da!\\tWe\\thave\\tjust\\treduced\\tthe\\tdataset’s\\tdimensionality\\tfrom\\t3D\\tto\\t2D.\\nNote\\tthat\\tthe\\taxes\\tcorrespond\\tto\\tnew\\tfeatures\\t\\nz\\n1\\n\\tand', 'z\\n1\\n\\tand\\t\\nz\\n2\\n\\t(the\\tcoordinates\\tof\\tthe\\tprojections\\ton\\tthe\\tplane).', 'Figure\\t8-3.\\t\\nThe\\tnew\\t2D\\tdataset\\tafter\\tprojection\\nHowever,\\tprojection\\tis\\tnot\\talways\\tthe\\tbest\\tapproach\\tto\\tdimensionality\\treduction.\\tIn\\tmany\\tcases\\tthe\\nsubspace\\tmay\\ttwist\\tand\\tturn,\\tsuch\\tas\\tin\\tthe\\tfamous\\t\\nSwiss\\troll\\n\\ttoy\\tdataset\\trepresented\\tin\\t\\nFigure\\t8-4\\n.', 'Figure\\t8-4.\\t\\nSwiss\\troll\\tdataset\\nSimply\\tprojecting\\tonto\\ta\\tplane\\t(e.g.,\\tby\\tdropping\\t\\nx\\n3\\n)\\twould\\tsquash\\tdifferent\\tlayers\\tof\\tthe\\tSwiss\\troll\\ntogether,\\tas\\tshown\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t8-5\\n.\\tHowever,\\twhat\\tyou\\treally\\twant\\tis\\tto\\tunroll\\tthe\\tSwiss\\troll\\tto\\nobtain\\tthe\\t2D\\tdataset\\ton\\tthe\\t\\nright\\tof\\t\\nFigure\\t8-5\\n.\\nFigure\\t8-5.\\t\\nSquashing\\tby\\tprojecting\\tonto\\ta\\tplane\\t(left)\\tversus\\tunrolling\\tthe\\tSwiss\\troll\\t(right)', 'Manifold\\tLearning\\nThe\\t\\nSwiss\\troll\\tis\\tan\\texample\\tof\\ta\\t2D\\t\\nmanifold\\n.\\tPut\\tsimply,\\ta\\t2D\\tmanifold\\tis\\ta\\t2D\\tshape\\tthat\\tcan\\tbe\\tbent\\nand\\ttwisted\\tin\\ta\\thigher-dimensional\\tspace.\\tMore\\tgenerally,\\ta\\t\\nd\\n-dimensional\\tmanifold\\tis\\ta\\tpart\\tof\\tan\\t\\nn\\n-\\ndimensional\\tspace\\t(where\\t\\nd\\n\\t<\\t\\nn\\n)\\tthat\\tlocally\\tresembles\\ta\\t\\nd\\n-dimensional\\t\\nhyperplane.\\tIn\\tthe\\tcase\\tof\\tthe\\nSwiss\\troll,\\t\\nd\\n\\t=\\t2\\tand\\t\\nn\\n\\t=\\t3:\\tit\\tlocally\\tresembles\\ta\\t2D\\tplane,\\tbut\\tit\\tis\\trolled\\tin\\tthe\\tthird\\tdimension.\\nMany\\tdimensionality\\treduction\\talgorithms\\twork\\tby\\tmodeling\\tthe\\t\\nmanifold\\n\\ton\\twhich\\tthe\\ttraining\\tinstances\\nlie;\\tthis\\tis\\tcalled\\t\\nManifold\\tLearning\\n.\\tIt\\trelies\\ton\\tthe\\t\\nmanifold\\tassumption\\n,\\talso\\tcalled\\tthe\\t\\nmanifold\\nhypothesis\\n,\\twhich\\t\\nholds\\tthat\\tmost\\treal-world\\thigh-dimensional\\tdatasets\\tlie\\tclose\\tto\\ta\\tmuch\\tlower-\\ndimensional\\tmanifold.\\tThis\\tassumption\\tis\\tvery\\toften\\tempirically\\tobserved.\\nOnce\\tagain,\\tthink\\tabout\\tthe\\tMNIST\\tdataset:\\tall\\thandwritten\\tdigit\\timages\\thave\\tsome\\tsimilarities.\\tThey\\tare', 'made\\tof\\tconnected\\tlines,\\tthe\\tborders\\tare\\twhite,\\tthey\\tare\\tmore\\tor\\tless\\tcentered,\\tand\\tso\\ton.\\tIf\\tyou\\trandomly\\ngenerated\\timages,\\tonly\\ta\\tridiculously\\ttiny\\tfraction\\tof\\tthem\\twould\\tlook\\tlike\\thandwritten\\tdigits.\\tIn\\tother\\nwords,\\tthe\\tdegrees\\tof\\tfreedom\\tavailable\\tto\\tyou\\tif\\tyou\\ttry\\tto\\tcreate\\ta\\tdigit\\timage\\tare\\tdramatically\\tlower\\nthan\\tthe\\tdegrees\\tof\\tfreedom\\tyou\\twould\\thave\\tif\\tyou\\twere\\tallowed\\tto\\tgenerate\\tany\\timage\\tyou\\twanted.\\nThese\\tconstraints\\ttend\\tto\\tsqueeze\\tthe\\tdataset\\tinto\\ta\\tlower-dimensional\\tmanifold.\\nThe\\tmanifold\\tassumption\\tis\\toften\\taccompanied\\tby\\tanother\\timplicit\\tassumption:\\tthat\\tthe\\ttask\\tat\\thand\\t(e.g.,\\nclassification\\tor\\tregression)\\twill\\tbe\\tsimpler\\tif\\texpressed\\tin\\tthe\\tlower-dimensional\\tspace\\tof\\tthe\\tmanifold.\\nFor\\texample,\\tin\\tthe\\ttop\\trow\\tof\\t\\nFigure\\t8-6\\n\\tthe\\tSwiss\\troll\\tis\\tsplit\\tinto\\ttwo\\tclasses:\\tin\\tthe\\t3D\\tspace\\t(on\\tthe\\nleft),\\tthe\\tdecision\\tboundary\\twould\\tbe\\tfairly\\tcomplex,\\tbut\\tin\\tthe\\t2D\\tunrolled\\tmanifold\\tspace\\t(on\\tthe\\tright),\\nthe\\tdecision\\tboundary\\tis\\ta\\tsimple\\tstraight\\tline.', 'However,\\tthis\\tassumption\\tdoes\\tnot\\talways\\thold.\\tFor\\texample,\\tin\\tthe\\tbottom\\trow\\tof\\t\\nFigure\\t8-6\\n,\\tthe\\ndecision\\tboundary\\tis\\tlocated\\tat\\t\\nx\\n1\\n\\t=\\t5.\\tThis\\tdecision\\tboundary\\tlooks\\tvery\\tsimple\\tin\\tthe\\toriginal\\t3D\\tspace\\n(a\\tvertical\\tplane),\\tbut\\tit\\tlooks\\tmore\\tcomplex\\tin\\tthe\\tunrolled\\tmanifold\\t(a\\tcollection\\tof\\tfour\\tindependent\\nline\\tsegments).\\nIn\\tshort,\\tif\\tyou\\treduce\\tthe\\tdimensionality\\tof\\tyour\\ttraining\\tset\\tbefore\\ttraining\\ta\\tmodel,\\tit\\twill\\tdefinitely\\nspeed\\tup\\ttraining,\\tbut\\tit\\tmay\\tnot\\talways\\tlead\\tto\\ta\\tbetter\\tor\\tsimpler\\tsolution;\\tit\\tall\\tdepends\\ton\\tthe\\tdataset.\\nHopefully\\tyou\\tnow\\thave\\ta\\tgood\\tsense\\tof\\twhat\\tthe\\tcurse\\tof\\tdimensionality\\tis\\tand\\thow\\tdimensionality\\nreduction\\talgorithms\\tcan\\tfight\\tit,\\tespecially\\twhen\\tthe\\tmanifold\\tassumption\\tholds.\\tThe\\trest\\tof\\tthis\\tchapter\\nwill\\tgo\\tthrough\\tsome\\tof\\tthe\\tmost\\tpopular\\talgorithms.', 'Figure\\t8-6.\\t\\nThe\\tdecision\\tboundary\\tmay\\tnot\\talways\\tbe\\tsimpler\\twith\\tlower\\tdimensions', 'PCA\\nPrincipal\\tComponent\\tAnalysis\\n\\t(PCA)\\t\\nis\\tby\\tfar\\tthe\\tmost\\tpopular\\tdimensionality\\treduction\\talgorithm.\\nFirst\\tit\\tidentifies\\tthe\\t\\nhyperplane\\tthat\\tlies\\tclosest\\tto\\tthe\\tdata,\\tand\\tthen\\tit\\tprojects\\tthe\\tdata\\tonto\\tit.', 'Preserving\\tthe\\tVariance\\nBefore\\t\\nyou\\tcan\\tproject\\tthe\\ttraining\\tset\\tonto\\ta\\tlower-dimensional\\thyperplane,\\tyou\\tfirst\\tneed\\tto\\tchoose\\tthe\\nright\\thyperplane.\\tFor\\texample,\\ta\\tsimple\\t2D\\tdataset\\tis\\trepresented\\ton\\tthe\\tleft\\tof\\t\\nFigure\\t8-7\\n,\\talong\\twith\\nthree\\tdifferent\\taxes\\t(i.e.,\\tone-dimensional\\thyperplanes).\\tOn\\tthe\\tright\\tis\\tthe\\tresult\\tof\\tthe\\tprojection\\tof\\tthe\\ndataset\\tonto\\teach\\tof\\tthese\\taxes.\\tAs\\tyou\\tcan\\tsee,\\tthe\\tprojection\\tonto\\tthe\\tsolid\\tline\\tpreserves\\tthe\\tmaximum\\nvariance,\\twhile\\tthe\\tprojection\\tonto\\tthe\\tdotted\\tline\\tpreserves\\tvery\\tlittle\\tvariance,\\tand\\tthe\\tprojection\\tonto\\nthe\\tdashed\\tline\\tpreserves\\tan\\tintermediate\\tamount\\tof\\tvariance.\\nFigure\\t8-7.\\t\\nSelecting\\tthe\\tsubspace\\tonto\\twhich\\tto\\tproject\\nIt\\tseems\\treasonable\\tto\\tselect\\tthe\\taxis\\tthat\\tpreserves\\tthe\\tmaximum\\tamount\\tof\\tvariance,\\tas\\tit\\twill\\tmost\\nlikely\\tlose\\tless\\tinformation\\tthan\\tthe\\tother\\tprojections.\\tAnother\\tway\\tto\\tjustify\\tthis\\tchoice\\tis\\tthat\\tit\\tis\\tthe\\naxis\\tthat\\tminimizes\\tthe\\tmean\\tsquared\\tdistance\\tbetween\\tthe\\toriginal\\tdataset\\tand\\tits\\tprojection\\tonto\\tthat', 'axis.\\tThis\\tis\\tthe\\trather\\tsimple\\tidea\\t\\nbehind\\t\\nPCA\\n.\\n4', 'Principal\\tComponents\\nPCA\\tidentifies\\t\\nthe\\taxis\\tthat\\taccounts\\tfor\\tthe\\tlargest\\tamount\\tof\\tvariance\\tin\\tthe\\ttraining\\tset.\\tIn\\t\\nFigure\\t8-7\\n,\\tit\\nis\\tthe\\tsolid\\tline.\\tIt\\talso\\tfinds\\ta\\tsecond\\taxis,\\torthogonal\\tto\\tthe\\tfirst\\tone,\\tthat\\taccounts\\tfor\\tthe\\tlargest\\tamount\\nof\\tremaining\\tvariance.\\tIn\\tthis\\t2D\\texample\\tthere\\tis\\tno\\tchoice:\\tit\\tis\\tthe\\tdotted\\tline.\\tIf\\tit\\twere\\ta\\thigher-\\ndimensional\\tdataset,\\tPCA\\twould\\talso\\tfind\\ta\\tthird\\taxis,\\torthogonal\\tto\\tboth\\tprevious\\taxes,\\tand\\ta\\tfourth,\\ta\\nfifth,\\tand\\tso\\ton\\t—\\tas\\tmany\\taxes\\tas\\tthe\\tnumber\\tof\\tdimensions\\tin\\tthe\\tdataset.\\nThe\\tunit\\tvector\\tthat\\tdefines\\tthe\\ti\\nth\\n\\taxis\\tis\\tcalled\\t\\nthe\\ti\\nth\\n\\t\\nprincipal\\tcomponent\\n\\t(PC).\\tIn\\t\\nFigure\\t8-7\\n,\\tthe\\t1\\nst\\nPC\\tis\\t\\nc\\n1\\n\\tand\\tthe\\t2\\nnd\\n\\tPC\\tis\\t\\nc\\n2\\n.\\tIn\\t\\nFigure\\t8-2\\n\\tthe\\tfirst\\ttwo\\tPCs\\tare\\trepresented\\tby\\tthe\\torthogonal\\tarrows\\tin\\nthe\\tplane,\\tand\\tthe\\tthird\\tPC\\twould\\tbe\\torthogonal\\tto\\tthe\\tplane\\t(pointing\\tup\\tor\\tdown).\\nNOTE\\nThe\\tdirection\\tof\\tthe\\tprincipal\\tcomponents\\tis\\tnot\\tstable:\\tif\\tyou\\tperturb\\tthe\\ttraining\\tset\\tslightly\\tand\\trun\\tPCA\\tagain,\\tsome\\tof\\tthe\\tnew', 'PCs\\tmay\\tpoint\\tin\\tthe\\topposite\\tdirection\\tof\\tthe\\toriginal\\tPCs.\\tHowever,\\tthey\\twill\\tgenerally\\tstill\\tlie\\ton\\tthe\\tsame\\taxes.\\tIn\\tsome\\tcases,\\na\\tpair\\tof\\tPCs\\tmay\\teven\\trotate\\tor\\tswap,\\tbut\\tthe\\tplane\\tthey\\tdefine\\twill\\tgenerally\\tremain\\tthe\\tsame.\\nSo\\thow\\tcan\\tyou\\tfind\\tthe\\tprincipal\\tcomponents\\tof\\ta\\ttraining\\tset?\\tLuckily,\\tthere\\tis\\ta\\tstandard\\tmatrix\\nfactorization\\ttechnique\\t\\ncalled\\t\\nSingular\\tValue\\tDecomposition\\n\\t(SVD)\\tthat\\tcan\\tdecompose\\tthe\\ttraining\\tset\\nmatrix\\t\\nX\\n\\tinto\\tthe\\tdot\\tproduct\\tof\\tthree\\tmatrices\\t\\nU\\n\\t·\\tΣ\\t·\\t\\nV\\nT\\n,\\twhere\\t\\nV\\nT\\n\\tcontains\\tall\\tthe\\tprincipal\\tcomponents\\nthat\\twe\\tare\\tlooking\\tfor,\\tas\\tshown\\tin\\t\\nEquation\\t8-1\\n.\\nEquation\\t8-1.\\t\\nPrincipal\\tcomponents\\tmatrix\\nThe\\tfollowing\\tPython\\tcode\\tuses\\tNumPy’s\\t\\nsvd()\\n\\tfunction\\t\\nto\\tobtain\\tall\\tthe\\tprincipal\\tcomponents\\tof\\tthe\\ntraining\\tset,\\tthen\\textracts\\tthe\\tfirst\\ttwo\\tPCs:\\nX_centered\\n\\t\\n=\\n\\t\\nX\\n\\t\\n-\\n\\t\\nX\\n.\\nmean\\n(\\naxis\\n=\\n0\\n)\\nU\\n,\\n\\t\\ns\\n,\\n\\t\\nV\\n\\t\\n=\\n\\t\\nnp\\n.\\nlinalg\\n.\\nsvd\\n(\\nX_centered\\n)\\nc1\\n\\t\\n=\\n\\t\\nV\\n.\\nT\\n[:,\\n\\t\\n0\\n]\\nc2\\n\\t\\n=\\n\\t\\nV\\n.\\nT\\n[:,\\n\\t\\n1\\n]', 'WARNING\\nPCA\\tassumes\\tthat\\tthe\\tdataset\\tis\\tcentered\\taround\\tthe\\torigin.\\tAs\\twe\\twill\\tsee,\\tScikit-Learn’s\\tPCA\\tclasses\\ttake\\tcare\\tof\\tcentering\\nthe\\tdata\\tfor\\tyou.\\tHowever,\\tif\\tyou\\timplement\\tPCA\\tyourself\\t(as\\tin\\tthe\\tpreceding\\texample),\\tor\\tif\\tyou\\tuse\\tother\\tlibraries,\\tdon’t\\nforget\\tto\\tcenter\\tthe\\t\\ndata\\tfirst.', 'Projecting\\tDown\\tto\\td\\tDimensions\\nOnce\\t\\nyou\\thave\\tidentified\\tall\\tthe\\tprincipal\\tcomponents,\\tyou\\tcan\\treduce\\tthe\\tdimensionality\\tof\\tthe\\tdataset\\ndown\\tto\\t\\nd\\n\\tdimensions\\tby\\tprojecting\\tit\\tonto\\tthe\\t\\nhyperplane\\tdefined\\tby\\tthe\\tfirst\\t\\nd\\n\\tprincipal\\tcomponents.\\nSelecting\\tthis\\thyperplane\\tensures\\tthat\\tthe\\tprojection\\twill\\tpreserve\\tas\\tmuch\\tvariance\\tas\\tpossible.\\tFor\\nexample,\\tin\\t\\nFigure\\t8-2\\n\\tthe\\t3D\\tdataset\\tis\\tprojected\\tdown\\tto\\tthe\\t2D\\tplane\\tdefined\\tby\\tthe\\tfirst\\ttwo\\tprincipal\\ncomponents,\\tpreserving\\ta\\tlarge\\tpart\\tof\\tthe\\tdataset’s\\tvariance.\\tAs\\ta\\tresult,\\tthe\\t2D\\tprojection\\tlooks\\tvery\\nmuch\\tlike\\tthe\\toriginal\\t3D\\tdataset.\\nTo\\tproject\\tthe\\ttraining\\tset\\tonto\\tthe\\thyperplane,\\tyou\\tcan\\tsimply\\tcompute\\tthe\\tdot\\tproduct\\tof\\tthe\\ttraining\\tset\\nmatrix\\t\\nX\\n\\tby\\tthe\\tmatrix\\t\\nW\\nd\\n,\\tdefined\\tas\\tthe\\tmatrix\\tcontaining\\tthe\\tfirst\\t\\nd\\n\\tprincipal\\tcomponents\\t(i.e.,\\tthe\\nmatrix\\tcomposed\\tof\\tthe\\tfirst\\t\\nd\\n\\tcolumns\\tof\\t\\nV\\nT\\n),\\tas\\tshown\\tin\\t\\nEquation\\t8-2\\n.\\nEquation\\t8-2.\\t\\nProjecting\\tthe\\ttraining\\tset\\tdown\\tto\\t\\nd\\n\\tdimensions', 'The\\tfollowing\\tPython\\tcode\\tprojects\\tthe\\ttraining\\tset\\tonto\\tthe\\tplane\\tdefined\\tby\\tthe\\tfirst\\ttwo\\tprincipal\\ncomponents:\\nW2\\n\\t\\n=\\n\\t\\nV\\n.\\nT\\n[:,\\n\\t\\n:\\n2\\n]\\nX2D\\n\\t\\n=\\n\\t\\nX_centered\\n.\\ndot\\n(\\nW2\\n)\\nThere\\tyou\\thave\\tit!\\tYou\\tnow\\tknow\\thow\\tto\\treduce\\tthe\\tdimensionality\\tof\\tany\\tdataset\\tdown\\tto\\tany\\tnumber\\tof\\ndimensions,\\twhile\\tpreserving\\tas\\tmuch\\tvariance\\tas\\tpossible.', 'Using\\tScikit-Learn\\nScikit-Learn’s\\t\\nPCA\\n\\tclass\\t\\nimplements\\tPCA\\tusing\\tSVD\\tdecomposition\\tjust\\tlike\\twe\\tdid\\tbefore.\\tThe\\nfollowing\\tcode\\tapplies\\tPCA\\tto\\treduce\\tthe\\tdimensionality\\tof\\tthe\\tdataset\\tdown\\tto\\ttwo\\tdimensions\\t(note\\nthat\\tit\\tautomatically\\ttakes\\tcare\\tof\\tcentering\\tthe\\tdata):\\nfrom\\n\\t\\nsklearn.decomposition\\n\\t\\nimport\\n\\t\\nPCA\\npca\\n\\t\\n=\\n\\t\\nPCA\\n(\\nn_components\\n\\t\\n=\\n\\t\\n2\\n)\\nX2D\\n\\t\\n=\\n\\t\\npca\\n.\\nfit_transform\\n(\\nX\\n)\\nAfter\\tfitting\\tthe\\t\\nPCA\\n\\ttransformer\\tto\\tthe\\tdataset,\\tyou\\tcan\\taccess\\tthe\\tprincipal\\tcomponents\\tusing\\tthe\\ncomponents_\\n\\t\\nvariable\\t(note\\tthat\\tit\\tcontains\\tthe\\tPCs\\tas\\thorizontal\\tvectors,\\tso,\\tfor\\texample,\\tthe\\tfirst\\nprincipal\\tcomponent\\tis\\tequal\\tto\\t\\npca.components_.T[:,\\t0]\\n).', 'Explained\\tVariance\\tRatio\\nAnother\\t\\nvery\\tuseful\\tpiece\\tof\\tinformation\\tis\\tthe\\t\\nexplained\\tvariance\\tratio\\n\\tof\\teach\\tprincipal\\tcomponent,\\navailable\\tvia\\tthe\\t\\nexplained_variance_ratio_\\n\\tvariable.\\tIt\\tindicates\\tthe\\tproportion\\tof\\tthe\\tdataset’s\\nvariance\\tthat\\tlies\\talong\\tthe\\taxis\\tof\\teach\\tprincipal\\tcomponent.\\tFor\\texample,\\tlet’s\\tlook\\tat\\tthe\\texplained\\nvariance\\tratios\\tof\\tthe\\tfirst\\ttwo\\tcomponents\\tof\\tthe\\t3D\\tdataset\\trepresented\\tin\\t\\nFigure\\t8-2\\n:\\n>>>\\t\\npca\\n.\\nexplained_variance_ratio_\\narray([\\t0.84248607,\\t\\t0.14631839])\\nThis\\ttells\\tyou\\tthat\\t84.2%\\tof\\tthe\\tdataset’s\\tvariance\\tlies\\talong\\tthe\\tfirst\\taxis,\\tand\\t14.6%\\tlies\\talong\\tthe\\nsecond\\taxis.\\tThis\\tleaves\\tless\\tthan\\t1.2%\\tfor\\tthe\\tthird\\taxis,\\tso\\tit\\tis\\treasonable\\tto\\tassume\\tthat\\tit\\tprobably\\ncarries\\tlittle\\tinformation.', 'Choosing\\tthe\\tRight\\tNumber\\tof\\tDimensions\\nInstead\\t\\nof\\tarbitrarily\\tchoosing\\tthe\\tnumber\\tof\\tdimensions\\tto\\treduce\\tdown\\tto,\\tit\\tis\\tgenerally\\tpreferable\\tto\\nchoose\\tthe\\tnumber\\tof\\tdimensions\\tthat\\tadd\\tup\\tto\\ta\\tsufficiently\\tlarge\\tportion\\tof\\tthe\\tvariance\\t(e.g.,\\t95%).\\nUnless,\\tof\\tcourse,\\tyou\\tare\\treducing\\tdimensionality\\tfor\\tdata\\tvisualization\\t—\\tin\\tthat\\tcase\\tyou\\twill\\ngenerally\\twant\\tto\\treduce\\tthe\\tdimensionality\\tdown\\tto\\t2\\tor\\t3.\\nThe\\tfollowing\\tcode\\tcomputes\\tPCA\\twithout\\treducing\\tdimensionality,\\tthen\\tcomputes\\tthe\\tminimum\\tnumber\\nof\\tdimensions\\trequired\\tto\\tpreserve\\t95%\\tof\\tthe\\ttraining\\tset’s\\tvariance:\\npca\\n\\t\\n=\\n\\t\\nPCA\\n()\\npca\\n.\\nfit\\n(\\nX_train\\n)\\ncumsum\\n\\t\\n=\\n\\t\\nnp\\n.\\ncumsum\\n(\\npca\\n.\\nexplained_variance_ratio_\\n)\\nd\\n\\t\\n=\\n\\t\\nnp\\n.\\nargmax\\n(\\ncumsum\\n\\t\\n>=\\n\\t\\n0.95\\n)\\n\\t\\n+\\n\\t\\n1\\nYou\\tcould\\tthen\\tset\\t\\nn_components=d\\n\\tand\\trun\\tPCA\\tagain.\\tHowever,\\tthere\\tis\\ta\\tmuch\\tbetter\\toption:\\tinstead\\nof\\tspecifying\\tthe\\tnumber\\tof\\tprincipal\\tcomponents\\tyou\\twant\\tto\\tpreserve,\\tyou\\t\\ncan\\tset\\t\\nn_components\\n\\tto\\tbe\\na\\tfloat\\tbetween\\t\\n0.0\\n\\tand\\t\\n1.0', 'and\\t\\n1.0\\n,\\tindicating\\tthe\\tratio\\tof\\tvariance\\tyou\\twish\\tto\\tpreserve:\\npca\\n\\t\\n=\\n\\t\\nPCA\\n(\\nn_components\\n=\\n0.95\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\npca\\n.\\nfit_transform\\n(\\nX_train\\n)\\nYet\\tanother\\toption\\tis\\tto\\tplot\\tthe\\t\\nexplained\\tvariance\\tas\\ta\\tfunction\\tof\\tthe\\tnumber\\tof\\tdimensions\\t(simply\\tplot\\ncumsum\\n;\\tsee\\t\\nFigure\\t8-8\\n).\\tThere\\twill\\tusually\\tbe\\tan\\telbow\\tin\\tthe\\tcurve,\\twhere\\tthe\\texplained\\tvariance\\tstops\\ngrowing\\tfast.\\tYou\\tcan\\tthink\\tof\\tthis\\tas\\tthe\\tintrinsic\\tdimensionality\\tof\\tthe\\tdataset.\\tIn\\tthis\\tcase,\\tyou\\tcan\\tsee\\nthat\\treducing\\tthe\\tdimensionality\\tdown\\tto\\tabout\\t100\\tdimensions\\twouldn’t\\tlose\\ttoo\\tmuch\\texplained\\nvariance.\\nFigure\\t8-8.\\t\\nExplained\\tvariance\\tas\\ta\\tfunction\\tof\\tthe\\tnumber\\tof\\tdimensions', 'PCA\\tfor\\tCompression\\nObviously\\t\\nafter\\tdimensionality\\treduction,\\tthe\\ttraining\\tset\\ttakes\\tup\\tmuch\\tless\\tspace.\\tFor\\texample,\\ttry\\napplying\\tPCA\\tto\\tthe\\tMNIST\\tdataset\\twhile\\tpreserving\\t95%\\tof\\tits\\tvariance.\\tYou\\tshould\\tfind\\tthat\\teach\\ninstance\\twill\\thave\\tjust\\tover\\t150\\tfeatures,\\tinstead\\tof\\tthe\\toriginal\\t784\\tfeatures.\\tSo\\twhile\\tmost\\tof\\tthe\\nvariance\\tis\\tpreserved,\\tthe\\tdataset\\tis\\tnow\\tless\\tthan\\t20%\\tof\\tits\\toriginal\\tsize!\\tThis\\tis\\ta\\treasonable\\ncompression\\tratio,\\tand\\tyou\\tcan\\tsee\\thow\\tthis\\tcan\\tspeed\\tup\\ta\\tclassification\\talgorithm\\t(such\\tas\\tan\\tSVM\\nclassifier)\\ttremendously.\\nIt\\tis\\talso\\tpossible\\tto\\tdecompress\\tthe\\treduced\\tdataset\\tback\\tto\\t784\\tdimensions\\tby\\tapplying\\tthe\\tinverse\\ntransformation\\tof\\tthe\\tPCA\\tprojection.\\tOf\\tcourse\\tthis\\twon’t\\tgive\\tyou\\tback\\tthe\\toriginal\\tdata,\\tsince\\tthe\\nprojection\\tlost\\ta\\tbit\\tof\\tinformation\\t(within\\tthe\\t5%\\tvariance\\tthat\\twas\\tdropped),\\tbut\\tit\\twill\\tlikely\\tbe\\tquite\\nclose\\tto\\tthe\\toriginal\\tdata.\\tThe\\tmean\\tsquared\\tdistance\\tbetween\\tthe\\toriginal\\tdata\\tand\\tthe\\treconstructed\\tdata', '(compressed\\tand\\tthen\\tdecompressed)\\tis\\tcalled\\tthe\\t\\nreconstruction\\terror\\n.\\t\\nFor\\texample,\\tthe\\tfollowing\\tcode\\ncompresses\\tthe\\tMNIST\\tdataset\\tdown\\tto\\t154\\tdimensions,\\tthen\\tuses\\tthe\\t\\ninverse_transform()\\n\\tmethod\\tto\\ndecompress\\tit\\tback\\tto\\t784\\tdimensions.\\t\\nFigure\\t8-9\\n\\tshows\\ta\\tfew\\tdigits\\tfrom\\tthe\\toriginal\\ttraining\\tset\\t(on\\tthe\\nleft),\\tand\\tthe\\tcorresponding\\tdigits\\tafter\\tcompression\\tand\\tdecompression.\\tYou\\tcan\\tsee\\tthat\\tthere\\tis\\ta\\tslight\\nimage\\tquality\\tloss,\\tbut\\tthe\\tdigits\\tare\\tstill\\tmostly\\tintact.\\npca\\n\\t\\n=\\n\\t\\nPCA\\n(\\nn_components\\n\\t\\n=\\n\\t\\n154\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\npca\\n.\\nfit_transform\\n(\\nX_train\\n)\\nX_recovered\\n\\t\\n=\\n\\t\\npca\\n.\\ninverse_transform\\n(\\nX_reduced\\n)\\nFigure\\t8-9.\\t\\nMNIST\\tcompression\\tpreserving\\t95%\\tof\\tthe\\tvariance\\nThe\\tequation\\tof\\tthe\\tinverse\\ttransformation\\tis\\tshown\\tin\\t\\nEquation\\t8-3\\n.\\nEquation\\t8-3.\\t\\nPCA\\tinverse\\ttransformation,\\tback\\tto\\tthe\\toriginal\\tnumber\\tof\\tdimensions', 'Incremental\\tPCA\\nOne\\t\\nproblem\\twith\\tthe\\tpreceding\\timplementation\\tof\\tPCA\\tis\\tthat\\tit\\trequires\\tthe\\twhole\\ttraining\\tset\\tto\\tfit\\tin\\nmemory\\tin\\torder\\tfor\\tthe\\tSVD\\talgorithm\\tto\\trun.\\tFortunately,\\t\\nIncremental\\tPCA\\n\\t(IPCA)\\talgorithms\\thave\\nbeen\\tdeveloped:\\tyou\\tcan\\tsplit\\tthe\\ttraining\\tset\\tinto\\tmini-batches\\tand\\tfeed\\tan\\tIPCA\\talgorithm\\tone\\tmini-\\nbatch\\tat\\ta\\ttime.\\tThis\\tis\\tuseful\\tfor\\tlarge\\ttraining\\tsets,\\tand\\talso\\tto\\tapply\\tPCA\\tonline\\t(i.e.,\\ton\\tthe\\tfly,\\tas\\tnew\\ninstances\\tarrive).\\nThe\\tfollowing\\tcode\\tsplits\\tthe\\tMNIST\\tdataset\\tinto\\t100\\tmini-batches\\t(using\\tNumPy’s\\t\\narray_split()\\nfunction)\\t\\nand\\tfeeds\\tthem\\tto\\tScikit-Learn’s\\t\\nIncrementalPCA\\n\\tclass\\n5\\n\\tto\\t\\nreduce\\tthe\\tdimensionality\\tof\\tthe\\nMNIST\\tdataset\\tdown\\tto\\t154\\tdimensions\\t(just\\tlike\\tbefore).\\tNote\\tthat\\tyou\\tmust\\tcall\\tthe\\t\\npartial_fit()\\nmethod\\twith\\teach\\tmini-batch\\trather\\tthan\\t\\nthe\\t\\nfit()\\n\\tmethod\\twith\\tthe\\twhole\\t\\ntraining\\tset:\\nfrom\\n\\t\\nsklearn.decomposition\\n\\t\\nimport\\n\\t\\nIncrementalPCA\\nn_batches\\n\\t\\n=\\n\\t\\n100\\ninc_pca\\n\\t\\n=\\n\\t\\nIncrementalPCA\\n(\\nn_components\\n=\\n154\\n)\\nfor\\n\\t\\nX_batch\\n\\t\\nin\\n\\t\\nnp', 'in\\n\\t\\nnp\\n.\\narray_split\\n(\\nX_train\\n,\\n\\t\\nn_batches\\n):\\n\\t\\t\\t\\t\\ninc_pca\\n.\\npartial_fit\\n(\\nX_batch\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\ninc_pca\\n.\\ntransform\\n(\\nX_train\\n)\\nAlternatively,\\tyou\\tcan\\tuse\\tNumPy’s\\t\\nmemmap\\n\\t\\nclass,\\twhich\\tallows\\tyou\\tto\\tmanipulate\\ta\\tlarge\\tarray\\tstored\\tin\\na\\tbinary\\tfile\\ton\\tdisk\\tas\\tif\\tit\\twere\\tentirely\\tin\\tmemory;\\tthe\\tclass\\tloads\\tonly\\tthe\\tdata\\tit\\tneeds\\tin\\tmemory,\\nwhen\\tit\\tneeds\\tit.\\tSince\\tthe\\t\\nIncrementalPCA\\n\\tclass\\tuses\\tonly\\ta\\tsmall\\tpart\\tof\\tthe\\tarray\\tat\\tany\\tgiven\\ttime,\\nthe\\tmemory\\tusage\\tremains\\tunder\\tcontrol.\\tThis\\tmakes\\tit\\tpossible\\tto\\tcall\\tthe\\tusual\\t\\nfit()\\n\\tmethod,\\tas\\tyou\\ncan\\tsee\\tin\\tthe\\tfollowing\\tcode:\\nX_mm\\n\\t\\n=\\n\\t\\nnp\\n.\\nmemmap\\n(\\nfilename\\n,\\n\\t\\ndtype\\n=\\n\"float32\"\\n,\\n\\t\\nmode\\n=\\n\"readonly\"\\n,\\n\\t\\nshape\\n=\\n(\\nm\\n,\\n\\t\\nn\\n))\\nbatch_size\\n\\t\\n=\\n\\t\\nm\\n\\t\\n//\\n\\t\\nn_batches\\ninc_pca\\n\\t\\n=\\n\\t\\nIncrementalPCA\\n(\\nn_components\\n=\\n154\\n,\\n\\t\\nbatch_size\\n=\\nbatch_size\\n)\\ninc_pca\\n.\\nfit\\n(\\nX_mm\\n)', 'Randomized\\tPCA\\nScikit-Learn\\t\\noffers\\tyet\\tanother\\toption\\tto\\tperform\\tPCA,\\tcalled\\t\\nRandomized\\tPCA\\n.\\tThis\\tis\\ta\\tstochastic\\nalgorithm\\tthat\\tquickly\\tfinds\\tan\\tapproximation\\tof\\tthe\\tfirst\\t\\nd\\n\\tprincipal\\tcomponents.\\tIts\\tcomputational\\ncomplexity\\tis\\t\\nO\\n(\\nm\\n\\t×\\t\\nd\\n2\\n)\\t+\\t\\nO\\n(\\nd\\n3\\n),\\tinstead\\tof\\t\\nO\\n(\\nm\\n\\t×\\t\\nn\\n2\\n)\\t+\\t\\nO\\n(\\nn\\n3\\n),\\tso\\tit\\tis\\tdramatically\\tfaster\\tthan\\tthe\\nprevious\\talgorithms\\twhen\\t\\nd\\n\\tis\\tmuch\\t\\nsmaller\\tthan\\t\\nn\\n.\\nrnd_pca\\n\\t\\n=\\n\\t\\nPCA\\n(\\nn_components\\n=\\n154\\n,\\n\\t\\nsvd_solver\\n=\\n\"randomized\"\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\nrnd_pca\\n.\\nfit_transform\\n(\\nX_train\\n)', 'Kernel\\tPCA\\nIn\\t\\nChapter\\t5\\n\\t\\nwe\\tdiscussed\\tthe\\t\\nkernel\\ttrick,\\ta\\tmathematical\\ttechnique\\tthat\\timplicitly\\tmaps\\tinstances\\tinto\\ta\\nvery\\t\\nhigh-dimensional\\tspace\\t(called\\tthe\\t\\nfeature\\tspace\\n),\\tenabling\\tnonlinear\\tclassification\\tand\\tregression\\nwith\\tSupport\\tVector\\tMachines.\\tRecall\\tthat\\ta\\tlinear\\tdecision\\tboundary\\tin\\tthe\\thigh-dimensional\\tfeature\\nspace\\tcorresponds\\tto\\ta\\tcomplex\\tnonlinear\\tdecision\\tboundary\\tin\\tthe\\t\\noriginal\\tspace\\n.\\nIt\\tturns\\tout\\tthat\\tthe\\tsame\\ttrick\\tcan\\tbe\\tapplied\\tto\\tPCA,\\tmaking\\tit\\tpossible\\tto\\tperform\\tcomplex\\tnonlinear\\nprojections\\tfor\\tdimensionality\\treduction.\\tThis\\tis\\tcalled\\t\\nKernel\\tPCA\\n\\t(kPCA)\\n.\\n6\\n\\tIt\\tis\\toften\\tgood\\tat\\npreserving\\tclusters\\tof\\tinstances\\tafter\\tprojection,\\tor\\tsometimes\\teven\\tunrolling\\tdatasets\\tthat\\tlie\\tclose\\tto\\ta\\ntwisted\\tmanifold.\\nFor\\texample,\\t\\nthe\\tfollowing\\tcode\\tuses\\tScikit-Learn’s\\t\\nKernelPCA\\n\\tclass\\tto\\tperform\\tkPCA\\twith\\tan\\tRBF\\nkernel\\t(see\\t\\nChapter\\t5\\n\\tfor\\tmore\\tdetails\\tabout\\tthe\\tRBF\\tkernel\\tand\\tthe\\tother\\tkernels):\\nfrom\\n\\t\\nsklearn.decomposition\\n\\t\\nimport\\n\\t\\nKernelPCA\\nrbf_pca\\n\\t\\n=', '=\\n\\t\\nKernelPCA\\n(\\nn_components\\n\\t\\n=\\n\\t\\n2\\n,\\n\\t\\nkernel\\n=\\n\"rbf\"\\n,\\n\\t\\ngamma\\n=\\n0.04\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\nrbf_pca\\n.\\nfit_transform\\n(\\nX\\n)\\nFigure\\t8-10\\n\\tshows\\tthe\\tSwiss\\troll,\\treduced\\tto\\ttwo\\tdimensions\\tusing\\ta\\tlinear\\tkernel\\t(equivalent\\tto\\tsimply\\nusing\\tthe\\t\\nPCA\\n\\tclass),\\tan\\tRBF\\tkernel,\\tand\\ta\\tsigmoid\\tkernel\\t(Logistic).\\nFigure\\t8-10.\\t\\nSwiss\\troll\\treduced\\tto\\t2D\\tusing\\tkPCA\\twith\\tvarious\\tkernels', 'Selecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nAs\\tkPCA\\tis\\tan\\tunsupervised\\tlearning\\talgorithm,\\tthere\\tis\\tno\\tobvious\\tperformance\\tmeasure\\tto\\thelp\\tyou\\nselect\\tthe\\tbest\\tkernel\\tand\\thyperparameter\\tvalues.\\tHowever,\\tdimensionality\\treduction\\tis\\toften\\ta\\npreparation\\tstep\\tfor\\ta\\tsupervised\\tlearning\\ttask\\t(e.g.,\\tclassification),\\tso\\tyou\\tcan\\tsimply\\tuse\\tgrid\\tsearch\\tto\\nselect\\tthe\\tkernel\\tand\\thyperparameters\\tthat\\tlead\\tto\\tthe\\tbest\\tperformance\\ton\\tthat\\ttask.\\tFor\\texample,\\tthe\\nfollowing\\tcode\\tcreates\\ta\\ttwo-step\\tpipeline,\\tfirst\\treducing\\tdimensionality\\tto\\ttwo\\tdimensions\\tusing\\tkPCA,\\nthen\\tapplying\\tLogistic\\tRegression\\tfor\\tclassification.\\tThen\\tit\\tuses\\t\\nGridSearchCV\\n\\t\\nto\\tfind\\tthe\\tbest\\tkernel\\nand\\tgamma\\tvalue\\tfor\\tkPCA\\tin\\torder\\tto\\tget\\tthe\\tbest\\tclassification\\taccuracy\\tat\\tthe\\tend\\tof\\tthe\\tpipeline:\\nfrom\\n\\t\\nsklearn.model_selection\\n\\t\\nimport\\n\\t\\nGridSearchCV\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nLogisticRegression\\nfrom\\n\\t\\nsklearn.pipeline\\n\\t\\nimport\\n\\t\\nPipeline\\nclf\\n\\t\\n=\\n\\t\\nPipeline\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"kpca\"\\n,\\n\\t\\nKernelPCA\\n(\\nn_components\\n=\\n2\\n)),', '=\\n2\\n)),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n(\\n\"log_reg\"\\n,\\n\\t\\nLogisticRegression\\n())\\n\\t\\t\\t\\t\\n])\\nparam_grid\\n\\t\\n=\\n\\t\\n[{\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\"kpca__gamma\"\\n:\\n\\t\\nnp\\n.\\nlinspace\\n(\\n0.03\\n,\\n\\t\\n0.05\\n,\\n\\t\\n10\\n),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\"kpca__kernel\"\\n:\\n\\t\\n[\\n\"rbf\"\\n,\\n\\t\\n\"sigmoid\"\\n]\\n\\t\\t\\t\\t\\n}]\\ngrid_search\\n\\t\\n=\\n\\t\\nGridSearchCV\\n(\\nclf\\n,\\n\\t\\nparam_grid\\n,\\n\\t\\ncv\\n=\\n3\\n)\\ngrid_search\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\nThe\\tbest\\tkernel\\tand\\thyperparameters\\tare\\tthen\\tavailable\\tthrough\\tthe\\t\\nbest_params_\\n\\tvariable:\\n>>>\\t\\nprint\\n(\\ngrid_search\\n.\\nbest_params_\\n)\\n{\\'kpca__gamma\\':\\t0.043333333333333335,\\t\\'kpca__kernel\\':\\t\\'rbf\\'}\\nAnother\\tapproach,\\tthis\\ttime\\tentirely\\tunsupervised,\\tis\\tto\\tselect\\tthe\\tkernel\\tand\\thyperparameters\\tthat\\tyield\\nthe\\tlowest\\treconstruction\\terror.\\tHowever,\\treconstruction\\tis\\tnot\\tas\\teasy\\tas\\twith\\tlinear\\tPCA.\\tHere’s\\twhy.\\nFigure\\t8-11\\n\\tshows\\tthe\\toriginal\\tSwiss\\troll\\t3D\\tdataset\\t(top\\tleft),\\tand\\tthe\\tresulting\\t2D\\tdataset\\tafter\\tkPCA\\tis\\napplied\\tusing\\tan\\tRBF\\tkernel\\t(top\\tright).\\tThanks\\tto\\tthe\\tkernel\\ttrick,\\tthis\\tis\\tmathematically\\tequivalent\\tto\\nmapping\\tthe\\ttraining\\tset\\tto\\tan\\tinfinite-dimensional', 'feature\\tspace\\t(bottom\\tright)\\tusing\\t\\nthe\\t\\nfeature\\tmap\\n\\tφ,\\nthen\\tprojecting\\tthe\\ttransformed\\ttraining\\tset\\tdown\\tto\\t2D\\tusing\\tlinear\\tPCA.\\tNotice\\tthat\\tif\\twe\\tcould\\tinvert\\nthe\\tlinear\\tPCA\\tstep\\tfor\\ta\\tgiven\\tinstance\\tin\\tthe\\treduced\\tspace,\\tthe\\treconstructed\\tpoint\\twould\\tlie\\tin\\tfeature\\nspace,\\tnot\\tin\\tthe\\toriginal\\tspace\\t(e.g.,\\tlike\\tthe\\tone\\trepresented\\tby\\tan\\tx\\tin\\tthe\\tdiagram).\\tSince\\tthe\\tfeature\\nspace\\tis\\tinfinite-dimensional,\\twe\\tcannot\\tcompute\\tthe\\treconstructed\\tpoint,\\tand\\ttherefore\\twe\\tcannot\\ncompute\\tthe\\ttrue\\treconstruction\\terror.\\tFortunately,\\tit\\tis\\tpossible\\tto\\tfind\\ta\\tpoint\\tin\\tthe\\toriginal\\tspace\\tthat\\nwould\\tmap\\tclose\\tto\\tthe\\treconstructed\\tpoint.\\tThis\\tis\\tcalled\\tthe\\t\\nreconstruction\\t\\npre-image\\n.\\tOnce\\tyou\\thave\\nthis\\tpre-image,\\tyou\\tcan\\tmeasure\\tits\\tsquared\\tdistance\\tto\\tthe\\toriginal\\tinstance.\\tYou\\tcan\\tthen\\tselect\\tthe\\nkernel\\tand\\thyperparameters\\tthat\\tminimize\\tthis\\treconstruction\\tpre-image\\terror.', 'Figure\\t8-11.\\t\\nKernel\\tPCA\\tand\\tthe\\treconstruction\\tpre-image\\terror\\nYou\\tmay\\tbe\\twondering\\thow\\tto\\tperform\\tthis\\treconstruction.\\tOne\\tsolution\\tis\\tto\\ttrain\\ta\\tsupervised\\nregression\\tmodel,\\twith\\tthe\\tprojected\\tinstances\\tas\\tthe\\ttraining\\tset\\tand\\tthe\\toriginal\\tinstances\\tas\\tthe\\ttargets.\\nScikit-Learn\\twill\\tdo\\tthis\\tautomatically\\tif\\tyou\\t\\nset\\t\\nfit_inverse_transform=True\\n,\\tas\\tshown\\tin\\tthe\\nfollowing\\tcode:\\n7\\nrbf_pca\\n\\t\\n=\\n\\t\\nKernelPCA\\n(\\nn_components\\n\\t\\n=\\n\\t\\n2\\n,\\n\\t\\nkernel\\n=\\n\"rbf\"\\n,\\n\\t\\ngamma\\n=\\n0.0433\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfit_inverse_transform\\n=\\nTrue\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\nrbf_pca\\n.\\nfit_transform\\n(\\nX\\n)\\nX_preimage\\n\\t\\n=\\n\\t\\nrbf_pca\\n.\\ninverse_transform\\n(\\nX_reduced\\n)\\nNOTE\\nBy\\tdefault,\\t\\nfit_inverse_transform=False\\n\\tand\\t\\nKernelPCA\\n\\thas\\tno\\t\\ninverse_transform()\\n\\tmethod.\\t\\nThis\\tmethod\\tonly\\tgets\\ncreated\\twhen\\tyou\\tset\\t\\nfit_inverse_transform=True\\n.\\nYou\\tcan\\tthen\\tcompute\\tthe\\treconstruction\\t\\npre-image\\terror:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\nmean_squared_error\\n>>>\\t\\nmean_squared_error\\n(\\nX\\n,\\n\\t\\nX_preimage\\n)\\n32.786308795766132', 'Now\\tyou\\tcan\\tuse\\tgrid\\tsearch\\twith\\tcross-validation\\tto\\tfind\\tthe\\tkernel\\tand\\thyperparameters\\tthat\\tminimize\\nthis\\tpre-image\\treconstruction\\t\\nerror.', 'LLE\\nLocally\\tLinear\\tEmbedding\\n\\t(LLE)\\n8\\n\\tis\\t\\nanother\\tvery\\tpowerful\\t\\nnonlinear\\tdimensionality\\treduction\\n(NLDR)\\t\\ntechnique.\\tIt\\tis\\ta\\t\\nManifold\\tLearning\\ttechnique\\tthat\\tdoes\\tnot\\trely\\ton\\tprojections\\tlike\\tthe\\tprevious\\nalgorithms.\\tIn\\ta\\tnutshell,\\tLLE\\tworks\\tby\\tfirst\\tmeasuring\\thow\\teach\\ttraining\\tinstance\\tlinearly\\trelates\\tto\\tits\\nclosest\\tneighbors\\t(c.n.),\\tand\\tthen\\tlooking\\tfor\\ta\\tlow-dimensional\\trepresentation\\tof\\tthe\\ttraining\\tset\\twhere\\nthese\\tlocal\\trelationships\\tare\\tbest\\tpreserved\\t(more\\tdetails\\tshortly).\\tThis\\tmakes\\tit\\tparticularly\\tgood\\tat\\nunrolling\\ttwisted\\tmanifolds,\\tespecially\\twhen\\tthere\\tis\\tnot\\ttoo\\tmuch\\tnoise.\\nFor\\texample,\\tthe\\tfollowing\\tcode\\tuses\\tScikit-Learn’s\\t\\nLocallyLinearEmbedding\\n\\tclass\\t\\nto\\tunroll\\tthe\\tSwiss\\nroll.\\tThe\\tresulting\\t2D\\tdataset\\tis\\tshown\\tin\\t\\nFigure\\t8-12\\n.\\tAs\\tyou\\tcan\\tsee,\\tthe\\tSwiss\\troll\\tis\\tcompletely\\nunrolled\\tand\\tthe\\tdistances\\tbetween\\tinstances\\tare\\tlocally\\twell\\tpreserved.\\tHowever,\\tdistances\\tare\\tnot', 'preserved\\ton\\ta\\tlarger\\tscale:\\tthe\\tleft\\tpart\\tof\\tthe\\tunrolled\\tSwiss\\troll\\tis\\tsqueezed,\\twhile\\tthe\\tright\\tpart\\tis\\nstretched.\\tNevertheless,\\tLLE\\tdid\\ta\\tpretty\\tgood\\tjob\\tat\\tmodeling\\tthe\\tmanifold.\\nfrom\\n\\t\\nsklearn.manifold\\n\\t\\nimport\\n\\t\\nLocallyLinearEmbedding\\nlle\\n\\t\\n=\\n\\t\\nLocallyLinearEmbedding\\n(\\nn_components\\n=\\n2\\n,\\n\\t\\nn_neighbors\\n=\\n10\\n)\\nX_reduced\\n\\t\\n=\\n\\t\\nlle\\n.\\nfit_transform\\n(\\nX\\n)\\nFigure\\t8-12.\\t\\nUnrolled\\tSwiss\\troll\\tusing\\tLLE\\nHere’s\\thow\\t\\nLLE\\tworks:\\tfirst,\\tfor\\teach\\ttraining\\tinstance\\t\\nx\\n(i)\\n,\\tthe\\talgorithm\\tidentifies\\tits\\t\\nk\\n\\tclosest\\nneighbors\\t(in\\tthe\\tpreceding\\tcode\\t\\nk\\n\\t=\\t10),\\tthen\\ttries\\tto\\treconstruct\\t\\nx\\n(i)\\n\\tas\\ta\\tlinear\\tfunction\\tof\\tthese\\nneighbors.\\tMore\\tspecifically,\\tit\\tfinds\\tthe\\tweights\\t\\nw\\ni,j\\n\\tsuch\\tthat\\tthe\\tsquared\\tdistance\\tbetween\\t\\nx\\n(i)\\n\\tand\\t\\n\\tis\\tas\\tsmall\\tas\\tpossible,\\tassuming\\t\\nw\\ni,j\\n\\t=\\t0\\t\\nif\\t\\nx\\n(j)\\n\\tis\\tnot\\tone\\tof\\tthe\\t\\nk\\n\\tclosest\\tneighbors\\tof\\t\\nx\\n(i)\\n.\\nThus\\tthe\\tfirst\\tstep\\tof\\tLLE\\tis\\tthe\\tconstrained\\toptimization\\tproblem\\tdescribed\\tin\\t\\nEquation\\t8-4\\n,\\twhere\\t\\nW\\n\\tis', 'the\\tweight\\tmatrix\\tcontaining\\tall\\tthe\\tweights\\t\\nw\\ni,j\\n.\\tThe\\tsecond\\tconstraint\\tsimply\\tnormalizes\\tthe\\tweights\\tfor\\neach\\ttraining\\tinstance\\t\\nx\\n(i)\\n.\\nEquation\\t8-4.\\t\\nLLE\\tstep\\t1:\\tlinearly\\tmodeling\\tlocal\\trelationships\\nAfter\\tthis\\tstep,\\tthe\\tweight\\tmatrix\\t\\n\\t(containing\\tthe\\tweights\\t\\n)\\tencodes\\tthe\\tlocal\\tlinear\\trelationships\\nbetween\\tthe\\ttraining\\tinstances.\\tNow\\tthe\\tsecond\\tstep\\tis\\tto\\tmap\\tthe\\ttraining\\tinstances\\tinto\\ta\\t\\nd\\n-dimensional\\nspace\\t(where\\t\\nd\\n\\t<\\t\\nn\\n)\\twhile\\tpreserving\\tthese\\tlocal\\trelationships\\tas\\tmuch\\tas\\tpossible.\\tIf\\t\\nz\\n(i)\\n\\tis\\tthe\\timage\\tof\\nx\\n(i)\\n\\tin\\tthis\\t\\nd\\n-dimensional\\tspace,\\tthen\\twe\\twant\\tthe\\tsquared\\tdistance\\tbetween\\t\\nz\\n(i)\\n\\tand\\t\\n\\tto\\tbe\\tas\\nsmall\\tas\\tpossible.\\tThis\\tidea\\tleads\\tto\\tthe\\tunconstrained\\toptimization\\tproblem\\tdescribed\\tin\\t\\nEquation\\t8-5\\n.\\tIt\\nlooks\\tvery\\tsimilar\\tto\\tthe\\tfirst\\tstep,\\tbut\\tinstead\\tof\\tkeeping\\tthe\\tinstances\\tfixed\\tand\\tfinding\\tthe\\toptimal\\nweights,\\twe\\tare\\tdoing\\tthe\\treverse:\\tkeeping\\tthe\\tweights\\tfixed\\tand\\tfinding\\tthe\\toptimal\\tposition\\tof\\tthe', 'instances’\\timages\\tin\\tthe\\tlow-dimensional\\tspace.\\tNote\\tthat\\t\\nZ\\n\\tis\\tthe\\tmatrix\\tcontaining\\tall\\t\\nz\\n(i)\\n.\\nEquation\\t8-5.\\t\\nLLE\\tstep\\t2:\\treducing\\tdimensionality\\twhile\\tpreserving\\trelationships\\nScikit-Learn’s\\tLLE\\timplementation\\thas\\tthe\\tfollowing\\tcomputational\\tcomplexity:\\t\\nO\\n(\\nm\\n\\tlog(\\nm\\n)\\nn\\n\\tlog(\\nk\\n))\\n\\tfor\\nfinding\\tthe\\t\\nk\\n\\tnearest\\tneighbors,\\t\\nO\\n(\\nmnk\\n3\\n)\\tfor\\toptimizing\\tthe\\tweights,\\tand\\t\\nO\\n(\\ndm\\n2\\n)\\tfor\\tconstructing\\tthe\\tlow-\\ndimensional\\trepresentations.\\tUnfortunately,\\tthe\\t\\nm\\n2\\n\\tin\\tthe\\tlast\\tterm\\tmakes\\tthis\\talgorithm\\t\\nscale\\tpoorly\\tto\\nvery\\tlarge\\tdatasets.', 'Other\\tDimensionality\\tReduction\\tTechniques\\nThere\\tare\\tmany\\tother\\tdimensionality\\treduction\\ttechniques,\\tseveral\\tof\\twhich\\tare\\tavailable\\tin\\tScikit-Learn.\\nHere\\tare\\tsome\\tof\\tthe\\tmost\\tpopular:\\nMultidimensional\\tScaling\\n\\t(MDS)\\t\\nreduces\\tdimensionality\\twhile\\ttrying\\tto\\tpreserve\\tthe\\tdistances\\nbetween\\tthe\\tinstances\\t(see\\t\\nFigure\\t8-13\\n).\\nIsomap\\n\\tcreates\\t\\na\\tgraph\\tby\\tconnecting\\teach\\tinstance\\tto\\tits\\tnearest\\tneighbors,\\tthen\\treduces\\ndimensionality\\twhile\\ttrying\\tto\\t\\npreserve\\tthe\\t\\ngeodesic\\tdistances\\n9\\n\\tbetween\\tthe\\tinstances.\\nt-Distributed\\tStochastic\\tNeighbor\\tEmbedding\\n\\t(t-SNE)\\t\\nreduces\\tdimensionality\\twhile\\ttrying\\tto\\tkeep\\nsimilar\\tinstances\\tclose\\tand\\tdissimilar\\tinstances\\tapart.\\tIt\\tis\\tmostly\\tused\\tfor\\tvisualization,\\tin\\nparticular\\tto\\tvisualize\\tclusters\\tof\\tinstances\\tin\\thigh-dimensional\\tspace\\t(e.g.,\\tto\\tvisualize\\tthe\\tMNIST\\nimages\\tin\\t2D).\\nLinear\\tDiscriminant\\tAnalysis\\n\\t(LDA)\\tis\\t\\nactually\\ta\\tclassification\\talgorithm,\\tbut\\tduring\\ttraining\\tit', 'learns\\tthe\\tmost\\tdiscriminative\\taxes\\tbetween\\tthe\\tclasses,\\tand\\tthese\\taxes\\tcan\\tthen\\tbe\\tused\\tto\\tdefine\\ta\\nhyperplane\\tonto\\twhich\\tto\\tproject\\tthe\\tdata.\\tThe\\tbenefit\\tis\\tthat\\tthe\\tprojection\\twill\\tkeep\\tclasses\\tas\\tfar\\napart\\tas\\tpossible,\\tso\\tLDA\\tis\\ta\\tgood\\ttechnique\\tto\\treduce\\tdimensionality\\tbefore\\trunning\\tanother\\nclassification\\talgorithm\\tsuch\\tas\\tan\\tSVM\\tclassifier.\\nFigure\\t8-13.\\t\\nReducing\\tthe\\tSwiss\\troll\\tto\\t2D\\tusing\\tvarious\\ttechniques', 'Exercises\\n1\\n.\\t\\nWhat\\tare\\tthe\\tmain\\tmotivations\\tfor\\treducing\\ta\\tdataset’s\\tdimensionality?\\tWhat\\tare\\tthe\\tmain\\ndrawbacks?\\n2\\n.\\t\\nWhat\\tis\\tthe\\tcurse\\tof\\tdimensionality?\\n3\\n.\\t\\nOnce\\ta\\tdataset’s\\tdimensionality\\thas\\tbeen\\treduced,\\tis\\tit\\tpossible\\tto\\treverse\\tthe\\toperation?\\tIf\\tso,\\nhow?\\tIf\\tnot,\\twhy?\\n4\\n.\\t\\nCan\\tPCA\\tbe\\tused\\tto\\treduce\\tthe\\tdimensionality\\tof\\ta\\thighly\\tnonlinear\\tdataset?\\n5\\n.\\t\\nSuppose\\tyou\\tperform\\tPCA\\ton\\ta\\t1,000-dimensional\\tdataset,\\tsetting\\tthe\\texplained\\tvariance\\tratio\\tto\\n95%.\\tHow\\tmany\\tdimensions\\twill\\tthe\\tresulting\\tdataset\\thave?\\n6\\n.\\t\\nIn\\twhat\\tcases\\twould\\tyou\\tuse\\tvanilla\\tPCA,\\tIncremental\\tPCA,\\tRandomized\\tPCA,\\tor\\tKernel\\tPCA?\\n7\\n.\\t\\nHow\\tcan\\tyou\\tevaluate\\tthe\\tperformance\\tof\\ta\\tdimensionality\\treduction\\talgorithm\\ton\\tyour\\tdataset?\\n8\\n.\\t\\nDoes\\tit\\tmake\\tany\\tsense\\tto\\tchain\\ttwo\\tdifferent\\tdimensionality\\treduction\\talgorithms?\\n9\\n.\\t\\nLoad\\tthe\\tMNIST\\tdataset\\t(introduced\\tin\\t\\nChapter\\t3\\n)\\tand\\tsplit\\tit\\tinto\\ta\\ttraining\\tset\\tand\\ta\\ttest\\tset\\t(take', 'the\\tfirst\\t60,000\\tinstances\\tfor\\ttraining,\\tand\\tthe\\tremaining\\t10,000\\tfor\\ttesting).\\tTrain\\ta\\tRandom\\tForest\\nclassifier\\ton\\tthe\\tdataset\\tand\\ttime\\thow\\tlong\\tit\\ttakes,\\tthen\\tevaluate\\tthe\\tresulting\\tmodel\\ton\\tthe\\ttest\\tset.\\nNext,\\tuse\\tPCA\\tto\\treduce\\tthe\\tdataset’s\\tdimensionality,\\twith\\tan\\texplained\\tvariance\\tratio\\tof\\t95%.\\tTrain\\na\\tnew\\tRandom\\tForest\\tclassifier\\ton\\tthe\\treduced\\tdataset\\tand\\tsee\\thow\\tlong\\tit\\ttakes.\\tWas\\ttraining\\tmuch\\nfaster?\\tNext\\tevaluate\\tthe\\tclassifier\\ton\\tthe\\ttest\\tset:\\thow\\tdoes\\tit\\tcompare\\tto\\tthe\\tprevious\\tclassifier?\\n10\\n.\\t\\nUse\\tt-SNE\\tto\\treduce\\tthe\\tMNIST\\tdataset\\tdown\\tto\\ttwo\\tdimensions\\tand\\tplot\\tthe\\tresult\\tusing\\nMatplotlib.\\tYou\\tcan\\tuse\\ta\\tscatterplot\\tusing\\t10\\tdifferent\\tcolors\\tto\\trepresent\\teach\\timage’s\\ttarget\\tclass.\\nAlternatively,\\tyou\\tcan\\twrite\\tcolored\\tdigits\\tat\\tthe\\tlocation\\tof\\teach\\tinstance,\\tor\\teven\\tplot\\tscaled-down\\nversions\\tof\\tthe\\tdigit\\timages\\tthemselves\\t(if\\tyou\\tplot\\tall\\tdigits,\\tthe\\tvisualization\\twill\\tbe\\ttoo\\tcluttered,', 'so\\tyou\\tshould\\teither\\tdraw\\ta\\trandom\\tsample\\tor\\tplot\\tan\\tinstance\\tonly\\tif\\tno\\tother\\tinstance\\thas\\talready\\nbeen\\tplotted\\tat\\ta\\tclose\\tdistance).\\tYou\\tshould\\tget\\ta\\tnice\\tvisualization\\twith\\twell-separated\\tclusters\\tof\\ndigits.\\tTry\\tusing\\tother\\tdimensionality\\treduction\\talgorithms\\tsuch\\tas\\tPCA,\\tLLE,\\tor\\tMDS\\tand\\tcompare\\nthe\\tresulting\\tvisualizations.\\nSolutions\\tto\\tthese\\texercises\\t\\nare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\nWell,\\tfour\\tdimensions\\tif\\tyou\\tcount\\ttime,\\tand\\ta\\tfew\\tmore\\tif\\tyou\\tare\\ta\\tstring\\ttheorist.\\nWatch\\ta\\trotating\\ttesseract\\tprojected\\tinto\\t3D\\tspace\\tat\\t\\nhttp://goo.gl/OM7ktJ\\n.\\tImage\\tby\\tWikipedia\\tuser\\tNerdBoy1392\\t(\\nCreative\\tCommons\\nBY-SA\\t3.0\\n).\\tReproduced\\tfrom\\t\\nhttps://en.wikipedia.org/wiki/Tesseract\\n.\\nFun\\tfact:\\tanyone\\tyou\\tknow\\tis\\tprobably\\tan\\textremist\\tin\\tat\\tleast\\tone\\tdimension\\t(e.g.,\\thow\\tmuch\\tsugar\\tthey\\tput\\tin\\ttheir\\tcoffee),\\tif\\tyou\\nconsider\\tenough\\tdimensions.\\n“On\\tLines\\tand\\tPlanes\\tof\\tClosest\\tFit\\tto\\tSystems\\tof\\tPoints\\tin\\tSpace,”\\tK.\\tPearson\\t(1901).', 'Scikit-Learn\\tuses\\tthe\\talgorithm\\tdescribed\\tin\\t“Incremental\\tLearning\\tfor\\tRobust\\tVisual\\tTracking,”\\tD.\\tRoss\\tet\\tal.\\t(2007).\\n1\\n2\\n3\\n4\\n5', '“Kernel\\tPrincipal\\tComponent\\tAnalysis,”\\tB.\\tSchölkopf,\\tA.\\tSmola,\\tK.\\tMüller\\t(1999).\\nScikit-Learn\\tuses\\tthe\\talgorithm\\tbased\\ton\\tKernel\\tRidge\\tRegression\\tdescribed\\tin\\tGokhan\\tH.\\tBakır,\\tJason\\tWeston,\\tand\\tBernhard\\tScholkopf,\\n“Learning\\tto\\tFind\\tPre-images”\\n\\t(Tubingen,\\tGermany:\\tMax\\tPlanck\\tInstitute\\tfor\\tBiological\\tCybernetics,\\t2004).\\n“Nonlinear\\tDimensionality\\tReduction\\tby\\tLocally\\tLinear\\tEmbedding,”\\tS.\\tRoweis,\\tL.\\tSaul\\t(2000).\\nThe\\tgeodesic\\tdistance\\tbetween\\ttwo\\tnodes\\tin\\ta\\tgraph\\tis\\tthe\\tnumber\\tof\\tnodes\\ton\\tthe\\tshortest\\tpath\\tbetween\\tthese\\tnodes.\\n6\\n7\\n8\\n9', 'Part\\tII.\\t\\nNeural\\tNetworks\\tand\\tDeep\\tLearning', 'Chapter\\t9.\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nTensorFlow\\n\\t\\nis\\ta\\tpowerful\\topen\\tsource\\tsoftware\\tlibrary\\tfor\\tnumerical\\tcomputation,\\tparticularly\\twell\\nsuited\\tand\\tfine-tuned\\tfor\\tlarge-scale\\tMachine\\tLearning.\\tIts\\tbasic\\tprinciple\\tis\\tsimple:\\tyou\\tfirst\\tdefine\\tin\\nPython\\ta\\tgraph\\tof\\tcomputations\\tto\\tperform\\t(for\\texample,\\tthe\\tone\\tin\\t\\nFigure\\t9-1\\n),\\tand\\tthen\\tTensorFlow\\ntakes\\tthat\\tgraph\\tand\\truns\\tit\\tefficiently\\tusing\\toptimized\\tC++\\tcode.\\nFigure\\t9-1.\\t\\nA\\tsimple\\tcomputation\\tgraph\\nMost\\timportantly,\\tit\\tis\\tpossible\\tto\\tbreak\\tup\\tthe\\tgraph\\tinto\\tseveral\\tchunks\\tand\\trun\\tthem\\tin\\tparallel\\tacross\\nmultiple\\tCPUs\\tor\\tGPUs\\t(as\\tshown\\tin\\t\\nFigure\\t9-2\\n).\\tTensorFlow\\talso\\tsupports\\tdistributed\\tcomputing,\\t\\nso\\nyou\\tcan\\ttrain\\tcolossal\\tneural\\tnetworks\\ton\\thumongous\\ttraining\\tsets\\tin\\ta\\treasonable\\tamount\\tof\\ttime\\tby\\nsplitting\\tthe\\tcomputations\\tacross\\thundreds\\tof\\tservers\\t(see\\t\\nChapter\\t12\\n).\\tTensorFlow\\tcan\\ttrain\\ta\\tnetwork\\nwith\\tmillions\\tof\\tparameters\\ton\\ta\\ttraining\\tset\\tcomposed\\tof\\tbillions\\tof\\tinstances\\twith\\tmillions\\tof\\tfeatures', 'each.\\tThis\\tshould\\tcome\\tas\\tno\\tsurprise,\\tsince\\tTensorFlow\\twas\\tdeveloped\\tby\\t\\nthe\\tGoogle\\tBrain\\tteam\\tand\\tit\\npowers\\tmany\\tof\\tGoogle’s\\tlarge-scale\\tservices,\\tsuch\\tas\\tGoogle\\tCloud\\tSpeech,\\tGoogle\\tPhotos,\\tand\\nGoogle\\tSearch.', 'Figure\\t9-2.\\t\\nParallel\\tcomputation\\ton\\tmultiple\\tCPUs/GPUs/servers\\nWhen\\tTensorFlow\\twas\\topen-sourced\\tin\\tNovember\\t2015,\\tthere\\twere\\talready\\tmany\\tpopular\\topen\\tsource\\nlibraries\\tfor\\t\\nDeep\\tLearning\\t(\\nTable\\t9-1\\n\\tlists\\ta\\tfew),\\tand\\tto\\tbe\\tfair\\tmost\\tof\\tTensorFlow’s\\tfeatures\\talready\\nexisted\\tin\\tone\\tlibrary\\tor\\tanother.\\tNevertheless,\\tTensorFlow’s\\tclean\\tdesign,\\tscalability,\\tflexibility,\\n1\\n\\tand\\ngreat\\tdocumentation\\t(not\\tto\\tmention\\tGoogle’s\\tname)\\tquickly\\tboosted\\tit\\tto\\tthe\\ttop\\tof\\tthe\\tlist.\\tIn\\tshort,\\nTensorFlow\\twas\\tdesigned\\tto\\tbe\\tflexible,\\tscalable,\\tand\\tproduction-ready,\\tand\\texisting\\tframeworks\\narguably\\t\\nhit\\tonly\\ttwo\\tout\\tof\\tthe\\tthree\\tof\\tthese.\\tHere\\tare\\tsome\\tof\\tTensorFlow’s\\thighlights:\\nIt\\truns\\tnot\\tonly\\ton\\tWindows,\\tLinux,\\tand\\tmacOS,\\tbut\\talso\\ton\\tmobile\\tdevices,\\tincluding\\tboth\\tiOS\\tand\\nAndroid.\\nIt\\tprovides\\ta\\tvery\\tsimple\\tPython\\tAPI\\t\\ncalled\\t\\nTF.Learn\\n2\\n\\t(\\ntensorflow.contrib.learn\\n),\\tcompatible\\nwith\\t\\nScikit-Learn.\\tAs\\tyou\\twill\\tsee,\\tyou\\tcan\\tuse\\tit\\tto\\ttrain\\tvarious\\ttypes\\tof\\tneural\\tnetworks\\tin\\tjust\\ta', 'few\\tlines\\tof\\tcode.\\tIt\\twas\\tpreviously\\tan\\tindependent\\tproject\\t\\ncalled\\t\\nScikit\\tFlow\\n\\t(or\\t\\nskflow\\n).\\nIt\\talso\\tprovides\\tanother\\tsimple\\tAPI\\t\\ncalled\\t\\nTF-slim\\n\\t(\\ntensorflow.contrib.slim\\n)\\tto\\tsimplify\\nbuilding,\\ttraining,\\tand\\tevaluating\\tneural\\tnetworks.\\nSeveral\\tother\\thigh-level\\tAPIs\\thave\\tbeen\\t\\nbuilt\\tindependently\\ton\\ttop\\tof\\tTensorFlow,\\tsuch\\tas\\t\\nKeras\\n(now\\tavailable\\tin\\t\\ntensorflow.contrib.keras\\n)\\tor\\t\\nPretty\\tTensor\\n.', 'Its\\tmain\\tPython\\tAPI\\toffers\\tmuch\\tmore\\tflexibility\\t(at\\tthe\\tcost\\tof\\thigher\\tcomplexity)\\tto\\tcreate\\tall\\tsorts\\nof\\tcomputations,\\tincluding\\tany\\tneural\\tnetwork\\tarchitecture\\tyou\\tcan\\tthink\\tof.\\nIt\\tincludes\\thighly\\tefficient\\tC++\\timplementations\\tof\\tmany\\tML\\toperations,\\tparticularly\\tthose\\tneeded\\tto\\nbuild\\tneural\\tnetworks.\\tThere\\tis\\talso\\ta\\tC++\\tAPI\\tto\\tdefine\\tyour\\town\\thigh-performance\\toperations.\\nIt\\tprovides\\tseveral\\tadvanced\\toptimization\\tnodes\\tto\\tsearch\\tfor\\tthe\\tparameters\\tthat\\tminimize\\ta\\tcost\\nfunction.\\tThese\\tare\\tvery\\teasy\\tto\\tuse\\tsince\\tTensorFlow\\tautomatically\\ttakes\\tcare\\tof\\tcomputing\\tthe\\ngradients\\tof\\tthe\\tfunctions\\tyou\\tdefine.\\tThis\\tis\\t\\ncalled\\t\\nautomatic\\tdifferentiating\\n\\t(or\\t\\nautodiff\\n).\\nIt\\talso\\tcomes\\twith\\ta\\tgreat\\tvisualization\\ttool\\t\\ncalled\\t\\nTensorBoard\\n\\tthat\\tallows\\tyou\\tto\\tbrowse\\tthrough\\nthe\\tcomputation\\tgraph,\\tview\\tlearning\\tcurves,\\tand\\tmore.\\nGoogle\\talso\\tlaunched\\ta\\t\\ncloud\\tservice\\tto\\trun\\tTensorFlow\\tgraphs\\n.\\nLast\\tbut\\tnot\\tleast,\\tit\\thas\\ta\\tdedicated\\tteam\\tof\\tpassionate\\tand\\thelpful\\tdevelopers,\\tand\\ta\\tgrowing', 'community\\tcontributing\\tto\\timproving\\tit.\\tIt\\tis\\tone\\tof\\tthe\\tmost\\tpopular\\topen\\tsource\\tprojects\\ton\\nGitHub,\\tand\\tmore\\tand\\tmore\\tgreat\\tprojects\\tare\\tbeing\\tbuilt\\ton\\ttop\\tof\\tit\\t(for\\texamples,\\tcheck\\tout\\tthe\\nresources\\tpage\\ton\\t\\nhttps://www.tensorflow.org/\\n,\\tor\\t\\nhttps://github.com/jtoy/awesome-tensorflow\\n).\\nTo\\task\\ttechnical\\tquestions,\\tyou\\tshould\\tuse\\t\\nhttp://stackoverflow.com/\\n\\tand\\ttag\\tyour\\tquestion\\twith\\n\"tensorflow\"\\n.\\tYou\\tcan\\tfile\\tbugs\\tand\\tfeature\\trequests\\tthrough\\tGitHub.\\tFor\\tgeneral\\tdiscussions,\\tjoin\\nthe\\t\\nGoogle\\tgroup\\n.\\nIn\\tthis\\tchapter,\\twe\\twill\\tgo\\tthrough\\tthe\\tbasics\\tof\\tTensorFlow,\\tfrom\\tinstallation\\tto\\tcreating,\\trunning,\\tsaving,\\nand\\tvisualizing\\tsimple\\tcomputational\\tgraphs.\\tMastering\\tthese\\tbasics\\tis\\timportant\\tbefore\\tyou\\tbuild\\tyour\\nfirst\\tneural\\tnetwork\\t\\n(which\\twe\\twill\\tdo\\tin\\tthe\\t\\nnext\\tchapter).\\nTable\\t9-1.\\t\\nOpen\\tsource\\tDeep\\tLearning\\tlibraries\\t(not\\tan\\texhaustive\\tlist)\\nLibrary\\nAPI\\nPlatforms\\nStarted\\tby\\nYear\\nCaffe\\nPython,\\tC++,\\tMatlab\\nLinux,\\tmacOS,\\tWindows\\nY.\\tJia,\\tUC\\tBerkeley\\t(BVLC)\\n2013\\nDeeplearning4j', 'Java,\\tScala,\\tClojure\\nLinux,\\tmacOS,\\tWindows,\\tAndroid\\nA.\\tGibson,\\tJ.Patterson\\n2014\\nH2O\\nPython,\\tR\\nLinux,\\tmacOS,\\tWindows\\nH2O.ai\\n2014\\nMXNet\\nPython,\\tC++,\\tothers\\nLinux,\\tmacOS,\\tWindows,\\tiOS,\\tAndroid\\nDMLC\\n2015\\nTensorFlow\\nPython,\\tC++\\nLinux,\\tmacOS,\\tWindows,\\tiOS,\\tAndroid\\nGoogle\\n2015\\nTheano\\nPython\\nLinux,\\tmacOS,\\tiOS\\nUniversity\\tof\\tMontreal\\n2010\\nTorch\\nC++,\\tLua\\nLinux,\\tmacOS,\\tiOS,\\tAndroid\\nR.\\tCollobert,\\tK.\\tKavukcuoglu,\\tC.\\tFarabet\\n2002', \"Installation\\nLet’s\\t\\nget\\tstarted!\\tAssuming\\tyou\\tinstalled\\tJupyter\\tand\\tScikit-Learn\\tby\\tfollowing\\tthe\\tinstallation\\ninstructions\\tin\\t\\nChapter\\t2\\n,\\tyou\\tcan\\tsimply\\tuse\\tpip\\tto\\tinstall\\tTensorFlow.\\tIf\\tyou\\tcreated\\tan\\tisolated\\nenvironment\\tusing\\tvirtualenv,\\tyou\\tfirst\\tneed\\tto\\tactivate\\tit:\\n$\\tcd\\t$ML_PATH\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t#\\tYour\\tML\\tworking\\tdirectory\\t(e.g.,\\t$HOME/ml)\\n$\\tsource\\tenv/bin/activate\\nNext,\\tinstall\\tTensorFlow:\\n$\\tpip3\\tinstall\\t--upgrade\\ttensorflow\\nNOTE\\nFor\\tGPU\\tsupport,\\tyou\\tneed\\tto\\tinstall\\t\\ntensorflow-gpu\\n\\tinstead\\tof\\t\\ntensorflow\\n.\\tSee\\t\\nChapter\\t12\\n\\tfor\\tmore\\tdetails.\\nTo\\ttest\\tyour\\tinstallation,\\ttype\\tthe\\tfollowing\\tcommand.\\tIt\\tshould\\toutput\\tthe\\tversion\\tof\\tTensorFlow\\tyou\\ninstalled.\\n$\\tpython3\\t-c\\t'import\\ttensorflow;\\tprint(tensorflow.__version__)'\\n1.0.0\", 'Creating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\nThe\\tfollowing\\tcode\\tcreates\\tthe\\t\\ngraph\\trepresented\\tin\\t\\nFigure\\t9-1\\n:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nx\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3\\n,\\n\\t\\nname\\n=\\n\"x\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n4\\n,\\n\\t\\nname\\n=\\n\"y\"\\n)\\nf\\n\\t\\n=\\n\\t\\nx\\n*\\nx\\n*\\ny\\n\\t\\n+\\n\\t\\ny\\n\\t\\n+\\n\\t\\n2\\nThat’s\\tall\\tthere\\tis\\tto\\tit!\\tThe\\tmost\\timportant\\tthing\\tto\\tunderstand\\tis\\tthat\\tthis\\tcode\\tdoes\\tnot\\tactually\\tperform\\nany\\tcomputation,\\teven\\tthough\\tit\\tlooks\\tlike\\tit\\tdoes\\t(especially\\tthe\\tlast\\tline).\\tIt\\tjust\\tcreates\\ta\\tcomputation\\ngraph.\\tIn\\tfact,\\teven\\tthe\\tvariables\\tare\\tnot\\tinitialized\\tyet.\\tTo\\tevaluate\\tthis\\tgraph,\\tyou\\tneed\\tto\\topen\\ta\\nTensorFlow\\t\\nsession\\n\\tand\\tuse\\tit\\tto\\tinitialize\\tthe\\tvariables\\tand\\tevaluate\\t\\nf\\n.\\tA\\tTensorFlow\\tsession\\ttakes\\tcare\\nof\\tplacing\\tthe\\toperations\\tonto\\t\\ndevices\\n\\tsuch\\tas\\tCPUs\\tand\\tGPUs\\tand\\trunning\\tthem,\\tand\\tit\\tholds\\tall\\tthe\\nvariable\\tvalues.\\n3\\n\\tThe\\tfollowing\\tcode\\tcreates\\ta\\tsession,\\tinitializes\\tthe\\tvariables,\\tand\\tevaluates,\\tand\\t\\nf\\n\\tthen\\ncloses\\tthe\\tsession\\t(which\\tfrees\\tup\\tresources):\\n>>>\\t\\nsess\\n\\t\\n=\\n\\t\\ntf\\n.\\nSession\\n()', '()\\n>>>\\t\\nsess\\n.\\nrun\\n(\\nx\\n.\\ninitializer\\n)\\n>>>\\t\\nsess\\n.\\nrun\\n(\\ny\\n.\\ninitializer\\n)\\n>>>\\t\\nresult\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\nf\\n)\\n>>>\\t\\nprint\\n(\\nresult\\n)\\n42\\n>>>\\t\\nsess\\n.\\nclose\\n()\\nHaving\\tto\\trepeat\\t\\nsess.run()\\n\\tall\\t\\nthe\\ttime\\tis\\ta\\tbit\\tcumbersome,\\t\\nbut\\tfortunately\\tthere\\tis\\ta\\tbetter\\tway:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nx\\n.\\ninitializer\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\ny\\n.\\ninitializer\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nresult\\n\\t\\n=\\n\\t\\nf\\n.\\neval\\n()\\nInside\\tthe\\t\\nwith\\n\\tblock,\\tthe\\tsession\\tis\\tset\\tas\\tthe\\t\\ndefault\\tsession.\\tCalling\\t\\nx.initializer.run()\\n\\tis\\nequivalent\\tto\\tcalling\\t\\ntf.get_default_session().run(x.initializer)\\n,\\tand\\tsimilarly\\t\\nf.eval()\\n\\tis\\nequivalent\\tto\\tcalling\\t\\ntf.get_default_session().run(f)\\n.\\tThis\\tmakes\\tthe\\tcode\\teasier\\tto\\tread.\\nMoreover,\\tthe\\tsession\\tis\\tautomatically\\tclosed\\tat\\tthe\\tend\\tof\\tthe\\tblock.\\nInstead\\tof\\tmanually\\trunning\\tthe\\t\\ninitializer\\tfor\\tevery\\tsingle\\tvariable,\\tyou\\tcan\\tuse\\tthe\\nglobal_variables_initializer()\\n\\t\\nfunction.\\tNote\\tthat\\tit\\tdoes\\tnot\\tactually\\tperform\\tthe\\tinitialization', 'immediately,\\tbut\\trather\\tcreates\\ta\\tnode\\tin\\tthe\\tgraph\\tthat\\twill\\tinitialize\\tall\\tvariables\\twhen\\tit\\tis\\trun:\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\n\\t\\t\\n#\\tprepare\\tan\\tinit\\tnode\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\n#\\tactually\\tinitialize\\tall\\tthe\\tvariables\\n\\t\\t\\t\\t\\nresult\\n\\t\\n=\\n\\t\\nf\\n.\\neval\\n()\\nInside\\tJupyter\\tor\\twithin\\ta\\tPython\\tshell\\tyou\\tmay\\tprefer\\tto\\t\\ncreate\\tan\\t\\nInteractiveSession\\n.\\tThe\\tonly\\ndifference\\tfrom\\ta\\tregular\\t\\nSession\\n\\tis\\tthat\\twhen\\tan\\t\\nInteractiveSession\\n\\tis\\tcreated\\tit\\tautomatically\\tsets', 'itself\\tas\\tthe\\tdefault\\tsession,\\tso\\tyou\\tdon’t\\tneed\\ta\\t\\nwith\\n\\tblock\\t(but\\tyou\\tdo\\tneed\\tto\\tclose\\tthe\\tsession\\nmanually\\twhen\\tyou\\tare\\tdone\\twith\\tit):\\n>>>\\t\\nsess\\n\\t\\n=\\n\\t\\ntf\\n.\\nInteractiveSession\\n()\\n>>>\\t\\ninit\\n.\\nrun\\n()\\n>>>\\t\\nresult\\n\\t\\n=\\n\\t\\nf\\n.\\neval\\n()\\n>>>\\t\\nprint\\n(\\nresult\\n)\\n42\\n>>>\\t\\nsess\\n.\\nclose\\n()\\nA\\tTensorFlow\\tprogram\\tis\\ttypically\\tsplit\\tinto\\ttwo\\tparts:\\tthe\\tfirst\\tpart\\tbuilds\\ta\\tcomputation\\tgraph\\t(this\\tis\\ncalled\\tthe\\t\\nconstruction\\tphase\\n),\\t\\nand\\tthe\\tsecond\\tpart\\truns\\tit\\t(this\\tis\\tthe\\t\\nexecution\\tphase\\n).\\tThe\\tconstruction\\nphase\\ttypically\\tbuilds\\ta\\tcomputation\\tgraph\\trepresenting\\tthe\\tML\\tmodel\\tand\\tthe\\tcomputations\\trequired\\tto\\ntrain\\tit.\\tThe\\texecution\\tphase\\tgenerally\\truns\\ta\\tloop\\tthat\\tevaluates\\ta\\ttraining\\tstep\\trepeatedly\\t(for\\texample,\\none\\tstep\\tper\\tmini-batch),\\tgradually\\timproving\\tthe\\t\\nmodel\\tparameters.\\tWe\\twill\\tgo\\tthrough\\tan\\texample\\nshortly.', 'Managing\\tGraphs\\nAny\\t\\nnode\\tyou\\tcreate\\tis\\tautomatically\\tadded\\tto\\tthe\\tdefault\\tgraph:\\n>>>\\t\\nx1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n1\\n)\\n>>>\\t\\nx1\\n.\\ngraph\\n\\t\\nis\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\nTrue\\nIn\\tmost\\tcases\\tthis\\tis\\tfine,\\tbut\\tsometimes\\tyou\\tmay\\twant\\tto\\tmanage\\tmultiple\\tindependent\\tgraphs.\\tYou\\tcan\\tdo\\nthis\\tby\\tcreating\\ta\\t\\nnew\\t\\nGraph\\n\\tand\\ttemporarily\\tmaking\\tit\\tthe\\tdefault\\tgraph\\tinside\\ta\\t\\nwith\\n\\tblock,\\tlike\\tso:\\n>>>\\t\\ngraph\\n\\t\\n=\\n\\t\\ntf\\n.\\nGraph\\n()\\n>>>\\t\\nwith\\n\\t\\ngraph\\n.\\nas_default\\n():\\n...\\t\\n\\t\\t\\t\\t\\nx2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n2\\n)\\n...\\n>>>\\t\\nx2\\n.\\ngraph\\n\\t\\nis\\n\\t\\ngraph\\nTrue\\n>>>\\t\\nx2\\n.\\ngraph\\n\\t\\nis\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\nFalse\\nTIP\\nIn\\tJupyter\\t(or\\tin\\ta\\tPython\\tshell),\\tit\\tis\\tcommon\\tto\\trun\\tthe\\tsame\\tcommands\\tmore\\tthan\\tonce\\twhile\\tyou\\tare\\texperimenting.\\tAs\\ta\\nresult,\\tyou\\tmay\\tend\\tup\\twith\\ta\\t\\ndefault\\tgraph\\tcontaining\\tmany\\tduplicate\\tnodes.\\tOne\\tsolution\\tis\\tto\\trestart\\tthe\\tJupyter\\tkernel\\t(or\\tthe\\nPython\\tshell),\\tbut\\ta\\tmore\\tconvenient\\tsolution\\tis\\tto\\tjust\\treset\\tthe\\tdefault\\tgraph\\tby\\t\\nrunning\\t\\ntf.reset_default_graph()\\n.', 'Lifecycle\\tof\\ta\\tNode\\tValue\\nWhen\\t\\nyou\\tevaluate\\ta\\tnode,\\tTensorFlow\\tautomatically\\tdetermines\\tthe\\tset\\tof\\tnodes\\tthat\\tit\\tdepends\\ton\\tand\\tit\\nevaluates\\tthese\\tnodes\\tfirst.\\tFor\\texample,\\tconsider\\tthe\\t\\nfollowing\\tcode:\\nw\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n3\\n)\\nx\\n\\t\\n=\\n\\t\\nw\\n\\t\\n+\\n\\t\\n2\\ny\\n\\t\\n=\\n\\t\\nx\\n\\t\\n+\\n\\t\\n5\\nz\\n\\t\\n=\\n\\t\\nx\\n\\t\\n*\\n\\t\\n3\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nprint\\n(\\ny\\n.\\neval\\n())\\n\\t\\t\\n#\\t10\\n\\t\\t\\t\\t\\nprint\\n(\\nz\\n.\\neval\\n())\\n\\t\\t\\n#\\t15\\nFirst,\\tthis\\tcode\\tdefines\\ta\\tvery\\tsimple\\tgraph.\\tThen\\tit\\tstarts\\ta\\tsession\\tand\\truns\\tthe\\tgraph\\tto\\tevaluate\\t\\ny\\n:\\nTensorFlow\\tautomatically\\tdetects\\tthat\\t\\ny\\n\\tdepends\\ton\\t\\nx\\n,\\twhich\\tdepends\\ton\\t\\nw\\n,\\tso\\tit\\tfirst\\tevaluates\\t\\nw\\n,\\tthen\\t\\nx\\n,\\nthen\\t\\ny\\n,\\tand\\treturns\\tthe\\tvalue\\tof\\t\\ny\\n.\\tFinally,\\tthe\\tcode\\truns\\tthe\\tgraph\\tto\\tevaluate\\t\\nz\\n.\\tOnce\\tagain,\\tTensorFlow\\ndetects\\tthat\\tit\\tmust\\tfirst\\tevaluate\\t\\nw\\n\\tand\\t\\nx\\n.\\tIt\\tis\\timportant\\tto\\tnote\\tthat\\tit\\twill\\t\\nnot\\n\\treuse\\tthe\\tresult\\tof\\tthe\\nprevious\\tevaluation\\tof\\t\\nw\\n\\tand\\t\\nx\\n.\\tIn\\tshort,\\tthe\\tpreceding\\tcode\\tevaluates\\t\\nw\\n\\tand\\t\\nx\\n\\ttwice.', 'x\\n\\ttwice.\\nAll\\tnode\\tvalues\\tare\\tdropped\\tbetween\\tgraph\\truns,\\texcept\\tvariable\\tvalues,\\twhich\\tare\\tmaintained\\tby\\tthe\\nsession\\tacross\\tgraph\\truns\\t(queues\\tand\\treaders\\talso\\tmaintain\\tsome\\tstate,\\tas\\twe\\twill\\tsee\\tin\\t\\nChapter\\t12\\n).\\tA\\nvariable\\tstarts\\tits\\tlife\\twhen\\tits\\tinitializer\\tis\\trun,\\tand\\tit\\tends\\twhen\\tthe\\tsession\\tis\\tclosed.\\nIf\\tyou\\twant\\tto\\tevaluate\\t\\ny\\n\\tand\\t\\nz\\n\\tefficiently,\\twithout\\tevaluating\\t\\nw\\n\\tand\\t\\nx\\n\\ttwice\\tas\\tin\\tthe\\tprevious\\tcode,\\tyou\\nmust\\task\\tTensorFlow\\tto\\tevaluate\\tboth\\t\\ny\\n\\tand\\t\\nz\\n\\tin\\tjust\\tone\\tgraph\\trun,\\tas\\tshown\\tin\\tthe\\tfollowing\\tcode:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ny_val\\n,\\n\\t\\nz_val\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n([\\ny\\n,\\n\\t\\nz\\n])\\n\\t\\t\\t\\t\\nprint\\n(\\ny_val\\n)\\n\\t\\t\\n#\\t10\\n\\t\\t\\t\\t\\nprint\\n(\\nz_val\\n)\\n\\t\\t\\n#\\t15\\nWARNING\\nIn\\tsingle-process\\tTensorFlow,\\tmultiple\\tsessions\\tdo\\tnot\\tshare\\tany\\tstate,\\teven\\tif\\tthey\\treuse\\tthe\\tsame\\tgraph\\t(each\\tsession\\twould\\nhave\\tits\\town\\tcopy\\tof\\tevery\\tvariable).\\tIn\\tdistributed\\tTensorFlow\\t(see\\t\\nChapter\\t12\\n),\\tvariable\\tstate\\tis\\tstored\\ton\\tthe\\tservers,\\tnot\\tin', 'the\\tsessions,\\tso\\tmultiple\\tsessions\\tcan\\tshare\\tthe\\tsame\\tvariables.', 'Linear\\tRegression\\twith\\tTensorFlow\\nTensorFlow\\t\\noperations\\t(also\\tcalled\\t\\nops\\n\\tfor\\tshort)\\tcan\\ttake\\tany\\tnumber\\tof\\tinputs\\tand\\tproduce\\tany\\tnumber\\nof\\toutputs.\\tFor\\texample,\\tthe\\taddition\\tand\\tmultiplication\\tops\\teach\\ttake\\ttwo\\tinputs\\tand\\tproduce\\tone\\toutput.\\nConstants\\tand\\tvariables\\ttake\\tno\\tinput\\t(they\\tare\\tcalled\\t\\nsource\\tops\\n).\\t\\nThe\\tinputs\\tand\\toutputs\\tare\\nmultidimensional\\tarrays,\\tcalled\\t\\ntensors\\n\\t(hence\\tthe\\tname\\t“tensor\\tflow”).\\tJust\\tlike\\tNumPy\\tarrays,\\ttensors\\nhave\\ta\\ttype\\tand\\ta\\tshape.\\tIn\\tfact,\\tin\\tthe\\tPython\\tAPI\\ttensors\\tare\\tsimply\\trepresented\\tby\\tNumPy\\tndarrays.\\nThey\\ttypically\\tcontain\\tfloats,\\tbut\\tyou\\tcan\\talso\\tuse\\tthem\\tto\\tcarry\\tstrings\\t(arbitrary\\tbyte\\tarrays).\\nIn\\tthe\\texamples\\tso\\tfar,\\tthe\\ttensors\\tjust\\tcontained\\ta\\tsingle\\tscalar\\tvalue,\\tbut\\tyou\\tcan\\tof\\tcourse\\tperform\\ncomputations\\ton\\tarrays\\tof\\tany\\tshape.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tmanipulates\\t2D\\tarrays\\tto\\tperform\\nLinear\\tRegression\\ton\\tthe\\tCalifornia\\thousing\\tdataset\\t(introduced\\tin\\t\\nChapter\\t2\\n).\\tIt\\tstarts\\tby\\tfetching\\tthe', 'dataset;\\tthen\\tit\\tadds\\tan\\textra\\tbias\\tinput\\tfeature\\t(\\nx\\n0\\n\\t=\\t1)\\tto\\tall\\ttraining\\tinstances\\t(it\\tdoes\\tso\\tusing\\tNumPy\\tso\\nit\\truns\\timmediately);\\tthen\\tit\\tcreates\\ttwo\\tTensorFlow\\tconstant\\tnodes,\\t\\nX\\n\\tand\\t\\ny\\n,\\tto\\thold\\tthis\\tdata\\tand\\tthe\\ntargets,\\n4\\n\\tand\\tit\\tuses\\tsome\\tof\\tthe\\tmatrix\\toperations\\tprovided\\tby\\tTensorFlow\\tto\\tdefine\\t\\ntheta\\n.\\tThese\\tmatrix\\nfunctions\\t—\\t\\ntranspose()\\n,\\n\\t\\nmatmul()\\n,\\tand\\t\\nmatrix_inverse()\\n\\t—\\tare\\tself-explanatory,\\tbut\\tas\\tusual\\tthey\\ndo\\tnot\\tperform\\tany\\tcomputations\\timmediately;\\tinstead,\\tthey\\tcreate\\tnodes\\tin\\tthe\\tgraph\\tthat\\twill\\tperform\\nthem\\twhen\\tthe\\tgraph\\tis\\trun.\\tYou\\tmay\\trecognize\\tthat\\tthe\\tdefinition\\tof\\t\\ntheta\\n\\tcorresponds\\tto\\tthe\\tNormal\\nEquation\\t(\\n\\t=\\t(\\nX\\nT\\n\\t·\\t\\nX\\n)\\n–1\\n\\t·\\t\\nX\\nT\\n\\t·\\t\\ny\\n;\\tsee\\t\\nChapter\\t4\\n).\\tFinally,\\tthe\\tcode\\tcreates\\ta\\tsession\\tand\\tuses\\tit\\t\\nto\\nevaluate\\t\\ntheta\\n.\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nfetch_california_housing\\nhousing\\n\\t\\n=\\n\\t\\nfetch_california_housing\\n()\\nm\\n,\\n\\t\\nn\\n\\t\\n=\\n\\t\\nhousing\\n.\\ndata\\n.\\nshape\\nhousing_data_plus_bias\\n\\t\\n=\\n\\t\\nnp\\n.\\nc_\\n[\\nnp\\n.\\nones\\n((\\nm\\n,', '((\\nm\\n,\\n\\t\\n1\\n)),\\n\\t\\nhousing\\n.\\ndata\\n]\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\nhousing_data_plus_bias\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"X\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\nhousing\\n.\\ntarget\\n.\\nreshape\\n(\\n-\\n1\\n,\\n\\t\\n1\\n),\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"y\"\\n)\\nXT\\n\\t\\n=\\n\\t\\ntf\\n.\\ntranspose\\n(\\nX\\n)\\ntheta\\n\\t\\n=\\n\\t\\ntf\\n.\\nmatmul\\n(\\ntf\\n.\\nmatmul\\n(\\ntf\\n.\\nmatrix_inverse\\n(\\ntf\\n.\\nmatmul\\n(\\nXT\\n,\\n\\t\\nX\\n)),\\n\\t\\nXT\\n),\\n\\t\\ny\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ntheta_value\\n\\t\\n=\\n\\t\\ntheta\\n.\\neval\\n()\\nThe\\tmain\\tbenefit\\tof\\tthis\\tcode\\tversus\\tcomputing\\tthe\\tNormal\\tEquation\\tdirectly\\tusing\\tNumPy\\tis\\tthat\\nTensorFlow\\twill\\tautomatically\\trun\\tthis\\ton\\tyour\\tGPU\\tcard\\tif\\tyou\\thave\\tone\\t(provided\\tyou\\tinstalled\\nTensorFlow\\twith\\tGPU\\tsupport,\\tof\\tcourse;\\tsee\\t\\nChapter\\t12\\n\\tfor\\t\\nmore\\tdetails).', 'Implementing\\tGradient\\tDescent\\nLet’s\\t\\ntry\\tusing\\tBatch\\tGradient\\tDescent\\t(introduced\\tin\\t\\nChapter\\t4\\n)\\tinstead\\tof\\tthe\\tNormal\\tEquation.\\tFirst\\twe\\nwill\\tdo\\tthis\\tby\\tmanually\\tcomputing\\tthe\\tgradients,\\tthen\\twe\\twill\\tuse\\tTensorFlow’s\\tautodiff\\tfeature\\tto\\tlet\\nTensorFlow\\tcompute\\tthe\\tgradients\\tautomatically,\\tand\\tfinally\\twe\\twill\\tuse\\ta\\tcouple\\tof\\tTensorFlow’s\\tout-\\nof-the-box\\toptimizers.\\nWARNING\\nWhen\\tusing\\tGradient\\tDescent,\\tremember\\tthat\\tit\\tis\\timportant\\tto\\tfirst\\tnormalize\\tthe\\tinput\\t\\nfeature\\tvectors,\\tor\\telse\\ttraining\\tmay\\tbe\\nmuch\\tslower.\\tYou\\tcan\\tdo\\tthis\\tusing\\tTensorFlow,\\tNumPy,\\t\\nScikit-Learn’s\\t\\nStandardScaler\\n,\\tor\\tany\\tother\\tsolution\\tyou\\tprefer.\\tThe\\nfollowing\\tcode\\tassumes\\tthat\\tthis\\tnormalization\\thas\\talready\\tbeen\\tdone.', 'Manually\\tComputing\\tthe\\tGradients\\nThe\\t\\nfollowing\\tcode\\tshould\\tbe\\tfairly\\tself-explanatory,\\texcept\\tfor\\ta\\tfew\\tnew\\telements:\\nThe\\t\\nrandom_uniform()\\n\\t\\nfunction\\tcreates\\ta\\tnode\\tin\\tthe\\tgraph\\tthat\\twill\\tgenerate\\ta\\ttensor\\tcontaining\\nrandom\\tvalues,\\tgiven\\tits\\tshape\\tand\\tvalue\\trange,\\tmuch\\tlike\\tNumPy’s\\t\\nrand()\\n\\tfunction.\\nThe\\t\\nassign()\\n\\t\\nfunction\\tcreates\\ta\\tnode\\tthat\\twill\\tassign\\ta\\tnew\\tvalue\\tto\\ta\\tvariable.\\tIn\\tthis\\tcase,\\tit\\nimplements\\tthe\\tBatch\\tGradient\\tDescent\\tstep\\t\\nθ\\n(next\\tstep)\\n\\t=\\t\\nθ\\n\\t–\\t\\nη\\nθ\\nMSE(\\nθ\\n).\\nThe\\tmain\\tloop\\texecutes\\tthe\\ttraining\\tstep\\tover\\tand\\tover\\tagain\\t(\\nn_epochs\\n\\ttimes),\\tand\\tevery\\t100\\niterations\\tit\\tprints\\tout\\tthe\\tcurrent\\t\\nMean\\tSquared\\tError\\t(\\nmse\\n).\\tYou\\tshould\\tsee\\tthe\\tMSE\\tgo\\tdown\\tat\\nevery\\titeration.\\nn_epochs\\n\\t\\n=\\n\\t\\n1000\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\nscaled_housing_data_plus_bias\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"X\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\nhousing\\n.\\ntarget\\n.\\nreshape\\n(\\n-\\n1\\n,\\n\\t\\n1\\n),\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"y\"\\n)\\ntheta\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_uniform\\n([\\nn', '([\\nn\\n\\t\\n+\\n\\t\\n1\\n,\\n\\t\\n1\\n],\\n\\t\\n-\\n1.0\\n,\\n\\t\\n1.0\\n),\\n\\t\\nname\\n=\\n\"theta\"\\n)\\ny_pred\\n\\t\\n=\\n\\t\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\ntheta\\n,\\n\\t\\nname\\n=\\n\"predictions\"\\n)\\nerror\\n\\t\\n=\\n\\t\\ny_pred\\n\\t\\n-\\n\\t\\ny\\nmse\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\nerror\\n),\\n\\t\\nname\\n=\\n\"mse\"\\n)\\ngradients\\n\\t\\n=\\n\\t\\n2\\n/\\nm\\n\\t\\n*\\n\\t\\ntf\\n.\\nmatmul\\n(\\ntf\\n.\\ntranspose\\n(\\nX\\n),\\n\\t\\nerror\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\ntf\\n.\\nassign\\n(\\ntheta\\n,\\n\\t\\ntheta\\n\\t\\n-\\n\\t\\nlearning_rate\\n\\t\\n*\\n\\t\\ngradients\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ninit\\n)\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nepoch\\n\\t\\n%\\n\\t\\n100\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nprint\\n(\\n\"Epoch\"\\n,\\n\\t\\nepoch\\n,\\n\\t\\n\"MSE\\t=\"\\n,\\n\\t\\nmse\\n.\\neval\\n())\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n)\\n\\t\\t\\t\\t\\nbest_theta\\n\\t\\n=\\n\\t\\ntheta\\n.\\neval\\n()', 'Using\\tautodiff\\nThe\\t\\npreceding\\tcode\\tworks\\tfine,\\tbut\\tit\\trequires\\tmathematically\\tderiving\\tthe\\tgradients\\tfrom\\tthe\\t\\ncost\\nfunction\\t(MSE).\\tIn\\tthe\\tcase\\tof\\tLinear\\tRegression,\\tit\\tis\\treasonably\\teasy,\\tbut\\tif\\tyou\\thad\\tto\\tdo\\tthis\\twith\\tdeep\\nneural\\tnetworks\\tyou\\twould\\tget\\tquite\\ta\\theadache:\\tit\\twould\\tbe\\ttedious\\tand\\terror-prone.\\tYou\\tcould\\tuse\\nsymbolic\\tdifferentiation\\n\\tto\\t\\nautomatically\\tfind\\tthe\\tequations\\tfor\\tthe\\tpartial\\tderivatives\\tfor\\tyou,\\tbut\\tthe\\nresulting\\tcode\\twould\\tnot\\tnecessarily\\tbe\\tvery\\tefficient.\\nTo\\tunderstand\\twhy,\\tconsider\\tthe\\tfunction\\t\\nf\\n(\\nx\\n)=\\texp(exp(exp(\\nx\\n))).\\tIf\\tyou\\tknow\\tcalculus,\\tyou\\tcan\\tfigure\\tout\\nits\\tderivative\\t\\nf′\\n(\\nx\\n)\\t=\\texp(\\nx\\n)\\t×\\texp(exp(\\nx\\n))\\t×\\texp(exp(exp(\\nx\\n))).\\tIf\\tyou\\tcode\\t\\nf\\n(\\nx\\n)\\tand\\t\\nf′\\n(\\nx\\n)\\tseparately\\tand\\nexactly\\tas\\tthey\\tappear,\\tyour\\tcode\\twill\\tnot\\tbe\\tas\\tefficient\\tas\\tit\\tcould\\tbe.\\tA\\tmore\\tefficient\\tsolution\\twould\\nbe\\tto\\twrite\\ta\\tfunction\\tthat\\tfirst\\tcomputes\\texp(\\nx\\n),\\tthen\\texp(exp(\\nx\\n)),\\tthen\\texp(exp(exp(\\nx\\n))),\\tand\\treturns\\tall\\nthree.\\tThis\\tgives\\tyou\\t\\nf\\n(\\nx', 'f\\n(\\nx\\n)\\tdirectly\\t(the\\tthird\\tterm),\\tand\\tif\\tyou\\tneed\\tthe\\tderivative\\tyou\\tcan\\tjust\\tmultiply\\tall\\nthree\\tterms\\tand\\tyou\\tare\\tdone.\\tWith\\tthe\\tnaïve\\tapproach\\tyou\\twould\\thave\\thad\\tto\\tcall\\tthe\\t\\nexp\\n\\tfunction\\tnine\\ntimes\\tto\\tcompute\\tboth\\t\\nf\\n(\\nx\\n)\\tand\\t\\nf′\\n(\\nx\\n).\\tWith\\tthis\\tapproach\\tyou\\tjust\\tneed\\tto\\tcall\\tit\\tthree\\ttimes.\\nIt\\tgets\\tworse\\twhen\\tyour\\tfunction\\tis\\tdefined\\tby\\tsome\\tarbitrary\\tcode.\\tCan\\tyou\\tfind\\tthe\\tequation\\t(or\\tthe\\ncode)\\tto\\tcompute\\tthe\\tpartial\\tderivatives\\tof\\tthe\\tfollowing\\tfunction?\\tHint:\\tdon’t\\teven\\ttry.\\ndef\\n\\t\\nmy_func\\n(\\na\\n,\\n\\t\\nb\\n):\\n\\t\\t\\t\\t\\nz\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\t\\t\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n100\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nz\\n\\t\\n=\\n\\t\\na\\n\\t\\n*\\n\\t\\nnp\\n.\\ncos\\n(\\nz\\n\\t\\n+\\n\\t\\ni\\n)\\n\\t\\n+\\n\\t\\nz\\n\\t\\n*\\n\\t\\nnp\\n.\\nsin\\n(\\nb\\n\\t\\n-\\n\\t\\ni\\n)\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nz\\nFortunately,\\tTensorFlow’s\\tautodiff\\tfeature\\tcomes\\tto\\tthe\\trescue:\\tit\\tcan\\tautomatically\\tand\\tefficiently\\ncompute\\tthe\\tgradients\\tfor\\tyou.\\tSimply\\treplace\\tthe\\t\\ngradients\\t=\\t...\\n\\tline\\tin\\tthe\\tGradient\\tDescent\\tcode\\tin\\nthe\\tprevious\\tsection\\twith\\tthe\\tfollowing\\tline,\\tand\\tthe\\tcode\\twill\\tcontinue\\tto\\twork\\tjust\\tfine:\\ngradients\\n\\t\\n=\\n\\t\\ntf\\n.\\ngradients', 'gradients\\n(\\nmse\\n,\\n\\t\\n[\\ntheta\\n])[\\n0\\n]\\nThe\\t\\ngradients()\\n\\t\\nfunction\\ttakes\\tan\\top\\t(in\\tthis\\tcase\\t\\nmse\\n)\\tand\\ta\\tlist\\tof\\tvariables\\t(in\\tthis\\tcase\\tjust\\t\\ntheta\\n),\\nand\\tit\\tcreates\\ta\\tlist\\tof\\tops\\t(one\\tper\\tvariable)\\tto\\tcompute\\tthe\\tgradients\\tof\\tthe\\top\\twith\\tregards\\tto\\teach\\nvariable.\\tSo\\tthe\\t\\ngradients\\n\\tnode\\twill\\tcompute\\tthe\\tgradient\\tvector\\tof\\tthe\\tMSE\\twith\\tregards\\tto\\t\\ntheta\\n.\\nThere\\tare\\tfour\\tmain\\tapproaches\\tto\\tcomputing\\tgradients\\tautomatically.\\tThey\\tare\\tsummarized\\tin\\t\\nTable\\t9-2\\n.\\nTensorFlow\\tuses\\t\\nreverse-mode\\tautodiff\\n,\\twhich\\tis\\tperfect\\t(efficient\\tand\\taccurate)\\twhen\\tthere\\tare\\tmany\\ninputs\\tand\\tfew\\toutputs,\\tas\\tis\\toften\\tthe\\tcase\\tin\\tneural\\tnetworks.\\tIt\\tcomputes\\tall\\tthe\\tpartial\\tderivatives\\tof\\nthe\\toutputs\\twith\\tregards\\tto\\tall\\tthe\\tinputs\\tin\\tjust\\t\\nn\\noutputs\\n\\t+\\t1\\tgraph\\ttraversals.\\nTable\\t9-2.\\t\\nMain\\tsolutions\\tto\\tcompute\\tgradients\\tautomatically\\nTechnique\\nNb\\tof\\tgraph\\ttraversals\\tto\\tcompute\\tall\\ngradients\\nAccuracy\\nSupports\\tarbitrary\\ncode\\nComment\\nNumerical\\ndifferentiation\\nn\\ninputs\\n\\t+\\t1\\nLow\\nYes\\nTrivial\\tto\\timplement', 'Symbolic\\tdifferentiation\\nN/A\\nHigh\\nNo\\nBuilds\\ta\\tvery\\tdifferent', 'graph\\nForward-mode\\tautodiff\\nn\\ninputs\\nHigh\\nYes\\nUses\\t\\ndual\\tnumbers\\nReverse-mode\\tautodiff\\nn\\noutputs\\n\\t+\\t1\\nHigh\\nYes\\nImplemented\\tby\\nTensorFlow\\nIf\\tyou\\tare\\tinterested\\tin\\thow\\tthis\\tmagic\\t\\nworks,\\tcheck\\tout\\t\\nAppendix\\tD\\n.', 'Using\\tan\\tOptimizer\\nSo\\t\\nTensorFlow\\tcomputes\\tthe\\tgradients\\tfor\\tyou.\\tBut\\tit\\tgets\\teven\\teasier:\\tit\\talso\\tprovides\\ta\\tnumber\\tof\\noptimizers\\tout\\tof\\tthe\\tbox,\\tincluding\\ta\\tGradient\\tDescent\\toptimizer.\\tYou\\tcan\\tsimply\\treplace\\tthe\\tpreceding\\ngradients\\t=\\t...\\n\\tand\\t\\ntraining_op\\t=\\t...\\n\\tlines\\twith\\tthe\\tfollowing\\tcode,\\tand\\tonce\\tagain\\teverything\\nwill\\tjust\\twork\\tfine:\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nGradientDescentOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nmse\\n)\\nIf\\tyou\\twant\\tto\\tuse\\ta\\tdifferent\\ttype\\tof\\toptimizer,\\tyou\\tjust\\tneed\\tto\\tchange\\tone\\tline.\\tFor\\texample,\\tyou\\tcan\\tuse\\na\\tmomentum\\toptimizer\\t(which\\toften\\tconverges\\tmuch\\tfaster\\tthan\\tGradient\\tDescent;\\tsee\\t\\nChapter\\t11\\n)\\tby\\ndefining\\tthe\\toptimizer\\t\\nlike\\tthis:\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nMomentumOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmomentum\\n=\\n0.9\\n)', 'Feeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\nLet’s\\t\\ntry\\tto\\tmodify\\tthe\\tprevious\\tcode\\tto\\timplement\\t\\nMini-batch\\tGradient\\tDescent.\\tFor\\tthis,\\twe\\tneed\\ta\\tway\\nto\\treplace\\t\\nX\\n\\tand\\t\\ny\\n\\tat\\tevery\\titeration\\twith\\tthe\\tnext\\tmini-batch.\\tThe\\tsimplest\\tway\\tto\\tdo\\tthis\\tis\\tto\\tuse\\nplaceholder\\tnodes.\\tThese\\tnodes\\tare\\tspecial\\tbecause\\tthey\\tdon’t\\tactually\\tperform\\tany\\tcomputation,\\tthey\\njust\\toutput\\tthe\\tdata\\tyou\\ttell\\tthem\\tto\\toutput\\tat\\truntime.\\tThey\\tare\\ttypically\\tused\\tto\\tpass\\tthe\\ttraining\\tdata\\tto\\nTensorFlow\\tduring\\ttraining.\\tIf\\tyou\\tdon’t\\tspecify\\ta\\tvalue\\tat\\truntime\\tfor\\ta\\tplaceholder,\\tyou\\tget\\tan\\nexception.\\nTo\\tcreate\\ta\\tplaceholder\\tnode,\\t\\nyou\\tmust\\tcall\\tthe\\t\\nplaceholder()\\n\\tfunction\\tand\\tspecify\\tthe\\toutput\\ttensor’s\\ndata\\ttype.\\tOptionally,\\tyou\\tcan\\talso\\tspecify\\tits\\tshape,\\tif\\tyou\\twant\\tto\\tenforce\\tit.\\tIf\\tyou\\tspecify\\t\\nNone\\n\\tfor\\ta\\ndimension,\\tit\\tmeans\\t“any\\tsize.”\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tcreates\\ta\\tplaceholder\\tnode\\t\\nA\\n,\\tand\\talso\\ta\\nnode\\t\\nB\\t=\\tA\\t+\\t5\\n.\\tWhen\\twe\\tevaluate\\t\\nB\\n,\\twe\\tpass\\ta\\t\\nfeed_dict\\n\\t\\nto\\tthe\\t\\neval()\\n\\t\\nmethod\\tthat\\tspecifies\\tthe', 'value\\tof\\t\\nA\\n.\\tNote\\tthat\\t\\nA\\n\\tmust\\thave\\trank\\t2\\t(i.e.,\\tit\\tmust\\tbe\\ttwo-dimensional)\\tand\\tthere\\tmust\\tbe\\tthree\\tcolumns\\n(or\\telse\\tan\\texception\\tis\\traised),\\tbut\\tit\\tcan\\thave\\tany\\tnumber\\tof\\trows.\\n>>>\\t\\nA\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\n3\\n))\\n>>>\\t\\nB\\n\\t\\n=\\n\\t\\nA\\n\\t\\n+\\n\\t\\n5\\n>>>\\t\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n...\\t\\n\\t\\t\\t\\t\\nB_val_1\\n\\t\\n=\\n\\t\\nB\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nA\\n:\\n\\t\\n[[\\n1\\n,\\n\\t\\n2\\n,\\n\\t\\n3\\n]]})\\n...\\t\\n\\t\\t\\t\\t\\nB_val_2\\n\\t\\n=\\n\\t\\nB\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nA\\n:\\n\\t\\n[[\\n4\\n,\\n\\t\\n5\\n,\\n\\t\\n6\\n],\\n\\t\\n[\\n7\\n,\\n\\t\\n8\\n,\\n\\t\\n9\\n]]})\\n...\\n>>>\\t\\nprint\\n(\\nB_val_1\\n)\\n[[\\t6.\\t\\t7.\\t\\t8.]]\\n>>>\\t\\nprint\\n(\\nB_val_2\\n)\\n[[\\t\\t9.\\t\\t10.\\t\\t11.]\\n\\t[\\t12.\\t\\t13.\\t\\t14.]]\\nNOTE\\nYou\\tcan\\tactually\\tfeed\\tthe\\toutput\\tof\\t\\nany\\n\\toperations,\\tnot\\tjust\\tplaceholders.\\tIn\\tthis\\tcase\\tTensorFlow\\tdoes\\tnot\\ttry\\tto\\tevaluate\\tthese\\noperations;\\tit\\tuses\\tthe\\tvalues\\tyou\\tfeed\\tit.\\nTo\\timplement\\tMini-batch\\tGradient\\tDescent,\\twe\\tonly\\tneed\\tto\\ttweak\\tthe\\texisting\\tcode\\tslightly.\\tFirst\\tchange\\nthe\\tdefinition\\tof\\t\\nX\\n\\tand\\t\\ny\\n\\tin\\tthe\\tconstruction\\tphase\\tto\\tmake\\tthem\\t\\nplaceholder\\tnodes:\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(', '(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn\\n\\t\\n+\\n\\t\\n1\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\n1\\n),\\n\\t\\nname\\n=\\n\"y\"\\n)\\nThen\\tdefine\\tthe\\tbatch\\tsize\\tand\\tcompute\\tthe\\ttotal\\tnumber\\tof\\tbatches:\\nbatch_size\\n\\t\\n=\\n\\t\\n100\\nn_batches\\n\\t\\n=\\n\\t\\nint\\n(\\nnp\\n.\\nceil\\n(\\nm\\n\\t\\n/\\n\\t\\nbatch_size\\n))\\nFinally,\\tin\\tthe\\texecution\\tphase,\\tfetch\\tthe\\tmini-batches\\tone\\tby\\tone,\\tthen\\tprovide\\tthe\\tvalue\\tof\\t\\nX\\n\\tand\\t\\ny\\n\\tvia\\nthe\\t\\nfeed_dict\\n\\tparameter\\twhen\\tevaluating\\ta\\tnode\\tthat\\tdepends\\ton\\teither\\tof\\tthem.\\ndef\\n\\t\\nfetch_batch\\n(\\nepoch\\n,\\n\\t\\nbatch_index\\n,\\n\\t\\nbatch_size\\n):', '[\\n...\\n]\\n\\t\\n#\\tload\\tthe\\tdata\\tfrom\\tdisk\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nX_batch\\n,\\n\\t\\ny_batch\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ninit\\n)\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nbatch_index\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_batches\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nfetch_batch\\n(\\nepoch\\n,\\n\\t\\nbatch_index\\n,\\n\\t\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\nbest_theta\\n\\t\\n=\\n\\t\\ntheta\\n.\\neval\\n()\\nNOTE\\nWe\\tdon’t\\tneed\\tto\\tpass\\tthe\\tvalue\\tof\\t\\nX\\n\\tand\\t\\ny\\n\\twhen\\tevaluating\\t\\ntheta\\n\\tsince\\tit\\tdoes\\tnot\\tdepend\\ton\\t\\neither\\tof\\tthem.', 'Saving\\tand\\tRestoring\\tModels\\nOnce\\t\\nyou\\thave\\ttrained\\tyour\\tmodel,\\tyou\\tshould\\tsave\\tits\\tparameters\\tto\\tdisk\\tso\\tyou\\tcan\\tcome\\tback\\tto\\tit\\nwhenever\\tyou\\twant,\\tuse\\tit\\tin\\tanother\\tprogram,\\tcompare\\tit\\tto\\tother\\tmodels,\\tand\\tso\\ton.\\tMoreover,\\tyou\\nprobably\\twant\\tto\\tsave\\tcheckpoints\\tat\\tregular\\tintervals\\tduring\\ttraining\\tso\\tthat\\tif\\tyour\\tcomputer\\tcrashes\\nduring\\ttraining\\tyou\\tcan\\tcontinue\\tfrom\\tthe\\tlast\\tcheckpoint\\trather\\tthan\\tstart\\tover\\tfrom\\tscratch.\\nTensorFlow\\tmakes\\tsaving\\tand\\trestoring\\ta\\tmodel\\tvery\\teasy.\\tJust\\tcreate\\ta\\t\\nSaver\\n\\tnode\\t\\nat\\tthe\\tend\\tof\\tthe\\nconstruction\\tphase\\t(after\\tall\\tvariable\\tnodes\\tare\\tcreated);\\tthen,\\tin\\tthe\\texecution\\tphase,\\tjust\\tcall\\tits\\t\\nsave()\\nmethod\\t\\nwhenever\\tyou\\twant\\tto\\tsave\\tthe\\tmodel,\\tpassing\\tit\\tthe\\tsession\\tand\\tpath\\tof\\t\\nthe\\tcheckpoint\\tfile:\\n[\\n...\\n]\\ntheta\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_uniform\\n([\\nn\\n\\t\\n+\\n\\t\\n1\\n,\\n\\t\\n1\\n],\\n\\t\\n-\\n1.0\\n,\\n\\t\\n1.0\\n),\\n\\t\\nname\\n=\\n\"theta\"\\n)\\n[\\n...\\n]\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.', 'sess\\n.\\nrun\\n(\\ninit\\n)\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nepoch\\n\\t\\n%\\n\\t\\n100\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\n#\\tcheckpoint\\tevery\\t100\\tepochs\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model.ckpt\"\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n)\\n\\t\\t\\t\\t\\nbest_theta\\n\\t\\n=\\n\\t\\ntheta\\n.\\neval\\n()\\n\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model_final.ckpt\"\\n)\\nRestoring\\ta\\tmodel\\tis\\tjust\\tas\\teasy:\\tyou\\tcreate\\ta\\t\\nSaver\\n\\tat\\tthe\\tend\\tof\\tthe\\tconstruction\\tphase\\tjust\\tlike\\tbefore,\\nbut\\tthen\\tat\\tthe\\tbeginning\\tof\\tthe\\texecution\\tphase,\\tinstead\\tof\\tinitializing\\tthe\\tvariables\\t\\nusing\\tthe\\t\\ninit\\n\\tnode,\\nyou\\tcall\\tthe\\t\\nrestore()\\n\\tmethod\\t\\nof\\tthe\\t\\nSaver\\n\\tobject:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\n[\\n...\\n]\\nBy\\tdefault\\ta\\t\\nSaver\\n\\tsaves\\tand\\trestores\\tall\\tvariables\\tunder\\ttheir\\town\\tname,\\tbut\\tif\\tyou\\tneed\\tmore\\tcontrol,\\nyou\\tcan\\tspecify\\twhich\\tvariables\\tto\\tsave\\tor\\trestore,\\tand\\twhat\\tnames\\tto\\tuse.\\tFor\\texample,\\tthe\\tfollowing\\nSaver\\n\\twill\\tsave\\tor\\trestore\\tonly\\tthe', 'theta\\n\\tvariable\\tunder\\tthe\\t\\nname\\t\\nweights\\n:\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n({\\n\"weights\"\\n:\\n\\t\\ntheta\\n})\\nBy\\tdefault,\\tthe\\t\\nsave()\\n\\tmethod\\talso\\tsaves\\tthe\\tstructure\\tof\\tthe\\tgraph\\tin\\ta\\tsecond\\tfile\\twith\\tthe\\tsame\\tname\\nplus\\ta\\t\\n.meta\\n\\textension.\\tYou\\tcan\\tload\\tthis\\tgraph\\tstructure\\tusing\\t\\ntf.train.import_meta_graph()\\n.\\tThis\\nadds\\tthe\\tgraph\\tto\\tthe\\tdefault\\tgraph,\\tand\\treturns\\ta\\t\\nSaver\\n\\tinstance\\tthat\\tyou\\tcan\\tthen\\tuse\\tto\\trestore\\tthe\\ngraph’s\\tstate\\t(i.e.,\\tthe\\tvariable\\tvalues):\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nimport_meta_graph\\n(\\n\"/tmp/my_model_final.ckpt.meta\"\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\n[\\n...\\n]', 'This\\tallows\\tyou\\tto\\tfully\\trestore\\ta\\tsaved\\tmodel,\\tincluding\\tboth\\tthe\\tgraph\\tstructure\\tand\\tthe\\tvariable\\tvalues,\\nwithout\\thaving\\tto\\tsearch\\tfor\\tthe\\tcode\\tthat\\tbuilt\\tit.', 'Visualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\nSo\\t\\nnow\\twe\\thave\\ta\\tcomputation\\tgraph\\tthat\\ttrains\\ta\\tLinear\\tRegression\\tmodel\\tusing\\tMini-batch\\tGradient\\nDescent,\\tand\\twe\\tare\\tsaving\\tcheckpoints\\tat\\tregular\\tintervals.\\tSounds\\tsophisticated,\\tdoesn’t\\tit?\\tHowever,\\nwe\\tare\\tstill\\trelying\\ton\\tthe\\t\\nprint()\\n\\tfunction\\tto\\tvisualize\\tprogress\\tduring\\ttraining.\\tThere\\tis\\ta\\tbetter\\tway:\\nenter\\tTensorBoard.\\tIf\\tyou\\tfeed\\tit\\tsome\\ttraining\\tstats,\\tit\\twill\\tdisplay\\tnice\\tinteractive\\tvisualizations\\tof\\nthese\\tstats\\tin\\tyour\\tweb\\tbrowser\\t(e.g.,\\tlearning\\tcurves).\\tYou\\tcan\\talso\\tprovide\\tit\\tthe\\tgraph’s\\tdefinition\\tand\\nit\\twill\\tgive\\tyou\\ta\\tgreat\\tinterface\\tto\\tbrowse\\tthrough\\tit.\\tThis\\tis\\tvery\\tuseful\\tto\\tidentify\\terrors\\tin\\tthe\\tgraph,\\tto\\nfind\\tbottlenecks,\\tand\\tso\\ton.\\nThe\\tfirst\\tstep\\tis\\tto\\ttweak\\tyour\\tprogram\\ta\\tbit\\tso\\tit\\twrites\\tthe\\tgraph\\tdefinition\\tand\\tsome\\ttraining\\tstats\\t—\\tfor\\nexample,\\tthe\\ttraining\\terror\\t(MSE)\\t—\\tto\\ta\\tlog\\tdirectory\\tthat\\tTensorBoard\\twill\\tread\\tfrom.\\tYou\\tneed\\tto\\tuse', 'a\\tdifferent\\tlog\\tdirectory\\tevery\\ttime\\tyou\\trun\\tyour\\tprogram,\\tor\\telse\\tTensorBoard\\twill\\tmerge\\tstats\\tfrom\\ndifferent\\truns,\\twhich\\twill\\tmess\\tup\\tthe\\tvisualizations.\\tThe\\tsimplest\\tsolution\\tfor\\tthis\\tis\\tto\\tinclude\\ta\\ntimestamp\\tin\\tthe\\tlog\\tdirectory\\tname.\\tAdd\\tthe\\tfollowing\\tcode\\tat\\tthe\\tbeginning\\tof\\tthe\\tprogram:\\nfrom\\n\\t\\ndatetime\\n\\t\\nimport\\n\\t\\ndatetime\\nnow\\n\\t\\n=\\n\\t\\ndatetime\\n.\\nutcnow\\n()\\n.\\nstrftime\\n(\\n\"\\n%Y%m%d%H%M%S\\n\"\\n)\\nroot_logdir\\n\\t\\n=\\n\\t\\n\"tf_logs\"\\nlogdir\\n\\t\\n=\\n\\t\\n\"{}/run-{}/\"\\n.\\nformat\\n(\\nroot_logdir\\n,\\n\\t\\nnow\\n)\\nNext,\\tadd\\tthe\\tfollowing\\tcode\\tat\\tthe\\tvery\\tend\\tof\\tthe\\t\\nconstruction\\tphase:\\nmse_summary\\n\\t\\n=\\n\\t\\ntf\\n.\\nsummary\\n.\\nscalar\\n(\\n\\'MSE\\'\\n,\\n\\t\\nmse\\n)\\nfile_writer\\n\\t\\n=\\n\\t\\ntf\\n.\\nsummary\\n.\\nFileWriter\\n(\\nlogdir\\n,\\n\\t\\ntf\\n.\\nget_default_graph\\n())\\nThe\\tfirst\\tline\\tcreates\\ta\\tnode\\tin\\tthe\\tgraph\\tthat\\twill\\tevaluate\\tthe\\tMSE\\tvalue\\tand\\twrite\\tit\\tto\\ta\\tTensorBoard-\\ncompatible\\tbinary\\tlog\\tstring\\tcalled\\ta\\t\\nsummary\\n.\\tThe\\tsecond\\tline\\tcreates\\ta\\t\\nFileWriter\\n\\tthat\\tyou\\twill\\tuse', 'to\\twrite\\tsummaries\\tto\\tlogfiles\\tin\\tthe\\tlog\\tdirectory.\\tThe\\tfirst\\tparameter\\tindicates\\tthe\\tpath\\tof\\tthe\\tlog\\ndirectory\\t(in\\tthis\\tcase\\tsomething\\tlike\\t\\ntf_logs/run-20160906091959/\\n,\\trelative\\tto\\tthe\\tcurrent\\tdirectory).\\nThe\\tsecond\\t(optional)\\tparameter\\tis\\tthe\\tgraph\\tyou\\twant\\tto\\tvisualize.\\tUpon\\tcreation,\\tthe\\t\\nFileWriter\\ncreates\\tthe\\tlog\\tdirectory\\tif\\tit\\tdoes\\tnot\\talready\\texist\\t(and\\tits\\tparent\\tdirectories\\tif\\tneeded),\\tand\\twrites\\tthe\\ngraph\\tdefinition\\tin\\ta\\tbinary\\tlogfile\\tcalled\\tan\\t\\nevents\\tfile\\n.\\nNext\\tyou\\tneed\\tto\\tupdate\\tthe\\texecution\\tphase\\tto\\tevaluate\\tthe\\t\\nmse_summary\\n\\tnode\\tregularly\\tduring\\ttraining\\n(e.g.,\\tevery\\t10\\tmini-batches).\\tThis\\twill\\toutput\\ta\\tsummary\\tthat\\tyou\\tcan\\tthen\\twrite\\tto\\tthe\\tevents\\tfile\\tusing\\nthe\\t\\nfile_writer\\n.\\tHere\\tis\\tthe\\tupdated\\tcode:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\nfor\\n\\t\\nbatch_index\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_batches\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nfetch_batch\\n(\\nepoch\\n,\\n\\t\\nbatch_index\\n,\\n\\t\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nbatch_index\\n\\t\\n%\\n\\t\\n10\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsummary_str\\n\\t\\n=\\n\\t\\nmse_summary\\n.\\neval\\n(\\nfeed_dict\\n=', '=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nstep\\n\\t\\n=\\n\\t\\nepoch\\n\\t\\n*\\n\\t\\nn_batches\\n\\t\\n+\\n\\t\\nbatch_index\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfile_writer\\n.\\nadd_summary\\n(\\nsummary_str\\n,\\n\\t\\nstep\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\n[\\n...\\n]', 'WARNING\\nAvoid\\tlogging\\ttraining\\tstats\\tat\\tevery\\tsingle\\ttraining\\tstep,\\tas\\tthis\\twould\\tsignificantly\\tslow\\tdown\\ttraining.\\nFinally,\\tyou\\twant\\tto\\tclose\\tthe\\t\\nFileWriter\\n\\tat\\tthe\\tend\\tof\\t\\nthe\\tprogram:\\nfile_writer\\n.\\nclose\\n()\\nNow\\trun\\tthis\\tprogram:\\tit\\twill\\tcreate\\tthe\\tlog\\tdirectory\\tand\\twrite\\tan\\tevents\\tfile\\tin\\tthis\\tdirectory,\\tcontaining\\nboth\\tthe\\tgraph\\tdefinition\\tand\\tthe\\tMSE\\tvalues.\\tOpen\\tup\\ta\\tshell\\tand\\tgo\\tto\\tyour\\tworking\\tdirectory,\\tthen\\ttype\\nls\\t-l\\ttf_logs/run*\\n\\tto\\tlist\\tthe\\tcontents\\tof\\tthe\\tlog\\tdirectory:\\n$\\tcd\\t$ML_PATH\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t#\\tYour\\tML\\tworking\\tdirectory\\t(e.g.,\\t$HOME/ml)\\n$\\tls\\t-l\\ttf_logs/run*\\ntotal\\t40\\n-rw-r--r--\\t1\\tageron\\tstaff\\t18620\\tSep\\t6\\t11:10\\tevents.out.tfevents.1472553182.mymac\\nIf\\tyou\\trun\\tthe\\tprogram\\ta\\tsecond\\ttime,\\tyou\\tshould\\tsee\\ta\\tsecond\\tdirectory\\tin\\tthe\\t\\ntf_logs/\\n\\tdirectory:\\n$\\tls\\t-l\\ttf_logs/\\ntotal\\t0\\ndrwxr-xr-x\\t\\t3\\tageron\\t\\tstaff\\t\\t102\\tSep\\t\\t6\\t10:07\\trun-20160906091959\\ndrwxr-xr-x\\t\\t3\\tageron\\t\\tstaff\\t\\t102\\tSep\\t\\t6\\t10:22\\trun-20160906092202', 'Great!\\tNow\\tit’s\\ttime\\tto\\tfire\\tup\\tthe\\tTensorBoard\\tserver.\\tYou\\tneed\\tto\\tactivate\\tyour\\tvirtualenv\\tenvironment\\nif\\tyou\\tcreated\\tone,\\tthen\\tstart\\tthe\\tserver\\tby\\trunning\\tthe\\t\\ntensorboard\\n\\tcommand,\\tpointing\\tit\\tto\\tthe\\troot\\tlog\\ndirectory.\\tThis\\tstarts\\tthe\\tTensorBoard\\tweb\\tserver,\\tlistening\\ton\\tport\\t6006\\t(which\\tis\\t“goog”\\twritten\\tupside\\ndown):\\n$\\tsource\\tenv/bin/activate\\n$\\ttensorboard\\t--logdir\\ttf_logs/\\nStarting\\tTensorBoard\\t\\ton\\tport\\t6006\\n(You\\tcan\\tnavigate\\tto\\thttp://0.0.0.0:6006)\\nNext\\topen\\ta\\tbrowser\\tand\\tgo\\tto\\t\\nhttp://0.0.0.0:6006/\\n\\t(or\\t\\nhttp://localhost:6006/\\n).\\tWelcome\\tto\\nTensorBoard!\\tIn\\tthe\\tEvents\\ttab\\tyou\\tshould\\tsee\\tMSE\\ton\\tthe\\tright.\\tIf\\tyou\\tclick\\ton\\tit,\\tyou\\twill\\tsee\\ta\\tplot\\tof\\nthe\\tMSE\\tduring\\ttraining,\\tfor\\tboth\\truns\\t(\\nFigure\\t9-3\\n).\\tYou\\tcan\\tcheck\\tor\\tuncheck\\tthe\\truns\\tyou\\twant\\tto\\tsee,\\nzoom\\tin\\tor\\tout,\\thover\\tover\\tthe\\tcurve\\tto\\tget\\tdetails,\\tand\\tso\\ton.', 'Figure\\t9-3.\\t\\nVisualizing\\ttraining\\tstats\\tusing\\tTensorBoard\\nNow\\tclick\\ton\\tthe\\tGraphs\\ttab.\\tYou\\tshould\\tsee\\tthe\\tgraph\\tshown\\tin\\t\\nFigure\\t9-4\\n.\\nTo\\treduce\\tclutter,\\tthe\\tnodes\\tthat\\thave\\t\\nmany\\t\\nedges\\n\\t(i.e.,\\tconnections\\tto\\tother\\tnodes)\\tare\\tseparated\\tout\\tto\\tan\\nauxiliary\\tarea\\ton\\tthe\\tright\\t(you\\tcan\\tmove\\ta\\tnode\\tback\\tand\\tforth\\tbetween\\tthe\\tmain\\tgraph\\tand\\tthe\\tauxiliary\\narea\\tby\\tright-clicking\\ton\\tit).\\tSome\\tparts\\tof\\tthe\\tgraph\\tare\\talso\\tcollapsed\\tby\\tdefault.\\tFor\\texample,\\ttry\\nhovering\\tover\\tthe\\t\\ngradients\\n\\tnode,\\tthen\\tclick\\ton\\tthe\\t\\n\\ticon\\tto\\texpand\\tthis\\tsubgraph.\\tNext,\\tin\\tthis\\nsubgraph,\\ttry\\texpanding\\tthe\\t\\nmse_grad\\n\\tsubgraph.\\nFigure\\t9-4.\\t\\nVisualizing\\tthe\\tgraph\\tusing\\tTensorBoard\\nTIP\\nIf\\tyou\\twant\\tto\\ttake\\ta\\tpeek\\tat\\tthe\\tgraph\\tdirectly\\twithin\\tJupyter,\\tyou\\tcan\\tuse\\tthe\\t\\nshow_graph()\\n\\t\\nfunction\\tavailable\\tin\\tthe\\tnotebook\\nfor\\tthis\\tchapter.\\tIt\\twas\\toriginally\\twritten\\tby\\tA.\\tMordvintsev\\tin\\this\\tgreat\\t\\ndeepdream\\ttutorial\\tnotebook\\n.\\tAnother\\toption\\tis\\tto\\tinstall\\nE.\\tJang’s\\t\\nTensorFlow\\tdebugger\\ttool\\n\\twhich\\tincludes\\ta\\tJupyter\\textension', 'for\\tgraph\\tvisualization\\t(and\\tmore).', 'Name\\tScopes\\nWhen\\t\\ndealing\\twith\\tmore\\tcomplex\\tmodels\\tsuch\\tas\\tneural\\tnetworks,\\tthe\\tgraph\\tcan\\teasily\\tbecome\\tcluttered\\nwith\\tthousands\\tof\\tnodes.\\tTo\\tavoid\\tthis,\\tyou\\tcan\\tcreate\\t\\nname\\tscopes\\n\\tto\\tgroup\\trelated\\tnodes.\\tFor\\texample,\\nlet’s\\tmodify\\tthe\\tprevious\\tcode\\tto\\tdefine\\tthe\\t\\nerror\\n\\tand\\t\\nmse\\n\\tops\\twithin\\ta\\tname\\t\\nscope\\tcalled\\t\\n\"loss\"\\n:\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"loss\"\\n)\\n\\t\\nas\\n\\t\\nscope\\n:\\n\\t\\t\\t\\t\\nerror\\n\\t\\n=\\n\\t\\ny_pred\\n\\t\\n-\\n\\t\\ny\\n\\t\\t\\t\\t\\nmse\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\nerror\\n),\\n\\t\\nname\\n=\\n\"mse\"\\n)\\nThe\\tname\\tof\\teach\\top\\tdefined\\twithin\\tthe\\tscope\\tis\\tnow\\tprefixed\\twith\\t\\n\"loss/\"\\n:\\n>>>\\t\\nprint\\n(\\nerror\\n.\\nop\\n.\\nname\\n)\\nloss/sub\\n>>>\\t\\nprint\\n(\\nmse\\n.\\nop\\n.\\nname\\n)\\nloss/mse\\nIn\\tTensorBoard,\\tthe\\t\\nmse\\n\\tand\\t\\nerror\\n\\tnodes\\tnow\\tappear\\tinside\\tthe\\t\\nloss\\n\\tnamespace,\\twhich\\tappears\\ncollapsed\\tby\\tdefault\\t(\\nFigure\\t9-5\\n).\\nFigure\\t9-5.\\t\\nA\\tcollapsed\\tnamescope\\tin\\tTensorBoard', 'Modularity\\nSuppose\\t\\nyou\\twant\\tto\\tcreate\\ta\\tgraph\\tthat\\tadds\\tthe\\toutput\\tof\\t\\ntwo\\t\\nrectified\\tlinear\\tunits\\n\\t(ReLU).\\tA\\tReLU\\ncomputes\\ta\\tlinear\\tfunction\\tof\\tthe\\tinputs,\\tand\\toutputs\\tthe\\tresult\\tif\\tit\\tis\\tpositive,\\tand\\t0\\totherwise,\\tas\\tshown\\nin\\t\\nEquation\\t9-1\\n.\\nEquation\\t9-1.\\t\\nRectified\\tlinear\\tunit\\nThe\\tfollowing\\tcode\\tdoes\\tthe\\tjob,\\tbut\\tit’s\\tquite\\t\\nrepetitive:\\nn_features\\n\\t\\n=\\n\\t\\n3\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nw1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_normal\\n((\\nn_features\\n,\\n\\t\\n1\\n)),\\n\\t\\nname\\n=\\n\"weights1\"\\n)\\nw2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_normal\\n((\\nn_features\\n,\\n\\t\\n1\\n)),\\n\\t\\nname\\n=\\n\"weights2\"\\n)\\nb1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"bias1\"\\n)\\nb2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"bias2\"\\n)\\nz1\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd\\n(\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\nw1\\n),\\n\\t\\nb1\\n,\\n\\t\\nname\\n=\\n\"z1\"\\n)\\nz2\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd\\n(\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\nw2\\n),\\n\\t\\nb2\\n,\\n\\t\\nname\\n=\\n\"z2\"\\n)\\nrelu1\\n\\t\\n=\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz1\\n,\\n\\t\\n0.\\n,\\n\\t\\nname\\n=\\n\"relu1\"\\n)\\nrelu2\\n\\t\\n=\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz1\\n,\\n\\t\\n0.\\n,\\n\\t\\nname\\n=\\n\"relu2\"\\n)\\noutput\\n\\t\\n=', '=\\n\\t\\ntf\\n.\\nadd\\n(\\nrelu1\\n,\\n\\t\\nrelu2\\n,\\n\\t\\nname\\n=\\n\"output\"\\n)\\nSuch\\trepetitive\\tcode\\tis\\thard\\tto\\tmaintain\\tand\\terror-prone\\t(in\\tfact,\\tthis\\tcode\\tcontains\\ta\\tcut-and-paste\\terror;\\ndid\\tyou\\tspot\\tit?).\\tIt\\twould\\tbecome\\teven\\tworse\\tif\\tyou\\twanted\\tto\\tadd\\ta\\tfew\\tmore\\tReLUs.\\tFortunately,\\nTensorFlow\\tlets\\tyou\\tstay\\t\\nDRY\\t(Don’t\\tRepeat\\tYourself):\\tsimply\\tcreate\\ta\\tfunction\\tto\\tbuild\\ta\\tReLU.\\tThe\\nfollowing\\tcode\\tcreates\\tfive\\tReLUs\\tand\\t\\noutputs\\ttheir\\tsum\\t(note\\tthat\\t\\nadd_n()\\n\\tcreates\\tan\\toperation\\tthat\\twill\\ncompute\\tthe\\tsum\\tof\\ta\\tlist\\tof\\ttensors):\\ndef\\n\\t\\nrelu\\n(\\nX\\n):\\n\\t\\t\\t\\t\\nw_shape\\n\\t\\n=\\n\\t\\n(\\nint\\n(\\nX\\n.\\nget_shape\\n()[\\n1\\n]),\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\nw\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_normal\\n(\\nw_shape\\n),\\n\\t\\nname\\n=\\n\"weights\"\\n)\\n\\t\\t\\t\\t\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"bias\"\\n)\\n\\t\\t\\t\\t\\nz\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd\\n(\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\nw\\n),\\n\\t\\nb\\n,\\n\\t\\nname\\n=\\n\"z\"\\n)\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz\\n,\\n\\t\\n0.\\n,\\n\\t\\nname\\n=\\n\"relu\"\\n)\\nn_features\\n\\t\\n=\\n\\t\\n3\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nrelus\\n\\t\\n=\\n\\t\\n[\\nrelu\\n(\\nX\\n)\\n\\t\\nfor\\n\\t\\ni\\n\\t\\nin', 'i\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n)]\\noutput\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n(\\nrelus\\n,\\n\\t\\nname\\n=\\n\"output\"\\n)\\nNote\\tthat\\twhen\\tyou\\tcreate\\ta\\tnode,\\tTensorFlow\\tchecks\\twhether\\tits\\tname\\talready\\texists,\\tand\\tif\\tit\\tdoes\\tit\\nappends\\tan\\tunderscore\\tfollowed\\tby\\tan\\tindex\\tto\\tmake\\tthe\\tname\\tunique.\\tSo\\tthe\\tfirst\\tReLU\\tcontains\\tnodes\\nnamed\\t\\n\"weights\"\\n,\\t\\n\"bias\"\\n,\\t\\n\"z\"\\n,\\tand\\t\\n\"relu\"\\n\\t(plus\\tmany\\tmore\\tnodes\\twith\\ttheir\\tdefault\\tname,\\tsuch\\tas\\n\"MatMul\"\\n);\\tthe\\tsecond\\tReLU\\tcontains\\tnodes\\tnamed\\t\\n\"weights_1\"\\n,\\t\\n\"bias_1\"\\n,\\tand\\tso\\ton;\\tthe\\tthird\\tReLU\\ncontains\\tnodes\\tnamed\\t\\n\"weights_2\"\\n,\\t\\n\"bias_2\"\\n,\\tand\\tso\\ton.\\tTensorBoard\\tidentifies\\tsuch\\tseries\\tand\\ncollapses\\tthem\\ttogether\\tto\\treduce\\tclutter\\t(as\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t9-6\\n).', 'Figure\\t9-6.\\t\\nCollapsed\\tnode\\tseries\\nUsing\\tname\\tscopes,\\tyou\\tcan\\tmake\\tthe\\tgraph\\tmuch\\tclearer.\\tSimply\\tmove\\tall\\tthe\\tcontent\\tof\\tthe\\t\\nrelu()\\nfunction\\tinside\\ta\\tname\\tscope.\\t\\nFigure\\t9-7\\n\\tshows\\tthe\\tresulting\\tgraph.\\tNotice\\tthat\\tTensorFlow\\talso\\tgives\\tthe\\nname\\tscopes\\tunique\\tnames\\tby\\tappending\\t\\n_1\\n,\\t\\n_2\\n,\\tand\\tso\\t\\non.\\ndef\\n\\t\\nrelu\\n(\\nX\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"relu\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]', 'Figure\\t9-7.\\t\\nA\\tclearer\\tgraph\\tusing\\tname-scoped\\tunits', 'Sharing\\tVariables\\nIf\\t\\nyou\\twant\\tto\\tshare\\ta\\tvariable\\tbetween\\tvarious\\tcomponents\\tof\\tyour\\tgraph,\\tone\\tsimple\\toption\\tis\\tto\\tcreate\\nit\\tfirst,\\tthen\\tpass\\tit\\tas\\ta\\tparameter\\tto\\tthe\\tfunctions\\tthat\\tneed\\tit.\\tFor\\texample,\\tsuppose\\tyou\\twant\\tto\\tcontrol\\nthe\\tReLU\\tthreshold\\t(currently\\thardcoded\\tto\\t0)\\tusing\\ta\\tshared\\t\\nthreshold\\n\\t\\nvariable\\tfor\\tall\\tReLUs.\\tYou\\ncould\\tjust\\tcreate\\tthat\\tvariable\\tfirst,\\tand\\tthen\\tpass\\tit\\tto\\tthe\\t\\nrelu()\\n\\tfunction:\\ndef\\n\\t\\nrelu\\n(\\nX\\n,\\n\\t\\nthreshold\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"relu\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz\\n,\\n\\t\\nthreshold\\n,\\n\\t\\nname\\n=\\n\"max\"\\n)\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"threshold\"\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nrelus\\n\\t\\n=\\n\\t\\n[\\nrelu\\n(\\nX\\n,\\n\\t\\nthreshold\\n)\\n\\t\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n)]\\noutput\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n(\\nrelus\\n,\\n\\t\\nname\\n=\\n\"output\"\\n)\\nThis\\tworks\\tfine:\\t\\nnow\\tyou\\tcan\\tcontrol\\tthe\\tthreshold\\tfor\\tall\\tReLUs\\tusing\\tthe\\t\\nthreshold\\n\\tvariable.', 'However,\\tif\\tthere\\tare\\tmany\\tshared\\tparameters\\tsuch\\tas\\tthis\\tone,\\tit\\twill\\tbe\\tpainful\\tto\\thave\\tto\\tpass\\tthem\\naround\\tas\\tparameters\\tall\\tthe\\ttime.\\tMany\\tpeople\\tcreate\\ta\\tPython\\tdictionary\\tcontaining\\tall\\tthe\\tvariables\\tin\\ntheir\\tmodel,\\tand\\tpass\\tit\\taround\\tto\\tevery\\tfunction.\\tOthers\\tcreate\\ta\\tclass\\tfor\\teach\\tmodule\\t(e.g.,\\ta\\t\\nReLU\\n\\tclass\\nusing\\tclass\\tvariables\\tto\\thandle\\tthe\\tshared\\tparameter).\\tYet\\tanother\\toption\\tis\\tto\\tset\\tthe\\tshared\\tvariable\\tas\\nan\\tattribute\\tof\\tthe\\t\\nrelu()\\n\\tfunction\\tupon\\tthe\\tfirst\\tcall,\\tlike\\tso:\\ndef\\n\\t\\nrelu\\n(\\nX\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"relu\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nnot\\n\\t\\nhasattr\\n(\\nrelu\\n,\\n\\t\\n\"threshold\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nrelu\\n.\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"threshold\"\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz\\n,\\n\\t\\nrelu\\n.\\nthreshold\\n,\\n\\t\\nname\\n=\\n\"max\"\\n)\\nTensorFlow\\t\\noffers\\tanother\\toption,\\twhich\\tmay\\tlead\\tto\\tslightly\\tcleaner\\tand\\tmore\\tmodular\\tcode\\tthan\\tthe\\nprevious\\tsolutions.\\n5\\n\\tThis\\tsolution\\tis\\ta\\tbit\\ttricky\\tto\\tunderstand\\tat\\tfirst,\\tbut\\tsince\\tit\\tis\\tused\\ta\\tlot\\tin', 'TensorFlow\\tit\\tis\\tworth\\tgoing\\tinto\\ta\\tbit\\tof\\tdetail.\\tThe\\tidea\\tis\\tto\\tuse\\tthe\\t\\nget_variable()\\n\\t\\nfunction\\tto\\ncreate\\tthe\\tshared\\tvariable\\tif\\tit\\tdoes\\tnot\\texist\\tyet,\\tor\\treuse\\tit\\tif\\tit\\talready\\texists.\\tThe\\tdesired\\tbehavior\\n(creating\\tor\\treusing)\\tis\\tcontrolled\\tby\\tan\\tattribute\\tof\\tthe\\t\\ncurrent\\t\\nvariable_scope()\\n.\\tFor\\texample,\\tthe\\nfollowing\\tcode\\twill\\tcreate\\ta\\tvariable\\tnamed\\t\\n\"relu/threshold\"\\n\\t(as\\ta\\tscalar,\\tsince\\t\\nshape=()\\n,\\tand\\tusing\\n0.0\\n\\tas\\tthe\\tinitial\\tvalue):\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n):\\n\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ninitializer\\n=\\ntf\\n.\\nconstant_initializer\\n(\\n0.0\\n))\\nNote\\tthat\\tif\\tthe\\tvariable\\thas\\talready\\tbeen\\tcreated\\tby\\tan\\tearlier\\tcall\\tto\\t\\nget_variable()\\n,\\tthis\\tcode\\twill\\nraise\\tan\\texception.\\tThis\\tbehavior\\tprevents\\treusing\\tvariables\\tby\\tmistake.\\tIf\\tyou\\twant\\tto\\treuse\\ta\\tvariable,\\nyou\\tneed\\tto\\texplicitly\\tsay\\tso\\tby\\tsetting\\tthe\\tvariable\\tscope’s\\t\\nreuse\\n\\tattribute\\tto\\t\\nTrue\\n\\t(in\\twhich\\tcase\\tyou', 'don’t\\thave\\tto\\tspecify\\tthe\\tshape\\tor\\tthe\\tinitializer):\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n,\\n\\t\\nreuse\\n=\\nTrue\\n):', 'threshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n)\\nThis\\tcode\\twill\\tfetch\\tthe\\texisting\\t\\n\"relu/threshold\"\\n\\tvariable,\\tor\\traise\\tan\\texception\\tif\\tit\\tdoes\\tnot\\texist\\tor\\nif\\tit\\twas\\tnot\\tcreated\\tusing\\t\\nget_variable()\\n.\\tAlternatively,\\tyou\\tcan\\tset\\tthe\\t\\nreuse\\n\\tattribute\\tto\\t\\nTrue\\n\\tinside\\nthe\\tblock\\tby\\tcalling\\tthe\\t\\nscope’s\\t\\nreuse_variables()\\n\\tmethod:\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n)\\n\\t\\nas\\n\\t\\nscope\\n:\\n\\t\\t\\t\\t\\nscope\\n.\\nreuse_variables\\n()\\n\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n)\\nWARNING\\nOnce\\t\\nreuse\\n\\tis\\tset\\tto\\t\\nTrue\\n,\\tit\\tcannot\\tbe\\tset\\tback\\tto\\t\\nFalse\\n\\twithin\\tthe\\tblock.\\tMoreover,\\tif\\tyou\\tdefine\\tother\\tvariable\\tscopes\\tinside\\nthis\\tone,\\tthey\\twill\\tautomatically\\tinherit\\t\\nreuse=True\\n.\\tLastly,\\tonly\\tvariables\\tcreated\\tby\\t\\nget_variable()\\n\\tcan\\tbe\\treused\\tthis\\tway.\\nNow\\tyou\\thave\\tall\\tthe\\tpieces\\tyou\\tneed\\tto\\tmake\\tthe\\t\\nrelu()\\n\\tfunction\\taccess\\tthe\\t\\nthreshold\\n\\tvariable\\nwithout\\thaving\\tto\\tpass\\tit\\tas\\ta\\tparameter:\\ndef\\n\\t\\nrelu\\n(\\nX\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n,\\n\\t\\nreuse\\n=\\nTrue\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf', '=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n)\\n\\t\\t\\n#\\treuse\\texisting\\tvariable\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz\\n,\\n\\t\\nthreshold\\n,\\n\\t\\nname\\n=\\n\"max\"\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n):\\n\\t\\t\\n#\\tcreate\\tthe\\tvariable\\n\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ninitializer\\n=\\ntf\\n.\\nconstant_initializer\\n(\\n0.0\\n))\\nrelus\\n\\t\\n=\\n\\t\\n[\\nrelu\\n(\\nX\\n)\\n\\t\\nfor\\n\\t\\nrelu_index\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n)]\\noutput\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n(\\nrelus\\n,\\n\\t\\nname\\n=\\n\"output\"\\n)\\nThis\\t\\ncode\\tfirst\\tdefines\\tthe\\t\\nrelu()\\n\\tfunction,\\tthen\\tcreates\\tthe\\t\\nrelu/threshold\\n\\tvariable\\t(as\\ta\\tscalar\\tthat\\nwill\\tlater\\tbe\\tinitialized\\tto\\t\\n0.0\\n)\\tand\\tbuilds\\tfive\\tReLUs\\tby\\tcalling\\tthe\\t\\nrelu()\\n\\tfunction.\\tThe\\t\\nrelu()\\nfunction\\treuses\\tthe\\t\\nrelu/threshold\\n\\tvariable,\\tand\\tcreates\\tthe\\tother\\tReLU\\tnodes.\\nNOTE\\nVariables\\tcreated\\tusing\\t\\nget_variable()\\n\\tare\\talways\\tnamed\\tusing\\tthe\\tname\\tof\\t\\ntheir\\t\\nvariable_scope\\n\\tas\\ta\\tprefix\\t(e.g.,', '\"relu/threshold\"\\n),\\tbut\\tfor\\tall\\tother\\tnodes\\t(including\\tvariables\\tcreated\\twith\\t\\ntf.Variable()\\n)\\tthe\\tvariable\\tscope\\tacts\\tlike\\ta\\tnew\\nname\\tscope.\\tIn\\tparticular,\\tif\\ta\\tname\\tscope\\twith\\tan\\tidentical\\tname\\twas\\talready\\tcreated,\\tthen\\ta\\tsuffix\\tis\\tadded\\tto\\tmake\\tthe\\tname\\nunique.\\tFor\\texample,\\tall\\tnodes\\tcreated\\tin\\tthe\\tpreceding\\tcode\\t(except\\tthe\\t\\nthreshold\\n\\tvariable)\\thave\\ta\\tname\\t\\nprefixed\\twith\\n\"relu_1/\"\\n\\tto\\t\\n\"relu_5/\"\\n,\\tas\\tshown\\tin\\t\\nFigure\\t9-8\\n.', 'Figure\\t9-8.\\t\\nFive\\tReLUs\\tsharing\\tthe\\tthreshold\\tvariable\\nIt\\tis\\tsomewhat\\tunfortunate\\tthat\\tthe\\t\\nthreshold\\n\\tvariable\\tmust\\tbe\\tdefined\\toutside\\tthe\\t\\nrelu()\\n\\tfunction,\\nwhere\\tall\\tthe\\trest\\tof\\tthe\\tReLU\\tcode\\tresides.\\tTo\\tfix\\tthis,\\tthe\\tfollowing\\tcode\\tcreates\\tthe\\t\\nthreshold\\nvariable\\twithin\\tthe\\t\\nrelu()\\n\\tfunction\\tupon\\tthe\\tfirst\\tcall,\\tthen\\treuses\\tit\\tin\\tsubsequent\\tcalls.\\tNow\\tthe\\t\\nrelu()\\nfunction\\tdoes\\tnot\\thave\\tto\\tworry\\tabout\\tname\\tscopes\\tor\\tvariable\\tsharing:\\tit\\tjust\\tcalls\\t\\nget_variable()\\n,\\nwhich\\twill\\tcreate\\tor\\treuse\\tthe\\t\\nthreshold\\n\\tvariable\\t(it\\tdoes\\tnot\\tneed\\tto\\tknow\\twhich\\tis\\tthe\\tcase).\\tThe\\trest\\nof\\tthe\\tcode\\tcalls\\t\\nrelu()\\n\\tfive\\ttimes,\\tmaking\\tsure\\tto\\tset\\t\\nreuse=False\\n\\ton\\tthe\\tfirst\\tcall,\\tand\\t\\nreuse=True\\nfor\\tthe\\tother\\t\\ncalls.\\ndef\\n\\t\\nrelu\\n(\\nX\\n):\\n\\t\\t\\t\\t\\nthreshold\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_variable\\n(\\n\"threshold\"\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ninitializer\\n=\\ntf\\n.\\nconstant_initializer\\n(\\n0.0\\n))\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\nz\\n,\\n\\t\\nthreshold\\n,\\n\\t\\nname\\n=\\n\"max\"\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,', ',\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nrelus\\n\\t\\n=\\n\\t\\n[]\\nfor\\n\\t\\nrelu_index\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"relu\"\\n,\\n\\t\\nreuse\\n=\\n(\\nrelu_index\\n\\t\\n>=\\n\\t\\n1\\n))\\n\\t\\nas\\n\\t\\nscope\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nrelus\\n.\\nappend\\n(\\nrelu\\n(\\nX\\n))\\noutput\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n(\\nrelus\\n,\\n\\t\\nname\\n=\\n\"output\"\\n)\\nThe\\tresulting\\tgraph\\tis\\tslightly\\tdifferent\\tthan\\tbefore,\\tsince\\tthe\\tshared\\tvariable\\tlives\\twithin\\tthe\\tfirst\\tReLU\\n(see\\t\\nFigure\\t9-9\\n).\\nFigure\\t9-9.\\t\\nFive\\tReLUs\\tsharing\\tthe\\tthreshold\\tvariable\\nThis\\tconcludes\\tthis\\tintroduction\\tto\\tTensorFlow.\\tWe\\twill\\tdiscuss\\tmore\\tadvanced\\ttopics\\tas\\twe\\tgo\\tthrough\\nthe\\tfollowing\\tchapters,\\tin\\tparticular\\tmany\\toperations\\trelated\\tto\\tdeep\\tneural\\tnetworks,\\tconvolutional\\nneural\\tnetworks,\\tand\\trecurrent\\tneural\\tnetworks\\tas\\twell\\tas\\thow\\tto\\tscale\\tup\\twith\\tTensorFlow\\tusing\\nmultithreading,\\tqueues,\\tmultiple\\tGPUs,\\tand\\t\\nmultiple\\tservers.', 'Exercises\\n1\\n.\\t\\nWhat\\tare\\tthe\\tmain\\tbenefits\\tof\\tcreating\\ta\\tcomputation\\tgraph\\trather\\tthan\\tdirectly\\texecuting\\tthe\\ncomputations?\\tWhat\\tare\\tthe\\tmain\\tdrawbacks?\\n2\\n.\\t\\nIs\\tthe\\tstatement\\t\\na_val\\n\\t\\n=\\n\\t\\na.eval(session=sess)\\n\\tequivalent\\tto\\t\\na_val\\n\\t\\n=\\n\\t\\nsess.run(a)\\n?\\n3\\n.\\t\\nIs\\tthe\\tstatement\\t\\na_val,\\tb_val\\n\\t\\n=\\n\\t\\na.eval(session=sess),\\tb.eval(session=sess)\\n\\tequivalent\\tto\\na_val,\\tb_val\\n\\t\\n=\\n\\t\\nsess.run([a,\\tb])\\n?\\n4\\n.\\t\\nCan\\tyou\\trun\\ttwo\\tgraphs\\tin\\tthe\\tsame\\tsession?\\n5\\n.\\t\\nIf\\tyou\\tcreate\\ta\\tgraph\\t\\ng\\n\\tcontaining\\ta\\tvariable\\t\\nw\\n,\\tthen\\tstart\\ttwo\\tthreads\\tand\\topen\\ta\\tsession\\tin\\teach\\nthread,\\tboth\\tusing\\tthe\\tsame\\tgraph\\t\\ng\\n,\\twill\\teach\\tsession\\thave\\tits\\town\\tcopy\\tof\\tthe\\tvariable\\t\\nw\\n\\tor\\twill\\tit\\nbe\\tshared?\\n6\\n.\\t\\nWhen\\tis\\ta\\tvariable\\tinitialized?\\tWhen\\tis\\tit\\tdestroyed?\\n7\\n.\\t\\nWhat\\tis\\tthe\\tdifference\\tbetween\\ta\\tplaceholder\\tand\\ta\\tvariable?\\n8\\n.\\t\\nWhat\\thappens\\twhen\\tyou\\trun\\tthe\\tgraph\\tto\\tevaluate\\tan\\toperation\\tthat\\tdepends\\ton\\ta\\tplaceholder\\tbut\\tyou\\ndon’t\\tfeed\\tits\\tvalue?\\tWhat\\thappens\\tif\\tthe\\toperation\\tdoes\\tnot\\tdepend\\ton\\tthe\\tplaceholder?\\n9\\n.', '9\\n.\\t\\nWhen\\tyou\\trun\\ta\\tgraph,\\tcan\\tyou\\tfeed\\tthe\\toutput\\tvalue\\tof\\tany\\toperation,\\tor\\tjust\\tthe\\tvalue\\tof\\nplaceholders?\\n10\\n.\\t\\nHow\\tcan\\tyou\\tset\\ta\\tvariable\\tto\\tany\\tvalue\\tyou\\twant\\t(during\\tthe\\texecution\\tphase)?\\n11\\n.\\t\\nHow\\tmany\\ttimes\\tdoes\\treverse-mode\\tautodiff\\tneed\\tto\\ttraverse\\tthe\\tgraph\\tin\\torder\\tto\\tcompute\\tthe\\ngradients\\tof\\tthe\\tcost\\tfunction\\twith\\tregards\\tto\\t10\\tvariables?\\tWhat\\tabout\\tforward-mode\\tautodiff?\\tAnd\\nsymbolic\\tdifferentiation?\\n12\\n.\\t\\nImplement\\tLogistic\\tRegression\\twith\\tMini-batch\\tGradient\\tDescent\\tusing\\tTensorFlow.\\tTrain\\tit\\tand\\nevaluate\\tit\\ton\\tthe\\tmoons\\tdataset\\t(introduced\\tin\\t\\nChapter\\t5\\n).\\tTry\\tadding\\tall\\tthe\\tbells\\tand\\twhistles:\\nDefine\\tthe\\tgraph\\twithin\\ta\\t\\nlogistic_regression()\\n\\tfunction\\tthat\\tcan\\tbe\\treused\\teasily.\\nSave\\tcheckpoints\\tusing\\ta\\t\\nSaver\\n\\tat\\tregular\\tintervals\\tduring\\ttraining,\\tand\\tsave\\tthe\\tfinal\\tmodel\\tat\\nthe\\tend\\tof\\ttraining.\\nRestore\\tthe\\tlast\\tcheckpoint\\tupon\\tstartup\\tif\\ttraining\\twas\\tinterrupted.\\nDefine\\tthe\\tgraph\\tusing\\tnice\\tscopes\\tso\\tthe\\tgraph\\tlooks\\tgood\\tin\\tTensorBoard.', 'Add\\tsummaries\\tto\\tvisualize\\tthe\\tlearning\\tcurves\\tin\\tTensorBoard.\\nTry\\ttweaking\\tsome\\thyperparameters\\tsuch\\tas\\tthe\\tlearning\\trate\\tor\\tthe\\tmini-batch\\tsize\\tand\\tlook\\tat\\nthe\\tshape\\tof\\tthe\\tlearning\\tcurve.', 'Solutions\\tto\\tthese\\texercises\\tare\\tavailable\\t\\nin\\t\\nAppendix\\tA\\n.\\nTensorFlow\\tis\\tnot\\tlimited\\tto\\tneural\\tnetworks\\tor\\teven\\tMachine\\tLearning;\\tyou\\tcould\\trun\\tquantum\\tphysics\\tsimulations\\tif\\tyou\\twanted.\\nNot\\tto\\tbe\\tconfused\\twith\\tthe\\tTFLearn\\tlibrary,\\twhich\\tis\\tan\\tindependent\\tproject.\\nIn\\tdistributed\\tTensorFlow,\\tvariable\\tvalues\\tare\\tstored\\ton\\tthe\\tservers\\tinstead\\tof\\tthe\\tsession,\\tas\\twe\\twill\\tsee\\tin\\t\\nChapter\\t12\\n.\\nNote\\tthat\\t\\nhousing.target\\n\\tis\\ta\\t1D\\tarray,\\tbut\\twe\\tneed\\tto\\treshape\\tit\\tto\\ta\\tcolumn\\tvector\\tto\\tcompute\\t\\ntheta\\n.\\tRecall\\tthat\\tNumPy’s\\t\\nreshape()\\nfunction\\taccepts\\t–1\\t(meaning\\t“unspecified”)\\tfor\\tone\\tof\\tthe\\tdimensions:\\tthat\\tdimension\\twill\\tbe\\tcomputed\\tbased\\ton\\tthe\\tarray’s\\tlength\\tand\\nthe\\tremaining\\tdimensions.\\nCreating\\ta\\t\\nReLU\\n\\tclass\\tis\\targuably\\tthe\\tcleanest\\toption,\\tbut\\tit\\tis\\trather\\theavyweight.\\n1\\n2\\n3\\n4\\n5', 'Chapter\\t10.\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\nNetworks\\nBirds\\t\\ninspired\\tus\\tto\\tfly,\\tburdock\\tplants\\tinspired\\tvelcro,\\tand\\tnature\\thas\\tinspired\\tmany\\tother\\tinventions.\\tIt\\nseems\\tonly\\tlogical,\\tthen,\\tto\\tlook\\tat\\tthe\\tbrain’s\\tarchitecture\\tfor\\tinspiration\\ton\\thow\\tto\\tbuild\\tan\\tintelligent\\nmachine.\\tThis\\tis\\tthe\\tkey\\tidea\\tthat\\tinspired\\t\\nartificial\\tneural\\tnetworks\\n\\t(ANNs).\\tHowever,\\talthough\\tplanes\\nwere\\tinspired\\tby\\tbirds,\\tthey\\tdon’t\\thave\\tto\\tflap\\ttheir\\twings.\\tSimilarly,\\tANNs\\thave\\tgradually\\tbecome\\tquite\\ndifferent\\tfrom\\ttheir\\tbiological\\tcousins.\\tSome\\tresearchers\\teven\\targue\\tthat\\twe\\tshould\\tdrop\\tthe\\tbiological\\nanalogy\\taltogether\\t(e.g.,\\tby\\tsaying\\t“units”\\trather\\tthan\\t“neurons”),\\tlest\\twe\\trestrict\\tour\\tcreativity\\tto\\nbiologically\\tplausible\\tsystems.\\n1\\nANNs\\tare\\tat\\tthe\\tvery\\tcore\\tof\\tDeep\\tLearning.\\tThey\\tare\\tversatile,\\tpowerful,\\tand\\tscalable,\\tmaking\\tthem\\nideal\\tto\\ttackle\\tlarge\\tand\\thighly\\tcomplex\\tMachine\\tLearning\\ttasks,\\tsuch\\tas\\tclassifying\\tbillions\\tof\\timages\\n(e.g.,\\tGoogle\\tImages),', 'powering\\tspeech\\trecognition\\tservices\\t(e.g.,\\tApple’s\\tSiri),\\t\\nrecommending\\tthe\\tbest\\nvideos\\tto\\twatch\\tto\\thundreds\\tof\\tmillions\\tof\\tusers\\tevery\\tday\\t(e.g.,\\tYouTube),\\t\\nor\\tlearning\\tto\\tbeat\\tthe\\tworld\\nchampion\\tat\\tthe\\tgame\\tof\\t\\nGo\\n\\tby\\texamining\\tmillions\\tof\\tpast\\tgames\\tand\\tthen\\tplaying\\tagainst\\t\\nitself\\n(DeepMind’s\\tAlphaGo).\\nIn\\tthis\\tchapter,\\twe\\twill\\tintroduce\\tartificial\\tneural\\tnetworks,\\tstarting\\twith\\ta\\tquick\\ttour\\tof\\tthe\\tvery\\tfirst\\nANN\\tarchitectures.\\tThen\\twe\\twill\\tpresent\\t\\nMulti-Layer\\tPerceptrons\\n\\t(MLPs)\\t\\nand\\timplement\\tone\\tusing\\nTensorFlow\\tto\\ttackle\\tthe\\tMNIST\\tdigit\\tclassification\\tproblem\\t(introduced\\tin\\t\\nChapter\\t3\\n).', 'From\\tBiological\\tto\\tArtificial\\tNeurons\\nSurprisingly,\\t\\nANNs\\thave\\tbeen\\taround\\tfor\\tquite\\ta\\twhile:\\tthey\\twere\\tfirst\\tintroduced\\tback\\tin\\t1943\\tby\\tthe\\nneurophysiologist\\tWarren\\tMcCulloch\\tand\\tthe\\tmathematician\\tWalter\\tPitts.\\tIn\\ttheir\\t\\nlandmark\\tpaper\\n,\\n2\\n\\t“A\\nLogical\\tCalculus\\tof\\tIdeas\\tImmanent\\tin\\tNervous\\tActivity,”\\tMcCulloch\\tand\\tPitts\\tpresented\\ta\\tsimplified\\ncomputational\\tmodel\\tof\\thow\\tbiological\\tneurons\\tmight\\twork\\ttogether\\tin\\tanimal\\tbrains\\tto\\tperform\\tcomplex\\ncomputations\\tusing\\t\\npropositional\\tlogic\\n.\\t\\nThis\\twas\\tthe\\tfirst\\tartificial\\tneural\\tnetwork\\tarchitecture.\\tSince\\nthen\\tmany\\tother\\tarchitectures\\thave\\tbeen\\tinvented,\\tas\\twe\\twill\\tsee.\\nThe\\tearly\\tsuccesses\\tof\\tANNs\\tuntil\\tthe\\t1960s\\tled\\tto\\tthe\\twidespread\\tbelief\\tthat\\twe\\twould\\tsoon\\tbe\\nconversing\\twith\\ttruly\\tintelligent\\tmachines.\\tWhen\\tit\\tbecame\\tclear\\tthat\\tthis\\tpromise\\twould\\tgo\\tunfulfilled\\n(at\\tleast\\tfor\\tquite\\ta\\twhile),\\tfunding\\tflew\\telsewhere\\tand\\tANNs\\tentered\\ta\\tlong\\tdark\\tera.\\tIn\\tthe\\tearly\\t1980s', 'there\\twas\\ta\\trevival\\tof\\tinterest\\tin\\tANNs\\tas\\tnew\\tnetwork\\tarchitectures\\twere\\tinvented\\tand\\tbetter\\ttraining\\ntechniques\\twere\\tdeveloped.\\tBut\\tby\\tthe\\t1990s,\\tpowerful\\talternative\\tMachine\\tLearning\\ttechniques\\tsuch\\tas\\nSupport\\tVector\\tMachines\\t(see\\t\\nChapter\\t5\\n)\\twere\\tfavored\\tby\\tmost\\tresearchers,\\tas\\tthey\\tseemed\\tto\\toffer\\nbetter\\tresults\\tand\\tstronger\\ttheoretical\\tfoundations.\\tFinally,\\twe\\tare\\tnow\\twitnessing\\tyet\\tanother\\twave\\tof\\ninterest\\tin\\tANNs.\\tWill\\tthis\\twave\\tdie\\tout\\tlike\\tthe\\tprevious\\tones\\tdid?\\tThere\\tare\\ta\\tfew\\tgood\\treasons\\tto\\nbelieve\\tthat\\tthis\\tone\\tis\\tdifferent\\tand\\twill\\thave\\ta\\tmuch\\tmore\\tprofound\\timpact\\ton\\tour\\tlives:\\nThere\\tis\\tnow\\ta\\thuge\\tquantity\\tof\\tdata\\tavailable\\tto\\ttrain\\tneural\\tnetworks,\\tand\\tANNs\\tfrequently\\noutperform\\tother\\tML\\ttechniques\\ton\\tvery\\tlarge\\tand\\tcomplex\\tproblems.\\nThe\\ttremendous\\tincrease\\tin\\tcomputing\\tpower\\tsince\\tthe\\t1990s\\tnow\\tmakes\\tit\\tpossible\\tto\\ttrain\\tlarge\\nneural\\tnetworks\\tin\\ta\\treasonable\\tamount\\tof\\ttime.\\tThis\\tis\\tin\\tpart\\tdue\\tto\\tMoore’s\\tLaw,\\tbut\\talso\\tthanks', 'to\\tthe\\tgaming\\tindustry,\\twhich\\thas\\tproduced\\tpowerful\\tGPU\\tcards\\tby\\tthe\\tmillions.\\nThe\\ttraining\\talgorithms\\thave\\tbeen\\timproved.\\tTo\\tbe\\tfair\\tthey\\tare\\tonly\\tslightly\\tdifferent\\tfrom\\tthe\\tones\\nused\\tin\\tthe\\t1990s,\\tbut\\tthese\\trelatively\\tsmall\\ttweaks\\thave\\ta\\thuge\\tpositive\\timpact.\\nSome\\ttheoretical\\tlimitations\\tof\\tANNs\\thave\\tturned\\tout\\tto\\tbe\\tbenign\\tin\\tpractice.\\tFor\\texample,\\tmany\\npeople\\tthought\\tthat\\tANN\\ttraining\\talgorithms\\twere\\tdoomed\\tbecause\\tthey\\twere\\tlikely\\tto\\tget\\tstuck\\tin\\nlocal\\toptima,\\tbut\\tit\\tturns\\tout\\tthat\\tthis\\tis\\trather\\trare\\tin\\tpractice\\t(or\\twhen\\tit\\tis\\tthe\\tcase,\\tthey\\tare\\tusually\\nfairly\\tclose\\tto\\tthe\\tglobal\\toptimum).\\nANNs\\tseem\\tto\\thave\\tentered\\ta\\tvirtuous\\tcircle\\tof\\tfunding\\tand\\tprogress.\\tAmazing\\tproducts\\tbased\\ton\\nANNs\\tregularly\\tmake\\tthe\\theadline\\tnews,\\twhich\\tpulls\\tmore\\tand\\tmore\\tattention\\tand\\tfunding\\ttoward\\nthem,\\tresulting\\tin\\tmore\\tand\\tmore\\tprogress,\\tand\\teven\\tmore\\tamazing\\t\\nproducts.', 'Biological\\tNeurons\\nBefore\\twe\\tdiscuss\\tartificial\\tneurons,\\tlet’s\\ttake\\ta\\tquick\\tlook\\tat\\ta\\tbiological\\tneuron\\t(represented\\tin\\nFigure\\t10-1\\n).\\tIt\\tis\\tan\\tunusual-looking\\tcell\\tmostly\\tfound\\tin\\tanimal\\tcerebral\\tcortexes\\t(e.g.,\\tyour\\tbrain),\\ncomposed\\tof\\ta\\t\\ncell\\tbody\\n\\tcontaining\\tthe\\tnucleus\\tand\\tmost\\tof\\tthe\\tcell’s\\tcomplex\\tcomponents,\\tand\\tmany\\nbranching\\textensions\\tcalled\\t\\ndendrites\\n,\\tplus\\tone\\tvery\\tlong\\textension\\tcalled\\tthe\\t\\naxon\\n.\\tThe\\taxon’s\\tlength\\nmay\\tbe\\tjust\\ta\\tfew\\ttimes\\tlonger\\tthan\\tthe\\tcell\\tbody,\\tor\\tup\\tto\\ttens\\tof\\tthousands\\tof\\ttimes\\tlonger.\\tNear\\tits\\nextremity\\tthe\\taxon\\tsplits\\toff\\tinto\\tmany\\tbranches\\tcalled\\t\\ntelodendria\\n,\\tand\\tat\\tthe\\ttip\\tof\\tthese\\tbranches\\tare\\nminuscule\\tstructures\\tcalled\\t\\nsynaptic\\tterminals\\n\\t(or\\tsimply\\t\\nsynapses\\n),\\twhich\\tare\\tconnected\\tto\\tthe\\ndendrites\\t(or\\tdirectly\\tto\\tthe\\tcell\\tbody)\\tof\\tother\\tneurons.\\tBiological\\tneurons\\treceive\\tshort\\telectrical\\nimpulses\\tcalled\\t\\nsignals\\n\\tfrom\\tother\\tneurons\\tvia\\tthese\\tsynapses.\\tWhen\\ta\\tneuron\\treceives\\ta\\tsufficient', 'number\\tof\\tsignals\\tfrom\\tother\\tneurons\\twithin\\ta\\tfew\\tmilliseconds,\\tit\\tfires\\tits\\town\\tsignals.\\nFigure\\t10-1.\\t\\nBiological\\tneuron\\n3\\nThus,\\tindividual\\tbiological\\tneurons\\tseem\\tto\\tbehave\\tin\\ta\\trather\\tsimple\\tway,\\tbut\\tthey\\tare\\torganized\\tin\\ta\\nvast\\tnetwork\\tof\\tbillions\\tof\\tneurons,\\teach\\tneuron\\ttypically\\tconnected\\tto\\tthousands\\tof\\tother\\tneurons.\\tHighly\\ncomplex\\tcomputations\\tcan\\tbe\\tperformed\\tby\\ta\\tvast\\tnetwork\\tof\\tfairly\\tsimple\\tneurons,\\tmuch\\tlike\\ta\\tcomplex\\nanthill\\tcan\\temerge\\tfrom\\tthe\\tcombined\\tefforts\\tof\\tsimple\\tants.\\tThe\\tarchitecture\\tof\\tbiological\\tneural\\nnetworks\\t(BNN)\\n4\\n\\tis\\tstill\\tthe\\tsubject\\tof\\tactive\\tresearch,\\tbut\\tsome\\tparts\\tof\\tthe\\tbrain\\thave\\tbeen\\tmapped,\\nand\\tit\\tseems\\tthat\\tneurons\\tare\\toften\\torganized\\tin\\tconsecutive\\tlayers,\\tas\\t\\nshown\\tin\\t\\nFigure\\t10-2\\n.', 'Figure\\t10-2.\\t\\nMultiple\\tlayers\\tin\\ta\\tbiological\\tneural\\tnetwork\\t(human\\tcortex)\\n5', 'Logical\\tComputations\\twith\\tNeurons\\nWarren\\tMcCulloch\\t\\nand\\tWalter\\tPitts\\tproposed\\ta\\tvery\\tsimple\\tmodel\\tof\\tthe\\tbiological\\tneuron,\\twhich\\tlater\\nbecame\\tknown\\tas\\t\\nan\\t\\nartificial\\tneuron\\n:\\tit\\thas\\tone\\tor\\tmore\\tbinary\\t(on/off)\\tinputs\\tand\\tone\\tbinary\\toutput.\\nThe\\tartificial\\tneuron\\tsimply\\tactivates\\tits\\toutput\\twhen\\tmore\\tthan\\ta\\tcertain\\tnumber\\tof\\tits\\tinputs\\tare\\tactive.\\nMcCulloch\\tand\\tPitts\\tshowed\\tthat\\teven\\twith\\tsuch\\ta\\tsimplified\\tmodel\\tit\\tis\\tpossible\\tto\\tbuild\\ta\\tnetwork\\tof\\nartificial\\tneurons\\tthat\\tcomputes\\tany\\tlogical\\tproposition\\tyou\\twant.\\tFor\\texample,\\tlet’s\\tbuild\\ta\\tfew\\tANNs\\nthat\\tperform\\tvarious\\tlogical\\tcomputations\\t(see\\t\\nFigure\\t10-3\\n),\\tassuming\\tthat\\ta\\tneuron\\tis\\tactivated\\twhen\\tat\\nleast\\ttwo\\tof\\tits\\tinputs\\tare\\tactive.\\nFigure\\t10-3.\\t\\nANNs\\tperforming\\tsimple\\tlogical\\tcomputations\\nThe\\tfirst\\tnetwork\\ton\\tthe\\tleft\\tis\\tsimply\\tthe\\tidentity\\tfunction:\\tif\\tneuron\\tA\\tis\\tactivated,\\tthen\\tneuron\\tC\\ngets\\tactivated\\tas\\twell\\t(since\\tit\\treceives\\ttwo\\tinput\\tsignals\\tfrom\\tneuron\\tA),\\tbut\\tif\\tneuron\\tA\\tis\\toff,\\tthen\\nneuron\\tC\\tis\\toff\\tas\\twell.', 'The\\tsecond\\tnetwork\\tperforms\\ta\\tlogical\\tAND:\\tneuron\\tC\\tis\\tactivated\\tonly\\twhen\\tboth\\tneurons\\tA\\tand\\tB\\nare\\tactivated\\t(a\\tsingle\\tinput\\tsignal\\tis\\tnot\\tenough\\tto\\tactivate\\tneuron\\tC).\\nThe\\tthird\\tnetwork\\tperforms\\ta\\tlogical\\tOR:\\tneuron\\tC\\tgets\\tactivated\\tif\\teither\\tneuron\\tA\\tor\\tneuron\\tB\\tis\\nactivated\\t(or\\tboth).\\nFinally,\\tif\\twe\\tsuppose\\tthat\\tan\\tinput\\tconnection\\tcan\\tinhibit\\tthe\\tneuron’s\\tactivity\\t(which\\tis\\tthe\\tcase\\nwith\\tbiological\\tneurons),\\tthen\\tthe\\tfourth\\tnetwork\\tcomputes\\ta\\tslightly\\tmore\\tcomplex\\tlogical\\nproposition:\\tneuron\\tC\\tis\\tactivated\\tonly\\tif\\tneuron\\tA\\tis\\tactive\\tand\\tif\\tneuron\\tB\\tis\\toff.\\tIf\\tneuron\\tA\\tis\\nactive\\tall\\tthe\\ttime,\\tthen\\tyou\\tget\\ta\\tlogical\\tNOT:\\tneuron\\tC\\tis\\tactive\\twhen\\tneuron\\tB\\tis\\toff,\\tand\\tvice\\nversa.\\nYou\\tcan\\teasily\\timagine\\thow\\tthese\\tnetworks\\tcan\\tbe\\tcombined\\tto\\tcompute\\tcomplex\\tlogical\\texpressions\\n(see\\tthe\\texercises\\tat\\tthe\\tend\\tof\\tthe\\tchapter).', 'The\\tPerceptron\\nThe\\t\\nPerceptron\\n\\t\\nis\\tone\\tof\\tthe\\tsimplest\\tANN\\tarchitectures,\\tinvented\\tin\\t1957\\tby\\tFrank\\tRosenblatt.\\tIt\\tis\\nbased\\ton\\ta\\tslightly\\tdifferent\\tartificial\\tneuron\\t(see\\t\\nFigure\\t10-4\\n)\\tcalled\\t\\na\\t\\nlinear\\tthreshold\\tunit\\n\\t(LTU):\\tthe\\ninputs\\tand\\toutput\\tare\\tnow\\tnumbers\\t(instead\\tof\\tbinary\\ton/off\\tvalues)\\tand\\teach\\tinput\\tconnection\\tis\\nassociated\\twith\\ta\\tweight.\\tThe\\tLTU\\tcomputes\\ta\\tweighted\\tsum\\tof\\tits\\tinputs\\t(\\nz\\n\\t=\\t\\nw\\n1\\n\\t\\nx\\n1\\n\\t+\\t\\nw\\n2\\n\\t\\nx\\n2\\n\\t+\\t\\t+\\t\\nw\\nn\\n\\t\\nx\\nn\\n=\\t\\nw\\nT\\n\\t·\\t\\nx\\n),\\tthen\\tapplies\\ta\\t\\nstep\\tfunction\\n\\t\\nto\\tthat\\tsum\\tand\\toutputs\\tthe\\tresult:\\t\\nh\\nw\\n(\\nx\\n)\\t=\\tstep\\t(\\nz\\n)\\t=\\tstep\\t(\\nw\\nT\\n\\t·\\t\\nx\\n).\\nFigure\\t10-4.\\t\\nLinear\\tthreshold\\tunit\\nThe\\tmost\\tcommon\\tstep\\tfunction\\tused\\tin\\tPerceptrons\\tis\\t\\nthe\\t\\nHeaviside\\tstep\\tfunction\\n\\t(see\\t\\nEquation\\t10-1\\n).\\nSometimes\\tthe\\tsign\\tfunction\\tis\\tused\\tinstead.\\nEquation\\t10-1.\\t\\nCommon\\tstep\\tfunctions\\tused\\tin\\tPerceptrons\\nA\\tsingle\\tLTU\\tcan\\tbe\\tused\\tfor\\tsimple\\tlinear\\tbinary\\tclassification.\\tIt\\tcomputes\\ta\\tlinear\\tcombination\\tof\\tthe', 'inputs\\tand\\tif\\tthe\\tresult\\texceeds\\ta\\tthreshold,\\tit\\toutputs\\tthe\\tpositive\\tclass\\tor\\telse\\toutputs\\tthe\\tnegative\\tclass\\n(just\\tlike\\ta\\tLogistic\\tRegression\\tclassifier\\tor\\ta\\tlinear\\tSVM).\\tFor\\texample,\\tyou\\tcould\\tuse\\ta\\tsingle\\tLTU\\tto\\nclassify\\tiris\\tflowers\\tbased\\ton\\tthe\\tpetal\\tlength\\tand\\twidth\\t(also\\tadding\\tan\\textra\\tbias\\tfeature\\t\\nx\\n0\\n\\t=\\t1,\\tjust\\tlike\\nwe\\tdid\\tin\\tprevious\\tchapters).\\tTraining\\tan\\tLTU\\tmeans\\tfinding\\tthe\\tright\\tvalues\\tfor\\t\\nw\\n0\\n,\\t\\nw\\n1\\n,\\tand\\t\\nw\\n2\\n\\t(the\\ntraining\\talgorithm\\tis\\tdiscussed\\tshortly).\\nA\\tPerceptron\\tis\\tsimply\\tcomposed\\tof\\ta\\tsingle\\tlayer\\tof\\tLTUs,\\n6\\n\\twith\\teach\\tneuron\\tconnected\\tto\\tall\\tthe\\tinputs.\\nThese\\tconnections\\tare\\toften\\trepresented\\tusing\\tspecial\\tpassthrough\\tneurons\\t\\ncalled\\t\\ninput\\tneurons\\n:\\tthey\\tjust\\noutput\\twhatever\\tinput\\tthey\\tare\\tfed.\\tMoreover,\\tan\\textra\\tbias\\tfeature\\tis\\tgenerally\\tadded\\t(\\nx\\n0\\n\\t=\\t1).\\tThis\\tbias\\nfeature\\tis\\ttypically\\trepresented\\tusing\\ta\\tspecial\\ttype\\tof\\tneuron\\tcalled\\ta\\t\\nbias\\tneuron\\n,\\t\\nwhich\\tjust\\toutputs\\t1\\nall\\tthe\\ttime.', 'A\\tPerceptron\\twith\\ttwo\\tinputs\\tand\\tthree\\toutputs\\tis\\trepresented\\tin\\t\\nFigure\\t10-5\\n.\\tThis\\tPerceptron\\tcan\\nclassify\\tinstances\\tsimultaneously\\tinto\\tthree\\tdifferent\\tbinary\\tclasses,\\twhich\\tmakes\\tit\\ta\\tmultioutput\\nclassifier.\\nFigure\\t10-5.\\t\\nPerceptron\\tdiagram\\nSo\\thow\\tis\\ta\\t\\nPerceptron\\ttrained?\\tThe\\tPerceptron\\ttraining\\talgorithm\\tproposed\\tby\\tFrank\\tRosenblatt\\twas\\nlargely\\tinspired\\t\\nby\\t\\nHebb’s\\trule\\n.\\tIn\\this\\tbook\\t\\nThe\\tOrganization\\tof\\tBehavior\\n,\\tpublished\\tin\\t1949,\\tDonald\\nHebb\\tsuggested\\tthat\\twhen\\ta\\tbiological\\tneuron\\toften\\ttriggers\\tanother\\tneuron,\\tthe\\tconnection\\tbetween\\tthese\\ntwo\\tneurons\\tgrows\\tstronger.\\tThis\\tidea\\twas\\tlater\\tsummarized\\tby\\tSiegrid\\tLöwel\\tin\\tthis\\tcatchy\\tphrase:\\n“Cells\\tthat\\tfire\\ttogether,\\twire\\ttogether.”\\tThis\\trule\\tlater\\tbecame\\tknown\\tas\\tHebb’s\\trule\\t\\n(or\\t\\nHebbian\\nlearning\\n);\\tthat\\tis,\\tthe\\tconnection\\tweight\\tbetween\\ttwo\\tneurons\\tis\\tincreased\\twhenever\\tthey\\thave\\tthe\\tsame\\noutput.\\tPerceptrons\\tare\\ttrained\\tusing\\ta\\tvariant\\tof\\tthis\\trule\\tthat\\ttakes\\tinto\\taccount\\tthe\\terror\\tmade\\tby\\tthe', 'network;\\tit\\tdoes\\tnot\\treinforce\\tconnections\\tthat\\tlead\\tto\\tthe\\twrong\\toutput.\\tMore\\tspecifically,\\tthe\\tPerceptron\\nis\\tfed\\tone\\ttraining\\tinstance\\tat\\ta\\ttime,\\tand\\tfor\\teach\\tinstance\\tit\\tmakes\\tits\\tpredictions.\\tFor\\tevery\\toutput\\nneuron\\tthat\\tproduced\\ta\\twrong\\tprediction,\\tit\\treinforces\\tthe\\tconnection\\tweights\\tfrom\\tthe\\tinputs\\tthat\\twould\\nhave\\tcontributed\\tto\\tthe\\tcorrect\\tprediction.\\tThe\\trule\\tis\\tshown\\tin\\t\\nEquation\\t10-2\\n.\\nEquation\\t10-2.\\t\\nPerceptron\\tlearning\\trule\\t(weight\\tupdate)\\nw\\ni\\n,\\t\\nj\\n\\tis\\tthe\\tconnection\\tweight\\tbetween\\tthe\\ti\\nth\\n\\tinput\\tneuron\\tand\\tthe\\tj\\nth\\n\\toutput\\tneuron.\\nx\\ni\\n\\tis\\tthe\\ti\\nth\\n\\tinput\\tvalue\\tof\\tthe\\tcurrent\\ttraining\\tinstance.\\nj\\n\\tis\\tthe\\toutput\\tof\\tthe\\tj\\nth\\n\\toutput\\tneuron\\tfor\\tthe\\tcurrent\\ttraining\\tinstance.\\ny\\nj\\n\\tis\\tthe\\ttarget\\toutput\\tof\\tthe\\tj\\nth\\n\\toutput\\tneuron\\tfor\\tthe\\tcurrent\\ttraining\\tinstance.', 'η\\n\\tis\\tthe\\tlearning\\trate.\\nThe\\tdecision\\tboundary\\tof\\teach\\toutput\\tneuron\\tis\\tlinear,\\tso\\tPerceptrons\\tare\\tincapable\\tof\\tlearning\\tcomplex\\npatterns\\t(just\\tlike\\tLogistic\\tRegression\\tclassifiers).\\tHowever,\\tif\\tthe\\ttraining\\tinstances\\tare\\tlinearly\\nseparable,\\tRosenblatt\\tdemonstrated\\tthat\\tthis\\talgorithm\\twould\\tconverge\\tto\\ta\\tsolution.\\n7\\n\\tThis\\tis\\t\\ncalled\\t\\nthe\\nPerceptron\\tconvergence\\ttheorem\\n.\\nScikit-Learn\\t\\nprovides\\ta\\t\\nPerceptron\\n\\tclass\\t\\nthat\\timplements\\ta\\tsingle\\tLTU\\tnetwork.\\tIt\\tcan\\tbe\\tused\\tpretty\\nmuch\\tas\\tyou\\twould\\texpect\\t—\\tfor\\texample,\\ton\\tthe\\tiris\\tdataset\\t(introduced\\tin\\t\\nChapter\\t4\\n):\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nload_iris\\nfrom\\n\\t\\nsklearn.linear_model\\n\\t\\nimport\\n\\t\\nPerceptron\\niris\\n\\t\\n=\\n\\t\\nload_iris\\n()\\nX\\n\\t\\n=\\n\\t\\niris\\n.\\ndata\\n[:,\\n\\t\\n(\\n2\\n,\\n\\t\\n3\\n)]\\n\\t\\t\\n#\\tpetal\\tlength,\\tpetal\\twidth\\ny\\n\\t\\n=\\n\\t\\n(\\niris\\n.\\ntarget\\n\\t\\n==\\n\\t\\n0\\n)\\n.\\nastype\\n(\\nnp\\n.\\nint\\n)\\n\\t\\t\\n#\\tIris\\tSetosa?\\nper_clf\\n\\t\\n=\\n\\t\\nPerceptron\\n(\\nrandom_state\\n=\\n42\\n)\\nper_clf\\n.\\nfit\\n(\\nX\\n,\\n\\t\\ny\\n)\\ny_pred\\n\\t\\n=\\n\\t\\nper_clf\\n.\\npredict\\n([[\\n2\\n,\\n\\t\\n0.5\\n]])', '0.5\\n]])\\nYou\\tmay\\thave\\trecognized\\tthat\\tthe\\tPerceptron\\tlearning\\talgorithm\\tstrongly\\tresembles\\t\\nStochastic\\tGradient\\nDescent.\\tIn\\tfact,\\tScikit-Learn’s\\t\\nPerceptron\\n\\tclass\\tis\\tequivalent\\tto\\tusing\\tan\\t\\nSGDClassifier\\n\\twith\\tthe\\nfollowing\\thyperparameters:\\t\\nloss=\"perceptron\"\\n,\\t\\nlearning_rate=\"constant\"\\n,\\t\\neta0=1\\n\\t(the\\tlearning\\nrate),\\tand\\t\\npenalty=None\\n\\t(no\\tregularization).\\nNote\\t\\nthat\\tcontrary\\tto\\tLogistic\\tRegression\\tclassifiers,\\tPerceptrons\\tdo\\tnot\\toutput\\ta\\tclass\\tprobability;\\trather,\\nthey\\tjust\\tmake\\tpredictions\\tbased\\ton\\ta\\thard\\tthreshold.\\tThis\\tis\\tone\\tof\\tthe\\tgood\\treasons\\tto\\tprefer\\tLogistic\\nRegression\\tover\\tPerceptrons.\\nIn\\ttheir\\t1969\\tmonograph\\ttitled\\t\\nPerceptrons\\n,\\tMarvin\\tMinsky\\tand\\tSeymour\\tPapert\\thighlighted\\ta\\tnumber\\tof\\nserious\\tweaknesses\\tof\\tPerceptrons,\\tin\\tparticular\\tthe\\tfact\\tthat\\tthey\\tare\\tincapable\\tof\\tsolving\\tsome\\ttrivial\\nproblems\\t(e.g.,\\tthe\\t\\nExclusive\\tOR\\n\\t(XOR)\\tclassification\\tproblem;\\tsee\\tthe\\tleft\\tside\\tof\\t\\nFigure\\t10-6\\n).\\tOf', ').\\tOf\\ncourse\\tthis\\tis\\ttrue\\tof\\tany\\tother\\tlinear\\tclassification\\tmodel\\tas\\twell\\t(such\\tas\\tLogistic\\tRegression\\nclassifiers),\\tbut\\tresearchers\\thad\\texpected\\tmuch\\tmore\\tfrom\\tPerceptrons,\\tand\\ttheir\\tdisappointment\\twas\\ngreat:\\tas\\ta\\tresult,\\tmany\\tresearchers\\tdropped\\t\\nconnectionism\\n\\t\\naltogether\\t(i.e.,\\tthe\\tstudy\\tof\\tneural\\tnetworks)\\nin\\tfavor\\tof\\thigher-level\\tproblems\\tsuch\\tas\\tlogic,\\tproblem\\tsolving,\\tand\\tsearch.\\nHowever,\\tit\\tturns\\tout\\tthat\\tsome\\tof\\tthe\\tlimitations\\tof\\tPerceptrons\\tcan\\tbe\\teliminated\\tby\\tstacking\\tmultiple\\nPerceptrons.\\tThe\\tresulting\\tANN\\tis\\tcalled\\ta\\t\\nMulti-Layer\\tPerceptron\\n\\t(MLP).\\t\\nIn\\tparticular,\\tan\\tMLP\\tcan\\nsolve\\tthe\\tXOR\\tproblem,\\tas\\tyou\\tcan\\tverify\\tby\\tcomputing\\tthe\\toutput\\tof\\tthe\\tMLP\\trepresented\\ton\\tthe\\tright\\tof\\nFigure\\t10-6\\n,\\tfor\\teach\\tcombination\\tof\\tinputs:\\twith\\tinputs\\t(0,\\t0)\\tor\\t(1,\\t1)\\tthe\\tnetwork\\toutputs\\t0,\\tand\\twith\\ninputs\\t(0,\\t1)\\tor\\t(1,\\t0)\\tit\\toutputs\\t1.', 'Figure\\t10-6.\\t\\nXOR\\tclassification\\tproblem\\tand\\tan\\tMLP\\tthat\\tsolves\\tit', 'Multi-Layer\\tPerceptron\\tand\\tBackpropagation\\nAn\\tMLP\\tis\\tcomposed\\tof\\tone\\t(passthrough)\\tinput\\tlayer,\\tone\\tor\\tmore\\tlayers\\tof\\tLTUs,\\tcalled\\t\\nhidden\\tlayers\\n,\\nand\\tone\\tfinal\\tlayer\\tof\\tLTUs\\tcalled\\t\\nthe\\t\\noutput\\tlayer\\n\\t(see\\t\\nFigure\\t10-7\\n).\\tEvery\\tlayer\\texcept\\tthe\\toutput\\tlayer\\nincludes\\ta\\tbias\\tneuron\\tand\\tis\\tfully\\tconnected\\tto\\tthe\\tnext\\tlayer.\\tWhen\\tan\\tANN\\thas\\ttwo\\tor\\tmore\\thidden\\nlayers,\\tit\\tis\\tcalled\\t\\na\\t\\ndeep\\tneural\\tnetwork\\n\\t(DNN).\\nFigure\\t10-7.\\t\\nMulti-Layer\\tPerceptron\\nFor\\tmany\\tyears\\tresearchers\\tstruggled\\tto\\tfind\\ta\\tway\\tto\\ttrain\\tMLPs,\\twithout\\tsuccess.\\tBut\\tin\\t1986,\\tD.\\tE.\\nRumelhart\\tet\\tal.\\tpublished\\ta\\t\\ngroundbreaking\\tarticle\\n8\\n\\tintroducing\\tthe\\t\\nbackpropagation\\n\\t\\ntraining\\talgorithm.\\n9\\nToday\\twe\\twould\\tdescribe\\tit\\tas\\tGradient\\tDescent\\tusing\\treverse-mode\\tautodiff\\t(Gradient\\tDescent\\twas\\nintroduced\\tin\\t\\nChapter\\t4\\n,\\tand\\tautodiff\\twas\\tdiscussed\\tin\\t\\nChapter\\t9\\n).\\nFor\\teach\\ttraining\\tinstance,\\tthe\\talgorithm\\tfeeds\\tit\\tto\\tthe\\tnetwork\\tand\\tcomputes\\tthe\\toutput\\tof\\tevery\\tneuron', 'in\\teach\\tconsecutive\\tlayer\\t(this\\tis\\tthe\\tforward\\tpass,\\tjust\\tlike\\twhen\\tmaking\\tpredictions).\\tThen\\tit\\tmeasures\\nthe\\tnetwork’s\\toutput\\terror\\t(i.e.,\\tthe\\tdifference\\tbetween\\tthe\\tdesired\\toutput\\tand\\tthe\\tactual\\toutput\\tof\\tthe\\nnetwork),\\tand\\tit\\tcomputes\\thow\\tmuch\\teach\\tneuron\\tin\\tthe\\tlast\\thidden\\tlayer\\tcontributed\\tto\\teach\\toutput\\nneuron’s\\terror.\\tIt\\tthen\\tproceeds\\tto\\tmeasure\\thow\\tmuch\\tof\\tthese\\terror\\tcontributions\\tcame\\tfrom\\teach\\tneuron\\nin\\tthe\\tprevious\\thidden\\tlayer\\t—\\tand\\tso\\ton\\tuntil\\tthe\\talgorithm\\treaches\\tthe\\tinput\\tlayer.\\tThis\\treverse\\tpass\\nefficiently\\tmeasures\\tthe\\terror\\tgradient\\tacross\\tall\\tthe\\tconnection\\tweights\\tin\\tthe\\tnetwork\\tby\\tpropagating\\tthe\\nerror\\tgradient\\tbackward\\tin\\tthe\\tnetwork\\t(hence\\tthe\\tname\\tof\\tthe\\talgorithm).\\tIf\\tyou\\tcheck\\tout\\tthe\\treverse-\\nmode\\tautodiff\\talgorithm\\tin\\t\\nAppendix\\tD\\n,\\tyou\\twill\\tfind\\tthat\\tthe\\tforward\\tand\\treverse\\tpasses\\tof\\nbackpropagation\\tsimply\\tperform\\treverse-mode\\tautodiff.\\tThe\\tlast\\tstep\\tof\\tthe\\tbackpropagation\\talgorithm\\tis', 'a\\tGradient\\tDescent\\tstep\\ton\\tall\\tthe\\tconnection\\tweights\\tin\\tthe\\tnetwork,\\tusing\\tthe\\terror\\tgradients\\tmeasured\\nearlier.', 'Let’s\\tmake\\tthis\\teven\\tshorter:\\tfor\\teach\\ttraining\\tinstance\\tthe\\tbackpropagation\\talgorithm\\tfirst\\tmakes\\ta\\nprediction\\t(forward\\tpass),\\tmeasures\\tthe\\terror,\\tthen\\tgoes\\tthrough\\teach\\tlayer\\tin\\treverse\\tto\\tmeasure\\tthe\\nerror\\tcontribution\\tfrom\\teach\\tconnection\\t(reverse\\tpass),\\tand\\tfinally\\tslightly\\ttweaks\\tthe\\tconnection\\tweights\\nto\\treduce\\tthe\\t\\nerror\\t(Gradient\\tDescent\\tstep).\\nIn\\torder\\tfor\\tthis\\talgorithm\\tto\\twork\\tproperly,\\tthe\\tauthors\\tmade\\ta\\tkey\\tchange\\tto\\tthe\\tMLP’s\\tarchitecture:\\tthey\\nreplaced\\tthe\\tstep\\tfunction\\twith\\tthe\\tlogistic\\tfunction,\\t\\nσ\\n(\\nz\\n)\\t=\\t1\\t/\\t(1\\t+\\texp(–\\nz\\n)).\\tThis\\twas\\tessential\\tbecause\\nthe\\tstep\\tfunction\\tcontains\\tonly\\tflat\\tsegments,\\tso\\tthere\\tis\\tno\\tgradient\\tto\\twork\\twith\\t(Gradient\\tDescent\\ncannot\\tmove\\ton\\ta\\tflat\\tsurface),\\twhile\\tthe\\tlogistic\\tfunction\\thas\\ta\\twell-defined\\tnonzero\\tderivative\\neverywhere,\\tallowing\\tGradient\\tDescent\\tto\\tmake\\tsome\\tprogress\\tat\\tevery\\tstep.\\tThe\\tbackpropagation\\nalgorithm\\tmay\\tbe\\tused\\twith\\tother\\t\\nactivation\\tfunctions\\n,\\t\\ninstead\\tof\\tthe\\tlogistic\\tfunction.\\tTwo\\tother\\tpopular', 'activation\\tfunctions\\tare:\\nThe\\t\\nhyperbolic\\ttangent\\n\\t\\nfunction\\ttanh\\t(\\nz\\n)\\t=\\t2\\nσ\\n(2\\nz\\n)\\t–\\t1\\nJust\\tlike\\tthe\\tlogistic\\tfunction\\tit\\tis\\tS-shaped,\\tcontinuous,\\tand\\tdifferentiable,\\tbut\\tits\\toutput\\tvalue\\nranges\\tfrom\\t–1\\tto\\t1\\t(instead\\tof\\t0\\tto\\t1\\tin\\tthe\\tcase\\tof\\tthe\\tlogistic\\tfunction),\\twhich\\ttends\\tto\\tmake\\teach\\nlayer’s\\toutput\\tmore\\tor\\tless\\tnormalized\\t(i.e.,\\tcentered\\taround\\t0)\\tat\\tthe\\tbeginning\\tof\\ttraining.\\tThis\\noften\\thelps\\tspeed\\tup\\tconvergence.\\nThe\\t\\nReLU\\tfunction\\t(introduced\\tin\\t\\nChapter\\t9\\n)\\nReLU\\t(\\nz\\n)\\t=\\tmax\\t(0,\\t\\nz\\n).\\tIt\\tis\\tcontinuous\\tbut\\tunfortunately\\tnot\\tdifferentiable\\tat\\t\\nz\\n\\t=\\t0\\t(the\\tslope\\tchanges\\nabruptly,\\twhich\\tcan\\tmake\\tGradient\\tDescent\\tbounce\\taround).\\tHowever,\\tin\\tpractice\\tit\\tworks\\tvery\\nwell\\tand\\thas\\tthe\\tadvantage\\tof\\tbeing\\tfast\\tto\\tcompute.\\tMost\\timportantly,\\tthe\\tfact\\tthat\\tit\\tdoes\\tnot\\thave\\ta\\nmaximum\\toutput\\tvalue\\talso\\thelps\\treduce\\tsome\\tissues\\tduring\\tGradient\\tDescent\\t(we\\twill\\tcome\\tback\\nto\\tthis\\tin\\t\\nChapter\\t11\\n).\\nThese\\tpopular\\tactivation\\tfunctions\\tand\\ttheir\\tderivatives\\tare\\trepresented\\tin\\t\\nFigure\\t10-8\\n.', '.\\nFigure\\t10-8.\\t\\nActivation\\tfunctions\\tand\\ttheir\\tderivatives\\nAn\\tMLP\\tis\\toften\\tused\\tfor\\tclassification,\\twith\\teach\\toutput\\tcorresponding\\tto\\ta\\tdifferent\\tbinary\\tclass\\t(e.g.,\\nspam/ham,\\turgent/not-urgent,\\tand\\tso\\ton).\\tWhen\\tthe\\tclasses\\tare\\texclusive\\t(e.g.,\\tclasses\\t0\\tthrough\\t9\\tfor\\ndigit\\timage\\tclassification),\\tthe\\toutput\\tlayer\\tis\\ttypically\\tmodified\\tby\\treplacing\\tthe\\tindividual\\tactivation\\nfunctions\\tby\\ta\\tshared\\t\\nsoftmax\\n\\t\\nfunction\\t(see\\t\\nFigure\\t10-9\\n).\\tThe\\tsoftmax\\tfunction\\twas\\tintroduced\\tin\\nChapter\\t3\\n.\\tThe\\toutput\\tof\\teach\\tneuron\\tcorresponds\\tto\\tthe\\testimated\\tprobability\\tof\\tthe\\tcorresponding\\tclass.\\nNote\\tthat\\tthe\\tsignal\\tflows\\tonly\\tin\\tone\\tdirection\\t(from\\tthe\\tinputs\\tto\\tthe\\toutputs),\\tso\\tthis\\tarchitecture\\tis\\tan\\nexample\\t\\nof\\ta\\t\\nfeedforward\\tneural\\tnetwork\\n\\t(FNN).', 'Figure\\t10-9.\\t\\nA\\tmodern\\tMLP\\t(including\\tReLU\\tand\\tsoftmax)\\tfor\\tclassification\\nNOTE\\nBiological\\tneurons\\tseem\\tto\\timplement\\ta\\troughly\\tsigmoid\\t(S-shaped)\\tactivation\\tfunction,\\tso\\tresearchers\\tstuck\\tto\\tsigmoid\\tfunctions\\nfor\\ta\\tvery\\tlong\\ttime.\\tBut\\tit\\tturns\\tout\\tthat\\tthe\\tReLU\\tactivation\\tfunction\\tgenerally\\tworks\\tbetter\\tin\\tANNs.\\tThis\\tis\\tone\\tof\\tthe\\tcases\\nwhere\\tthe\\tbiological\\tanalogy\\twas\\t\\nmisleading.', 'Training\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nThe\\t\\nsimplest\\tway\\tto\\ttrain\\tan\\tMLP\\twith\\tTensorFlow\\tis\\tto\\tuse\\tthe\\thigh-level\\tAPI\\tTF.Learn,\\twhich\\toffers\\ta\\nScikit-Learn–compatible\\tAPI.\\tThe\\t\\nDNNClassifier\\n\\t\\nclass\\tmakes\\tit\\tfairly\\teasy\\tto\\ttrain\\ta\\tdeep\\tneural\\nnetwork\\twith\\tany\\tnumber\\tof\\thidden\\tlayers,\\tand\\ta\\tsoftmax\\toutput\\tlayer\\tto\\toutput\\testimated\\tclass\\nprobabilities.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\ttrains\\ta\\tDNN\\tfor\\tclassification\\twith\\ttwo\\thidden\\tlayers\\n(one\\twith\\t300\\tneurons,\\tand\\tthe\\tother\\twith\\t100\\tneurons)\\tand\\ta\\tsoftmax\\toutput\\tlayer\\twith\\t10\\tneurons:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nfeature_cols\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlearn\\n.\\ninfer_real_valued_columns_from_input\\n(\\nX_train\\n)\\ndnn_clf\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlearn\\n.\\nDNNClassifier\\n(\\nhidden_units\\n=\\n[\\n300\\n,\\n100\\n],\\n\\t\\nn_classes\\n=\\n10\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeature_columns\\n=\\nfeature_cols\\n)\\ndnn_clf\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlearn\\n.\\nSKCompat\\n(\\ndnn_clf\\n)\\n\\t\\t\\n#\\tif\\tTensorFlow\\t>=\\t1.1\\ndnn_clf\\n.\\nfit\\n(\\nX_train\\n,\\n\\t\\ny_train\\n,\\n\\t\\nbatch_size\\n=\\n50\\n,\\n\\t\\nsteps\\n=', \"steps\\n=\\n40000\\n)\\nThe\\tcode\\tfirst\\tcreates\\ta\\tset\\tof\\treal\\tvalued\\tcolumns\\tfrom\\tthe\\ttraining\\tset\\t(other\\ttypes\\tof\\tcolumns,\\tsuch\\tas\\ncategorical\\tcolumns,\\tare\\tavailable).\\tThen\\twe\\tcreate\\tthe\\t\\nDNNClassifier\\n,\\tand\\twe\\twrap\\tit\\tin\\ta\\tScikit-\\nLearn\\tcompatibility\\thelper.\\tFinally,\\twe\\trun\\t40,000\\ttraining\\titerations\\tusing\\tbatches\\tof\\t50\\tinstances.\\nIf\\t\\nyou\\trun\\tthis\\tcode\\ton\\tthe\\tMNIST\\tdataset\\t(after\\tscaling\\tit,\\te.g.,\\tby\\tusing\\tScikit-Learn’s\\nStandardScaler\\n),\\t\\nyou\\twill\\tactually\\tget\\ta\\tmodel\\tthat\\tachieves\\taround\\t98.2%\\taccuracy\\ton\\tthe\\ttest\\tset!\\nThat’s\\tbetter\\tthan\\tthe\\tbest\\tmodel\\twe\\t\\ntrained\\tin\\t\\nChapter\\t3\\n:\\n>>>\\t\\nfrom\\n\\t\\nsklearn.metrics\\n\\t\\nimport\\n\\t\\naccuracy_score\\n>>>\\t\\ny_pred\\n\\t\\n=\\n\\t\\ndnn_clf\\n.\\npredict\\n(\\nX_test\\n)\\n>>>\\t\\naccuracy_score\\n(\\ny_test\\n,\\n\\t\\ny_pred\\n[\\n'classes'\\n])\\n0.98250000000000004\\nWARNING\\nThe\\t\\ntensorflow.contrib\\n\\tpackage\\t\\ncontains\\tmany\\tuseful\\tfunctions,\\tbut\\tit\\tis\\ta\\tplace\\tfor\\texperimental\\tcode\\tthat\\thas\\tnot\\tyet\\ngraduated\\tto\\tbe\\tpart\\tof\\tthe\\tcore\\tTensorFlow\\tAPI.\\tSo\\tthe\\t\\nDNNClassifier\\n\\tclass\\t(and\\tany\\tother\\t\\ncontrib\", 'contrib\\n\\tcode)\\tmay\\tchange\\nwithout\\tnotice\\tin\\tthe\\tfuture.\\nUnder\\tthe\\thood,\\tthe\\t\\nDNNClassifier\\n\\tclass\\tcreates\\tall\\tthe\\tneuron\\tlayers,\\tbased\\ton\\tthe\\tReLU\\tactivation\\nfunction\\t(we\\tcan\\tchange\\tthis\\tby\\tsetting\\tthe\\t\\nactivation_fn\\n\\thyperparameter).\\tThe\\toutput\\tlayer\\trelies\\ton\\nthe\\tsoftmax\\tfunction,\\t\\nand\\tthe\\t\\ncost\\tfunction\\tis\\tcross\\tentropy\\t(introduced\\tin\\t\\nChapter\\t4\\n).', 'Training\\ta\\tDNN\\tUsing\\tPlain\\tTensorFlow\\nIf\\t\\nyou\\twant\\tmore\\tcontrol\\tover\\tthe\\tarchitecture\\tof\\tthe\\tnetwork,\\tyou\\tmay\\tprefer\\tto\\tuse\\tTensorFlow’s\\tlower-\\nlevel\\tPython\\tAPI\\t(introduced\\tin\\t\\nChapter\\t9\\n).\\tIn\\tthis\\tsection\\twe\\twill\\tbuild\\tthe\\tsame\\tmodel\\tas\\tbefore\\tusing\\nthis\\tAPI,\\tand\\twe\\twill\\timplement\\tMini-batch\\tGradient\\tDescent\\tto\\ttrain\\tit\\ton\\tthe\\tMNIST\\tdataset.\\tThe\\tfirst\\nstep\\tis\\tthe\\tconstruction\\tphase,\\tbuilding\\tthe\\tTensorFlow\\tgraph.\\tThe\\tsecond\\tstep\\tis\\tthe\\texecution\\tphase,\\nwhere\\tyou\\tactually\\trun\\tthe\\tgraph\\tto\\ttrain\\tthe\\tmodel.', 'Construction\\tPhase\\nLet’s\\tstart.\\t\\nFirst\\twe\\tneed\\tto\\timport\\tthe\\t\\ntensorflow\\n\\tlibrary.\\tThen\\twe\\tmust\\tspecify\\tthe\\tnumber\\tof\\tinputs\\nand\\toutputs,\\tand\\tset\\tthe\\tnumber\\tof\\thidden\\tneurons\\tin\\teach\\tlayer:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nn_inputs\\n\\t\\n=\\n\\t\\n28\\n*\\n28\\n\\t\\t\\n#\\tMNIST\\nn_hidden1\\n\\t\\n=\\n\\t\\n300\\nn_hidden2\\n\\t\\n=\\n\\t\\n100\\nn_outputs\\n\\t\\n=\\n\\t\\n10\\nNext,\\tjust\\tlike\\tyou\\tdid\\tin\\t\\nChapter\\t9\\n,\\tyou\\tcan\\tuse\\tplaceholder\\tnodes\\tto\\trepresent\\tthe\\ttraining\\tdata\\tand\\ntargets.\\tThe\\tshape\\tof\\t\\nX\\n\\tis\\tonly\\tpartially\\tdefined.\\tWe\\tknow\\tthat\\tit\\twill\\tbe\\ta\\t2D\\ttensor\\t(i.e.,\\ta\\tmatrix),\\twith\\ninstances\\talong\\tthe\\tfirst\\tdimension\\tand\\tfeatures\\talong\\tthe\\tsecond\\tdimension,\\tand\\twe\\tknow\\tthat\\tthe\\tnumber\\nof\\tfeatures\\tis\\tgoing\\tto\\tbe\\t28\\tx\\t28\\t(one\\tfeature\\tper\\tpixel),\\tbut\\twe\\tdon’t\\tknow\\tyet\\thow\\tmany\\tinstances\\teach\\ntraining\\tbatch\\twill\\tcontain.\\tSo\\tthe\\tshape\\tof\\t\\nX\\n\\tis\\t\\n(None,\\tn_inputs)\\n.\\tSimilarly,\\twe\\tknow\\tthat\\t\\ny\\n\\twill\\tbe\\ta\\n1D\\ttensor\\twith\\tone\\tentry\\tper\\tinstance,\\tbut\\tagain\\twe\\tdon’t\\tknow\\tthe\\tsize\\tof\\tthe\\ttraining\\tbatch\\tat\\tthis\\tpoint,\\nso\\tthe\\t\\nshape\\tis\\t\\n(None)\\n.\\nX\\n\\t\\n=\\n\\t\\ntf\\n.', '=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_inputs\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint64\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n),\\n\\t\\nname\\n=\\n\"y\"\\n)\\nNow\\tlet’s\\tcreate\\tthe\\tactual\\tneural\\tnetwork.\\tThe\\tplaceholder\\t\\nX\\n\\twill\\tact\\tas\\tthe\\tinput\\tlayer;\\tduring\\tthe\\nexecution\\tphase,\\tit\\twill\\tbe\\treplaced\\twith\\tone\\ttraining\\tbatch\\tat\\ta\\ttime\\t(note\\tthat\\tall\\tthe\\tinstances\\tin\\ta\\ntraining\\tbatch\\twill\\tbe\\tprocessed\\tsimultaneously\\tby\\tthe\\tneural\\tnetwork).\\tNow\\tyou\\tneed\\tto\\tcreate\\tthe\\ttwo\\nhidden\\tlayers\\tand\\tthe\\toutput\\tlayer.\\tThe\\ttwo\\thidden\\tlayers\\tare\\talmost\\tidentical:\\tthey\\tdiffer\\tonly\\tby\\tthe\\ninputs\\tthey\\tare\\tconnected\\tto\\tand\\tby\\tthe\\tnumber\\tof\\tneurons\\tthey\\tcontain.\\tThe\\toutput\\tlayer\\tis\\talso\\tvery\\nsimilar,\\tbut\\tit\\tuses\\ta\\tsoftmax\\tactivation\\tfunction\\tinstead\\tof\\ta\\tReLU\\tactivation\\tfunction.\\tSo\\tlet’s\\tcreate\\ta\\nneuron_layer()\\n\\tfunction\\tthat\\twe\\twill\\tuse\\tto\\tcreate\\tone\\tlayer\\tat\\ta\\ttime.\\tIt\\twill\\tneed\\tparameters\\tto\\nspecify\\tthe\\tinputs,\\tthe\\tnumber\\tof\\tneurons,\\tthe\\tactivation\\tfunction,\\t\\nand\\tthe\\tname\\tof\\tthe\\tlayer:\\ndef', 'def\\n\\t\\nneuron_layer\\n(\\nX\\n,\\n\\t\\nn_neurons\\n,\\n\\t\\nname\\n,\\n\\t\\nactivation\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\nname\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nn_inputs\\n\\t\\n=\\n\\t\\nint\\n(\\nX\\n.\\nget_shape\\n()[\\n1\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nstddev\\n\\t\\n=\\n\\t\\n2\\n\\t\\n/\\n\\t\\nnp\\n.\\nsqrt\\n(\\nn_inputs\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\ntruncated_normal\\n((\\nn_inputs\\n,\\n\\t\\nn_neurons\\n),\\n\\t\\nstddev\\n=\\nstddev\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nW\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ninit\\n,\\n\\t\\nname\\n=\\n\"kernel\"\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n([\\nn_neurons\\n]),\\n\\t\\nname\\n=\\n\"bias\"\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nZ\\n\\t\\n=\\n\\t\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\nW\\n)\\n\\t\\n+\\n\\t\\nb\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nactivation\\n\\t\\nis\\n\\t\\nnot\\n\\t\\nNone\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nactivation\\n(\\nZ\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nZ\\nLet’s\\tgo\\tthrough\\tthis\\tcode\\tline\\tby\\tline:\\n1\\n.\\t\\nFirst\\twe\\tcreate\\ta\\tname\\tscope\\tusing\\tthe\\tname\\tof\\tthe\\tlayer:\\tit\\twill\\tcontain\\tall\\tthe\\tcomputation\\tnodes\\nfor\\tthis\\tneuron\\tlayer.\\tThis\\tis\\toptional,\\tbut\\tthe\\tgraph\\twill\\tlook\\tmuch\\tnicer\\tin\\tTensorBoard\\tif\\tits\\tnodes\\nare\\twell\\torganized.', '2\\n.\\t\\nNext,\\twe\\tget\\tthe\\tnumber\\tof\\tinputs\\tby\\tlooking\\tup\\tthe\\tinput\\tmatrix’s\\tshape\\tand\\tgetting\\tthe\\tsize\\tof\\tthe\\nsecond\\tdimension\\t(the\\tfirst\\tdimension\\tis\\tfor\\tinstances).\\n3\\n.\\t\\nThe\\tnext\\tthree\\tlines\\tcreate\\ta\\t\\nW\\n\\tvariable\\tthat\\twill\\thold\\tthe\\tweights\\tmatrix\\t(often\\tcalled\\tthe\\tlayer’s\\nkernel\\n).\\tIt\\twill\\tbe\\ta\\t2D\\ttensor\\tcontaining\\tall\\tthe\\tconnection\\tweights\\tbetween\\teach\\tinput\\tand\\teach\\nneuron;\\thence,\\tits\\tshape\\twill\\tbe\\t\\n(n_inputs,\\tn_neurons)\\n.\\tIt\\twill\\tbe\\tinitialized\\trandomly,\\tusing\\ta\\ntruncated\\n10\\n\\tnormal\\t(Gaussian)\\tdistribution\\twith\\ta\\tstandard\\tdeviation\\tof\\t\\n.\\tUsing\\tthis\\nspecific\\tstandard\\tdeviation\\thelps\\tthe\\talgorithm\\tconverge\\tmuch\\tfaster\\t(we\\twill\\tdiscuss\\tthis\\tfurther\\tin\\nChapter\\t11\\n;\\tit\\tis\\tone\\tof\\tthose\\tsmall\\ttweaks\\tto\\tneural\\tnetworks\\tthat\\thave\\thad\\ta\\ttremendous\\timpact\\ton\\ntheir\\tefficiency).\\tIt\\tis\\timportant\\tto\\tinitialize\\tconnection\\tweights\\trandomly\\tfor\\tall\\thidden\\tlayers\\tto\\navoid\\tany\\tsymmetries\\tthat\\tthe\\tGradient\\tDescent\\talgorithm\\twould\\tbe\\tunable\\tto\\tbreak.\\n11\\n4\\n.\\t\\nThe\\tnext\\tline\\tcreates\\ta\\t\\nb', 'b\\n\\tvariable\\tfor\\tbiases,\\tinitialized\\tto\\t0\\t(no\\tsymmetry\\tissue\\tin\\tthis\\tcase),\\twith\\none\\tbias\\tparameter\\tper\\tneuron.\\n5\\n.\\t\\nThen\\twe\\tcreate\\ta\\tsubgraph\\tto\\tcompute\\t\\nZ\\n\\t=\\t\\nX\\n\\t·\\t\\nW\\n\\t+\\t\\nb\\n.\\tThis\\tvectorized\\timplementation\\twill\\nefficiently\\tcompute\\tthe\\tweighted\\tsums\\tof\\tthe\\tinputs\\tplus\\tthe\\tbias\\tterm\\tfor\\teach\\tand\\tevery\\tneuron\\tin\\nthe\\tlayer,\\tfor\\tall\\tthe\\tinstances\\tin\\tthe\\tbatch\\tin\\tjust\\tone\\tshot.\\n6\\n.\\t\\nFinally,\\tif\\tan\\t\\nactivation\\n\\tparameter\\tis\\tprovided,\\tsuch\\tas\\t\\ntf.nn.relu\\n\\t\\n(i.e.,\\tmax\\t(0,\\t\\nZ\\n)),\\tthen\\tthe\\ncode\\treturns\\t\\nactivation(Z)\\n,\\tor\\telse\\tit\\tjust\\treturns\\t\\nZ\\n.\\nOkay,\\tso\\tnow\\tyou\\thave\\ta\\tnice\\tfunction\\tto\\tcreate\\ta\\tneuron\\tlayer.\\tLet’s\\tuse\\tit\\tto\\tcreate\\tthe\\tdeep\\tneural\\nnetwork!\\tThe\\tfirst\\thidden\\tlayer\\ttakes\\t\\nX\\n\\tas\\tits\\tinput.\\tThe\\tsecond\\ttakes\\tthe\\toutput\\tof\\tthe\\tfirst\\thidden\\tlayer\\tas\\nits\\tinput.\\tAnd\\tfinally,\\tthe\\toutput\\tlayer\\ttakes\\tthe\\toutput\\tof\\tthe\\tsecond\\thidden\\tlayer\\tas\\tits\\tinput.\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):\\n\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\nneuron_layer\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n,', 'activation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\nneuron_layer\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\nneuron_layer\\n(\\nhidden2\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\nNotice\\tthat\\tonce\\tagain\\twe\\tused\\ta\\t\\nname\\tscope\\tfor\\tclarity.\\tAlso\\tnote\\tthat\\t\\nlogits\\n\\tis\\tthe\\toutput\\tof\\tthe\\tneural\\nnetwork\\t\\nbefore\\n\\tgoing\\tthrough\\tthe\\tsoftmax\\tactivation\\tfunction:\\tfor\\toptimization\\treasons,\\twe\\twill\\thandle\\tthe\\nsoftmax\\tcomputation\\tlater.\\nAs\\tyou\\tmight\\texpect,\\tTensorFlow\\tcomes\\twith\\tmany\\thandy\\tfunctions\\tto\\tcreate\\t\\nstandard\\n\\tneural\\tnetwork\\nlayers,\\tso\\tthere’s\\toften\\tno\\tneed\\tto\\tdefine\\tyour\\town\\t\\nneuron_layer()\\n\\t\\nfunction\\tlike\\twe\\tjust\\tdid.\\tFor\\nexample,\\tTensorFlow’s\\t\\ntf.layers.dense()\\n\\t\\nfunction\\t(previously\\tcalled\\ntf.contrib.layers.fully_connected()\\n)\\tcreates\\ta\\tfully\\tconnected\\tlayer,\\twhere\\tall\\tthe\\tinputs\\tare\\nconnected\\tto\\tall\\tthe\\tneurons\\tin\\tthe\\t\\nlayer.\\tIt\\ttakes\\tcare\\tof\\tcreating\\tthe\\t\\nweights\\tand\\t\\nbiases\\tvariables,\\tnamed\\nkernel\\n\\tand\\t\\nbias', 'bias\\n\\trespectively,\\tusing\\tthe\\tappropriate\\tinitialization\\tstrategy,\\tand\\tyou\\tcan\\tset\\tthe\\tactivation\\nfunction\\tusing\\tthe\\t\\nactivation\\n\\targument.\\tAs\\twe\\twill\\tsee\\tin\\t\\nChapter\\t11\\n,\\tit\\talso\\tsupports\\tregularization\\nparameters.\\tLet’s\\ttweak\\tthe\\tpreceding\\tcode\\tto\\tuse\\tthe\\t\\ndense()\\n\\tfunction\\tinstead\\tof\\tour\\t\\nneuron_layer()\\nfunction.\\tSimply\\treplace\\tthe\\t\\ndnn\\n\\tconstruction\\tsection\\twith\\tthe\\tfollowing\\tcode:\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):', 'hidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden2\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\nNow\\tthat\\twe\\thave\\tthe\\tneural\\tnetwork\\tmodel\\tready\\tto\\tgo,\\twe\\tneed\\tto\\tdefine\\tthe\\t\\ncost\\tfunction\\tthat\\twe\\twill\\nuse\\tto\\ttrain\\tit.\\tJust\\tas\\twe\\tdid\\tfor\\tSoftmax\\tRegression\\tin\\t\\nChapter\\t4\\n,\\twe\\twill\\tuse\\tcross\\tentropy.\\tAs\\twe\\ndiscussed\\tearlier,\\tcross\\tentropy\\twill\\tpenalize\\tmodels\\tthat\\testimate\\ta\\tlow\\tprobability\\tfor\\tthe\\ttarget\\tclass.\\nTensorFlow\\tprovides\\t\\nseveral\\n\\tfunctions\\tto\\tcompute\\tcross\\tentropy.\\t\\nWe\\twill\\tuse\\nsparse_softmax_cross_entropy_with_logits()\\n:\\tit\\tcomputes\\tthe\\tcross\\tentropy\\tbased\\ton\\tthe\\t“logits”\\n(i.e.,\\tthe\\toutput\\tof\\tthe\\tnetwork\\t\\nbefore\\n\\tgoing\\tthrough\\tthe\\tsoftmax\\tactivation\\tfunction),\\tand\\tit\\texpects\\tlabels', 'in\\tthe\\tform\\tof\\tintegers\\tranging\\tfrom\\t0\\tto\\tthe\\tnumber\\tof\\tclasses\\tminus\\t1\\t(in\\tour\\tcase,\\tfrom\\t0\\tto\\t9).\\tThis\\nwill\\tgive\\tus\\ta\\t1D\\ttensor\\tcontaining\\tthe\\tcross\\tentropy\\tfor\\teach\\tinstance.\\tWe\\tcan\\tthen\\tuse\\tTensorFlow’s\\nreduce_mean()\\n\\tfunction\\t\\nto\\tcompute\\tthe\\tmean\\tcross\\tentropy\\tover\\tall\\tinstances.\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"loss\"\\n):\\n\\t\\t\\t\\t\\nxentropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsparse_softmax_cross_entropy_with_logits\\n(\\nlabels\\n=\\ny\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlogits\\n=\\nlogits\\n)\\n\\t\\t\\t\\t\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\nxentropy\\n,\\n\\t\\nname\\n=\\n\"loss\"\\n)\\nNOTE\\nThe\\t\\nsparse_softmax_cross_entropy_with_logits()\\n\\tfunction\\tis\\tequivalent\\t\\nto\\tapplying\\tthe\\tsoftmax\\tactivation\\tfunction\\tand\\tthen\\ncomputing\\tthe\\tcross\\tentropy,\\tbut\\tit\\tis\\tmore\\tefficient,\\tand\\tit\\tproperly\\ttakes\\tcare\\tof\\tcorner\\tcases\\tlike\\tlogits\\tequal\\tto\\t0.\\tThis\\tis\\twhy\\nwe\\tdid\\tnot\\tapply\\tthe\\tsoftmax\\tactivation\\tfunction\\tearlier.\\tThere\\tis\\talso\\tanother\\tfunction\\tcalled\\nsoftmax_cross_entropy_with_logits()', ',\\twhich\\ttakes\\tlabels\\tin\\tthe\\tform\\tof\\tone-hot\\tvectors\\t(instead\\tof\\tints\\tfrom\\t0\\tto\\tthe\\tnumber\\nof\\tclasses\\tminus\\t1).\\nWe\\thave\\tthe\\tneural\\tnetwork\\tmodel,\\twe\\thave\\tthe\\t\\ncost\\tfunction,\\tand\\tnow\\twe\\tneed\\tto\\tdefine\\ta\\nGradientDescentOptimizer\\n\\tthat\\t\\nwill\\ttweak\\tthe\\t\\nmodel\\tparameters\\tto\\tminimize\\tthe\\tcost\\tfunction.\\nNothing\\tnew;\\tit’s\\tjust\\tlike\\twe\\tdid\\tin\\t\\nChapter\\t9\\n:\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"train\"\\n):\\n\\t\\t\\t\\t\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nGradientDescentOptimizer\\n(\\nlearning_rate\\n)\\n\\t\\t\\t\\t\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\nThe\\tlast\\timportant\\tstep\\tin\\tthe\\tconstruction\\tphase\\tis\\tto\\tspecify\\thow\\tto\\tevaluate\\tthe\\tmodel.\\t\\nWe\\twill\\tsimply\\nuse\\taccuracy\\tas\\tour\\tperformance\\tmeasure.\\tFirst,\\tfor\\teach\\tinstance,\\tdetermine\\tif\\tthe\\tneural\\tnetwork’s\\nprediction\\tis\\tcorrect\\tby\\tchecking\\twhether\\tor\\tnot\\tthe\\thighest\\tlogit\\tcorresponds\\tto\\tthe\\ttarget\\tclass.\\tFor\\tthis\\nyou\\tcan\\tuse\\tthe\\t\\nin_top_k()\\n\\tfunction.\\t\\nThis\\treturns\\ta\\t1D\\ttensor\\tfull\\tof\\tboolean\\tvalues,\\tso\\twe\\tneed\\tto\\tcast', 'these\\tbooleans\\tto\\tfloats\\tand\\tthen\\tcompute\\tthe\\taverage.\\tThis\\twill\\tgive\\tus\\tthe\\tnetwork’s\\toverall\\taccuracy.\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"eval\"\\n):\\n\\t\\t\\t\\t\\ncorrect\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nin_top_k\\n(\\nlogits\\n,\\n\\t\\ny\\n,\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\naccuracy\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\ncast\\n(\\ncorrect\\n,\\n\\t\\ntf\\n.\\nfloat32\\n))\\nAnd,\\tas\\tusual,\\t\\nwe\\tneed\\tto\\tcreate\\ta\\tnode\\tto\\tinitialize\\tall\\tvariables,\\tand\\twe\\twill\\talso\\tcreate\\ta\\t\\nSaver\\n\\tto', 'save\\tour\\ttrained\\tmodel\\tparameters\\tto\\tdisk:\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\nPhew!\\tThis\\tconcludes\\tthe\\tconstruction\\tphase.\\tThis\\twas\\tfewer\\tthan\\t40\\tlines\\tof\\tcode,\\tbut\\tit\\twas\\tpretty\\nintense:\\twe\\tcreated\\tplaceholders\\tfor\\tthe\\tinputs\\tand\\tthe\\ttargets,\\twe\\tcreated\\ta\\tfunction\\tto\\tbuild\\ta\\tneuron\\nlayer,\\twe\\tused\\tit\\tto\\tcreate\\tthe\\tDNN,\\twe\\tdefined\\tthe\\tcost\\tfunction,\\twe\\tcreated\\tan\\toptimizer,\\tand\\tfinally\\twe\\ndefined\\tthe\\tperformance\\tmeasure.\\tNow\\ton\\tto\\tthe\\t\\nexecution\\tphase.', 'Execution\\tPhase\\nThis\\t\\npart\\tis\\tmuch\\tshorter\\tand\\tsimpler.\\tFirst,\\tlet’s\\tload\\tMNIST.\\tWe\\tcould\\tuse\\tScikit-Learn\\tfor\\tthat\\tas\\twe\\ndid\\tin\\tprevious\\tchapters,\\tbut\\tTensorFlow\\toffers\\tits\\town\\thelper\\tthat\\tfetches\\tthe\\tdata,\\tscales\\tit\\t(between\\t0\\nand\\t1),\\tshuffles\\tit,\\tand\\tprovides\\ta\\tsimple\\tfunction\\tto\\tload\\tone\\tmini-batch\\ta\\ttime.\\tSo\\tlet’s\\tuse\\tit\\tinstead:\\nfrom\\n\\t\\ntensorflow.examples.tutorials.mnist\\n\\t\\nimport\\n\\t\\ninput_data\\nmnist\\n\\t\\n=\\n\\t\\ninput_data\\n.\\nread_data_sets\\n(\\n\"/tmp/data/\"\\n)\\nNow\\twe\\tdefine\\tthe\\tnumber\\tof\\tepochs\\tthat\\twe\\twant\\tto\\trun,\\tas\\twell\\tas\\tthe\\tsize\\tof\\tthe\\tmini-batches:\\nn_epochs\\n\\t\\n=\\n\\t\\n40\\nbatch_size\\n\\t\\n=\\n\\t\\n50\\nAnd\\tnow\\twe\\tcan\\ttrain\\tthe\\tmodel:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})', 'acc_train\\n\\t\\n=\\n\\t\\naccuracy\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nacc_test\\n\\t\\n=\\n\\t\\naccuracy\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nmnist\\n.\\ntest\\n.\\nimages\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ny\\n:\\n\\t\\nmnist\\n.\\ntest\\n.\\nlabels\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nprint\\n(\\nepoch\\n,\\n\\t\\n\"Train\\taccuracy:\"\\n,\\n\\t\\nacc_train\\n,\\n\\t\\n\"Test\\taccuracy:\"\\n,\\n\\t\\nacc_test\\n)\\n\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\nThis\\tcode\\topens\\ta\\tTensorFlow\\tsession,\\tand\\tit\\truns\\tthe\\t\\ninit\\n\\tnode\\tthat\\tinitializes\\tall\\tthe\\tvariables.\\tThen\\tit\\nruns\\tthe\\tmain\\ttraining\\tloop:\\tat\\teach\\tepoch,\\tthe\\tcode\\titerates\\tthrough\\ta\\tnumber\\tof\\tmini-batches\\tthat\\ncorresponds\\tto\\tthe\\ttraining\\tset\\tsize.\\tEach\\tmini-batch\\tis\\tfetched\\tvia\\tthe\\t\\nnext_batch()\\n\\t\\nmethod,\\tand\\tthen\\nthe\\tcode\\tsimply\\truns\\tthe\\ttraining\\toperation,\\tfeeding\\tit\\tthe\\tcurrent\\tmini-batch\\tinput\\tdata\\tand\\ttargets.\\tNext,\\nat\\tthe\\tend\\tof\\teach\\tepoch,\\tthe\\tcode\\tevaluates\\tthe\\tmodel\\ton\\tthe\\tlast\\tmini-batch\\tand\\ton\\tthe\\tfull\\ttest\\tset,\\tand\\tit', 'prints\\tout\\tthe\\tresult.\\tFinally,\\tthe\\tmodel\\tparameters\\tare\\tsaved\\tto\\tdisk.', 'Using\\tthe\\tNeural\\tNetwork\\nNow\\t\\nthat\\tthe\\tneural\\tnetwork\\tis\\ttrained,\\tyou\\tcan\\tuse\\tit\\tto\\tmake\\tpredictions.\\tTo\\tdo\\tthat,\\tyou\\tcan\\treuse\\tthe\\nsame\\tconstruction\\tphase,\\tbut\\tchange\\tthe\\texecution\\tphase\\tlike\\tthis:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\nX_new_scaled\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\t\\n#\\tsome\\tnew\\timages\\t(scaled\\tfrom\\t0\\tto\\t1)\\n\\t\\t\\t\\t\\nZ\\n\\t\\n=\\n\\t\\nlogits\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_new_scaled\\n})\\n\\t\\t\\t\\t\\ny_pred\\n\\t\\n=\\n\\t\\nnp\\n.\\nargmax\\n(\\nZ\\n,\\n\\t\\naxis\\n=\\n1\\n)\\nFirst\\tthe\\tcode\\tloads\\tthe\\tmodel\\tparameters\\tfrom\\tdisk.\\tThen\\tit\\tloads\\tsome\\tnew\\timages\\tthat\\tyou\\twant\\tto\\nclassify.\\tRemember\\tto\\tapply\\tthe\\tsame\\tfeature\\tscaling\\tas\\tfor\\tthe\\ttraining\\tdata\\t(in\\tthis\\tcase,\\tscale\\tit\\tfrom\\t0\\nto\\t1).\\tThen\\tthe\\tcode\\tevaluates\\tthe\\t\\nlogits\\n\\tnode.\\tIf\\tyou\\twanted\\tto\\tknow\\tall\\tthe\\testimated\\tclass\\nprobabilities,\\tyou\\twould\\tneed\\tto\\tapply\\tthe\\t\\nsoftmax()\\n\\tfunction\\tto\\tthe\\tlogits,\\tbut\\tif\\tyou\\tjust\\twant\\tto\\tpredict\\na\\tclass,\\tyou\\tcan\\tsimply\\tpick\\tthe\\tclass\\tthat\\thas\\tthe\\thighest\\tlogit\\tvalue\\t\\n(using\\tthe\\t\\nargmax()', 'argmax()\\n\\tfunction\\tdoes\\nthe\\ttrick).', 'Fine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\nThe\\t\\nflexibility\\tof\\tneural\\tnetworks\\tis\\talso\\tone\\tof\\ttheir\\tmain\\tdrawbacks:\\tthere\\tare\\tmany\\thyperparameters\\tto\\ntweak.\\tNot\\tonly\\tcan\\tyou\\tuse\\tany\\t\\nimaginable\\t\\nnetwork\\ttopology\\n\\t(how\\tneurons\\tare\\tinterconnected),\\tbut\\teven\\nin\\ta\\tsimple\\tMLP\\tyou\\tcan\\tchange\\tthe\\tnumber\\tof\\tlayers,\\tthe\\tnumber\\tof\\tneurons\\tper\\tlayer,\\tthe\\ttype\\tof\\nactivation\\tfunction\\tto\\tuse\\tin\\teach\\tlayer,\\tthe\\tweight\\tinitialization\\tlogic,\\tand\\tmuch\\tmore.\\tHow\\tdo\\tyou\\tknow\\nwhat\\tcombination\\tof\\thyperparameters\\tis\\tthe\\tbest\\tfor\\tyour\\ttask?\\nOf\\tcourse,\\tyou\\tcan\\tuse\\tgrid\\tsearch\\twith\\tcross-validation\\tto\\tfind\\tthe\\tright\\thyperparameters,\\tlike\\tyou\\tdid\\tin\\nprevious\\tchapters,\\tbut\\tsince\\tthere\\tare\\tmany\\thyperparameters\\tto\\ttune,\\tand\\tsince\\ttraining\\ta\\tneural\\tnetwork\\non\\ta\\tlarge\\tdataset\\ttakes\\ta\\tlot\\tof\\ttime,\\tyou\\twill\\tonly\\tbe\\table\\tto\\texplore\\ta\\ttiny\\tpart\\tof\\tthe\\thyperparameter\\nspace\\tin\\ta\\treasonable\\tamount\\tof\\ttime.\\tIt\\tis\\tmuch\\tbetter\\tto\\tuse\\t\\nrandomized\\tsearch\\n,\\t\\nas\\twe\\tdiscussed\\tin\\nChapter\\t2\\n.\\tAnother\\toption\\tis\\tto\\tuse\\ta\\ttool\\tsuch\\tas', 'Oscar\\n,\\twhich\\timplements\\tmore\\tcomplex\\talgorithms\\tto\\nhelp\\tyou\\tfind\\ta\\tgood\\tset\\tof\\thyperparameters\\tquickly.\\nIt\\thelps\\tto\\thave\\tan\\tidea\\tof\\twhat\\tvalues\\tare\\treasonable\\tfor\\teach\\thyperparameter,\\tso\\tyou\\tcan\\trestrict\\tthe\\nsearch\\tspace.\\tLet’s\\tstart\\twith\\tthe\\tnumber\\tof\\thidden\\tlayers.', 'Number\\tof\\tHidden\\tLayers\\nFor\\t\\nmany\\tproblems,\\tyou\\tcan\\tjust\\tbegin\\twith\\ta\\tsingle\\thidden\\tlayer\\tand\\tyou\\twill\\tget\\treasonable\\tresults.\\tIt\\nhas\\tactually\\tbeen\\tshown\\tthat\\tan\\tMLP\\twith\\tjust\\tone\\thidden\\tlayer\\tcan\\tmodel\\teven\\tthe\\tmost\\tcomplex\\nfunctions\\tprovided\\tit\\thas\\tenough\\tneurons.\\tFor\\ta\\tlong\\ttime,\\tthese\\tfacts\\tconvinced\\tresearchers\\tthat\\tthere\\nwas\\tno\\tneed\\tto\\tinvestigate\\tany\\tdeeper\\tneural\\tnetworks.\\tBut\\tthey\\toverlooked\\tthe\\tfact\\tthat\\tdeep\\tnetworks\\nhave\\ta\\tmuch\\thigher\\t\\nparameter\\tefficiency\\n\\t\\nthan\\tshallow\\tones:\\tthey\\tcan\\tmodel\\tcomplex\\tfunctions\\tusing\\nexponentially\\tfewer\\tneurons\\tthan\\tshallow\\tnets,\\tmaking\\tthem\\tmuch\\tfaster\\tto\\ttrain.\\nTo\\tunderstand\\twhy,\\tsuppose\\tyou\\tare\\tasked\\tto\\tdraw\\ta\\tforest\\tusing\\tsome\\tdrawing\\tsoftware,\\tbut\\tyou\\tare\\nforbidden\\tto\\tuse\\tcopy/paste.\\tYou\\twould\\thave\\tto\\tdraw\\teach\\ttree\\tindividually,\\tbranch\\tper\\tbranch,\\tleaf\\tper\\nleaf.\\tIf\\tyou\\tcould\\tinstead\\tdraw\\tone\\tleaf,\\tcopy/paste\\tit\\tto\\tdraw\\ta\\tbranch,\\tthen\\tcopy/paste\\tthat\\tbranch\\tto', 'create\\ta\\ttree,\\tand\\tfinally\\tcopy/paste\\tthis\\ttree\\tto\\tmake\\ta\\tforest,\\tyou\\twould\\tbe\\tfinished\\tin\\tno\\ttime.\\tReal-\\nworld\\tdata\\tis\\toften\\tstructured\\tin\\tsuch\\ta\\thierarchical\\tway\\tand\\tDNNs\\tautomatically\\ttake\\tadvantage\\tof\\tthis\\nfact:\\tlower\\thidden\\tlayers\\tmodel\\tlow-level\\tstructures\\t(e.g.,\\tline\\tsegments\\tof\\tvarious\\tshapes\\tand\\norientations),\\tintermediate\\thidden\\tlayers\\tcombine\\tthese\\tlow-level\\tstructures\\tto\\tmodel\\tintermediate-level\\nstructures\\t(e.g.,\\tsquares,\\tcircles),\\tand\\tthe\\thighest\\thidden\\tlayers\\tand\\tthe\\toutput\\tlayer\\tcombine\\tthese\\nintermediate\\tstructures\\tto\\tmodel\\thigh-level\\tstructures\\t(e.g.,\\tfaces).\\nNot\\tonly\\tdoes\\tthis\\thierarchical\\tarchitecture\\thelp\\tDNNs\\tconverge\\tfaster\\tto\\ta\\tgood\\tsolution,\\tit\\talso\\nimproves\\ttheir\\tability\\tto\\tgeneralize\\tto\\tnew\\tdatasets.\\tFor\\texample,\\tif\\tyou\\thave\\talready\\ttrained\\ta\\tmodel\\tto\\nrecognize\\tfaces\\tin\\tpictures,\\tand\\tyou\\tnow\\twant\\tto\\ttrain\\ta\\tnew\\tneural\\tnetwork\\tto\\trecognize\\thairstyles,\\tthen', 'you\\tcan\\tkickstart\\ttraining\\tby\\treusing\\tthe\\tlower\\tlayers\\tof\\tthe\\tfirst\\tnetwork.\\tInstead\\tof\\trandomly\\tinitializing\\nthe\\tweights\\tand\\tbiases\\tof\\tthe\\tfirst\\tfew\\tlayers\\tof\\tthe\\tnew\\tneural\\tnetwork,\\tyou\\tcan\\tinitialize\\tthem\\tto\\tthe\\nvalue\\tof\\tthe\\tweights\\tand\\tbiases\\tof\\tthe\\tlower\\tlayers\\tof\\tthe\\tfirst\\tnetwork.\\tThis\\tway\\tthe\\tnetwork\\twill\\tnot\\nhave\\tto\\tlearn\\tfrom\\tscratch\\tall\\tthe\\tlow-level\\tstructures\\tthat\\toccur\\tin\\tmost\\tpictures;\\tit\\twill\\tonly\\thave\\tto\\nlearn\\tthe\\thigher-level\\tstructures\\t(e.g.,\\thairstyles).\\nIn\\tsummary,\\tfor\\tmany\\tproblems\\tyou\\tcan\\tstart\\twith\\tjust\\tone\\tor\\ttwo\\thidden\\tlayers\\tand\\tit\\twill\\twork\\tjust\\tfine\\n(e.g.,\\tyou\\tcan\\teasily\\treach\\tabove\\t97%\\taccuracy\\ton\\tthe\\tMNIST\\tdataset\\tusing\\tjust\\tone\\thidden\\tlayer\\twith\\ta\\nfew\\thundred\\tneurons,\\tand\\tabove\\t98%\\taccuracy\\tusing\\ttwo\\thidden\\tlayers\\twith\\tthe\\tsame\\ttotal\\tamount\\tof\\nneurons,\\tin\\troughly\\tthe\\tsame\\tamount\\tof\\ttraining\\ttime).\\tFor\\tmore\\tcomplex\\tproblems,\\tyou\\tcan\\tgradually\\nramp\\tup\\tthe\\tnumber\\tof\\thidden\\tlayers,\\tuntil\\tyou\\tstart\\toverfitting\\tthe\\ttraining\\tset.\\tVery\\tcomplex\\ttasks,\\tsuch', 'as\\tlarge\\timage\\tclassification\\tor\\tspeech\\trecognition,\\ttypically\\trequire\\tnetworks\\twith\\tdozens\\tof\\tlayers\\t(or\\neven\\thundreds,\\tbut\\tnot\\tfully\\tconnected\\tones,\\tas\\twe\\twill\\tsee\\tin\\t\\nChapter\\t13\\n),\\tand\\tthey\\tneed\\ta\\thuge\\tamount\\nof\\ttraining\\tdata.\\tHowever,\\tyou\\twill\\trarely\\thave\\tto\\ttrain\\tsuch\\tnetworks\\tfrom\\tscratch:\\tit\\tis\\tmuch\\tmore\\ncommon\\tto\\treuse\\tparts\\tof\\ta\\tpretrained\\tstate-of-the-art\\tnetwork\\tthat\\tperforms\\ta\\tsimilar\\ttask.\\tTraining\\twill\\nbe\\ta\\tlot\\tfaster\\tand\\trequire\\tmuch\\tless\\t\\ndata\\t(we\\twill\\tdiscuss\\tthis\\tin\\t\\nChapter\\t11\\n).', 'Number\\tof\\tNeurons\\tper\\tHidden\\tLayer\\nObviously\\t\\nthe\\tnumber\\tof\\tneurons\\tin\\tthe\\tinput\\tand\\toutput\\tlayers\\tis\\tdetermined\\tby\\tthe\\ttype\\tof\\tinput\\tand\\noutput\\tyour\\ttask\\trequires.\\tFor\\texample,\\tthe\\tMNIST\\ttask\\trequires\\t28\\tx\\t28\\t=\\t784\\tinput\\tneurons\\tand\\t10\\noutput\\tneurons.\\tAs\\tfor\\tthe\\thidden\\tlayers,\\ta\\tcommon\\tpractice\\tis\\tto\\tsize\\tthem\\tto\\tform\\ta\\tfunnel,\\twith\\tfewer\\nand\\tfewer\\tneurons\\tat\\teach\\tlayer\\t—\\tthe\\trationale\\tbeing\\tthat\\tmany\\tlow-level\\tfeatures\\tcan\\tcoalesce\\tinto\\tfar\\nfewer\\thigh-level\\tfeatures.\\tFor\\texample,\\ta\\ttypical\\tneural\\tnetwork\\tfor\\tMNIST\\tmay\\thave\\ttwo\\thidden\\tlayers,\\nthe\\tfirst\\twith\\t300\\tneurons\\tand\\tthe\\tsecond\\twith\\t100.\\tHowever,\\tthis\\tpractice\\tis\\tnot\\tas\\tcommon\\tnow,\\tand\\nyou\\tmay\\tsimply\\tuse\\tthe\\tsame\\tsize\\tfor\\tall\\thidden\\tlayers\\t—\\tfor\\texample,\\tall\\thidden\\tlayers\\twith\\t150\\nneurons:\\tthat’s\\tjust\\tone\\thyperparameter\\tto\\ttune\\tinstead\\tof\\tone\\tper\\tlayer.\\tJust\\tlike\\tfor\\tthe\\tnumber\\tof\\tlayers,\\nyou\\tcan\\ttry\\tincreasing\\tthe\\tnumber\\tof\\tneurons\\tgradually\\tuntil\\tthe\\tnetwork\\tstarts\\toverfitting.\\tIn\\tgeneral\\tyou', 'will\\tget\\tmore\\tbang\\tfor\\tthe\\tbuck\\tby\\tincreasing\\tthe\\tnumber\\tof\\tlayers\\tthan\\tthe\\tnumber\\tof\\tneurons\\tper\\tlayer.\\nUnfortunately,\\tas\\tyou\\tcan\\tsee,\\tfinding\\tthe\\tperfect\\tamount\\tof\\tneurons\\tis\\tstill\\tsomewhat\\tof\\ta\\tblack\\tart.\\nA\\tsimpler\\tapproach\\tis\\tto\\tpick\\ta\\tmodel\\twith\\tmore\\tlayers\\tand\\tneurons\\tthan\\tyou\\tactually\\tneed,\\tthen\\tuse\\t\\nearly\\nstopping\\tto\\tprevent\\tit\\tfrom\\t\\noverfitting\\t(and\\tother\\tregularization\\ttechniques,\\tespecially\\t\\ndropout\\n,\\tas\\t\\nwe\\twill\\nsee\\tin\\t\\nChapter\\t11\\n).\\tThis\\thas\\tbeen\\tdubbed\\tthe\\t“stretch\\tpants”\\tapproach:\\n12\\n\\tinstead\\tof\\twasting\\ttime\\tlooking\\nfor\\tpants\\tthat\\tperfectly\\tmatch\\tyour\\tsize,\\tjust\\tuse\\tlarge\\tstretch\\tpants\\tthat\\twill\\tshrink\\tdown\\tto\\tthe\\tright\\tsize.', 'Activation\\tFunctions\\nIn\\t\\nmost\\tcases\\tyou\\tcan\\tuse\\tthe\\t\\nReLU\\tactivation\\tfunction\\tin\\tthe\\thidden\\tlayers\\t(or\\tone\\tof\\tits\\tvariants,\\tas\\twe\\nwill\\tsee\\tin\\t\\nChapter\\t11\\n).\\tIt\\tis\\ta\\tbit\\tfaster\\tto\\tcompute\\tthan\\tother\\tactivation\\tfunctions,\\tand\\tGradient\\tDescent\\ndoes\\tnot\\tget\\tstuck\\tas\\tmuch\\ton\\tplateaus,\\tthanks\\tto\\tthe\\tfact\\tthat\\tit\\tdoes\\tnot\\tsaturate\\tfor\\tlarge\\tinput\\tvalues\\t(as\\nopposed\\tto\\tthe\\tlogistic\\tfunction\\tor\\tthe\\t\\nhyperbolic\\ttangent\\tfunction,\\twhich\\tsaturate\\tat\\t1).\\nFor\\tthe\\toutput\\tlayer,\\tthe\\tsoftmax\\tactivation\\tfunction\\tis\\tgenerally\\ta\\tgood\\tchoice\\tfor\\tclassification\\ttasks\\n(when\\tthe\\tclasses\\tare\\tmutually\\texclusive).\\tFor\\tregression\\ttasks,\\tyou\\tcan\\tsimply\\tuse\\tno\\tactivation\\tfunction\\nat\\tall.\\nThis\\tconcludes\\tthis\\tintroduction\\tto\\tartificial\\tneural\\tnetworks.\\tIn\\tthe\\tfollowing\\tchapters,\\twe\\twill\\tdiscuss\\ntechniques\\tto\\ttrain\\tvery\\tdeep\\tnets,\\tand\\tdistribute\\ttraining\\tacross\\tmultiple\\tservers\\tand\\tGPUs.\\tThen\\twe\\twill\\nexplore\\ta\\tfew\\tother\\tpopular\\tneural\\tnetwork\\tarchitectures:\\tconvolutional\\tneural\\tnetworks,\\trecurrent\\tneural\\nnetworks,\\tand', 'autoencoders.\\n13', 'Exercises\\n1\\n.\\t\\nDraw\\tan\\tANN\\tusing\\tthe\\toriginal\\tartificial\\tneurons\\t(like\\tthe\\tones\\tin\\t\\nFigure\\t10-3\\n)\\tthat\\tcomputes\\t\\nA\\n\\t\\nB\\n\\t(where\\t\\n\\trepresents\\tthe\\tXOR\\toperation).\\tHint:\\t\\nA\\n\\t\\n\\t\\nB\\n\\t=\\t(\\nA\\n\\t\\n\\t¬\\t\\nB\\n)\\t\\n\\t(¬\\t\\nA\\n\\t\\n\\t\\nB\\n).\\n2\\n.\\t\\nWhy\\tis\\tit\\tgenerally\\tpreferable\\tto\\tuse\\ta\\tLogistic\\tRegression\\tclassifier\\trather\\tthan\\ta\\tclassical\\nPerceptron\\t(i.e.,\\ta\\tsingle\\tlayer\\tof\\tlinear\\tthreshold\\tunits\\ttrained\\tusing\\tthe\\tPerceptron\\ttraining\\nalgorithm)?\\tHow\\tcan\\tyou\\ttweak\\ta\\tPerceptron\\tto\\tmake\\tit\\tequivalent\\tto\\ta\\tLogistic\\tRegression\\nclassifier?\\n3\\n.\\t\\nWhy\\twas\\tthe\\tlogistic\\tactivation\\tfunction\\ta\\tkey\\tingredient\\tin\\ttraining\\tthe\\tfirst\\tMLPs?\\n4\\n.\\t\\nName\\tthree\\tpopular\\tactivation\\tfunctions.\\tCan\\tyou\\tdraw\\tthem?\\n5\\n.\\t\\nSuppose\\tyou\\thave\\tan\\tMLP\\tcomposed\\tof\\tone\\tinput\\tlayer\\twith\\t10\\tpassthrough\\tneurons,\\tfollowed\\tby\\none\\thidden\\tlayer\\twith\\t50\\tartificial\\tneurons,\\tand\\tfinally\\tone\\toutput\\tlayer\\twith\\t3\\tartificial\\tneurons.\\tAll\\nartificial\\tneurons\\tuse\\tthe\\tReLU\\tactivation\\tfunction.\\nWhat\\tis\\tthe\\tshape\\tof\\tthe\\tinput\\tmatrix\\t\\nX\\n?', 'X\\n?\\nWhat\\tabout\\tthe\\tshape\\tof\\tthe\\thidden\\tlayer’s\\tweight\\tvector\\t\\nW\\nh\\n,\\tand\\tthe\\tshape\\tof\\tits\\tbias\\tvector\\nb\\nh\\n?\\nWhat\\tis\\tthe\\tshape\\tof\\tthe\\toutput\\tlayer’s\\tweight\\tvector\\t\\nW\\no\\n,\\tand\\tits\\tbias\\tvector\\t\\nb\\no\\n?\\nWhat\\tis\\tthe\\tshape\\tof\\tthe\\tnetwork’s\\toutput\\tmatrix\\t\\nY\\n?\\nWrite\\tthe\\tequation\\tthat\\tcomputes\\tthe\\tnetwork’s\\toutput\\tmatrix\\t\\nY\\n\\tas\\ta\\tfunction\\tof\\t\\nX\\n,\\t\\nW\\nh\\n,\\t\\nb\\nh\\n,\\t\\nW\\no\\nand\\t\\nb\\no\\n.\\n6\\n.\\t\\nHow\\tmany\\tneurons\\tdo\\tyou\\tneed\\tin\\tthe\\toutput\\tlayer\\tif\\tyou\\twant\\tto\\tclassify\\temail\\tinto\\tspam\\tor\\tham?\\nWhat\\tactivation\\tfunction\\tshould\\tyou\\tuse\\tin\\tthe\\toutput\\tlayer?\\tIf\\tinstead\\tyou\\twant\\tto\\ttackle\\tMNIST,\\nhow\\tmany\\tneurons\\tdo\\tyou\\tneed\\tin\\tthe\\toutput\\tlayer,\\tusing\\twhat\\tactivation\\tfunction?\\tAnswer\\tthe\\tsame\\nquestions\\tfor\\tgetting\\tyour\\tnetwork\\tto\\tpredict\\thousing\\tprices\\tas\\tin\\t\\nChapter\\t2\\n.\\n7\\n.\\t\\nWhat\\tis\\tbackpropagation\\tand\\thow\\tdoes\\tit\\twork?\\tWhat\\tis\\tthe\\tdifference\\tbetween\\tbackpropagation\\nand\\treverse-mode\\tautodiff?\\n8\\n.\\t\\nCan\\tyou\\tlist\\tall\\tthe\\thyperparameters\\tyou\\tcan\\ttweak\\tin\\tan\\tMLP?\\tIf\\tthe\\tMLP\\toverfits\\tthe\\ttraining\\tdata,', 'how\\tcould\\tyou\\ttweak\\tthese\\thyperparameters\\tto\\ttry\\tto\\tsolve\\tthe\\tproblem?\\n9\\n.\\t\\nTrain\\ta\\tdeep\\tMLP\\ton\\tthe\\tMNIST\\tdataset\\tand\\tsee\\tif\\tyou\\tcan\\tget\\tover\\t98%\\tprecision.\\tJust\\tlike\\tin\\tthe\\nlast\\texercise\\tof\\t\\nChapter\\t9\\n,\\ttry\\tadding\\tall\\tthe\\tbells\\tand\\twhistles\\t(i.e.,\\tsave\\tcheckpoints,\\trestore\\tthe\\nlast\\tcheckpoint\\tin\\tcase\\tof\\tan\\tinterruption,\\tadd\\tsummaries,\\tplot\\tlearning\\tcurves\\tusing\\tTensorBoard,\\nand\\tso\\ton).\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.', 'You\\tcan\\tget\\tthe\\tbest\\tof\\tboth\\tworlds\\tby\\tbeing\\topen\\tto\\tbiological\\tinspirations\\twithout\\tbeing\\tafraid\\tto\\tcreate\\tbiologically\\tunrealistic\\tmodels,\\tas\\nlong\\tas\\tthey\\twork\\twell.\\n“A\\tLogical\\tCalculus\\tof\\tIdeas\\tImmanent\\tin\\tNervous\\tActivity,”\\tW.\\tMcCulloch\\tand\\tW.\\tPitts\\t(1943).\\nImage\\tby\\tBruce\\tBlaus\\t(\\nCreative\\tCommons\\t3.0\\n).\\tReproduced\\tfrom\\t\\nhttps://en.wikipedia.org/wiki/Neuron\\n.\\nIn\\tthe\\tcontext\\tof\\tMachine\\tLearning,\\tthe\\tphrase\\t“neural\\tnetworks”\\tgenerally\\trefers\\tto\\tANNs,\\tnot\\tBNNs.\\nDrawing\\tof\\ta\\tcortical\\tlamination\\tby\\tS.\\tRamon\\ty\\tCajal\\t(public\\tdomain).\\tReproduced\\tfrom\\t\\nhttps://en.wikipedia.org/wiki/Cerebral_cortex\\n.\\nThe\\tname\\t\\nPerceptron\\n\\tis\\tsometimes\\tused\\tto\\tmean\\ta\\ttiny\\tnetwork\\twith\\ta\\tsingle\\tLTU.\\nNote\\tthat\\tthis\\tsolution\\tis\\tgenerally\\tnot\\tunique:\\tin\\tgeneral\\twhen\\tthe\\tdata\\tare\\tlinearly\\tseparable,\\tthere\\tis\\tan\\tinfinity\\tof\\thyperplanes\\tthat\\tcan\\nseparate\\tthem.\\n“Learning\\tInternal\\tRepresentations\\tby\\tError\\tPropagation,”\\tD.\\tRumelhart,\\tG.\\tHinton,\\tR.\\tWilliams\\t(1986).', 'This\\talgorithm\\twas\\tactually\\tinvented\\tseveral\\ttimes\\tby\\tvarious\\tresearchers\\tin\\tdifferent\\tfields,\\tstarting\\twith\\tP.\\tWerbos\\tin\\t1974.\\nUsing\\ta\\ttruncated\\tnormal\\tdistribution\\trather\\tthan\\ta\\tregular\\tnormal\\tdistribution\\tensures\\tthat\\tthere\\twon’t\\tbe\\tany\\tlarge\\tweights,\\twhich\\tcould\\nslow\\tdown\\ttraining.\\nFor\\texample,\\tif\\tyou\\tset\\tall\\tthe\\tweights\\tto\\t0,\\tthen\\tall\\tneurons\\twill\\toutput\\t0,\\tand\\tthe\\terror\\tgradient\\twill\\tbe\\tthe\\tsame\\tfor\\tall\\tneurons\\tin\\ta\\tgiven\\nhidden\\tlayer.\\tThe\\tGradient\\tDescent\\tstep\\twill\\tthen\\tupdate\\tall\\tthe\\tweights\\tin\\texactly\\tthe\\tsame\\tway\\tin\\teach\\tlayer,\\tso\\tthey\\twill\\tall\\tremain\\nequal.\\tIn\\tother\\twords,\\tdespite\\thaving\\thundreds\\tof\\tneurons\\tper\\tlayer,\\tyour\\tmodel\\twill\\tact\\tas\\tif\\tthere\\twere\\tonly\\tone\\tneuron\\tper\\tlayer.\\tIt\\tis\\tnot\\ngoing\\tto\\tfly.\\nBy\\tVincent\\tVanhoucke\\tin\\this\\t\\nDeep\\tLearning\\tclass\\n\\ton\\tUdacity.com.\\nA\\tfew\\textra\\tANN\\tarchitectures\\tare\\tpresented\\tin\\t\\nAppendix\\tE\\n.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13', 'Chapter\\t11.\\t\\nTraining\\tDeep\\tNeural\\tNets\\nIn\\t\\nChapter\\t10\\n\\t\\nwe\\tintroduced\\tartificial\\tneural\\tnetworks\\tand\\ttrained\\tour\\tfirst\\tdeep\\tneural\\tnetwork.\\tBut\\tit\\nwas\\ta\\tvery\\tshallow\\tDNN,\\twith\\tonly\\ttwo\\thidden\\tlayers.\\tWhat\\tif\\tyou\\tneed\\tto\\ttackle\\ta\\tvery\\tcomplex\\nproblem,\\tsuch\\tas\\tdetecting\\thundreds\\tof\\ttypes\\tof\\tobjects\\tin\\thigh-resolution\\timages?\\tYou\\tmay\\tneed\\tto\\ttrain\\na\\tmuch\\tdeeper\\tDNN,\\tperhaps\\twith\\t(say)\\t10\\tlayers,\\teach\\tcontaining\\thundreds\\tof\\tneurons,\\tconnected\\tby\\nhundreds\\tof\\tthousands\\tof\\tconnections.\\tThis\\twould\\tnot\\tbe\\ta\\twalk\\tin\\tthe\\tpark:\\nFirst,\\tyou\\twould\\tbe\\tfaced\\twith\\tthe\\t\\ntricky\\t\\nvanishing\\tgradients\\n\\tproblem\\t(or\\tthe\\trelated\\t\\nexploding\\ngradients\\n\\tproblem)\\tthat\\taffects\\tdeep\\tneural\\tnetworks\\tand\\tmakes\\tlower\\tlayers\\tvery\\thard\\tto\\ttrain.\\nSecond,\\twith\\tsuch\\ta\\tlarge\\tnetwork,\\ttraining\\twould\\tbe\\textremely\\tslow.\\nThird,\\ta\\tmodel\\twith\\tmillions\\tof\\tparameters\\twould\\tseverely\\trisk\\toverfitting\\tthe\\ttraining\\tset.\\nIn\\tthis\\tchapter,\\twe\\twill\\tgo\\tthrough\\teach\\tof\\tthese\\tproblems\\tin\\tturn\\tand\\tpresent\\ttechniques\\tto\\tsolve\\tthem.', 'We\\twill\\tstart\\tby\\texplaining\\tthe\\tvanishing\\tgradients\\tproblem\\tand\\texploring\\tsome\\tof\\tthe\\tmost\\tpopular\\nsolutions\\tto\\tthis\\tproblem.\\tNext\\twe\\twill\\tlook\\tat\\tvarious\\toptimizers\\tthat\\tcan\\tspeed\\tup\\ttraining\\tlarge\\tmodels\\ntremendously\\tcompared\\tto\\tplain\\t\\nGradient\\tDescent.\\tFinally,\\twe\\twill\\tgo\\tthrough\\ta\\tfew\\tpopular\\nregularization\\ttechniques\\tfor\\tlarge\\tneural\\tnetworks.\\nWith\\tthese\\ttools,\\tyou\\twill\\tbe\\table\\tto\\ttrain\\tvery\\tdeep\\tnets:\\twelcome\\tto\\tDeep\\tLearning!', 'Vanishing/Exploding\\tGradients\\tProblems\\nAs\\twe\\tdiscussed\\tin\\t\\nChapter\\t10\\n,\\tthe\\t\\nbackpropagation\\talgorithm\\tworks\\tby\\tgoing\\tfrom\\tthe\\toutput\\tlayer\\tto\\tthe\\ninput\\tlayer,\\tpropagating\\tthe\\terror\\tgradient\\ton\\tthe\\tway.\\tOnce\\tthe\\talgorithm\\thas\\tcomputed\\tthe\\tgradient\\tof\\tthe\\ncost\\tfunction\\twith\\tregards\\tto\\teach\\tparameter\\tin\\tthe\\tnetwork,\\tit\\tuses\\tthese\\tgradients\\tto\\tupdate\\teach\\nparameter\\twith\\ta\\tGradient\\tDescent\\tstep.\\nUnfortunately,\\tgradients\\toften\\tget\\tsmaller\\tand\\tsmaller\\tas\\tthe\\talgorithm\\tprogresses\\tdown\\tto\\tthe\\tlower\\nlayers.\\tAs\\ta\\tresult,\\tthe\\tGradient\\tDescent\\tupdate\\tleaves\\tthe\\tlower\\tlayer\\tconnection\\tweights\\tvirtually\\nunchanged,\\tand\\ttraining\\tnever\\tconverges\\tto\\ta\\tgood\\tsolution.\\tThis\\t\\nis\\tcalled\\tthe\\t\\nvanishing\\tgradients\\nproblem.\\tIn\\tsome\\tcases,\\tthe\\topposite\\tcan\\thappen:\\tthe\\tgradients\\tcan\\tgrow\\tbigger\\tand\\tbigger,\\tso\\tmany\\nlayers\\tget\\tinsanely\\tlarge\\tweight\\tupdates\\tand\\tthe\\talgorithm\\tdiverges.\\tThis\\tis\\tthe\\t\\nexploding\\tgradients\\nproblem,\\t\\nwhich\\tis\\tmostly\\tencountered\\tin\\trecurrent\\tneural\\tnetworks\\t(see\\t\\nChapter\\t14', ').\\tMore\\tgenerally,\\ndeep\\tneural\\tnetworks\\tsuffer\\tfrom\\tunstable\\tgradients;\\tdifferent\\tlayers\\tmay\\tlearn\\tat\\twidely\\tdifferent\\tspeeds.\\nAlthough\\tthis\\tunfortunate\\tbehavior\\thas\\tbeen\\tempirically\\tobserved\\tfor\\tquite\\ta\\twhile\\t(it\\twas\\tone\\tof\\tthe\\nreasons\\twhy\\tdeep\\tneural\\tnetworks\\twere\\tmostly\\tabandoned\\tfor\\ta\\tlong\\ttime),\\tit\\tis\\tonly\\taround\\t2010\\tthat\\nsignificant\\tprogress\\twas\\tmade\\tin\\tunderstanding\\tit.\\tA\\tpaper\\ttitled\\t\\n“Understanding\\tthe\\tDifficulty\\tof\\tTraining\\nDeep\\tFeedforward\\tNeural\\tNetworks”\\n\\tby\\tXavier\\tGlorot\\tand\\tYoshua\\tBengio\\n1\\n\\tfound\\ta\\tfew\\tsuspects,\\nincluding\\tthe\\tcombination\\tof\\tthe\\tpopular\\tlogistic\\tsigmoid\\tactivation\\tfunction\\tand\\tthe\\tweight\\tinitialization\\ntechnique\\tthat\\twas\\tmost\\tpopular\\tat\\tthe\\ttime,\\tnamely\\t\\nrandom\\tinitialization\\tusing\\ta\\tnormal\\tdistribution\\twith\\na\\tmean\\tof\\t0\\tand\\ta\\tstandard\\tdeviation\\tof\\t1.\\tIn\\tshort,\\tthey\\tshowed\\tthat\\twith\\tthis\\tactivation\\tfunction\\tand\\tthis\\ninitialization\\tscheme,\\tthe\\tvariance\\tof\\tthe\\toutputs\\tof\\teach\\tlayer\\tis\\tmuch\\tgreater\\tthan\\tthe\\tvariance\\tof\\tits', 'inputs.\\tGoing\\tforward\\tin\\tthe\\tnetwork,\\tthe\\tvariance\\tkeeps\\tincreasing\\tafter\\teach\\tlayer\\tuntil\\tthe\\tactivation\\nfunction\\tsaturates\\tat\\tthe\\ttop\\tlayers.\\tThis\\tis\\tactually\\tmade\\tworse\\tby\\tthe\\tfact\\tthat\\tthe\\tlogistic\\tfunction\\thas\\ta\\nmean\\tof\\t0.5,\\tnot\\t0\\t\\n(the\\thyperbolic\\ttangent\\tfunction\\thas\\ta\\tmean\\tof\\t0\\tand\\tbehaves\\tslightly\\tbetter\\tthan\\tthe\\nlogistic\\tfunction\\tin\\tdeep\\tnetworks).\\nLooking\\tat\\tthe\\tlogistic\\tactivation\\tfunction\\t(see\\t\\nFigure\\t11-1\\n),\\tyou\\tcan\\tsee\\tthat\\twhen\\tinputs\\tbecome\\tlarge\\n(negative\\tor\\tpositive),\\tthe\\tfunction\\tsaturates\\tat\\t0\\tor\\t1,\\twith\\ta\\tderivative\\textremely\\tclose\\tto\\t0.\\tThus\\twhen\\nbackpropagation\\tkicks\\tin,\\tit\\thas\\tvirtually\\tno\\tgradient\\tto\\tpropagate\\tback\\tthrough\\tthe\\tnetwork,\\tand\\twhat\\nlittle\\tgradient\\texists\\tkeeps\\tgetting\\tdiluted\\tas\\tbackpropagation\\tprogresses\\tdown\\tthrough\\tthe\\ttop\\tlayers,\\tso\\nthere\\tis\\treally\\tnothing\\tleft\\tfor\\tthe\\tlower\\tlayers.', 'Figure\\t11-1.\\t\\nLogistic\\tactivation\\tfunction\\tsaturation', 'Xavier\\tand\\tHe\\tInitialization\\nIn\\ttheir\\tpaper,\\tGlorot\\tand\\tBengio\\tpropose\\ta\\tway\\tto\\tsignificantly\\talleviate\\tthis\\tproblem.\\tWe\\tneed\\tthe\\nsignal\\tto\\tflow\\tproperly\\tin\\tboth\\tdirections:\\tin\\tthe\\tforward\\tdirection\\twhen\\tmaking\\tpredictions,\\tand\\tin\\tthe\\nreverse\\tdirection\\twhen\\tbackpropagating\\tgradients.\\tWe\\tdon’t\\twant\\tthe\\tsignal\\tto\\tdie\\tout,\\tnor\\tdo\\twe\\twant\\tit\\nto\\texplode\\tand\\tsaturate.\\tFor\\tthe\\tsignal\\tto\\tflow\\tproperly,\\tthe\\tauthors\\targue\\tthat\\twe\\tneed\\tthe\\tvariance\\tof\\tthe\\noutputs\\tof\\teach\\tlayer\\tto\\tbe\\tequal\\tto\\tthe\\tvariance\\tof\\tits\\tinputs,\\n2\\n\\tand\\twe\\talso\\tneed\\tthe\\tgradients\\tto\\thave\\nequal\\tvariance\\tbefore\\tand\\tafter\\tflowing\\tthrough\\ta\\tlayer\\tin\\tthe\\treverse\\tdirection\\t(please\\tcheck\\tout\\tthe\\npaper\\tif\\tyou\\tare\\tinterested\\tin\\tthe\\tmathematical\\tdetails).\\tIt\\tis\\tactually\\tnot\\tpossible\\tto\\tguarantee\\tboth\\tunless\\nthe\\tlayer\\thas\\tan\\tequal\\tnumber\\tof\\tinput\\tand\\toutput\\tconnections,\\tbut\\tthey\\tproposed\\ta\\tgood\\tcompromise\\tthat\\nhas\\tproven\\tto\\twork\\tvery\\twell\\tin\\tpractice:\\tthe\\tconnection\\tweights\\tmust\\tbe\\tinitialized\\trandomly\\tas\\ndescribed\\tin\\t\\nEquation\\t11-1', ',\\twhere\\t\\nn\\ninputs\\n\\tand\\t\\nn\\noutputs\\n\\tare\\tthe\\tnumber\\tof\\tinput\\tand\\toutput\\tconnections\\tfor\\nthe\\tlayer\\twhose\\tweights\\tare\\tbeing\\tinitialized\\t\\n(also\\tcalled\\t\\nfan-in\\n\\tand\\t\\nfan-out\\n).\\tThis\\tinitialization\\tstrategy\\nis\\toften\\tcalled\\t\\nXavier\\tinitialization\\n\\t(after\\tthe\\tauthor’s\\tfirst\\tname),\\tor\\tsometimes\\t\\nGlorot\\tinitialization\\n.\\nEquation\\t11-1.\\t\\nXavier\\tinitialization\\t(when\\tusing\\tthe\\tlogistic\\tactivation\\tfunction)\\nWhen\\tthe\\tnumber\\tof\\tinput\\tconnections\\tis\\troughly\\tequal\\tto\\tthe\\tnumber\\tof\\toutput\\tconnections,\\tyou\\tget\\nsimpler\\tequations\\t(e.g.,\\t\\n\\tor\\t\\n).\\tWe\\tused\\tthis\\tsimplified\\tstrategy\\tin\\nChapter\\t10\\n.\\n3\\nUsing\\tthe\\tXavier\\tinitialization\\tstrategy\\tcan\\tspeed\\tup\\ttraining\\tconsiderably,\\tand\\tit\\tis\\tone\\tof\\tthe\\ttricks\\tthat\\nled\\tto\\tthe\\tcurrent\\tsuccess\\tof\\tDeep\\tLearning.\\tSome\\t\\nrecent\\tpapers\\n4\\n\\thave\\tprovided\\tsimilar\\tstrategies\\tfor\\ndifferent\\tactivation\\tfunctions,\\tas\\tshown\\tin\\t\\nTable\\t11-1\\n.\\tThe\\tinitialization\\tstrategy\\tfor\\tthe\\t\\nReLU\\tactivation\\nfunction\\t(and\\tits\\tvariants,\\tincluding\\tthe\\tELU\\tactivation\\tdescribed\\tshortly)\\tis\\tsometimes', 'called\\t\\nHe\\ninitialization\\n\\t(after\\tthe\\tlast\\tname\\tof\\tits\\tauthor).\\nTable\\t11-1.\\t\\nInitialization\\tparameters\\tfor\\teach\\ttype\\tof\\nactivation\\tfunction\\nActivation\\tfunction\\nUniform\\tdistribution\\t[–r,\\tr]\\nNormal\\tdistribution\\nLogistic\\nHyperbolic\\ttangent\\nReLU\\t(and\\tits\\tvariants)\\nBy\\tdefault,\\tthe\\t\\ntf.layers.dense()\\n\\tfunction\\t\\n(introduced\\tin\\t\\nChapter\\t10\\n)\\tuses\\tXavier\\tinitialization\\t(with\\na\\tuniform\\tdistribution).\\tYou\\tcan\\tchange\\tthis\\tto\\tHe\\tinitialization\\tby\\tusing\\tthe', 'variance_scaling_initializer()\\n\\t\\nfunction\\tlike\\tthis:\\nhe_init\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\nhe_init\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\nNOTE\\nHe\\tinitialization\\tconsiders\\tonly\\tthe\\tfan-in,\\t\\nnot\\tthe\\taverage\\tbetween\\tfan-in\\tand\\tfan-out\\tlike\\tin\\tXavier\\tinitialization.\\tThis\\tis\\talso\\tthe\\ndefault\\tfor\\tthe\\t\\nvariance_scaling_initializer()\\n\\tfunction,\\tbut\\tyou\\tcan\\tchange\\tthis\\tby\\tsetting\\tthe\\t\\nargument\\t\\nmode=\"FAN_AVG\"\\n.', 'Nonsaturating\\tActivation\\tFunctions\\nOne\\t\\nof\\tthe\\tinsights\\tin\\tthe\\t2010\\tpaper\\tby\\tGlorot\\tand\\tBengio\\twas\\tthat\\tthe\\tvanishing/exploding\\tgradients\\nproblems\\twere\\tin\\tpart\\tdue\\tto\\ta\\tpoor\\tchoice\\tof\\tactivation\\tfunction.\\tUntil\\tthen\\tmost\\tpeople\\thad\\tassumed\\nthat\\tif\\tMother\\tNature\\thad\\tchosen\\tto\\tuse\\troughly\\tsigmoid\\tactivation\\tfunctions\\tin\\tbiological\\tneurons,\\tthey\\nmust\\tbe\\tan\\texcellent\\tchoice.\\tBut\\tit\\tturns\\tout\\tthat\\tother\\tactivation\\tfunctions\\tbehave\\tmuch\\tbetter\\tin\\tdeep\\nneural\\tnetworks,\\tin\\tparticular\\tthe\\tReLU\\tactivation\\tfunction,\\tmostly\\tbecause\\tit\\tdoes\\tnot\\tsaturate\\tfor\\npositive\\tvalues\\t(and\\talso\\tbecause\\tit\\tis\\tquite\\tfast\\tto\\tcompute).\\nUnfortunately,\\tthe\\tReLU\\tactivation\\tfunction\\tis\\tnot\\tperfect.\\tIt\\tsuffers\\tfrom\\ta\\tproblem\\tknown\\tas\\t\\nthe\\t\\ndying\\nReLUs\\n:\\tduring\\ttraining,\\tsome\\tneurons\\teffectively\\tdie,\\tmeaning\\tthey\\tstop\\toutputting\\tanything\\tother\\tthan\\t0.\\nIn\\tsome\\tcases,\\tyou\\tmay\\tfind\\tthat\\thalf\\tof\\tyour\\tnetwork’s\\tneurons\\tare\\tdead,\\tespecially\\tif\\tyou\\tused\\ta\\tlarge', 'learning\\trate.\\tDuring\\ttraining,\\tif\\ta\\tneuron’s\\tweights\\tget\\tupdated\\tsuch\\tthat\\tthe\\tweighted\\tsum\\tof\\tthe\\tneuron’s\\ninputs\\tis\\tnegative,\\tit\\twill\\tstart\\toutputting\\t0.\\tWhen\\tthis\\thappen,\\tthe\\tneuron\\tis\\tunlikely\\tto\\tcome\\tback\\tto\\tlife\\nsince\\tthe\\tgradient\\tof\\tthe\\tReLU\\tfunction\\tis\\t0\\twhen\\tits\\tinput\\tis\\tnegative.\\nTo\\tsolve\\tthis\\tproblem,\\tyou\\tmay\\twant\\tto\\tuse\\ta\\tvariant\\tof\\tthe\\tReLU\\tfunction,\\tsuch\\tas\\tthe\\t\\nleaky\\tReLU\\n.\\t\\nThis\\nfunction\\tis\\tdefined\\tas\\tLeakyReLU\\nα\\n(\\nz\\n)\\t=\\tmax(\\nαz\\n,\\t\\nz\\n)\\t(see\\t\\nFigure\\t11-2\\n).\\tThe\\thyperparameter\\t\\nα\\n\\tdefines\\thow\\nmuch\\tthe\\tfunction\\t“leaks”:\\tit\\tis\\tthe\\tslope\\tof\\tthe\\tfunction\\tfor\\t\\nz\\n\\t<\\t0,\\tand\\tis\\ttypically\\tset\\tto\\t0.01.\\tThis\\tsmall\\nslope\\tensures\\tthat\\tleaky\\tReLUs\\tnever\\tdie;\\tthey\\tcan\\tgo\\tinto\\ta\\tlong\\tcoma,\\tbut\\tthey\\thave\\ta\\tchance\\tto\\neventually\\twake\\tup.\\tA\\t\\nrecent\\tpaper\\n5\\n\\tcompared\\tseveral\\tvariants\\tof\\tthe\\tReLU\\tactivation\\tfunction\\tand\\tone\\nof\\tits\\tconclusions\\twas\\tthat\\tthe\\tleaky\\tvariants\\talways\\toutperformed\\tthe\\tstrict\\tReLU\\tactivation\\tfunction.\\tIn\\nfact,\\tsetting\\t\\nα', 'α\\n\\t=\\t0.2\\t(huge\\tleak)\\tseemed\\tto\\tresult\\tin\\tbetter\\tperformance\\tthan\\t\\nα\\n\\t=\\t0.01\\t(small\\tleak).\\tThey\\nalso\\tevaluated\\tthe\\t\\nrandomized\\tleaky\\tReLU\\n\\t(RReLU),\\twhere\\t\\nα\\n\\tis\\tpicked\\trandomly\\tin\\ta\\tgiven\\trange\\tduring\\ntraining,\\tand\\tit\\tis\\tfixed\\tto\\tan\\taverage\\tvalue\\tduring\\ttesting.\\tIt\\talso\\tperformed\\tfairly\\twell\\tand\\tseemed\\tto\\tact\\nas\\ta\\tregularizer\\t(reducing\\tthe\\trisk\\tof\\toverfitting\\t\\nthe\\ttraining\\tset).\\tFinally,\\tthey\\talso\\tevaluated\\tthe\\nparametric\\tleaky\\tReLU\\n\\t(PReLU),\\t\\nwhere\\t\\nα\\n\\tis\\tauthorized\\tto\\tbe\\tlearned\\tduring\\ttraining\\t(instead\\tof\\tbeing\\ta\\nhyperparameter,\\tit\\tbecomes\\ta\\tparameter\\tthat\\tcan\\tbe\\tmodified\\tby\\tbackpropagation\\tlike\\tany\\tother\\nparameter).\\tThis\\twas\\treported\\tto\\tstrongly\\toutperform\\tReLU\\ton\\tlarge\\timage\\tdatasets,\\tbut\\ton\\tsmaller\\ndatasets\\tit\\truns\\tthe\\trisk\\tof\\toverfitting\\tthe\\ttraining\\tset.', 'Figure\\t11-2.\\t\\nLeaky\\tReLU\\nLast\\tbut\\tnot\\tleast,\\ta\\t\\n2015\\tpaper\\n\\tby\\tDjork-Arné\\tClevert\\tet\\tal.\\n6\\n\\tproposed\\ta\\tnew\\tactivation\\tfunction\\tcalled\\nthe\\t\\nexponential\\tlinear\\tunit\\n\\t(ELU)\\t\\nthat\\toutperformed\\tall\\tthe\\tReLU\\tvariants\\tin\\ttheir\\texperiments:\\ttraining\\ntime\\twas\\treduced\\tand\\tthe\\tneural\\tnetwork\\tperformed\\tbetter\\ton\\tthe\\ttest\\tset.\\tIt\\tis\\trepresented\\tin\\t\\nFigure\\t11-3\\n,\\nand\\t\\nEquation\\t11-2\\n\\tshows\\tits\\tdefinition.\\nEquation\\t11-2.\\t\\nELU\\tactivation\\tfunction', 'Figure\\t11-3.\\t\\nELU\\tactivation\\tfunction\\nIt\\tlooks\\ta\\tlot\\tlike\\tthe\\tReLU\\tfunction,\\twith\\ta\\tfew\\tmajor\\tdifferences:\\nFirst\\tit\\ttakes\\ton\\tnegative\\tvalues\\twhen\\t\\nz\\n\\t<\\t0,\\twhich\\tallows\\tthe\\tunit\\tto\\thave\\tan\\taverage\\toutput\\tcloser\\nto\\t0.\\tThis\\thelps\\talleviate\\tthe\\tvanishing\\tgradients\\tproblem,\\tas\\tdiscussed\\tearlier.\\tThe\\thyperparameter\\nα\\n\\tdefines\\tthe\\tvalue\\tthat\\tthe\\tELU\\tfunction\\tapproaches\\twhen\\t\\nz\\n\\tis\\ta\\tlarge\\tnegative\\tnumber.\\tIt\\tis\\tusually\\nset\\tto\\t1,\\tbut\\tyou\\tcan\\ttweak\\tit\\tlike\\tany\\tother\\thyperparameter\\tif\\tyou\\twant.\\nSecond,\\tit\\thas\\ta\\tnonzero\\tgradient\\tfor\\t\\nz\\n\\t<\\t0,\\twhich\\tavoids\\tthe\\tdying\\tunits\\tissue.\\nThird,\\tthe\\tfunction\\tis\\tsmooth\\teverywhere,\\tincluding\\taround\\t\\nz\\n\\t=\\t0,\\twhich\\thelps\\tspeed\\tup\\tGradient\\nDescent,\\tsince\\tit\\tdoes\\tnot\\tbounce\\tas\\tmuch\\tleft\\tand\\tright\\tof\\t\\nz\\n\\t=\\t0.\\nThe\\tmain\\tdrawback\\tof\\tthe\\tELU\\tactivation\\tfunction\\tis\\tthat\\tit\\tis\\tslower\\tto\\tcompute\\tthan\\tthe\\tReLU\\tand\\tits\\nvariants\\t(due\\tto\\tthe\\tuse\\tof\\tthe\\texponential\\tfunction),\\tbut\\tduring\\ttraining\\tthis\\tis\\tcompensated\\tby\\tthe\\tfaster', 'convergence\\trate.\\tHowever,\\tat\\ttest\\ttime\\tan\\tELU\\tnetwork\\twill\\tbe\\tslower\\tthan\\ta\\tReLU\\tnetwork.\\nTIP\\nSo\\twhich\\tactivation\\tfunction\\tshould\\tyou\\tuse\\tfor\\tthe\\thidden\\tlayers\\tof\\tyour\\tdeep\\tneural\\tnetworks?\\tAlthough\\tyour\\tmileage\\twill\\tvary,\\nin\\tgeneral\\tELU\\t>\\tleaky\\tReLU\\t(and\\tits\\tvariants)\\t>\\tReLU\\t>\\ttanh\\t>\\tlogistic.\\tIf\\tyou\\tcare\\ta\\tlot\\tabout\\truntime\\tperformance,\\tthen\\tyou\\nmay\\tprefer\\tleaky\\tReLUs\\tover\\tELUs.\\tIf\\tyou\\tdon’t\\twant\\tto\\ttweak\\tyet\\tanother\\thyperparameter,\\tyou\\tmay\\tjust\\tuse\\tthe\\tdefault\\t\\nα\\nvalues\\tsuggested\\tearlier\\t(0.01\\tfor\\tthe\\tleaky\\tReLU,\\tand\\t1\\tfor\\tELU).\\tIf\\tyou\\thave\\tspare\\ttime\\tand\\tcomputing\\tpower,\\tyou\\tcan\\tuse\\ncross-validation\\tto\\tevaluate\\tother\\tactivation\\tfunctions,\\tin\\tparticular\\tRReLU\\tif\\tyour\\tnetwork\\tis\\toverfitting,\\tor\\tPReLU\\tif\\tyou\\thave\\ta\\nhuge\\ttraining\\tset.\\nTensorFlow\\toffers\\tan\\t\\nelu()\\n\\tfunction\\tthat\\tyou\\tcan\\tuse\\tto\\tbuild\\tyour\\tneural\\tnetwork.\\tSimply\\tset\\tthe\\nactivation\\n\\targument\\twhen\\tcalling\\tthe\\t\\ndense()\\n\\tfunction,\\t\\nlike\\tthis:', 'hidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nelu\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\nTensorFlow\\tdoes\\tnot\\thave\\ta\\tpredefined\\tfunction\\tfor\\tleaky\\tReLUs,\\tbut\\tit\\tis\\teasy\\tenough\\tto\\t\\ndefine:\\ndef\\n\\t\\nleaky_relu\\n(\\nz\\n,\\n\\t\\nname\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ntf\\n.\\nmaximum\\n(\\n0.01\\n\\t\\n*\\n\\t\\nz\\n,\\n\\t\\nz\\n,\\n\\t\\nname\\n=\\nname\\n)\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\nleaky_relu\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)', 'Batch\\tNormalization\\nAlthough\\t\\nusing\\tHe\\tinitialization\\talong\\twith\\tELU\\t(or\\tany\\tvariant\\tof\\tReLU)\\tcan\\tsignificantly\\treduce\\tthe\\nvanishing/exploding\\tgradients\\tproblems\\tat\\tthe\\tbeginning\\tof\\ttraining,\\tit\\tdoesn’t\\tguarantee\\tthat\\tthey\\twon’t\\ncome\\tback\\tduring\\ttraining.\\nIn\\ta\\t\\n2015\\tpaper\\n,\\n7\\n\\tSergey\\tIoffe\\tand\\tChristian\\tSzegedy\\tproposed\\ta\\ttechnique\\tcalled\\t\\nBatch\\tNormalization\\n(BN)\\tto\\taddress\\tthe\\tvanishing/exploding\\tgradients\\tproblems,\\tand\\tmore\\tgenerally\\tthe\\tproblem\\tthat\\tthe\\ndistribution\\tof\\teach\\tlayer’s\\tinputs\\tchanges\\tduring\\ttraining,\\tas\\tthe\\tparameters\\tof\\tthe\\tprevious\\tlayers\\t\\nchange\\n(which\\tthey\\tcall\\tthe\\t\\nInternal\\tCovariate\\tShift\\n\\tproblem).\\nThe\\ttechnique\\tconsists\\tof\\tadding\\tan\\toperation\\tin\\tthe\\tmodel\\tjust\\tbefore\\tthe\\tactivation\\tfunction\\tof\\teach\\nlayer,\\tsimply\\tzero-centering\\tand\\tnormalizing\\tthe\\tinputs,\\tthen\\tscaling\\tand\\tshifting\\tthe\\tresult\\tusing\\ttwo\\tnew\\nparameters\\tper\\tlayer\\t(one\\tfor\\tscaling,\\tthe\\tother\\tfor\\tshifting).\\tIn\\tother\\twords,\\tthis\\toperation\\tlets\\tthe\\tmodel', 'learn\\tthe\\toptimal\\tscale\\tand\\tmean\\tof\\tthe\\tinputs\\tfor\\teach\\tlayer.\\nIn\\torder\\tto\\tzero-center\\tand\\tnormalize\\tthe\\tinputs,\\tthe\\talgorithm\\tneeds\\tto\\testimate\\tthe\\tinputs’\\tmean\\tand\\nstandard\\tdeviation.\\tIt\\tdoes\\tso\\tby\\tevaluating\\tthe\\tmean\\tand\\tstandard\\tdeviation\\tof\\tthe\\tinputs\\tover\\tthe\\tcurrent\\nmini-batch\\t(hence\\tthe\\tname\\t“Batch\\tNormalization”).\\tThe\\twhole\\t\\noperation\\tis\\tsummarized\\tin\\t\\nEquation\\t11-\\n3\\n.\\nEquation\\t11-3.\\t\\nBatch\\tNormalization\\talgorithm\\nμ\\nB\\n\\tis\\tthe\\tempirical\\tmean,\\tevaluated\\tover\\tthe\\twhole\\tmini-batch\\t\\nB\\n.', 'σ\\nB\\n\\tis\\tthe\\tempirical\\tstandard\\tdeviation,\\talso\\tevaluated\\tover\\tthe\\twhole\\tmini-batch.\\nm\\nB\\n\\tis\\tthe\\tnumber\\tof\\tinstances\\tin\\tthe\\tmini-batch.\\n(i)\\n\\tis\\tthe\\tzero-centered\\tand\\tnormalized\\tinput.\\nγ\\n\\tis\\tthe\\tscaling\\tparameter\\tfor\\tthe\\tlayer.\\nβ\\n\\tis\\tthe\\tshifting\\tparameter\\t(offset)\\tfor\\tthe\\tlayer.\\nϵ\\n\\tis\\ta\\ttiny\\tnumber\\tto\\tavoid\\tdivision\\tby\\tzero\\t(typically\\t10\\n–5\\n).\\tThis\\tis\\tcalled\\t\\na\\t\\nsmoothing\\tterm\\n.\\nz\\n(i)\\n\\tis\\tthe\\toutput\\tof\\tthe\\tBN\\toperation:\\tit\\tis\\ta\\tscaled\\tand\\tshifted\\tversion\\tof\\tthe\\tinputs.\\nAt\\ttest\\ttime,\\tthere\\tis\\tno\\tmini-batch\\tto\\tcompute\\tthe\\tempirical\\tmean\\tand\\tstandard\\tdeviation,\\tso\\tinstead\\tyou\\nsimply\\tuse\\tthe\\twhole\\ttraining\\tset’s\\tmean\\tand\\tstandard\\tdeviation.\\tThese\\tare\\ttypically\\tefficiently\\tcomputed\\nduring\\ttraining\\tusing\\ta\\tmoving\\taverage.\\tSo,\\tin\\ttotal,\\tfour\\tparameters\\tare\\tlearned\\tfor\\teach\\tbatch-\\nnormalized\\tlayer:\\t\\nγ\\n\\t(scale),\\t\\nβ\\n\\t(offset),\\t\\nμ\\n\\t(mean),\\tand\\t\\nσ\\n\\t(standard\\tdeviation).\\nThe\\tauthors\\tdemonstrated\\tthat\\tthis\\ttechnique\\tconsiderably\\timproved\\tall\\tthe\\tdeep\\tneural\\tnetworks\\tthey', 'experimented\\twith.\\tThe\\tvanishing\\tgradients\\tproblem\\twas\\tstrongly\\treduced,\\tto\\tthe\\tpoint\\tthat\\tthey\\tcould\\tuse\\nsaturating\\tactivation\\tfunctions\\tsuch\\tas\\tthe\\ttanh\\tand\\teven\\tthe\\tlogistic\\tactivation\\tfunction.\\tThe\\tnetworks\\nwere\\talso\\tmuch\\tless\\tsensitive\\tto\\tthe\\tweight\\tinitialization.\\tThey\\twere\\table\\tto\\tuse\\tmuch\\tlarger\\tlearning\\nrates,\\tsignificantly\\tspeeding\\tup\\tthe\\tlearning\\tprocess.\\tSpecifically,\\tthey\\tnote\\tthat\\t“Applied\\tto\\ta\\tstate-of-\\nthe-art\\timage\\tclassification\\tmodel,\\tBatch\\tNormalization\\tachieves\\tthe\\tsame\\taccuracy\\twith\\t14\\ttimes\\tfewer\\ntraining\\tsteps,\\tand\\tbeats\\tthe\\toriginal\\tmodel\\tby\\ta\\tsignificant\\tmargin.\\t[…]\\tUsing\\tan\\tensemble\\tof\\tbatch-\\nnormalized\\tnetworks,\\twe\\timprove\\tupon\\tthe\\tbest\\tpublished\\tresult\\ton\\tImageNet\\tclassification:\\treaching\\n4.9%\\ttop-5\\tvalidation\\terror\\t(and\\t4.8%\\ttest\\terror),\\texceeding\\tthe\\taccuracy\\tof\\thuman\\traters.”\\tFinally,\\tlike\\ta\\ngift\\tthat\\tkeeps\\ton\\tgiving,\\tBatch\\tNormalization\\talso\\tacts\\tlike\\ta\\tregularizer,\\treducing\\tthe\\tneed\\tfor\\tother', 'regularization\\ttechniques\\t(such\\tas\\tdropout,\\tdescribed\\tlater\\tin\\tthe\\tchapter).\\nBatch\\tNormalization\\tdoes,\\thowever,\\tadd\\tsome\\tcomplexity\\tto\\tthe\\tmodel\\t(although\\tit\\tremoves\\tthe\\tneed\\tfor\\nnormalizing\\tthe\\tinput\\tdata\\tsince\\tthe\\tfirst\\thidden\\tlayer\\twill\\ttake\\tcare\\tof\\tthat,\\tprovided\\tit\\tis\\tbatch-\\nnormalized).\\tMoreover,\\tthere\\tis\\ta\\truntime\\tpenalty:\\tthe\\tneural\\tnetwork\\tmakes\\tslower\\tpredictions\\tdue\\tto\\nthe\\textra\\tcomputations\\trequired\\tat\\teach\\tlayer.\\tSo\\tif\\tyou\\tneed\\tpredictions\\tto\\tbe\\tlightning-fast,\\tyou\\tmay\\nwant\\tto\\tcheck\\thow\\twell\\tplain\\tELU\\t+\\tHe\\tinitialization\\tperform\\tbefore\\tplaying\\twith\\tBatch\\tNormalization.\\nNOTE\\nYou\\tmay\\tfind\\tthat\\ttraining\\tis\\trather\\tslow\\tat\\tfirst\\twhile\\tGradient\\tDescent\\tis\\tsearching\\tfor\\tthe\\toptimal\\tscales\\tand\\toffsets\\tfor\\teach\\nlayer,\\tbut\\tit\\taccelerates\\tonce\\tit\\thas\\tfound\\treasonably\\tgood\\tvalues.\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\nTensorFlow\\t\\nprovides\\ta\\t\\ntf.nn.batch_normalization()\\n\\tfunction\\tthat\\tsimply\\tcenters\\tand\\tnormalizes\\tthe', 'inputs,\\tbut\\tyou\\tmust\\tcompute\\tthe\\tmean\\tand\\tstandard\\tdeviation\\tyourself\\t(based\\ton\\tthe\\tmini-batch\\tdata', 'during\\ttraining\\tor\\ton\\tthe\\tfull\\tdataset\\tduring\\ttesting,\\tas\\tjust\\tdiscussed)\\tand\\tpass\\tthem\\tas\\tparameters\\tto\\tthis\\nfunction,\\tand\\tyou\\tmust\\talso\\thandle\\tthe\\tcreation\\tof\\tthe\\tscaling\\tand\\toffset\\tparameters\\t(and\\tpass\\tthem\\tto\\tthis\\nfunction).\\tIt\\tis\\tdoable,\\tbut\\tnot\\tthe\\tmost\\tconvenient\\tapproach.\\tInstead,\\tyou\\tshould\\tuse\\tthe\\ntf.layers.batch_normalization()\\n\\tfunction,\\t\\nwhich\\thandles\\tall\\tthis\\tfor\\tyou,\\tas\\tin\\tthe\\tfollowing\\tcode:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nn_inputs\\n\\t\\n=\\n\\t\\n28\\n\\t\\n*\\n\\t\\n28\\nn_hidden1\\n\\t\\n=\\n\\t\\n300\\nn_hidden2\\n\\t\\n=\\n\\t\\n100\\nn_outputs\\n\\t\\n=\\n\\t\\n10\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_inputs\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\ntraining\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder_with_default\\n(\\nFalse\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\nname\\n=\\n\\'training\\'\\n)\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\nbn1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\nbatch_normalization\\n(\\nhidden1\\n,\\n\\t\\ntraining\\n=\\ntraining\\n,\\n\\t\\nmomentum\\n=\\n0.9\\n)\\nbn1_act\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nelu\\n(\\nbn1\\n)\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nbn1_act\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n)', ')\\nbn2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\nbatch_normalization\\n(\\nhidden2\\n,\\n\\t\\ntraining\\n=\\ntraining\\n,\\n\\t\\nmomentum\\n=\\n0.9\\n)\\nbn2_act\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nelu\\n(\\nbn2\\n)\\nlogits_before_bn\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nbn2_act\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\nbatch_normalization\\n(\\nlogits_before_bn\\n,\\n\\t\\ntraining\\n=\\ntraining\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmomentum\\n=\\n0.9\\n)\\nLet’s\\twalk\\tthrough\\tthis\\tcode.\\tThe\\tfirst\\tlines\\tare\\tfairly\\tself-explanatory,\\tuntil\\twe\\tdefine\\tthe\\t\\ntraining\\nplaceholder:\\twe\\twill\\tset\\tit\\tto\\t\\nTrue\\n\\tduring\\ttraining,\\tbut\\totherwise\\tit\\twill\\tdefault\\tto\\t\\nFalse\\n.\\tThis\\twill\\tbe\\nused\\tto\\ttell\\tthe\\t\\ntf.layers.batch_normalization()\\n\\tfunction\\twhether\\tit\\tshould\\tuse\\tthe\\tcurrent\\tmini-\\nbatch’s\\tmean\\tand\\tstandard\\tdeviation\\t(during\\ttraining)\\tor\\tthe\\twhole\\ttraining\\tset’s\\tmean\\tand\\tstandard\\ndeviation\\t(during\\ttesting).\\nThen,\\twe\\talternate\\tfully\\tconnected\\tlayers\\tand\\tbatch\\tnormalization\\tlayers:\\tthe\\tfully\\tconnected\\tlayers\\tare\\ncreated\\tusing\\tthe\\t\\ntf.layers.dense()\\n\\t\\nfunction,\\tjust\\tlike\\twe\\tdid\\tin', 'Chapter\\t10\\n.\\tNote\\tthat\\twe\\tdon’t\\nspecify\\tany\\tactivation\\tfunction\\tfor\\tthe\\tfully\\tconnected\\tlayers\\tbecause\\twe\\twant\\tto\\tapply\\tthe\\tactivation\\nfunction\\tafter\\teach\\tbatch\\tnormalization\\tlayer.\\n8\\n\\tWe\\tcreate\\tthe\\tbatch\\tnormalization\\tlayers\\tusing\\tthe\\ntf.layers.batch_normalization()\\n\\t\\nfunction,\\tsetting\\tits\\t\\ntraining\\n\\tand\\t\\nmomentum\\n\\tparameters.\\tThe\\tBN\\nalgorithm\\tuses\\t\\nexponential\\tdecay\\n\\tto\\t\\ncompute\\tthe\\trunning\\taverages,\\twhich\\tis\\twhy\\tit\\trequires\\tthe\\nmomentum\\n\\tparameter:\\tgiven\\ta\\tnew\\tvalue\\t\\nv\\n,\\tthe\\trunning\\taverage\\t\\n\\tis\\tupdated\\tthrough\\tthe\\tequation:\\nA\\tgood\\tmomentum\\tvalue\\tis\\ttypically\\tclose\\tto\\t1\\t—\\tfor\\texample,\\t0.9,\\t0.99,\\tor\\t0.999\\t(you\\twant\\tmore\\t9s\\tfor\\nlarger\\tdatasets\\tand\\tsmaller\\tmini-batches).\\nYou\\tmay\\thave\\tnoticed\\tthat\\tthe\\tcode\\tis\\tquite\\trepetitive,\\twith\\tthe\\tsame\\tbatch\\tnormalization\\tparameters\\nappearing\\tover\\tand\\tover\\tagain.\\tTo\\tavoid\\tthis\\trepetition,\\tyou\\tcan\\tuse\\tthe\\t\\npartial()\\n\\tfunction\\tfrom\\tthe\\nfunctools\\n\\tmodule\\t(part\\tof\\tPython’s\\tstandard\\tlibrary).\\tIt\\tcreates\\ta\\tthin\\twrapper\\taround\\ta\\tfunction\\tand', 'allows\\tyou\\tto\\tdefine\\tdefault\\tvalues\\tfor\\tsome\\tparameters.\\tThe\\tcreation\\tof\\tthe\\tnetwork\\tlayers\\tin\\tthe\\npreceding\\tcode\\tcan\\tbe\\tmodified\\tlike\\tso:\\nfrom\\n\\t\\nfunctools\\n\\t\\nimport\\n\\t\\npartial\\nmy_batch_norm_layer\\n\\t\\n=\\n\\t\\npartial\\n(\\ntf\\n.\\nlayers\\n.\\nbatch_normalization\\n,', 'training\\n=\\ntraining\\n,\\n\\t\\nmomentum\\n=\\n0.9\\n)\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\nbn1\\n\\t\\n=\\n\\t\\nmy_batch_norm_layer\\n(\\nhidden1\\n)\\nbn1_act\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nelu\\n(\\nbn1\\n)\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nbn1_act\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n)\\nbn2\\n\\t\\n=\\n\\t\\nmy_batch_norm_layer\\n(\\nhidden2\\n)\\nbn2_act\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nelu\\n(\\nbn2\\n)\\nlogits_before_bn\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nbn2_act\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\nlogits\\n\\t\\n=\\n\\t\\nmy_batch_norm_layer\\n(\\nlogits_before_bn\\n)\\nIt\\tmay\\tnot\\tlook\\tmuch\\tbetter\\tthan\\tbefore\\tin\\tthis\\tsmall\\texample,\\tbut\\tif\\tyou\\thave\\t10\\tlayers\\tand\\twant\\tto\\tuse\\nthe\\tsame\\tactivation\\tfunction,\\tinitializer,\\tregularizer,\\tand\\tso\\ton,\\tin\\tall\\tlayers,\\tthis\\ttrick\\twill\\tmake\\tyour\\tcode\\nmuch\\tmore\\treadable.\\nThe\\trest\\tof\\tthe\\tconstruction\\tphase\\tis\\tthe\\tsame\\tas\\tin\\t\\nChapter\\t10\\n:\\tdefine\\t\\nthe\\tcost\\tfunction,\\tcreate\\tan\\noptimizer,\\ttell\\tit\\tto\\tminimize\\tthe\\tcost\\tfunction,\\tdefine\\tthe\\tevaluation\\toperations,\\tcreate\\ta\\t\\nSaver\\n,\\tand\\tso\\ton.', 'The\\texecution\\tphase\\tis\\talso\\tpretty\\tmuch\\tthe\\tsame,\\twith\\ttwo\\texceptions.\\tFirst,\\tduring\\ttraining,\\twhenever\\nyou\\trun\\tan\\toperation\\tthat\\tdepends\\ton\\tthe\\t\\nbatch_normalization()\\n\\tlayer,\\tyou\\tneed\\tto\\tset\\tthe\\t\\ntraining\\nplaceholder\\tto\\t\\nTrue\\n.\\tSecond,\\tthe\\t\\nbatch_normalization()\\n\\tfunction\\tcreates\\ta\\tfew\\toperations\\tthat\\tmust\\nbe\\tevaluated\\tat\\teach\\tstep\\tduring\\ttraining\\tin\\torder\\tto\\tupdate\\tthe\\tmoving\\taverages\\t(recall\\tthat\\tthese\\tmoving\\naverages\\tare\\tneeded\\tto\\tevaluate\\tthe\\ttraining\\tset’s\\tmean\\tand\\tstandard\\tdeviation).\\tThese\\toperations\\tare\\nautomatically\\tadded\\tto\\tthe\\t\\nUPDATE_OPS\\n\\tcollection,\\tso\\tall\\twe\\tneed\\tto\\tdo\\tis\\tget\\tthe\\tlist\\tof\\toperations\\tin\\tthat\\ncollection\\tand\\trun\\tthem\\tat\\teach\\ttraining\\titeration:\\nextra_update_ops\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nUPDATE_OPS\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain', '.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n([\\ntraining_op\\n,\\n\\t\\nextra_update_ops\\n],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n=\\n{\\ntraining\\n:\\n\\t\\nTrue\\n,\\n\\t\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\naccuracy_val\\n\\t\\n=\\n\\t\\naccuracy\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nmnist\\n.\\ntest\\n.\\nimages\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ny\\n:\\n\\t\\nmnist\\n.\\ntest\\n.\\nlabels\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nprint\\n(\\nepoch\\n,\\n\\t\\n\"Test\\taccuracy:\"\\n,\\n\\t\\naccuracy_val\\n)\\n\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\nThat’s\\tall!\\tIn\\tthis\\ttiny\\texample\\twith\\tjust\\ttwo\\tlayers,\\tit’s\\tunlikely\\tthat\\tBatch\\tNormalization\\twill\\thave\\ta\\nvery\\tpositive\\timpact,\\tbut\\tfor\\tdeeper\\tnetworks\\tit\\tcan\\tmake\\ta\\ttremendous\\t\\ndifference.', 'Gradient\\tClipping\\nA\\tpopular\\t\\ntechnique\\tto\\tlessen\\tthe\\texploding\\tgradients\\tproblem\\tis\\tto\\tsimply\\tclip\\tthe\\tgradients\\tduring\\nbackpropagation\\tso\\tthat\\tthey\\tnever\\texceed\\tsome\\tthreshold\\t(this\\tis\\tmostly\\tuseful\\tfor\\trecurrent\\tneural\\nnetworks;\\tsee\\t\\nChapter\\t14\\n).\\tThis\\tis\\tcalled\\t\\nGradient\\tClipping\\n.\\n9\\n\\tIn\\tgeneral\\tpeople\\tnow\\tprefer\\tBatch\\nNormalization,\\tbut\\tit’s\\tstill\\tuseful\\tto\\tknow\\tabout\\tGradient\\tClipping\\tand\\thow\\tto\\timplement\\tit.\\nIn\\tTensorFlow,\\tthe\\toptimizer’s\\t\\nminimize()\\n\\t\\nfunction\\ttakes\\tcare\\tof\\tboth\\tcomputing\\tthe\\tgradients\\tand\\napplying\\tthem,\\tso\\tyou\\tmust\\tinstead\\tcall\\t\\nthe\\toptimizer’s\\t\\ncompute_gradients()\\n\\t\\nmethod\\tfirst,\\tthen\\tcreate\\nan\\toperation\\tto\\tclip\\tthe\\tgradients\\tusing\\tthe\\t\\nclip_by_value()\\n\\t\\nfunction,\\tand\\tfinally\\tcreate\\tan\\toperation\\tto\\napply\\tthe\\tclipped\\tgradients\\tusing\\tthe\\toptimizer’s\\t\\napply_gradients()\\n\\t\\nmethod:\\nthreshold\\n\\t\\n=\\n\\t\\n1.0\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nGradientDescentOptimizer\\n(\\nlearning_rate\\n)\\ngrads_and_vars\\n\\t\\n=\\n\\t\\noptimizer\\n.\\ncompute_gradients\\n(\\nloss\\n)\\ncapped_gvs\\n\\t\\n=\\n\\t\\n[(\\ntf\\n.', '[(\\ntf\\n.\\nclip_by_value\\n(\\ngrad\\n,\\n\\t\\n-\\nthreshold\\n,\\n\\t\\nthreshold\\n),\\n\\t\\nvar\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ngrad\\n,\\n\\t\\nvar\\n\\t\\nin\\n\\t\\ngrads_and_vars\\n]\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\napply_gradients\\n(\\ncapped_gvs\\n)\\nYou\\twould\\tthen\\trun\\tthis\\t\\ntraining_op\\n\\tat\\tevery\\ttraining\\tstep,\\tas\\tusual.\\tIt\\twill\\tcompute\\tthe\\tgradients,\\tclip\\nthem\\tbetween\\t–1.0\\tand\\t1.0,\\tand\\tapply\\tthem.\\tThe\\tthreshold\\tis\\ta\\thyperparameter\\tyou\\t\\ncan\\ttune.', 'Reusing\\tPretrained\\tLayers\\nIt\\t\\nis\\tgenerally\\tnot\\ta\\tgood\\tidea\\tto\\ttrain\\ta\\tvery\\tlarge\\tDNN\\tfrom\\tscratch:\\tinstead,\\tyou\\tshould\\talways\\ttry\\tto\\nfind\\tan\\texisting\\tneural\\tnetwork\\tthat\\taccomplishes\\ta\\tsimilar\\ttask\\tto\\tthe\\tone\\tyou\\tare\\ttrying\\tto\\ttackle,\\tthen\\njust\\treuse\\tthe\\tlower\\tlayers\\tof\\tthis\\tnetwork:\\tthis\\tis\\tcalled\\t\\ntransfer\\tlearning\\n.\\tIt\\twill\\tnot\\tonly\\tspeed\\tup\\ntraining\\tconsiderably,\\tbut\\twill\\talso\\trequire\\tmuch\\tless\\ttraining\\tdata.\\nFor\\texample,\\tsuppose\\tthat\\tyou\\thave\\taccess\\tto\\ta\\tDNN\\tthat\\twas\\ttrained\\tto\\tclassify\\tpictures\\tinto\\t100\\ndifferent\\tcategories,\\tincluding\\tanimals,\\tplants,\\tvehicles,\\tand\\teveryday\\tobjects.\\tYou\\tnow\\twant\\tto\\ttrain\\ta\\nDNN\\tto\\tclassify\\tspecific\\ttypes\\tof\\tvehicles.\\tThese\\ttasks\\tare\\tvery\\tsimilar,\\tso\\tyou\\tshould\\ttry\\tto\\treuse\\tparts\\nof\\tthe\\tfirst\\tnetwork\\t(see\\t\\nFigure\\t11-4\\n).\\nFigure\\t11-4.\\t\\nReusing\\tpretrained\\tlayers\\nNOTE\\nIf\\tthe\\tinput\\tpictures\\tof\\tyour\\tnew\\ttask\\tdon’t\\thave\\tthe\\tsame\\tsize\\tas\\tthe\\tones\\tused\\tin\\tthe\\toriginal\\ttask,\\tyou\\twill\\thave\\tto\\tadd\\ta', 'preprocessing\\tstep\\tto\\tresize\\tthem\\tto\\tthe\\tsize\\texpected\\tby\\tthe\\toriginal\\tmodel.\\tMore\\tgenerally,\\ttransfer\\tlearning\\twill\\tonly\\twork\\twell\\nif\\tthe\\tinputs\\thave\\tsimilar\\tlow-level\\tfeatures.', 'Reusing\\ta\\tTensorFlow\\tModel\\nIf\\t\\nthe\\toriginal\\tmodel\\twas\\ttrained\\tusing\\tTensorFlow,\\tyou\\tcan\\tsimply\\trestore\\tit\\tand\\ttrain\\tit\\ton\\tthe\\tnew\\ttask.\\nAs\\twe\\tdiscussed\\tin\\t\\nChapter\\t9\\n,\\tyou\\tcan\\tuse\\tthe\\t\\nimport_meta_graph()\\n\\tfunction\\tto\\timport\\tthe\\toperations\\ninto\\tthe\\tdefault\\tgraph.\\tThis\\treturns\\ta\\t\\nSaver\\n\\tthat\\tyou\\tcan\\tlater\\tuse\\tto\\tload\\tthe\\tmodel’s\\tstate:\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nimport_meta_graph\\n(\\n\"./my_model_final.ckpt.meta\"\\n)\\nYou\\tmust\\tthen\\tget\\ta\\thandle\\ton\\tthe\\toperations\\tand\\ttensors\\tyou\\twill\\tneed\\tfor\\ttraining.\\tFor\\tthis,\\tyou\\tcan\\tuse\\nthe\\tgraph’s\\t\\nget_operation_by_name()\\n\\tand\\t\\nget_tensor_by_name()\\n\\tmethods.\\tThe\\tname\\tof\\ta\\ttensor\\tis\\nthe\\tname\\tof\\tthe\\toperation\\tthat\\toutputs\\tit\\tfollowed\\tby\\t\\n:0\\n\\t(or\\t\\n:1\\n\\tif\\tit\\tis\\tthe\\tsecond\\toutput,\\t\\n:2\\n\\tif\\tit\\tis\\tthe\\nthird,\\tand\\tso\\ton):\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"X:0\"\\n)\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"y:0\"\\n)\\naccuracy\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"eval/accuracy:0\"\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\ntf\\n.', '=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_operation_by_name\\n(\\n\"GradientDescent\"\\n)\\nIf\\tthe\\tpretrained\\tmodel\\tis\\tnot\\twell\\tdocumented,\\tthen\\tyou\\twill\\thave\\tto\\texplore\\tthe\\tgraph\\tto\\tfind\\tthe\\tnames\\nof\\tthe\\toperations\\tyou\\twill\\tneed.\\tIn\\tthis\\tcase,\\t\\nyou\\tcan\\teither\\texplore\\tthe\\tgraph\\tusing\\tTensorBoard\\t(for\\tthis\\nyou\\tmust\\tfirst\\texport\\tthe\\tgraph\\tusing\\ta\\t\\nFileWriter\\n,\\tas\\tdiscussed\\tin\\t\\nChapter\\t9\\n),\\tor\\tyou\\tcan\\tuse\\tthe\\tgraph’s\\nget_operations()\\n\\tmethod\\tto\\tlist\\tall\\tthe\\toperations:\\nfor\\n\\t\\nop\\n\\t\\nin\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_operations\\n():\\n\\t\\t\\t\\t\\nprint\\n(\\nop\\n.\\nname\\n)\\nIf\\tyou\\tare\\tthe\\tauthor\\tof\\tthe\\toriginal\\tmodel,\\tyou\\tcould\\tmake\\tthings\\teasier\\tfor\\tpeople\\twho\\twill\\treuse\\tyour\\nmodel\\tby\\tgiving\\toperations\\tvery\\tclear\\tnames\\tand\\tdocumenting\\tthem.\\tAnother\\tapproach\\tis\\tto\\tcreate\\ta\\ncollection\\tcontaining\\tall\\tthe\\timportant\\toperations\\tthat\\tpeople\\twill\\twant\\tto\\tget\\ta\\thandle\\ton:\\nfor\\n\\t\\nop\\n\\t\\nin\\n\\t\\n(\\nX\\n,\\n\\t\\ny\\n,\\n\\t\\naccuracy\\n,\\n\\t\\ntraining_op\\n):\\n\\t\\t\\t\\t\\ntf\\n.\\nadd_to_collection\\n(\\n\"my_important_ops\"\\n,\\n\\t\\nop\\n)', ',\\n\\t\\nop\\n)\\nThis\\tway\\tpeople\\twho\\treuse\\tyour\\tmodel\\twill\\tbe\\table\\tto\\tsimply\\twrite:\\nX\\n,\\n\\t\\ny\\n,\\n\\t\\naccuracy\\n,\\n\\t\\ntraining_op\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\n\"my_important_ops\"\\n)\\nYou\\tcan\\tthen\\trestore\\tthe\\tmodel’s\\tstate\\tusing\\tthe\\t\\nSaver\\n\\tand\\tcontinue\\ttraining\\tusing\\tyour\\town\\tdata:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\ttrain\\tthe\\tmodel\\ton\\tyour\\town\\tdata\\nAlternatively,\\tif\\tyou\\thave\\taccess\\tto\\tthe\\tPython\\tcode\\tthat\\tbuilt\\tthe\\toriginal\\tgraph,\\tyou\\tcan\\tuse\\tit\\tinstead\\tof\\nimport_meta_graph()\\n.\\nIn\\tgeneral,\\tyou\\twill\\twant\\tto\\treuse\\tonly\\tpart\\tof\\tthe\\toriginal\\tmodel,\\ttypically\\tthe\\tlower\\tlayers.\\tIf\\tyou\\tuse\\nimport_meta_graph()\\n\\tto\\trestore\\tthe\\tgraph,\\tit\\twill\\tload\\tthe\\tentire\\toriginal\\tgraph,\\tbut\\tnothing\\tprevents', 'you\\tfrom\\tjust\\tignoring\\tthe\\tlayers\\tyou\\tdo\\tnot\\tcare\\tabout.\\tFor\\texample,\\tas\\tshown\\tin\\t\\nFigure\\t11-4\\n,\\tyou\\tcould\\nbuild\\tnew\\tlayers\\t(e.g.,\\tone\\thidden\\tlayer\\tand\\tone\\toutput\\tlayer)\\ton\\ttop\\tof\\ta\\tpretrained\\tlayer\\t(e.g.,\\tpretrained\\nhidden\\tlayer\\t3).\\tYou\\twould\\talso\\tneed\\tto\\tcompute\\tthe\\tloss\\tfor\\tthis\\tnew\\toutput,\\tand\\tcreate\\tan\\toptimizer\\tto\\nminimize\\tthat\\tloss.\\nIf\\tyou\\thave\\taccess\\tto\\tthe\\tpretrained\\tgraph’s\\tPython\\tcode,\\tyou\\tcan\\tjust\\treuse\\tthe\\tparts\\tyou\\tneed\\tand\\tchop\\nout\\tthe\\trest.\\tHowever,\\tin\\tthis\\tcase\\tyou\\tneed\\ta\\t\\nSaver\\n\\tto\\trestore\\tthe\\tpretrained\\tmodel\\t(specifying\\twhich\\nvariables\\tyou\\twant\\tto\\trestore;\\totherwise,\\tTensorFlow\\twill\\tcomplain\\tthat\\tthe\\tgraphs\\tdo\\tnot\\tmatch),\\tand\\nanother\\t\\nSaver\\n\\tto\\tsave\\tthe\\tnew\\tmodel.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\trestores\\tonly\\thidden\\tlayers\\t1,\\t2,\\nand\\t3:\\n[\\n...\\n]\\n\\t\\n#\\tbuild\\tthe\\tnew\\tmodel\\twith\\tthe\\tsame\\thidden\\tlayers\\t1-3\\tas\\tbefore\\nreuse_vars\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nGLOBAL_VARIABLES\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscope\\n=\\n\"hidden[123]\"\\n)\\n\\t\\n#\\tregular\\texpression', 'reuse_vars_dict\\n\\t\\n=\\n\\t\\ndict\\n([(\\nvar\\n.\\nop\\n.\\nname\\n,\\n\\t\\nvar\\n)\\n\\t\\nfor\\n\\t\\nvar\\n\\t\\nin\\n\\t\\nreuse_vars\\n])\\nrestore_saver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n(\\nreuse_vars_dict\\n)\\n\\t\\n#\\tto\\trestore\\tlayers\\t1-3\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\n\\t\\n#\\tto\\tinit\\tall\\tvariables,\\told\\tand\\tnew\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\n\\t\\n#\\tto\\tsave\\tthe\\tnew\\tmodel\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nrestore_saver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\ttrain\\tthe\\tmodel\\n\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"./my_new_model_final.ckpt\"\\n)\\nFirst\\t\\nwe\\tbuild\\tthe\\tnew\\tmodel,\\tmaking\\tsure\\tto\\tcopy\\tthe\\toriginal\\tmodel’s\\thidden\\tlayers\\t1\\tto\\t3.\\tThen\\twe\\tget\\nthe\\tlist\\tof\\tall\\tvariables\\tin\\thidden\\tlayers\\t1\\tto\\t3,\\tusing\\tthe\\tregular\\texpression\\t\\n\"hidden[123]\"\\n.\\tNext,\\twe\\ncreate\\ta\\tdictionary\\tthat\\tmaps\\tthe\\tname\\tof\\teach\\tvariable\\tin\\tthe\\toriginal\\tmodel\\tto\\tits\\tname\\tin\\tthe\\tnew\\tmodel\\n(generally\\tyou\\twant\\tto\\tkeep\\tthe\\texact\\tsame\\tnames).\\tThen\\twe\\tcreate\\ta\\t\\nSaver\\n\\tthat\\twill\\trestore\\tonly\\tthese', 'variables.\\tWe\\talso\\tcreate\\tan\\toperation\\tto\\tinitialize\\tall\\tthe\\tvariables\\t(old\\tand\\tnew)\\tand\\ta\\tsecond\\t\\nSaver\\n\\tto\\nsave\\tthe\\tentire\\tnew\\tmodel,\\tnot\\tjust\\tlayers\\t1\\tto\\t3.\\tWe\\tthen\\tstart\\ta\\tsession\\tand\\tinitialize\\tall\\tvariables\\tin\\tthe\\nmodel,\\tthen\\trestore\\tthe\\tvariable\\tvalues\\tfrom\\tthe\\toriginal\\tmodel’s\\tlayers\\t1\\tto\\t3.\\tFinally,\\twe\\ttrain\\tthe\\tmodel\\non\\tthe\\tnew\\ttask\\tand\\tsave\\tit.\\nTIP\\nThe\\tmore\\tsimilar\\tthe\\ttasks\\tare,\\tthe\\tmore\\tlayers\\tyou\\twant\\tto\\treuse\\t(starting\\twith\\tthe\\tlower\\tlayers).\\tFor\\tvery\\tsimilar\\ttasks,\\tyou\\tcan\\ntry\\tkeeping\\tall\\tthe\\thidden\\tlayers\\tand\\tjust\\treplace\\tthe\\toutput\\t\\nlayer.', 'Reusing\\tModels\\tfrom\\tOther\\tFrameworks\\nIf\\tthe\\t\\nmodel\\twas\\ttrained\\tusing\\tanother\\tframework,\\tyou\\twill\\tneed\\tto\\tload\\tthe\\tmodel\\tparameters\\tmanually\\n(e.g.,\\tusing\\tTheano\\tcode\\tif\\tit\\twas\\ttrained\\twith\\tTheano),\\tthen\\tassign\\tthem\\tto\\tthe\\tappropriate\\tvariables.\\nThis\\tcan\\tbe\\tquite\\ttedious.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tshows\\thow\\tyou\\twould\\tcopy\\tthe\\tweight\\tand\\nbiases\\tfrom\\tthe\\tfirst\\thidden\\tlayer\\tof\\ta\\t\\nmodel\\ttrained\\tusing\\tanother\\tframework:\\noriginal_w\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\n#\\tLoad\\tthe\\tweights\\tfrom\\tthe\\tother\\tframework\\noriginal_b\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\n#\\tLoad\\tthe\\tbiases\\tfrom\\tthe\\tother\\tframework\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_inputs\\n),\\n\\t\\nname\\n=\\n\"X\"\\n)\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\n[\\n...\\n]\\n\\t\\n#\\tBuild\\tthe\\trest\\tof\\tthe\\tmodel\\n#\\tGet\\ta\\thandle\\ton\\tthe\\tassignment\\tnodes\\tfor\\tthe\\thidden1\\tvariables\\ngraph\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\nassign_kernel\\n\\t\\n=\\n\\t\\ngraph\\n.\\nget_operation_by_name\\n(\\n\"hidden1/kernel/Assign\"\\n)\\nassign_bias\\n\\t\\n=', '=\\n\\t\\ngraph\\n.\\nget_operation_by_name\\n(\\n\"hidden1/bias/Assign\"\\n)\\ninit_kernel\\n\\t\\n=\\n\\t\\nassign_kernel\\n.\\ninputs\\n[\\n1\\n]\\ninit_bias\\n\\t\\n=\\n\\t\\nassign_bias\\n.\\ninputs\\n[\\n1\\n]\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ninit\\n,\\n\\t\\nfeed_dict\\n=\\n{\\ninit_kernel\\n:\\n\\t\\noriginal_w\\n,\\n\\t\\ninit_bias\\n:\\n\\t\\noriginal_b\\n})\\n\\t\\t\\t\\t\\n#\\t[...]\\tTrain\\tthe\\tmodel\\ton\\tyour\\tnew\\ttask\\nIn\\tthis\\timplementation,\\twe\\tfirst\\tload\\tthe\\tpretrained\\tmodel\\tusing\\tthe\\tother\\tframework\\t(not\\tshown\\there),\\nand\\twe\\textract\\tfrom\\tit\\tthe\\tmodel\\tparameters\\twe\\twant\\tto\\treuse.\\tNext,\\twe\\tbuild\\tour\\tTensorFlow\\tmodel\\tas\\nusual.\\tThen\\tcomes\\tthe\\ttricky\\tpart:\\tevery\\tTensorFlow\\tvariable\\thas\\tan\\tassociated\\tassignment\\toperation\\tthat\\nis\\tused\\tto\\tinitialize\\tit.\\tWe\\tstart\\tby\\tgetting\\ta\\thandle\\ton\\tthese\\tassignment\\toperations\\t(they\\thave\\tthe\\tsame\\nname\\tas\\tthe\\tvariable,\\tplus\\t\\n\"/Assign\"\\n).\\tWe\\talso\\tget\\ta\\thandle\\ton\\teach\\tassignment\\toperation’s\\tsecond\\ninput:\\tin\\tthe\\tcase\\tof\\tan\\tassignment\\toperation,\\tthe\\tsecond\\tinput\\tcorresponds\\tto\\tthe\\tvalue\\tthat\\twill\\tbe', 'assigned\\tto\\tthe\\tvariable,\\tso\\tin\\tthis\\tcase\\tit\\tis\\tthe\\tvariable’s\\tinitialization\\tvalue.\\tOnce\\twe\\tstart\\tthe\\tsession,\\nwe\\trun\\tthe\\tusual\\tinitialization\\toperation,\\tbut\\tthis\\ttime\\twe\\tfeed\\tit\\tthe\\tvalues\\twe\\twant\\tfor\\tthe\\tvariables\\twe\\nwant\\tto\\treuse.\\tAlternatively,\\twe\\tcould\\thave\\tcreated\\tnew\\tassignment\\toperations\\tand\\tplaceholders,\\tand\\nused\\tthem\\tto\\tset\\tthe\\tvalues\\tof\\tthe\\tvariables\\tafter\\tinitialization.\\tBut\\twhy\\tcreate\\tnew\\tnodes\\tin\\tthe\\tgraph\\nwhen\\teverything\\twe\\tneed\\tis\\talready\\tthere?', 'Freezing\\tthe\\tLower\\tLayers\\nIt\\t\\nis\\tlikely\\tthat\\tthe\\tlower\\tlayers\\tof\\tthe\\tfirst\\tDNN\\thave\\tlearned\\tto\\tdetect\\tlow-level\\tfeatures\\tin\\tpictures\\tthat\\nwill\\tbe\\tuseful\\tacross\\tboth\\timage\\tclassification\\ttasks,\\tso\\tyou\\tcan\\tjust\\treuse\\tthese\\tlayers\\tas\\tthey\\tare.\\tIt\\tis\\ngenerally\\ta\\tgood\\tidea\\tto\\t“freeze”\\ttheir\\t\\nweights\\twhen\\ttraining\\tthe\\tnew\\tDNN:\\tif\\tthe\\tlower-layer\\tweights\\nare\\tfixed,\\tthen\\tthe\\thigher-layer\\tweights\\twill\\tbe\\teasier\\tto\\ttrain\\t(because\\tthey\\twon’t\\thave\\tto\\tlearn\\ta\\tmoving\\ntarget).\\tTo\\tfreeze\\tthe\\tlower\\tlayers\\tduring\\ttraining,\\tone\\tsolution\\tis\\tto\\tgive\\tthe\\toptimizer\\tthe\\tlist\\tof\\nvariables\\tto\\ttrain,\\texcluding\\tthe\\tvariables\\t\\nfrom\\tthe\\tlower\\tlayers:\\ntrain_vars\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nTRAINABLE_VARIABLES\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscope\\n=\\n\"hidden[34]|outputs\"\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n,\\n\\t\\nvar_list\\n=\\ntrain_vars\\n)\\nThe\\tfirst\\tline\\tgets\\tthe\\tlist\\tof\\tall\\ttrainable\\tvariables\\tin\\thidden\\tlayers\\t3\\tand\\t4\\tand\\tin\\tthe\\toutput\\tlayer.\\tThis', 'leaves\\tout\\tthe\\tvariables\\tin\\tthe\\thidden\\tlayers\\t1\\tand\\t2.\\tNext\\twe\\tprovide\\tthis\\trestricted\\tlist\\tof\\ttrainable\\nvariables\\tto\\tthe\\toptimizer’s\\t\\nminimize()\\n\\t\\nfunction.\\tTa-da!\\tLayers\\t1\\tand\\t2\\tare\\tnow\\tfrozen:\\tthey\\twill\\tnot\\nbudge\\tduring\\ttraining\\t(these\\tare\\toften\\tcalled\\t\\nfrozen\\tlayers\\n).\\nAnother\\toption\\tis\\tto\\tadd\\ta\\t\\nstop_gradient()\\n\\tlayer\\tin\\tthe\\tgraph.\\tAny\\tlayer\\tbelow\\tit\\twill\\tbe\\tfrozen:\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):\\n\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden1\"\\n)\\n\\t\\n#\\treused\\tfrozen\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden2\"\\n)\\n\\t\\n#\\treused\\tfrozen\\n\\t\\t\\t\\t\\nhidden2_stop\\n\\t\\n=\\n\\t\\ntf\\n.\\nstop_gradient\\n(\\nhidden2\\n)\\n\\t\\t\\t\\t\\nhidden3\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden2_stop\\n,\\n\\t\\nn_hidden3\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden3\"\\n)\\n\\t\\n#\\treused,\\tnot\\tfrozen\\n\\t\\t\\t\\t\\nhidden4\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.', 'layers\\n.\\ndense\\n(\\nhidden3\\n,\\n\\t\\nn_hidden4\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden4\"\\n)\\n\\t\\n#\\tnew!\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden4\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\n\\t\\n#\\tnew!', 'Caching\\tthe\\tFrozen\\tLayers\\nSince\\t\\nthe\\tfrozen\\tlayers\\twon’t\\tchange,\\tit\\tis\\tpossible\\tto\\tcache\\tthe\\toutput\\tof\\tthe\\ttopmost\\tfrozen\\tlayer\\tfor\\teach\\ntraining\\tinstance.\\tSince\\ttraining\\tgoes\\tthrough\\tthe\\twhole\\tdataset\\tmany\\ttimes,\\tthis\\twill\\tgive\\tyou\\ta\\thuge\\nspeed\\tboost\\tas\\tyou\\twill\\tonly\\tneed\\tto\\tgo\\tthrough\\tthe\\tfrozen\\tlayers\\tonce\\tper\\ttraining\\tinstance\\t(instead\\tof\\nonce\\tper\\tepoch).\\tFor\\texample,\\tyou\\tcould\\tfirst\\trun\\tthe\\twhole\\ttraining\\tset\\tthrough\\tthe\\tlower\\tlayers\\n(assuming\\tyou\\thave\\tenough\\tRAM),\\tthen\\tduring\\ttraining,\\tinstead\\tof\\tbuilding\\tbatches\\tof\\ttraining\\tinstances,\\nyou\\twould\\tbuild\\tbatches\\tof\\toutputs\\tfrom\\thidden\\tlayer\\t2\\tand\\tfeed\\tthem\\tto\\tthe\\ttraining\\toperation:\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nn_batches\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nrestore_saver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"./my_model_final.ckpt\"\\n)\\n\\t\\t\\t\\t\\nh2_cache\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\nhidden2\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nmnist\\n.\\ntrain\\n.\\nimages\\n})\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):', 'shuffled_idx\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\npermutation\\n(\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nhidden2_batches\\n\\t\\n=\\n\\t\\nnp\\n.\\narray_split\\n(\\nh2_cache\\n[\\nshuffled_idx\\n],\\n\\t\\nn_batches\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ny_batches\\n\\t\\n=\\n\\t\\nnp\\n.\\narray_split\\n(\\nmnist\\n.\\ntrain\\n.\\nlabels\\n[\\nshuffled_idx\\n],\\n\\t\\nn_batches\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nhidden2_batch\\n,\\n\\t\\ny_batch\\n\\t\\nin\\n\\t\\nzip\\n(\\nhidden2_batches\\n,\\n\\t\\ny_batches\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nhidden2\\n:\\nhidden2_batch\\n,\\n\\t\\ny\\n:\\ny_batch\\n})\\n\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"./my_new_model_final.ckpt\"\\n)\\nThe\\tlast\\tline\\tof\\tthe\\ttraining\\tloop\\truns\\tthe\\ttraining\\toperation\\tdefined\\tearlier\\t(which\\tdoes\\tnot\\ttouch\\tlayers\\t1\\nand\\t2),\\tand\\tfeeds\\tit\\ta\\tbatch\\tof\\toutputs\\tfrom\\tthe\\tsecond\\thidden\\tlayer\\t(as\\twell\\tas\\tthe\\ttargets\\tfor\\tthat\\tbatch).\\nSince\\twe\\tgive\\tTensorFlow\\tthe\\toutput\\tof\\thidden\\tlayer\\t2,\\tit\\tdoes\\tnot\\ttry\\tto\\tevaluate\\t\\nit\\t(or\\tany\\tnode\\tit\\ndepends\\ton).', 'Tweaking,\\tDropping,\\tor\\tReplacing\\tthe\\tUpper\\tLayers\\nThe\\t\\noutput\\tlayer\\tof\\tthe\\toriginal\\tmodel\\tshould\\tusually\\tbe\\treplaced\\tsince\\tit\\tis\\tmost\\tlikely\\tnot\\tuseful\\tat\\tall\\nfor\\tthe\\tnew\\ttask,\\tand\\tit\\tmay\\tnot\\teven\\thave\\tthe\\tright\\tnumber\\tof\\toutputs\\tfor\\tthe\\tnew\\ttask.\\nSimilarly,\\tthe\\tupper\\thidden\\tlayers\\tof\\tthe\\toriginal\\tmodel\\tare\\tless\\tlikely\\tto\\tbe\\tas\\tuseful\\tas\\tthe\\tlower\\tlayers,\\nsince\\tthe\\thigh-level\\tfeatures\\tthat\\tare\\tmost\\tuseful\\tfor\\tthe\\tnew\\ttask\\tmay\\tdiffer\\tsignificantly\\tfrom\\tthe\\tones\\nthat\\twere\\tmost\\tuseful\\tfor\\tthe\\toriginal\\ttask.\\tYou\\twant\\tto\\tfind\\tthe\\tright\\tnumber\\tof\\tlayers\\tto\\treuse.\\nTry\\tfreezing\\tall\\tthe\\tcopied\\tlayers\\tfirst,\\tthen\\ttrain\\tyour\\tmodel\\tand\\tsee\\thow\\tit\\tperforms.\\tThen\\ttry\\tunfreezing\\none\\tor\\ttwo\\tof\\tthe\\ttop\\thidden\\tlayers\\tto\\tlet\\tbackpropagation\\ttweak\\tthem\\tand\\tsee\\tif\\tperformance\\timproves.\\nThe\\tmore\\ttraining\\tdata\\tyou\\thave,\\tthe\\tmore\\tlayers\\tyou\\tcan\\tunfreeze.\\nIf\\tyou\\tstill\\tcannot\\tget\\tgood\\tperformance,\\tand\\tyou\\thave\\tlittle\\ttraining\\tdata,\\ttry\\tdropping\\tthe\\ttop\\thidden', 'layer(s)\\tand\\tfreeze\\tall\\tremaining\\thidden\\tlayers\\tagain.\\tYou\\tcan\\titerate\\tuntil\\tyou\\tfind\\tthe\\tright\\tnumber\\tof\\nlayers\\tto\\treuse.\\tIf\\tyou\\thave\\tplenty\\tof\\ttraining\\tdata,\\tyou\\tmay\\ttry\\treplacing\\tthe\\ttop\\thidden\\tlayers\\tinstead\\tof\\ndropping\\tthem,\\tand\\teven\\tadd\\tmore\\thidden\\tlayers.', 'Model\\tZoos\\nWhere\\t\\ncan\\tyou\\tfind\\ta\\tneural\\tnetwork\\ttrained\\tfor\\ta\\ttask\\tsimilar\\tto\\tthe\\tone\\tyou\\twant\\tto\\ttackle?\\tThe\\tfirst\\nplace\\tto\\tlook\\tis\\tobviously\\tin\\tyour\\town\\tcatalog\\tof\\tmodels.\\tThis\\tis\\tone\\tgood\\treason\\tto\\tsave\\tall\\tyour\\tmodels\\nand\\torganize\\tthem\\tso\\tyou\\tcan\\tretrieve\\tthem\\tlater\\teasily.\\tAnother\\toption\\tis\\tto\\tsearch\\tin\\ta\\t\\nmodel\\tzoo\\n.\\tMany\\npeople\\ttrain\\tMachine\\tLearning\\tmodels\\tfor\\tvarious\\ttasks\\tand\\tkindly\\trelease\\ttheir\\tpretrained\\tmodels\\tto\\tthe\\npublic.\\nTensorFlow\\t\\nhas\\tits\\town\\tmodel\\tzoo\\tavailable\\tat\\t\\nhttps://github.com/tensorflow/models\\n.\\tIn\\tparticular,\\tit\\ncontains\\tmost\\tof\\tthe\\tstate-of-the-art\\timage\\tclassification\\tnets\\tsuch\\tas\\tVGG,\\tInception,\\tand\\t\\nResNet\\t(see\\nChapter\\t13\\n,\\tand\\tcheck\\tout\\tthe\\t\\nmodels/slim\\n\\tdirectory),\\tincluding\\tthe\\tcode,\\tthe\\tpretrained\\tmodels,\\tand\\ttools\\nto\\tdownload\\tpopular\\timage\\tdatasets.\\nAnother\\tpopular\\tmodel\\tzoo\\tis\\t\\nCaffe’s\\t\\nModel\\tZoo\\n.\\tIt\\talso\\tcontains\\tmany\\tcomputer\\tvision\\tmodels\\t(e.g.,\\nLeNet,\\tAlexNet,\\tZFNet,\\tGoogLeNet,\\tVGGNet,\\tinception)\\ttrained\\ton\\tvarious\\tdatasets\\t(e.g.,\\tImageNet,', 'Places\\tDatabase,\\tCIFAR10,\\tetc.).\\tSaumitro\\tDasgupta\\twrote\\ta\\tconverter,\\twhich\\tis\\tavailable\\tat\\nhttps://github.com/ethereon/caffe-tensorflow\\n.', 'Unsupervised\\tPretraining\\nSuppose\\t\\nyou\\twant\\tto\\ttackle\\ta\\tcomplex\\ttask\\tfor\\twhich\\tyou\\tdon’t\\thave\\tmuch\\tlabeled\\ttraining\\tdata,\\tbut\\nunfortunately\\tyou\\tcannot\\tfind\\ta\\tmodel\\ttrained\\ton\\ta\\tsimilar\\ttask.\\tDon’t\\tlose\\tall\\thope!\\tFirst,\\tyou\\tshould\\tof\\ncourse\\ttry\\tto\\tgather\\tmore\\tlabeled\\ttraining\\tdata,\\tbut\\tif\\tthis\\tis\\ttoo\\thard\\tor\\ttoo\\texpensive,\\tyou\\tmay\\tstill\\tbe\\nable\\tto\\tperform\\t\\nunsupervised\\tpretraining\\n\\t(see\\t\\nFigure\\t11-5\\n).\\tThat\\tis,\\tif\\tyou\\thave\\tplenty\\tof\\tunlabeled\\ntraining\\tdata,\\tyou\\tcan\\ttry\\tto\\ttrain\\tthe\\tlayers\\tone\\tby\\tone,\\tstarting\\twith\\tthe\\tlowest\\tlayer\\tand\\tthen\\tgoing\\tup,\\nusing\\tan\\tunsupervised\\tfeature\\tdetector\\talgorithm\\tsuch\\t\\nas\\t\\nRestricted\\tBoltzmann\\tMachines\\n\\t(RBMs;\\tsee\\nAppendix\\tE\\n)\\tor\\tautoencoders\\t(see\\t\\nChapter\\t15\\n).\\tEach\\tlayer\\tis\\ttrained\\ton\\tthe\\toutput\\tof\\tthe\\tpreviously\\ntrained\\tlayers\\t(all\\tlayers\\texcept\\tthe\\tone\\tbeing\\ttrained\\tare\\tfrozen).\\tOnce\\tall\\tlayers\\thave\\tbeen\\ttrained\\tthis\\nway,\\tyou\\tcan\\tfine-tune\\tthe\\tnetwork\\tusing\\tsupervised\\tlearning\\t(i.e.,\\twith\\tbackpropagation).', 'This\\tis\\ta\\trather\\tlong\\tand\\ttedious\\tprocess,\\tbut\\tit\\toften\\tworks\\twell;\\tin\\tfact,\\tit\\tis\\tthis\\ttechnique\\tthat\\tGeoffrey\\nHinton\\tand\\this\\tteam\\tused\\tin\\t2006\\tand\\twhich\\tled\\tto\\tthe\\trevival\\tof\\tneural\\tnetworks\\tand\\tthe\\tsuccess\\tof\\tDeep\\nLearning.\\tUntil\\t2010,\\tunsupervised\\tpretraining\\t(typically\\tusing\\tRBMs)\\twas\\tthe\\tnorm\\tfor\\tdeep\\tnets,\\tand\\tit\\nwas\\tonly\\tafter\\tthe\\tvanishing\\tgradients\\tproblem\\twas\\talleviated\\tthat\\tit\\tbecame\\tmuch\\tmore\\tcommon\\tto\\ttrain\\nDNNs\\tpurely\\tusing\\t\\nbackpropagation.\\tHowever,\\tunsupervised\\tpretraining\\t(today\\ttypically\\tusing\\nautoencoders\\trather\\tthan\\tRBMs)\\tis\\tstill\\ta\\tgood\\toption\\twhen\\tyou\\thave\\ta\\tcomplex\\ttask\\tto\\tsolve,\\tno\\tsimilar\\nmodel\\tyou\\tcan\\treuse,\\tand\\tlittle\\tlabeled\\ttraining\\tdata\\tbut\\tplenty\\tof\\tunlabeled\\t\\ntraining\\tdata.\\n10\\nFigure\\t11-5.\\t\\nUnsupervised\\tpretraining', 'Pretraining\\ton\\tan\\tAuxiliary\\tTask\\nOne\\t\\nlast\\toption\\tis\\tto\\ttrain\\ta\\tfirst\\tneural\\tnetwork\\ton\\tan\\tauxiliary\\ttask\\tfor\\twhich\\tyou\\tcan\\teasily\\tobtain\\tor\\ngenerate\\tlabeled\\ttraining\\tdata,\\tthen\\treuse\\tthe\\tlower\\tlayers\\tof\\tthat\\tnetwork\\tfor\\tyour\\tactual\\ttask.\\tThe\\tfirst\\nneural\\tnetwork’s\\tlower\\tlayers\\twill\\tlearn\\tfeature\\tdetectors\\tthat\\twill\\tlikely\\tbe\\treusable\\tby\\tthe\\tsecond\\nneural\\tnetwork.\\nFor\\texample,\\tif\\tyou\\twant\\tto\\tbuild\\ta\\tsystem\\tto\\trecognize\\tfaces,\\tyou\\tmay\\tonly\\thave\\ta\\tfew\\tpictures\\tof\\teach\\nindividual\\t—\\tclearly\\tnot\\tenough\\tto\\ttrain\\ta\\tgood\\tclassifier.\\tGathering\\thundreds\\tof\\tpictures\\tof\\teach\\tperson\\nwould\\tnot\\tbe\\tpractical.\\tHowever,\\tyou\\tcould\\tgather\\ta\\tlot\\tof\\tpictures\\tof\\trandom\\tpeople\\ton\\tthe\\tinternet\\tand\\ntrain\\ta\\tfirst\\tneural\\tnetwork\\tto\\tdetect\\twhether\\tor\\tnot\\ttwo\\tdifferent\\tpictures\\tfeature\\tthe\\tsame\\tperson.\\tSuch\\ta\\nnetwork\\twould\\tlearn\\tgood\\tfeature\\tdetectors\\tfor\\tfaces,\\tso\\treusing\\tits\\tlower\\tlayers\\twould\\tallow\\tyou\\tto\\ntrain\\ta\\tgood\\tface\\tclassifier\\tusing\\tlittle\\ttraining\\tdata.', 'It\\tis\\toften\\trather\\tcheap\\tto\\tgather\\tunlabeled\\ttraining\\texamples,\\tbut\\tquite\\texpensive\\tto\\tlabel\\tthem.\\tIn\\tthis\\nsituation,\\ta\\tcommon\\ttechnique\\tis\\tto\\tlabel\\tall\\tyour\\ttraining\\texamples\\tas\\t“good,”\\tthen\\tgenerate\\tmany\\tnew\\ntraining\\tinstances\\tby\\tcorrupting\\tthe\\tgood\\tones,\\tand\\tlabel\\tthese\\tcorrupted\\tinstances\\tas\\t“bad.”\\tThen\\tyou\\tcan\\ntrain\\ta\\tfirst\\tneural\\tnetwork\\tto\\tclassify\\tinstances\\tas\\tgood\\tor\\tbad.\\tFor\\texample,\\tyou\\tcould\\tdownload\\nmillions\\tof\\tsentences,\\tlabel\\tthem\\tas\\t“good,”\\tthen\\trandomly\\tchange\\ta\\tword\\tin\\teach\\tsentence\\tand\\tlabel\\tthe\\nresulting\\tsentences\\tas\\t“bad.”\\tIf\\ta\\tneural\\tnetwork\\tcan\\ttell\\tthat\\t“The\\tdog\\tsleeps”\\tis\\ta\\tgood\\tsentence\\tbut\\n“The\\tdog\\tthey”\\tis\\tbad,\\tit\\tprobably\\tknows\\tquite\\ta\\tlot\\tabout\\tlanguage.\\tReusing\\tits\\tlower\\tlayers\\twill\\tlikely\\nhelp\\tin\\tmany\\tlanguage\\tprocessing\\ttasks.\\nAnother\\tapproach\\tis\\tto\\ttrain\\ta\\tfirst\\tnetwork\\tto\\toutput\\ta\\tscore\\tfor\\teach\\ttraining\\tinstance,\\tand\\tuse\\ta\\t\\ncost\\nfunction\\tthat\\tensures\\tthat\\ta\\tgood\\tinstance’s\\tscore\\tis\\tgreater\\tthan\\ta\\tbad\\tinstance’s\\tscore\\tby\\tat\\tleast\\tsome', 'margin.\\tThis\\tis\\t\\ncalled\\t\\nmax\\tmargin\\tlearning\\n.', 'Faster\\tOptimizers\\nTraining\\t\\na\\tvery\\tlarge\\tdeep\\tneural\\tnetwork\\tcan\\tbe\\tpainfully\\tslow.\\tSo\\tfar\\twe\\thave\\tseen\\tfour\\tways\\tto\\tspeed\\nup\\ttraining\\t(and\\treach\\ta\\tbetter\\tsolution):\\tapplying\\ta\\tgood\\tinitialization\\tstrategy\\tfor\\tthe\\tconnection\\tweights,\\nusing\\ta\\tgood\\tactivation\\tfunction,\\tusing\\tBatch\\tNormalization,\\tand\\treusing\\tparts\\tof\\ta\\tpretrained\\tnetwork.\\nAnother\\thuge\\tspeed\\tboost\\tcomes\\tfrom\\tusing\\ta\\tfaster\\toptimizer\\tthan\\tthe\\tregular\\tGradient\\tDescent\\noptimizer.\\tIn\\tthis\\tsection\\twe\\twill\\tpresent\\tthe\\tmost\\tpopular\\tones:\\tMomentum\\toptimization,\\tNesterov\\nAccelerated\\tGradient,\\tAdaGrad,\\tRMSProp,\\tand\\tfinally\\tAdam\\toptimization.', 'Momentum\\tOptimization\\nImagine\\t\\na\\tbowling\\tball\\trolling\\tdown\\ta\\tgentle\\tslope\\ton\\ta\\tsmooth\\tsurface:\\tit\\twill\\tstart\\tout\\tslowly,\\tbut\\tit\\nwill\\tquickly\\tpick\\tup\\tmomentum\\tuntil\\tit\\teventually\\treaches\\tterminal\\tvelocity\\t(if\\tthere\\tis\\tsome\\tfriction\\tor\\nair\\tresistance).\\tThis\\tis\\tthe\\tvery\\tsimple\\tidea\\tbehind\\t\\nMomentum\\toptimization\\n,\\t\\nproposed\\tby\\tBoris\\tPolyak\\nin\\t1964\\n.\\n11\\n\\tIn\\tcontrast,\\tregular\\tGradient\\tDescent\\twill\\tsimply\\ttake\\tsmall\\tregular\\tsteps\\tdown\\tthe\\tslope,\\tso\\tit\\nwill\\ttake\\tmuch\\tmore\\ttime\\tto\\treach\\tthe\\tbottom.\\nRecall\\tthat\\t\\nGradient\\tDescent\\tsimply\\tupdates\\tthe\\tweights\\t\\nθ\\n\\tby\\tdirectly\\tsubtracting\\tthe\\tgradient\\tof\\tthe\\tcost\\nfunction\\t\\nJ\\n(\\nθ\\n)\\twith\\tregards\\tto\\tthe\\tweights\\t(\\nθ\\nJ\\n(\\nθ\\n))\\tmultiplied\\tby\\tthe\\tlearning\\trate\\t\\nη\\n.\\tThe\\tequation\\tis:\\t\\nθ\\n\\t←\\nθ\\n\\t–\\t\\nη\\nθ\\nJ\\n(\\nθ\\n).\\tIt\\tdoes\\tnot\\tcare\\tabout\\twhat\\tthe\\tearlier\\tgradients\\twere.\\tIf\\tthe\\tlocal\\tgradient\\tis\\ttiny,\\tit\\tgoes\\nvery\\tslowly.\\nMomentum\\toptimization\\tcares\\ta\\tgreat\\tdeal\\tabout\\twhat\\tprevious\\tgradients\\twere:\\tat\\teach\\titeration,\\tit\\nsubtracts\\tthe\\tlocal\\tgradient\\tfrom\\tthe', 'momentum\\tvector\\n\\t\\nm\\n\\t(multiplied\\tby\\tthe\\tlearning\\trate\\t\\nη\\n),\\tand\\tit\\nupdates\\tthe\\tweights\\tby\\tsimply\\tadding\\tthis\\tmomentum\\tvector\\t(see\\t\\nEquation\\t11-4\\n).\\tIn\\tother\\twords,\\tthe\\ngradient\\tis\\tused\\tas\\tan\\tacceleration,\\tnot\\tas\\ta\\tspeed.\\tTo\\tsimulate\\tsome\\tsort\\tof\\tfriction\\tmechanism\\tand\\nprevent\\tthe\\tmomentum\\tfrom\\tgrowing\\ttoo\\tlarge,\\tthe\\talgorithm\\tintroduces\\ta\\tnew\\thyperparameter\\t\\nβ\\n,\\tsimply\\ncalled\\tthe\\t\\nmomentum\\n,\\twhich\\tmust\\tbe\\tset\\tbetween\\t0\\t(high\\tfriction)\\tand\\t1\\t(no\\tfriction).\\tA\\ttypical\\nmomentum\\tvalue\\tis\\t0.9.\\nEquation\\t11-4.\\t\\nMomentum\\talgorithm\\nYou\\tcan\\teasily\\tverify\\tthat\\tif\\tthe\\tgradient\\tremains\\tconstant,\\tthe\\tterminal\\tvelocity\\t(i.e.,\\tthe\\tmaximum\\tsize\\tof\\nthe\\tweight\\tupdates)\\tis\\tequal\\tto\\tthat\\tgradient\\tmultiplied\\tby\\tthe\\tlearning\\trate\\t\\nη\\n\\tmultiplied\\tby\\t\\n\\t(ignoring\\nthe\\tsign).\\tFor\\texample,\\tif\\t\\nβ\\n\\t=\\t0.9,\\tthen\\tthe\\tterminal\\tvelocity\\tis\\tequal\\tto\\t10\\ttimes\\tthe\\tgradient\\ttimes\\tthe\\nlearning\\trate,\\tso\\tMomentum\\toptimization\\tends\\tup\\tgoing\\t10\\ttimes\\tfaster\\tthan\\tGradient\\tDescent!\\tThis', 'allows\\tMomentum\\toptimization\\tto\\tescape\\tfrom\\tplateaus\\tmuch\\tfaster\\tthan\\tGradient\\tDescent.\\tIn\\tparticular,\\nwe\\tsaw\\tin\\t\\nChapter\\t4\\n\\tthat\\twhen\\tthe\\tinputs\\thave\\tvery\\tdifferent\\tscales\\tthe\\t\\ncost\\tfunction\\twill\\tlook\\tlike\\tan\\nelongated\\tbowl\\t(see\\t\\nFigure\\t4-7\\n).\\tGradient\\tDescent\\tgoes\\tdown\\tthe\\tsteep\\tslope\\tquite\\tfast,\\tbut\\tthen\\tit\\ttakes\\na\\tvery\\tlong\\ttime\\tto\\tgo\\tdown\\tthe\\tvalley.\\tIn\\tcontrast,\\tMomentum\\toptimization\\twill\\troll\\tdown\\tthe\\tbottom\\tof\\nthe\\tvalley\\tfaster\\tand\\tfaster\\tuntil\\tit\\treaches\\tthe\\tbottom\\t(the\\toptimum).\\tIn\\tdeep\\tneural\\tnetworks\\tthat\\tdon’t\\nuse\\tBatch\\tNormalization,\\tthe\\tupper\\tlayers\\twill\\toften\\tend\\tup\\thaving\\tinputs\\twith\\tvery\\tdifferent\\tscales,\\tso\\nusing\\tMomentum\\toptimization\\thelps\\ta\\tlot.\\tIt\\tcan\\talso\\thelp\\troll\\tpast\\tlocal\\toptima.', 'NOTE\\nDue\\tto\\tthe\\tmomentum,\\tthe\\toptimizer\\tmay\\tovershoot\\ta\\tbit,\\tthen\\tcome\\tback,\\tovershoot\\tagain,\\tand\\toscillate\\tlike\\tthis\\tmany\\ttimes\\nbefore\\tstabilizing\\tat\\tthe\\tminimum.\\tThis\\tis\\tone\\tof\\tthe\\treasons\\twhy\\tit\\tis\\tgood\\tto\\thave\\ta\\tbit\\tof\\tfriction\\tin\\tthe\\tsystem:\\tit\\tgets\\trid\\tof\\nthese\\toscillations\\tand\\tthus\\tspeeds\\tup\\tconvergence.\\nImplementing\\tMomentum\\toptimization\\tin\\t\\nTensorFlow\\tis\\ta\\tno-brainer:\\tjust\\treplace\\tthe\\nGradientDescentOptimizer\\n\\twith\\tthe\\t\\nMomentumOptimizer\\n,\\tthen\\tlie\\tback\\tand\\tprofit!\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nMomentumOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmomentum\\n=\\n0.9\\n)\\nThe\\tone\\tdrawback\\tof\\tMomentum\\toptimization\\tis\\tthat\\tit\\tadds\\tyet\\tanother\\thyperparameter\\tto\\ttune.\\tHowever,\\nthe\\tmomentum\\tvalue\\tof\\t0.9\\tusually\\tworks\\twell\\tin\\tpractice\\tand\\talmost\\talways\\tgoes\\tfaster\\t\\nthan\\tGradient\\nDescent.', 'Nesterov\\tAccelerated\\tGradient\\nOne\\t\\nsmall\\tvariant\\tto\\tMomentum\\toptimization,\\tproposed\\tby\\t\\nYurii\\tNesterov\\tin\\t1983\\n,\\n12\\n\\tis\\talmost\\talways\\nfaster\\tthan\\tvanilla\\tMomentum\\toptimization.\\tThe\\t\\nidea\\tof\\t\\nNesterov\\tMomentum\\toptimization\\n,\\tor\\t\\nNesterov\\nAccelerated\\tGradient\\n\\t(NAG),\\tis\\tto\\tmeasure\\tthe\\tgradient\\tof\\tthe\\tcost\\tfunction\\tnot\\tat\\tthe\\tlocal\\tposition\\tbut\\nslightly\\tahead\\tin\\tthe\\tdirection\\tof\\tthe\\tmomentum\\t(see\\t\\nEquation\\t11-5\\n).\\tThe\\tonly\\tdifference\\tfrom\\tvanilla\\nMomentum\\toptimization\\tis\\tthat\\tthe\\tgradient\\tis\\tmeasured\\tat\\t\\nθ\\n\\t+\\t\\nβ\\nm\\n\\trather\\tthan\\tat\\t\\nθ\\n.\\nEquation\\t11-5.\\t\\nNesterov\\tAccelerated\\tGradient\\talgorithm\\nThis\\tsmall\\ttweak\\tworks\\tbecause\\tin\\tgeneral\\tthe\\tmomentum\\tvector\\twill\\tbe\\tpointing\\tin\\tthe\\tright\\tdirection\\n(i.e.,\\ttoward\\tthe\\toptimum),\\tso\\tit\\twill\\tbe\\tslightly\\tmore\\taccurate\\tto\\tuse\\tthe\\tgradient\\tmeasured\\ta\\tbit\\tfarther\\tin\\nthat\\tdirection\\trather\\tthan\\tusing\\tthe\\tgradient\\tat\\tthe\\toriginal\\tposition,\\tas\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t11-6\\n\\t(where\\n1\\n\\trepresents\\tthe\\tgradient\\tof\\tthe\\tcost\\tfunction\\tmeasured\\t\\nat\\tthe\\tstarting\\tpoint\\t\\nθ', 'θ\\n,\\tand\\t\\n2\\n\\trepresents\\tthe\\ngradient\\tat\\tthe\\tpoint\\tlocated\\tat\\t\\nθ\\n\\t+\\t\\nβ\\nm\\n).\\tAs\\tyou\\tcan\\tsee,\\tthe\\tNesterov\\tupdate\\tends\\tup\\tslightly\\tcloser\\tto\\tthe\\noptimum.\\tAfter\\ta\\twhile,\\tthese\\tsmall\\timprovements\\tadd\\tup\\tand\\tNAG\\tends\\tup\\tbeing\\tsignificantly\\tfaster\\tthan\\nregular\\tMomentum\\toptimization.\\tMoreover,\\tnote\\tthat\\twhen\\tthe\\tmomentum\\tpushes\\tthe\\tweights\\tacross\\ta\\nvalley,\\t\\n1\\n\\tcontinues\\tto\\tpush\\tfurther\\tacross\\tthe\\tvalley,\\twhile\\t\\n2\\n\\tpushes\\tback\\ttoward\\tthe\\tbottom\\tof\\tthe\\nvalley.\\tThis\\thelps\\treduce\\toscillations\\tand\\tthus\\tconverges\\tfaster.\\nNAG\\twill\\talmost\\talways\\tspeed\\tup\\ttraining\\tcompared\\tto\\t\\nregular\\tMomentum\\toptimization.\\tTo\\tuse\\tit,\\nsimply\\t\\nset\\t\\nuse_nesterov=True\\n\\twhen\\tcreating\\tthe\\t\\nMomentumOptimizer\\n:\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nMomentumOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmomentum\\n=\\n0.9\\n,\\n\\t\\nuse_nesterov\\n=\\nTrue\\n)', 'Figure\\t11-6.\\t\\nRegular\\tversus\\tNesterov\\tMomentum\\toptimization', 'AdaGrad\\nConsider\\t\\nthe\\telongated\\tbowl\\tproblem\\tagain:\\t\\nGradient\\tDescent\\tstarts\\tby\\tquickly\\tgoing\\tdown\\tthe\\tsteepest\\nslope,\\tthen\\tslowly\\tgoes\\tdown\\tthe\\tbottom\\tof\\tthe\\tvalley.\\tIt\\twould\\tbe\\tnice\\tif\\tthe\\talgorithm\\tcould\\tdetect\\tthis\\nearly\\ton\\tand\\tcorrect\\tits\\tdirection\\tto\\tpoint\\ta\\tbit\\tmore\\ttoward\\tthe\\tglobal\\toptimum.\\nThe\\t\\nAdaGrad\\n\\talgorithm\\n13\\n\\tachieves\\tthis\\tby\\tscaling\\tdown\\tthe\\tgradient\\tvector\\talong\\tthe\\tsteepest\\tdimensions\\n(see\\t\\nEquation\\t11-6\\n):\\nEquation\\t11-6.\\t\\nAdaGrad\\talgorithm\\nThe\\tfirst\\tstep\\taccumulates\\tthe\\tsquare\\tof\\tthe\\tgradients\\tinto\\tthe\\tvector\\t\\ns\\n\\t(the\\t\\n\\tsymbol\\trepresents\\tthe\\nelement-wise\\tmultiplication).\\tThis\\tvectorized\\tform\\tis\\tequivalent\\tto\\tcomputing\\t\\ns\\ni\\n\\t←\\t\\ns\\ni\\n\\t+\\t(∂\\t\\nJ\\n(\\nθ\\n)\\t/\\t∂\\t\\nθ\\ni\\n)\\n2\\nfor\\teach\\telement\\t\\ns\\ni\\n\\tof\\tthe\\tvector\\t\\ns\\n;\\tin\\tother\\twords,\\teach\\t\\ns\\ni\\n\\taccumulates\\tthe\\tsquares\\tof\\tthe\\tpartial\\nderivative\\tof\\tthe\\t\\ncost\\tfunction\\twith\\tregards\\tto\\tparameter\\t\\nθ\\ni\\n.\\tIf\\tthe\\tcost\\tfunction\\tis\\tsteep\\talong\\tthe\\ti\\nth\\ndimension,\\tthen\\t\\ns\\ni\\n\\twill\\tget\\tlarger\\tand\\tlarger\\tat\\teach\\titeration.', 'The\\tsecond\\tstep\\tis\\talmost\\tidentical\\tto\\tGradient\\tDescent,\\tbut\\twith\\tone\\tbig\\tdifference:\\tthe\\tgradient\\tvector\\nis\\tscaled\\tdown\\tby\\ta\\tfactor\\tof\\t\\n\\t(the\\t\\tsymbol\\trepresents\\tthe\\telement-wise\\tdivision,\\tand\\tϵ\\tis\\ta\\nsmoothing\\tterm\\tto\\tavoid\\tdivision\\tby\\tzero,\\ttypically\\tset\\tto\\t10\\n–10\\n).\\tThis\\tvectorized\\tform\\tis\\tequivalent\\tto\\ncomputing\\t\\n\\tfor\\tall\\tparameters\\t\\nθ\\ni\\n\\t(simultaneously).\\nIn\\tshort,\\tthis\\talgorithm\\tdecays\\tthe\\tlearning\\trate,\\tbut\\tit\\tdoes\\tso\\tfaster\\tfor\\tsteep\\tdimensions\\tthan\\tfor\\ndimensions\\twith\\tgentler\\tslopes.\\tThis\\tis\\tcalled\\tan\\t\\nadaptive\\tlearning\\trate\\n.\\t\\nIt\\thelps\\tpoint\\tthe\\tresulting\\nupdates\\tmore\\tdirectly\\ttoward\\tthe\\tglobal\\toptimum\\t(see\\t\\nFigure\\t11-7\\n).\\tOne\\tadditional\\tbenefit\\tis\\tthat\\tit\\nrequires\\tmuch\\tless\\ttuning\\tof\\tthe\\tlearning\\trate\\thyperparameter\\t\\nη\\n.', 'Figure\\t11-7.\\t\\nAdaGrad\\tversus\\tGradient\\tDescent\\nAdaGrad\\toften\\tperforms\\twell\\tfor\\tsimple\\tquadratic\\tproblems,\\tbut\\tunfortunately\\tit\\toften\\tstops\\ttoo\\tearly\\nwhen\\ttraining\\tneural\\tnetworks.\\tThe\\tlearning\\trate\\tgets\\tscaled\\tdown\\tso\\tmuch\\tthat\\tthe\\talgorithm\\tends\\tup\\nstopping\\tentirely\\tbefore\\treaching\\tthe\\tglobal\\toptimum.\\tSo\\teven\\tthough\\tTensorFlow\\thas\\tan\\nAdagradOptimizer\\n,\\tyou\\tshould\\tnot\\tuse\\tit\\tto\\ttrain\\tdeep\\tneural\\tnetworks\\t(it\\tmay\\tbe\\tefficient\\tfor\\tsimpler\\ntasks\\tsuch\\tas\\tLinear\\tRegression,\\t\\nthough).', 'RMSProp\\nAlthough\\t\\nAdaGrad\\tslows\\tdown\\ta\\tbit\\ttoo\\tfast\\tand\\tends\\tup\\tnever\\tconverging\\tto\\tthe\\tglobal\\toptimum,\\tthe\\nRMSProp\\n\\talgorithm\\n14\\n\\tfixes\\tthis\\tby\\taccumulating\\tonly\\tthe\\tgradients\\tfrom\\tthe\\tmost\\trecent\\titerations\\t(as\\nopposed\\tto\\tall\\tthe\\tgradients\\tsince\\tthe\\tbeginning\\tof\\ttraining).\\tIt\\tdoes\\tso\\tby\\tusing\\texponential\\tdecay\\tin\\tthe\\nfirst\\tstep\\t(see\\t\\nEquation\\t11-7\\n).\\nEquation\\t11-7.\\t\\nRMSProp\\talgorithm\\nThe\\tdecay\\trate\\t\\nβ\\n\\tis\\ttypically\\tset\\tto\\t0.9.\\tYes,\\tit\\tis\\tonce\\tagain\\ta\\tnew\\thyperparameter,\\tbut\\tthis\\tdefault\\tvalue\\noften\\tworks\\twell,\\tso\\tyou\\tmay\\tnot\\tneed\\tto\\ttune\\tit\\tat\\tall.\\nAs\\tyou\\tmight\\texpect,\\tTensorFlow\\thas\\t\\nan\\t\\nRMSPropOptimizer\\n\\tclass:\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nRMSPropOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmomentum\\n=\\n0.9\\n,\\n\\t\\ndecay\\n=\\n0.9\\n,\\n\\t\\nepsilon\\n=\\n1e-10\\n)\\nExcept\\ton\\tvery\\tsimple\\tproblems,\\tthis\\toptimizer\\talmost\\talways\\tperforms\\tmuch\\tbetter\\tthan\\tAdaGrad.\\tIt\\nalso\\tgenerally\\tconverges\\tfaster\\tthan\\tMomentum\\toptimization\\tand\\tNesterov\\tAccelerated\\tGradients.\\tIn\\tfact,', 'it\\twas\\tthe\\tpreferred\\toptimization\\talgorithm\\tof\\tmany\\tresearchers\\tuntil\\tAdam\\toptimization\\tcame\\taround.', 'Adam\\tOptimization\\nAdam\\n,\\n15\\n\\twhich\\t\\nstands\\tfor\\t\\nadaptive\\tmoment\\testimation\\n,\\t\\ncombines\\tthe\\tideas\\tof\\tMomentum\\toptimization\\nand\\tRMSProp:\\tjust\\tlike\\tMomentum\\toptimization\\tit\\tkeeps\\ttrack\\tof\\tan\\texponentially\\tdecaying\\taverage\\tof\\npast\\tgradients,\\tand\\tjust\\tlike\\tRMSProp\\tit\\tkeeps\\ttrack\\tof\\tan\\texponentially\\tdecaying\\taverage\\tof\\tpast\\tsquared\\ngradients\\t(see\\t\\nEquation\\t11-8\\n).\\n16\\nEquation\\t11-8.\\t\\nAdam\\talgorithm\\nT\\n\\trepresents\\tthe\\titeration\\tnumber\\t(starting\\tat\\t1).\\nIf\\tyou\\tjust\\tlook\\tat\\tsteps\\t1,\\t2,\\tand\\t5,\\tyou\\twill\\tnotice\\tAdam’s\\tclose\\tsimilarity\\tto\\tboth\\tMomentum\\noptimization\\tand\\tRMSProp.\\tThe\\tonly\\tdifference\\tis\\tthat\\tstep\\t1\\tcomputes\\tan\\texponentially\\tdecaying\\naverage\\trather\\tthan\\tan\\texponentially\\tdecaying\\tsum,\\tbut\\tthese\\tare\\tactually\\tequivalent\\texcept\\tfor\\ta\\tconstant\\nfactor\\t(the\\tdecaying\\taverage\\tis\\tjust\\t1\\t–\\t\\nβ\\n1\\n\\ttimes\\tthe\\tdecaying\\tsum).\\tSteps\\t3\\tand\\t4\\tare\\tsomewhat\\tof\\ta\\ntechnical\\tdetail:\\tsince\\t\\nm\\n\\tand\\t\\ns\\n\\tare\\tinitialized\\tat\\t0,\\tthey\\twill\\tbe\\tbiased\\ttoward\\t0\\tat\\tthe\\tbeginning\\tof', 'training,\\tso\\tthese\\ttwo\\tsteps\\twill\\thelp\\tboost\\t\\nm\\n\\tand\\t\\ns\\n\\tat\\tthe\\tbeginning\\tof\\ttraining.\\nThe\\tmomentum\\tdecay\\thyperparameter\\t\\nβ\\n1\\n\\tis\\ttypically\\tinitialized\\tto\\t0.9,\\twhile\\tthe\\tscaling\\tdecay\\nhyperparameter\\t\\nβ\\n2\\n\\tis\\toften\\tinitialized\\tto\\t0.999.\\tAs\\tearlier,\\tthe\\t\\nsmoothing\\tterm\\t\\nϵ\\n\\tis\\tusually\\tinitialized\\tto\\ta\\ntiny\\tnumber\\tsuch\\tas\\t10\\n–8\\n.\\tThese\\tare\\tthe\\tdefault\\tvalues\\tfor\\tTensorFlow’s\\t\\nAdamOptimizer\\n\\tclass,\\t\\nso\\tyou\\ncan\\tsimply\\tuse:\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n)\\nIn\\tfact,\\tsince\\tAdam\\tis\\tan\\tadaptive\\tlearning\\trate\\talgorithm\\t(like\\tAdaGrad\\tand\\tRMSProp),\\tit\\trequires\\tless\\ntuning\\tof\\tthe\\tlearning\\trate\\thyperparameter\\t\\nη\\n.\\tYou\\tcan\\toften\\tuse\\tthe\\tdefault\\tvalue\\t\\nη\\n\\t=\\t0.001,\\tmaking\\tAdam\\neven\\teasier\\tto\\tuse\\tthan\\tGradient\\tDescent.', 'WARNING\\nThis\\tbook\\tinitially\\trecommended\\tusing\\t\\nAdam\\toptimization,\\tbecause\\tit\\twas\\tgenerally\\tconsidered\\tfaster\\tand\\tbetter\\tthan\\tother\\nmethods.\\tHowever,\\ta\\t\\n2017\\tpaper\\n17\\n\\tby\\tAshia\\tC.\\tWilson\\tet\\tal.\\tshowed\\tthat\\tadaptive\\toptimization\\tmethods\\t(i.e.,\\tAdaGrad,\\nRMSProp\\tand\\tAdam\\toptimization)\\tcan\\tlead\\tto\\tsolutions\\tthat\\tgeneralize\\tpoorly\\ton\\tsome\\tdatasets.\\tSo\\tyou\\tmay\\twant\\tto\\tstick\\tto\\nMomentum\\toptimization\\tor\\tNesterov\\tAccelerated\\tGradient\\tfor\\tnow,\\tuntil\\tresearchers\\thave\\ta\\tbetter\\tunderstanding\\tof\\tthis\\tissue.\\nAll\\tthe\\toptimization\\ttechniques\\tdiscussed\\tso\\tfar\\tonly\\trely\\ton\\tthe\\t\\nfirst-order\\tpartial\\tderivatives\\n(\\nJacobians\\n).\\tThe\\t\\noptimization\\tliterature\\tcontains\\tamazing\\talgorithms\\tbased\\ton\\tthe\\t\\nsecond-order\\tpartial\\nderivatives\\n\\t(the\\t\\nHessians\\n).\\t\\nUnfortunately,\\tthese\\talgorithms\\tare\\tvery\\thard\\tto\\tapply\\tto\\tdeep\\tneural\\tnetworks\\nbecause\\tthere\\tare\\t\\nn\\n2\\n\\tHessians\\tper\\toutput\\t(where\\t\\nn\\n\\tis\\tthe\\tnumber\\tof\\tparameters),\\tas\\topposed\\tto\\tjust\\t\\nn', 'n\\nJacobians\\tper\\toutput.\\tSince\\tDNNs\\ttypically\\thave\\ttens\\tof\\tthousands\\tof\\tparameters,\\tthe\\tsecond-order\\noptimization\\talgorithms\\toften\\tdon’t\\teven\\tfit\\tin\\tmemory,\\tand\\teven\\twhen\\tthey\\tdo,\\tcomputing\\tthe\\tHessians\\tis\\njust\\ttoo\\tslow.\\nTRAINING\\tSPARSE\\tMODELS\\nAll\\tthe\\t\\noptimization\\talgorithms\\tjust\\tpresented\\tproduce\\tdense\\tmodels,\\tmeaning\\tthat\\tmost\\tparameters\\twill\\tbe\\tnonzero.\\tIf\\tyou\\tneed\\ta\\nblazingly\\tfast\\tmodel\\tat\\truntime,\\tor\\tif\\tyou\\tneed\\tit\\tto\\ttake\\tup\\tless\\tmemory,\\tyou\\tmay\\tprefer\\tto\\tend\\tup\\twith\\ta\\tsparse\\tmodel\\tinstead.\\nOne\\ttrivial\\tway\\tto\\tachieve\\tthis\\tis\\tto\\ttrain\\tthe\\tmodel\\tas\\tusual,\\tthen\\tget\\trid\\tof\\tthe\\ttiny\\tweights\\t(set\\tthem\\tto\\t0).\\nAnother\\toption\\tis\\tto\\tapply\\tstrong\\tℓ\\n1\\n\\t\\nregularization\\tduring\\ttraining,\\tas\\tit\\tpushes\\tthe\\toptimizer\\tto\\tzero\\tout\\tas\\tmany\\tweights\\tas\\tit\\tcan\\t(as\\ndiscussed\\tin\\t\\nChapter\\t4\\n\\tabout\\tLasso\\tRegression).\\nHowever,\\tin\\tsome\\tcases\\tthese\\ttechniques\\tmay\\tremain\\tinsufficient.\\tOne\\tlast\\toption\\tis\\tto\\tapply\\t\\nDual\\tAveraging\\n,\\t\\noften\\tcalled\\t\\nFollow\\tThe\\nRegularized\\tLeader\\n\\t(FTRL),\\ta', 'technique\\tproposed\\tby\\tYurii\\tNesterov\\n.\\n18\\n\\tWhen\\tused\\twith\\tℓ\\n1\\n\\tregularization,\\tthis\\ttechnique\\toften\\tleads\\tto\\nvery\\tsparse\\tmodels.\\tTensorFlow\\timplements\\ta\\tvariant\\tof\\tFTRL\\tcalled\\t\\nFTRL-Proximal\\n19\\n\\tin\\tthe\\t\\nFTRLOptimizer\\n\\tclass.', 'Learning\\tRate\\tScheduling\\nFinding\\t\\na\\tgood\\tlearning\\trate\\tcan\\tbe\\ttricky.\\tIf\\tyou\\tset\\tit\\tway\\ttoo\\thigh,\\ttraining\\tmay\\tactually\\tdiverge\\t(as\\twe\\ndiscussed\\tin\\t\\nChapter\\t4\\n).\\tIf\\tyou\\tset\\tit\\ttoo\\tlow,\\ttraining\\twill\\teventually\\tconverge\\tto\\tthe\\toptimum,\\tbut\\tit\\twill\\ntake\\ta\\tvery\\tlong\\ttime.\\tIf\\tyou\\tset\\tit\\tslightly\\ttoo\\thigh,\\tit\\twill\\tmake\\tprogress\\tvery\\tquickly\\tat\\tfirst,\\tbut\\tit\\twill\\nend\\tup\\tdancing\\taround\\tthe\\toptimum,\\tnever\\tsettling\\tdown\\t(unless\\tyou\\tuse\\tan\\tadaptive\\tlearning\\trate\\noptimization\\talgorithm\\tsuch\\tas\\tAdaGrad,\\tRMSProp,\\tor\\tAdam,\\tbut\\teven\\tthen\\tit\\tmay\\ttake\\ttime\\tto\\tsettle).\\tIf\\nyou\\thave\\ta\\tlimited\\tcomputing\\tbudget,\\tyou\\tmay\\thave\\tto\\tinterrupt\\ttraining\\tbefore\\tit\\thas\\tconverged\\tproperly,\\nyielding\\ta\\tsuboptimal\\tsolution\\t(see\\t\\nFigure\\t11-8\\n).\\nFigure\\t11-8.\\t\\nLearning\\tcurves\\tfor\\tvarious\\tlearning\\trates\\tη\\nYou\\tmay\\tbe\\table\\tto\\tfind\\ta\\tfairly\\tgood\\tlearning\\trate\\tby\\ttraining\\tyour\\tnetwork\\tseveral\\ttimes\\tduring\\tjust\\ta\\nfew\\tepochs\\tusing\\tvarious\\tlearning\\trates\\tand\\tcomparing\\tthe\\tlearning\\tcurves.\\tThe\\tideal\\tlearning\\trate\\twill', 'learn\\tquickly\\tand\\tconverge\\tto\\tgood\\tsolution.\\nHowever,\\tyou\\tcan\\tdo\\tbetter\\tthan\\ta\\tconstant\\tlearning\\trate:\\tif\\tyou\\tstart\\twith\\ta\\thigh\\tlearning\\trate\\tand\\tthen\\nreduce\\tit\\tonce\\tit\\tstops\\tmaking\\tfast\\tprogress,\\tyou\\tcan\\treach\\ta\\tgood\\tsolution\\tfaster\\tthan\\twith\\tthe\\toptimal\\nconstant\\tlearning\\trate.\\tThere\\tare\\tmany\\tdifferent\\tstrategies\\tto\\treduce\\tthe\\tlearning\\trate\\tduring\\ttraining.\\nThese\\tstrategies\\tare\\tcalled\\t\\nlearning\\tschedules\\n\\t(we\\tbriefly\\tintroduced\\tthis\\tconcept\\tin\\t\\nChapter\\t4\\n),\\tthe\\tmost\\ncommon\\tof\\twhich\\tare:\\nPredetermined\\tpiecewise\\tconstant\\tlearning\\trate\\nFor\\texample,\\tset\\tthe\\tlearning\\trate\\tto\\t\\nη\\n0\\n\\t=\\t0.1\\tat\\tfirst,\\tthen\\tto\\t\\nη\\n1\\n\\t=\\t0.001\\tafter\\t50\\tepochs.\\tAlthough\\nthis\\tsolution\\tcan\\twork\\tvery\\twell,\\tit\\toften\\trequires\\tfiddling\\taround\\tto\\tfigure\\tout\\tthe\\tright\\tlearning\\nrates\\tand\\twhen\\tto\\tuse\\tthem.\\nPerformance\\tscheduling\\nMeasure\\tthe\\tvalidation\\terror\\tevery\\t\\nN\\n\\tsteps\\t(just\\tlike\\tfor\\tearly\\tstopping)\\tand\\treduce\\tthe\\tlearning\\trate\\nby\\ta\\tfactor\\tof\\t\\nλ\\n\\twhen\\tthe\\terror\\tstops\\tdropping.\\nExponential\\tscheduling', 'Set\\tthe\\tlearning\\trate\\tto\\ta\\tfunction\\tof\\tthe\\titeration\\tnumber\\t\\nt\\n:\\t\\nη\\n(\\nt\\n)\\t=\\t\\nη\\n0\\n\\t10\\n–t/r\\n.\\tThis\\tworks\\tgreat,\\tbut\\tit\\nrequires\\ttuning\\t\\nη\\n0\\n\\tand\\t\\nr\\n.\\tThe\\tlearning\\trate\\twill\\tdrop\\tby\\ta\\tfactor\\tof\\t10\\tevery\\t\\nr\\n\\tsteps.\\nPower\\tscheduling\\nSet\\tthe\\tlearning\\trate\\tto\\t\\nη\\n(\\nt\\n)\\t=\\t\\nη\\n0\\n\\t(1\\t+\\t\\nt\\n/\\nr\\n)\\n–c\\n.\\tThe\\thyperparameter\\t\\nc\\n\\tis\\ttypically\\tset\\tto\\t1.\\tThis\\tis\\nsimilar\\tto\\texponential\\tscheduling,\\tbut\\tthe\\tlearning\\trate\\tdrops\\tmuch\\tmore\\tslowly.\\nA\\t\\n2013\\tpaper\\n20\\n\\tby\\tAndrew\\tSenior\\tet\\tal.\\tcompared\\tthe\\tperformance\\tof\\tsome\\tof\\tthe\\tmost\\tpopular\\tlearning\\nschedules\\twhen\\ttraining\\tdeep\\tneural\\tnetworks\\tfor\\tspeech\\trecognition\\tusing\\tMomentum\\toptimization.\\tThe\\nauthors\\tconcluded\\tthat,\\tin\\tthis\\tsetting,\\tboth\\tperformance\\tscheduling\\tand\\texponential\\tscheduling\\tperformed\\nwell,\\tbut\\tthey\\tfavored\\texponential\\tscheduling\\tbecause\\tit\\tis\\tsimpler\\tto\\timplement,\\tis\\teasy\\tto\\ttune,\\tand\\nconverged\\tslightly\\tfaster\\tto\\tthe\\toptimal\\tsolution.\\nImplementing\\ta\\t\\nlearning\\tschedule\\twith\\tTensorFlow\\tis\\tfairly\\tstraightforward:\\ninitial_learning_rate\\n\\t\\n=\\n\\t\\n0.1', '=\\n\\t\\n0.1\\ndecay_steps\\n\\t\\n=\\n\\t\\n10000\\ndecay_rate\\n\\t\\n=\\n\\t\\n1\\n/\\n10\\nglobal_step\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0\\n,\\n\\t\\ntrainable\\n=\\nFalse\\n,\\n\\t\\nname\\n=\\n\"global_step\"\\n)\\nlearning_rate\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nexponential_decay\\n(\\ninitial_learning_rate\\n,\\n\\t\\nglobal_step\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ndecay_steps\\n,\\n\\t\\ndecay_rate\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nMomentumOptimizer\\n(\\nlearning_rate\\n,\\n\\t\\nmomentum\\n=\\n0.9\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n,\\n\\t\\nglobal_step\\n=\\nglobal_step\\n)\\nAfter\\tsetting\\tthe\\thyperparameter\\tvalues,\\twe\\tcreate\\ta\\tnontrainable\\tvariable\\t\\nglobal_step\\n\\t(initialized\\tto\\t0)\\nto\\tkeep\\ttrack\\tof\\tthe\\tcurrent\\ttraining\\t\\niteration\\tnumber.\\tThen\\twe\\tdefine\\tan\\texponentially\\tdecaying\\tlearning\\nrate\\t(with\\t\\nη\\n0\\n\\t=\\t0.1\\tand\\t\\nr\\n\\t=\\t10,000)\\tusing\\tTensorFlow’s\\t\\nexponential_decay()\\n\\tfunction.\\tNext,\\twe\\tcreate\\nan\\toptimizer\\t(in\\tthis\\texample,\\ta\\t\\nMomentumOptimizer\\n)\\tusing\\tthis\\tdecaying\\tlearning\\trate.\\tFinally,\\twe\\ncreate\\tthe\\ttraining\\toperation\\tby\\tcalling\\tthe\\toptimizer’s\\t\\nminimize()\\n\\tmethod;\\tsince\\twe\\tpass\\tit\\tthe', 'global_step\\n\\tvariable,\\tit\\twill\\tkindly\\ttake\\tcare\\tof\\t\\nincrementing\\tit.\\tThat’s\\tit!\\nSince\\tAdaGrad,\\tRMSProp,\\tand\\tAdam\\toptimization\\tautomatically\\treduce\\tthe\\tlearning\\trate\\tduring\\ttraining,\\nit\\tis\\tnot\\tnecessary\\tto\\tadd\\tan\\textra\\tlearning\\tschedule.\\tFor\\tother\\toptimization\\talgorithms,\\tusing\\texponential\\ndecay\\tor\\tperformance\\tscheduling\\tcan\\tconsiderably\\tspeed\\t\\nup\\tconvergence.', 'Avoiding\\tOverfitting\\tThrough\\tRegularization\\nWith\\tfour\\tparameters\\tI\\tcan\\tfit\\tan\\telephant\\tand\\twith\\tfive\\tI\\tcan\\tmake\\thim\\twiggle\\this\\ttrunk.\\nJohn\\tvon\\tNeumann,\\t\\ncited\\tby\\tEnrico\\tFermi\\tin\\tNature\\t427\\nDeep\\tneural\\tnetworks\\t\\ntypically\\thave\\ttens\\tof\\tthousands\\tof\\tparameters,\\tsometimes\\teven\\tmillions.\\tWith\\tso\\nmany\\tparameters,\\tthe\\tnetwork\\thas\\tan\\tincredible\\tamount\\tof\\tfreedom\\tand\\tcan\\tfit\\ta\\thuge\\tvariety\\tof\\tcomplex\\ndatasets.\\tBut\\tthis\\tgreat\\tflexibility\\talso\\tmeans\\tthat\\tit\\tis\\tprone\\tto\\toverfitting\\tthe\\ttraining\\tset.\\nWith\\tmillions\\tof\\tparameters\\tyou\\tcan\\tfit\\tthe\\twhole\\tzoo.\\tIn\\tthis\\tsection\\twe\\twill\\tpresent\\tsome\\tof\\tthe\\tmost\\npopular\\tregularization\\ttechniques\\tfor\\tneural\\tnetworks,\\tand\\thow\\tto\\timplement\\tthem\\twith\\tTensorFlow:\\nearly\\tstopping,\\tℓ\\n1\\n\\tand\\tℓ\\n2\\n\\t\\nregularization,\\tdropout,\\tmax-norm\\tregularization,\\tand\\tdata\\taugmentation.', 'Early\\tStopping\\nTo\\t\\navoid\\toverfitting\\tthe\\ttraining\\tset,\\ta\\tgreat\\tsolution\\tis\\tearly\\tstopping\\t(introduced\\tin\\t\\nChapter\\t4\\n):\\tjust\\ninterrupt\\ttraining\\twhen\\tits\\tperformance\\ton\\tthe\\tvalidation\\tset\\tstarts\\tdropping.\\nOne\\tway\\tto\\timplement\\tthis\\twith\\tTensorFlow\\tis\\tto\\tevaluate\\tthe\\tmodel\\ton\\ta\\tvalidation\\tset\\tat\\tregular\\nintervals\\t(e.g.,\\tevery\\t50\\tsteps),\\tand\\tsave\\ta\\t“winner”\\tsnapshot\\tif\\tit\\toutperforms\\tprevious\\t“winner”\\nsnapshots.\\tCount\\tthe\\tnumber\\tof\\tsteps\\tsince\\tthe\\tlast\\t“winner”\\tsnapshot\\twas\\tsaved,\\tand\\tinterrupt\\ttraining\\nwhen\\tthis\\tnumber\\treaches\\tsome\\tlimit\\t(e.g.,\\t2,000\\tsteps).\\tThen\\trestore\\tthe\\tlast\\t“winner”\\tsnapshot.\\nAlthough\\tearly\\tstopping\\tworks\\tvery\\twell\\tin\\tpractice,\\tyou\\tcan\\tusually\\tget\\tmuch\\thigher\\tperformance\\tout\\tof\\nyour\\tnetwork\\tby\\tcombining\\tit\\twith\\tother\\tregularization\\ttechniques.', 'ℓ\\n1\\n\\tand\\tℓ\\n2\\n\\tRegularization\\nJust\\t\\nlike\\tyou\\tdid\\tin\\t\\nChapter\\t4\\n\\tfor\\tsimple\\tlinear\\tmodels,\\tyou\\tcan\\tuse\\tℓ\\n1\\n\\tand\\tℓ\\n2\\n\\tregularization\\tto\\tconstrain\\ta\\nneural\\tnetwork’s\\tconnection\\tweights\\t(but\\ttypically\\tnot\\tits\\tbiases).\\nOne\\tway\\tto\\tdo\\tthis\\tusing\\tTensorFlow\\t\\nis\\tto\\tsimply\\tadd\\tthe\\tappropriate\\tregularization\\tterms\\tto\\tyour\\tcost\\nfunction.\\tFor\\texample,\\tassuming\\tyou\\thave\\tjust\\tone\\thidden\\tlayer\\twith\\tweights\\t\\nW1\\n\\tand\\tone\\toutput\\tlayer\\twith\\nweights\\t\\nW2\\n,\\tthen\\tyou\\tcan\\tapply\\tℓ\\n1\\n\\tregularization\\t\\nlike\\tthis:\\n[\\n...\\n]\\n\\t\\n#\\tconstruct\\tthe\\tneural\\tnetwork\\nW1\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"hidden1/kernel:0\"\\n)\\nW2\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"outputs/kernel:0\"\\n)\\nscale\\n\\t\\n=\\n\\t\\n0.001\\n\\t\\n#\\tl1\\tregularization\\thyperparameter\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"loss\"\\n):\\n\\t\\t\\t\\t\\nxentropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsparse_softmax_cross_entropy_with_logits\\n(\\nlabels\\n=\\ny\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlogits\\n=\\nlogits\\n)\\n\\t\\t\\t\\t\\nbase_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\nxentropy\\n,', ',\\n\\t\\nname\\n=\\n\"avg_xentropy\"\\n)\\n\\t\\t\\t\\t\\nreg_losses\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\ntf\\n.\\nabs\\n(\\nW1\\n))\\n\\t\\n+\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\ntf\\n.\\nabs\\n(\\nW2\\n))\\n\\t\\t\\t\\t\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd\\n(\\nbase_loss\\n,\\n\\t\\nscale\\n\\t\\n*\\n\\t\\nreg_losses\\n,\\n\\t\\nname\\n=\\n\"loss\"\\n)\\nHowever,\\tif\\tthere\\tare\\tmany\\tlayers,\\tthis\\tapproach\\tis\\tnot\\tvery\\tconvenient.\\tFortunately,\\tTensorFlow\\nprovides\\ta\\tbetter\\toption.\\tMany\\tfunctions\\tthat\\tcreate\\tvariables\\t(such\\tas\\t\\nget_variable()\\n\\tor\\ntf.layers.dense()\\n)\\taccept\\ta\\t\\n*_regularizer\\n\\targument\\tfor\\teach\\tcreated\\tvariable\\t(e.g.,\\nkernel_regularizer\\n).\\tYou\\tcan\\tpass\\tany\\tfunction\\tthat\\ttakes\\tweights\\tas\\tan\\targument\\tand\\treturns\\tthe\\ncorresponding\\tregularization\\tloss.\\tThe\\t\\nl1_regularizer()\\n,\\t\\nl2_regularizer()\\n,\\tand\\nl1_l2_regularizer()\\n\\tfunctions\\treturn\\t\\nsuch\\tfunctions.\\tThe\\tfollowing\\tcode\\tputs\\tall\\tthis\\ttogether:\\nmy_dense_layer\\n\\t\\n=\\n\\t\\npartial\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\nkernel_regularizer\\n=\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nl1_regularizer\\n(\\nscale\\n))\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):\\n\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\nmy_dense_layer', '(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n)\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden2\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nactivation\\n=\\nNone\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"outputs\"\\n)\\nThis\\tcode\\tcreates\\ta\\tneural\\tnetwork\\twith\\ttwo\\thidden\\tlayers\\tand\\tone\\toutput\\tlayer,\\tand\\tit\\talso\\tcreates\\tnodes\\nin\\tthe\\tgraph\\tto\\tcompute\\tthe\\tℓ\\n1\\n\\tregularization\\tloss\\tcorresponding\\tto\\teach\\tlayer’s\\tweights.\\tTensorFlow\\nautomatically\\tadds\\tthese\\tnodes\\tto\\ta\\tspecial\\tcollection\\tcontaining\\tall\\tthe\\tregularization\\t\\nlosses.\\tYou\\tjust\\nneed\\tto\\tadd\\tthese\\tregularization\\tlosses\\tto\\tyour\\toverall\\tloss,\\tlike\\tthis:\\nreg_losses\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nREGULARIZATION_LOSSES\\n)\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n([\\nbase_loss\\n]\\n\\t\\n+\\n\\t\\nreg_losses\\n,\\n\\t\\nname\\n=\\n\"loss\"\\n)\\nWARNING\\nDon’t\\tforget\\tto\\tadd\\tthe\\tregularization\\tlosses\\tto\\tyour\\toverall\\tloss,\\tor\\telse\\tthey\\twill\\tsimply\\tbe\\t\\nignored.', 'Dropout\\nThe\\t\\nmost\\tpopular\\tregularization\\ttechnique\\tfor\\tdeep\\tneural\\tnetworks\\tis\\targuably\\t\\ndropout\\n.\\tIt\\twas\\nproposed\\n21\\n\\tby\\tG.\\tE.\\tHinton\\tin\\t2012\\tand\\tfurther\\tdetailed\\tin\\ta\\t\\npaper\\n22\\n\\tby\\tNitish\\tSrivastava\\tet\\tal.,\\tand\\tit\\thas\\nproven\\tto\\tbe\\thighly\\tsuccessful:\\teven\\tthe\\tstate-of-the-art\\tneural\\tnetworks\\tgot\\ta\\t1–2%\\taccuracy\\tboost\\nsimply\\tby\\tadding\\tdropout.\\tThis\\tmay\\tnot\\tsound\\tlike\\ta\\tlot,\\tbut\\twhen\\ta\\tmodel\\talready\\thas\\t95%\\taccuracy,\\ngetting\\ta\\t2%\\taccuracy\\tboost\\tmeans\\tdropping\\tthe\\terror\\trate\\tby\\talmost\\t40%\\t(going\\tfrom\\t5%\\terror\\tto\\nroughly\\t3%).\\nIt\\tis\\ta\\tfairly\\tsimple\\talgorithm:\\tat\\tevery\\ttraining\\tstep,\\tevery\\tneuron\\t(including\\tthe\\tinput\\tneurons\\tbut\\nexcluding\\tthe\\toutput\\tneurons)\\thas\\ta\\tprobability\\t\\np\\n\\tof\\tbeing\\ttemporarily\\t“dropped\\tout,”\\tmeaning\\tit\\twill\\tbe\\nentirely\\tignored\\tduring\\tthis\\ttraining\\tstep,\\tbut\\tit\\tmay\\tbe\\tactive\\tduring\\tthe\\tnext\\tstep\\t(see\\t\\nFigure\\t11-9\\n).\\tThe\\nhyperparameter\\t\\np\\n\\tis\\tcalled\\tthe\\t\\ndropout\\trate\\n,\\t\\nand\\tit\\tis\\ttypically\\tset\\tto\\t50%.\\tAfter\\ttraining,\\tneurons\\tdon’t', 'get\\tdropped\\tanymore.\\tAnd\\tthat’s\\tall\\t(except\\tfor\\ta\\ttechnical\\tdetail\\twe\\twill\\tdiscuss\\tmomentarily).\\nFigure\\t11-9.\\t\\nDropout\\tregularization\\nIt\\tis\\tquite\\tsurprising\\tat\\tfirst\\tthat\\tthis\\trather\\tbrutal\\ttechnique\\tworks\\tat\\tall.\\tWould\\ta\\tcompany\\tperform\\tbetter\\nif\\tits\\temployees\\twere\\ttold\\tto\\ttoss\\ta\\tcoin\\tevery\\tmorning\\tto\\tdecide\\twhether\\tor\\tnot\\tto\\tgo\\tto\\twork?\\tWell,\\twho\\nknows;\\tperhaps\\tit\\twould!\\tThe\\tcompany\\twould\\tobviously\\tbe\\tforced\\tto\\tadapt\\tits\\torganization;\\tit\\tcould\\tnot\\nrely\\ton\\tany\\tsingle\\tperson\\tto\\tfill\\tin\\tthe\\tcoffee\\tmachine\\tor\\tperform\\tany\\tother\\tcritical\\ttasks,\\tso\\tthis\\texpertise\\nwould\\thave\\tto\\tbe\\tspread\\tacross\\tseveral\\tpeople.\\tEmployees\\twould\\thave\\tto\\tlearn\\tto\\tcooperate\\twith\\tmany\\nof\\ttheir\\tcoworkers,\\tnot\\tjust\\ta\\thandful\\tof\\tthem.\\tThe\\tcompany\\twould\\tbecome\\tmuch\\tmore\\tresilient.\\tIf\\tone\\nperson\\tquit,\\tit\\twouldn’t\\tmake\\tmuch\\tof\\ta\\tdifference.\\tIt’s\\tunclear\\twhether\\tthis\\tidea\\twould\\tactually\\twork\\tfor\\ncompanies,\\tbut\\tit\\tcertainly\\tdoes\\tfor\\tneural\\tnetworks.\\tNeurons\\ttrained\\twith\\tdropout\\tcannot\\tco-adapt\\twith', 'their\\tneighboring\\tneurons;\\tthey\\thave\\tto\\tbe\\tas\\tuseful\\tas\\tpossible\\ton\\ttheir\\town.\\tThey\\talso\\tcannot\\trely', 'excessively\\ton\\tjust\\ta\\tfew\\tinput\\tneurons;\\tthey\\tmust\\tpay\\tattention\\tto\\teach\\tof\\ttheir\\tinput\\tneurons.\\tThey\\tend\\tup\\nbeing\\tless\\tsensitive\\tto\\tslight\\tchanges\\tin\\tthe\\tinputs.\\tIn\\tthe\\tend\\tyou\\tget\\ta\\tmore\\trobust\\tnetwork\\tthat\\ngeneralizes\\tbetter.\\nAnother\\tway\\tto\\tunderstand\\tthe\\tpower\\tof\\tdropout\\tis\\tto\\trealize\\tthat\\ta\\tunique\\tneural\\tnetwork\\tis\\tgenerated\\tat\\neach\\ttraining\\tstep.\\tSince\\teach\\tneuron\\tcan\\tbe\\teither\\tpresent\\tor\\tabsent,\\tthere\\tis\\ta\\ttotal\\tof\\t2\\nN\\n\\tpossible\\nnetworks\\t(where\\t\\nN\\n\\tis\\tthe\\ttotal\\tnumber\\tof\\tdroppable\\tneurons).\\tThis\\tis\\tsuch\\ta\\thuge\\tnumber\\tthat\\tit\\tis\\nvirtually\\timpossible\\tfor\\tthe\\tsame\\tneural\\tnetwork\\tto\\tbe\\tsampled\\ttwice.\\tOnce\\tyou\\thave\\trun\\ta\\t10,000\\ntraining\\tsteps,\\tyou\\thave\\tessentially\\ttrained\\t10,000\\tdifferent\\tneural\\tnetworks\\t(each\\twith\\tjust\\tone\\ttraining\\ninstance).\\tThese\\tneural\\tnetworks\\tare\\tobviously\\tnot\\tindependent\\tsince\\tthey\\tshare\\tmany\\tof\\ttheir\\tweights,\\nbut\\tthey\\tare\\tnevertheless\\tall\\tdifferent.\\tThe\\tresulting\\tneural\\tnetwork\\tcan\\tbe\\tseen\\tas\\tan\\taveraging\\tensemble\\nof\\tall\\tthese\\tsmaller\\tneural\\tnetworks.', 'There\\tis\\tone\\tsmall\\tbut\\timportant\\ttechnical\\tdetail.\\tSuppose\\t\\np\\n\\t=\\t50%,\\tin\\twhich\\tcase\\tduring\\ttesting\\ta\\tneuron\\nwill\\tbe\\tconnected\\tto\\ttwice\\tas\\tmany\\tinput\\tneurons\\tas\\tit\\twas\\t(on\\taverage)\\tduring\\ttraining.\\tTo\\tcompensate\\nfor\\tthis\\tfact,\\twe\\tneed\\tto\\tmultiply\\teach\\tneuron’s\\tinput\\tconnection\\tweights\\tby\\t0.5\\tafter\\ttraining.\\tIf\\twe\\tdon’t,\\neach\\tneuron\\twill\\tget\\ta\\ttotal\\tinput\\tsignal\\troughly\\ttwice\\tas\\tlarge\\tas\\twhat\\tthe\\tnetwork\\twas\\ttrained\\ton,\\tand\\tit\\nis\\tunlikely\\tto\\tperform\\twell.\\tMore\\tgenerally,\\twe\\tneed\\tto\\tmultiply\\teach\\tinput\\tconnection\\tweight\\tby\\t\\nthe\\t\\nkeep\\nprobability\\n\\t(1\\t–\\t\\np\\n)\\tafter\\ttraining.\\tAlternatively,\\twe\\tcan\\tdivide\\teach\\tneuron’s\\toutput\\tby\\tthe\\tkeep\\nprobability\\tduring\\ttraining\\t(these\\talternatives\\tare\\tnot\\tperfectly\\tequivalent,\\tbut\\tthey\\twork\\tequally\\twell).\\nTo\\timplement\\tdropout\\tusing\\t\\nTensorFlow,\\tyou\\tcan\\tsimply\\tapply\\tthe\\t\\ntf.layers.dropout()\\n\\t\\nfunction\\tto\\tthe\\ninput\\tlayer\\tand/or\\tto\\tthe\\toutput\\tof\\tany\\thidden\\tlayer\\tyou\\twant.\\tDuring\\ttraining,\\tthis\\tfunction\\trandomly', 'drops\\tsome\\titems\\t(setting\\tthem\\tto\\t0)\\tand\\tdivides\\tthe\\tremaining\\titems\\tby\\tthe\\tkeep\\tprobability.\\tAfter\\ntraining,\\tthis\\tfunction\\tdoes\\tnothing\\tat\\tall.\\tThe\\tfollowing\\tcode\\tapplies\\tdropout\\tregularization\\tto\\tour\\tthree-\\nlayer\\t\\nneural\\tnetwork:\\n[\\n...\\n]\\ntraining\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder_with_default\\n(\\nFalse\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\nname\\n=\\n\\'training\\'\\n)\\ndropout_rate\\n\\t\\n=\\n\\t\\n0.5\\n\\t\\t\\n#\\t==\\t1\\t-\\tkeep_prob\\nX_drop\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndropout\\n(\\nX\\n,\\n\\t\\ndropout_rate\\n,\\n\\t\\ntraining\\n=\\ntraining\\n)\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):\\n\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX_drop\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden1\"\\n)\\n\\t\\t\\t\\t\\nhidden1_drop\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndropout\\n(\\nhidden1\\n,\\n\\t\\ndropout_rate\\n,\\n\\t\\ntraining\\n=\\ntraining\\n)\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden1_drop\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden2\"\\n)\\n\\t\\t\\t\\t\\nhidden2_drop\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndropout\\n(\\nhidden2\\n,\\n\\t\\ndropout_rate\\n,\\n\\t\\ntraining\\n=\\ntraining\\n)\\n\\t\\t\\t\\t\\nlogits', 'logits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden2_drop\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)\\nWARNING\\nYou\\twant\\tto\\tuse\\tthe\\t\\ntf.layers.dropout()\\n\\tfunction,\\tnot\\t\\ntf.nn.dropout()\\n.\\tThe\\tfirst\\tone\\tturns\\toff\\t(no-op)\\twhen\\tnot\\ttraining,\\nwhich\\tis\\twhat\\tyou\\twant,\\twhile\\tthe\\tsecond\\tone\\tdoes\\tnot.\\nOf\\tcourse,\\tjust\\tlike\\tyou\\tdid\\tearlier\\tfor\\tBatch\\tNormalization,\\tyou\\tneed\\tto\\tset\\t\\ntraining\\n\\tto\\t\\nTrue\\n\\twhen\\ntraining,\\tand\\tleave\\tthe\\tdefault\\t\\nFalse\\n\\tvalue\\twhen\\ttesting.\\nIf\\tyou\\tobserve\\tthat\\tthe\\tmodel\\tis\\toverfitting,\\tyou\\tcan\\tincrease\\tthe\\tdropout\\trate.\\tConversely,\\tyou\\tshould\\ttry', 'decreasing\\tthe\\tdropout\\trate\\tif\\tthe\\tmodel\\tunderfits\\tthe\\ttraining\\tset.\\tIt\\tcan\\talso\\thelp\\tto\\tincrease\\tthe\\tdropout\\nrate\\tfor\\tlarge\\tlayers,\\tand\\treduce\\tit\\tfor\\tsmall\\tones.\\nDropout\\tdoes\\ttend\\tto\\tsignificantly\\tslow\\tdown\\tconvergence,\\tbut\\tit\\tusually\\tresults\\tin\\ta\\tmuch\\tbetter\\tmodel\\nwhen\\ttuned\\tproperly.\\tSo,\\tit\\tis\\tgenerally\\twell\\tworth\\tthe\\textra\\ttime\\tand\\teffort.\\nNOTE\\nDropconnect\\n\\t\\nis\\ta\\tvariant\\tof\\tdropout\\twhere\\tindividual\\tconnections\\tare\\tdropped\\trandomly\\trather\\tthan\\twhole\\tneurons.\\tIn\\tgeneral\\ndropout\\tperforms\\t\\nbetter.', 'Max-Norm\\tRegularization\\nAnother\\t\\nregularization\\ttechnique\\tthat\\tis\\tquite\\tpopular\\tfor\\tneural\\tnetworks\\tis\\tcalled\\t\\nmax-norm\\nregularization\\n:\\tfor\\teach\\tneuron,\\tit\\tconstrains\\tthe\\tweights\\t\\nw\\n\\tof\\tthe\\tincoming\\tconnections\\tsuch\\tthat\\t\\t\\nw\\n\\t\\n2\\n\\t≤\\nr\\n,\\twhere\\t\\nr\\n\\tis\\tthe\\tmax-norm\\thyperparameter\\tand\\t\\n\\t·\\t\\n2\\n\\tis\\tthe\\n\\tℓ\\n2\\n\\tnorm.\\nWe\\ttypically\\timplement\\tthis\\tconstraint\\tby\\tcomputing\\t\\nw\\n2\\n\\tafter\\teach\\ttraining\\tstep\\tand\\tclipping\\t\\nw\\n\\tif\\nneeded\\t(\\n).\\nReducing\\t\\nr\\n\\tincreases\\tthe\\tamount\\tof\\tregularization\\tand\\thelps\\treduce\\toverfitting.\\tMax-norm\\tregularization\\ncan\\talso\\thelp\\talleviate\\tthe\\tvanishing/exploding\\tgradients\\tproblems\\t(if\\tyou\\tare\\tnot\\tusing\\tBatch\\nNormalization).\\nTensorFlow\\t\\ndoes\\tnot\\tprovide\\tan\\toff-the-shelf\\tmax-norm\\tregularizer,\\tbut\\tit\\tis\\tnot\\ttoo\\thard\\tto\\timplement.\\nThe\\tfollowing\\tcode\\tgets\\ta\\thandle\\ton\\tthe\\tweights\\tof\\tthe\\tfirst\\thidden\\tlayer,\\tthen\\tit\\tuses\\tthe\\nclip_by_norm()\\n\\tfunction\\t\\nto\\tcreate\\tan\\toperation\\tthat\\twill\\tclip\\tthe\\tweights\\talong\\tthe\\tsecond\\taxis\\tso\\tthat', 'each\\trow\\tvector\\tends\\tup\\twith\\ta\\tmaximum\\tnorm\\tof\\t1.0.\\tThe\\tlast\\tline\\tcreates\\tan\\tassignment\\toperation\\tthat\\nwill\\tassign\\tthe\\tclipped\\tweights\\tto\\tthe\\tweights\\tvariable:\\nthreshold\\n\\t\\n=\\n\\t\\n1.0\\nweights\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_default_graph\\n()\\n.\\nget_tensor_by_name\\n(\\n\"hidden1/kernel:0\"\\n)\\nclipped_weights\\n\\t\\n=\\n\\t\\ntf\\n.\\nclip_by_norm\\n(\\nweights\\n,\\n\\t\\nclip_norm\\n=\\nthreshold\\n,\\n\\t\\naxes\\n=\\n1\\n)\\nclip_weights\\n\\t\\n=\\n\\t\\ntf\\n.\\nassign\\n(\\nweights\\n,\\n\\t\\nclipped_weights\\n)\\nThen\\tyou\\tjust\\tapply\\tthis\\toperation\\tafter\\teach\\ttraining\\tstep,\\tlike\\tso:\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\nclip_weights\\n.\\neval\\n()\\nIn\\tgeneral,\\tyou\\twould\\tdo\\tthis\\tfor\\tevery\\thidden\\tlayer.\\tAlthough\\tthis\\tsolution\\tshould\\twork\\tfine,\\tit\\tis\\ta\\tbit\\nmessy.\\tA\\tcleaner\\tsolution\\tis\\tto\\t\\ncreate\\ta\\t\\nmax_norm_regularizer()\\n\\tfunction\\t\\nand\\tuse\\tit\\tjust\\tlike\\tthe\\tearlier\\nl1_regularizer()\\n\\tfunction:\\ndef\\n\\t\\nmax_norm_regularizer\\n(\\nthreshold\\n,\\n\\t\\naxes\\n=\\n1\\n,\\n\\t\\nname\\n=\\n\"max_norm\"\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncollection\\n=\\n\"max_norm\"\\n):\\n\\t\\t\\t\\t\\ndef\\n\\t\\nmax_norm\\n(\\nweights\\n):', '):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nclipped\\n\\t\\n=\\n\\t\\ntf\\n.\\nclip_by_norm\\n(\\nweights\\n,\\n\\t\\nclip_norm\\n=\\nthreshold\\n,\\n\\t\\naxes\\n=\\naxes\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nclip_weights\\n\\t\\n=\\n\\t\\ntf\\n.\\nassign\\n(\\nweights\\n,\\n\\t\\nclipped\\n,\\n\\t\\nname\\n=\\nname\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntf\\n.\\nadd_to_collection\\n(\\ncollection\\n,\\n\\t\\nclip_weights\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nNone\\n\\t\\t\\n#\\tthere\\tis\\tno\\tregularization\\tloss\\tterm\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nmax_norm\\nThis\\tfunction\\treturns\\ta\\tparametrized\\t\\nmax_norm()\\n\\t\\nfunction\\tthat\\tyou\\tcan\\tuse\\tlike\\tany\\tother\\tregularizer:\\nmax_norm_reg\\n\\t\\n=\\n\\t\\nmax_norm_regularizer\\n(\\nthreshold\\n=\\n1.0\\n)\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"dnn\"\\n):\\n\\t\\t\\t\\t\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_regularizer\\n=\\nmax_norm_reg\\n,\\n\\t\\nname\\n=\\n\"hidden1\"\\n)\\n\\t\\t\\t\\t\\nhidden2\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_regularizer\\n=\\nmax_norm_reg\\n,\\n\\t\\nname\\n=\\n\"hidden2\"\\n)\\n\\t\\t\\t\\t\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden2\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nname\\n=\\n\"outputs\"\\n)', 'Note\\tthat\\tmax-norm\\tregularization\\tdoes\\tnot\\trequire\\tadding\\ta\\tregularization\\tloss\\tterm\\tto\\tyour\\toverall\\tloss\\nfunction,\\twhich\\tis\\twhy\\tthe\\t\\nmax_norm()\\n\\tfunction\\treturns\\t\\nNone\\n.\\tBut\\tyou\\tstill\\tneed\\tto\\tbe\\table\\tto\\trun\\tthe\\nclip_weights\\n\\toperations\\tafter\\teach\\ttraining\\tstep,\\tso\\tyou\\tneed\\tto\\tbe\\table\\tto\\tget\\ta\\thandle\\ton\\tthem.\\tThis\\tis\\nwhy\\tthe\\t\\nmax_norm()\\n\\tfunction\\tadds\\tthe\\t\\nclip_weights\\n\\toperation\\tto\\ta\\tcollection\\tof\\tmax-norm\\tclipping\\noperations.\\tYou\\tneed\\tto\\tfetch\\tthese\\tclipping\\toperations\\tand\\trun\\tthem\\tafter\\t\\neach\\ttraining\\tstep:\\nclip_all_weights\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\n\"max_norm\"\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nclip_all_weights\\n)\\nMuch', ')\\nMuch\\n\\tcleaner\\tcode,\\tisn’t\\t\\nit?', 'Data\\tAugmentation\\nOne\\t\\nlast\\tregularization\\ttechnique,\\tdata\\taugmentation,\\tconsists\\tof\\tgenerating\\tnew\\ttraining\\tinstances\\tfrom\\nexisting\\tones,\\tartificially\\tboosting\\tthe\\tsize\\tof\\tthe\\ttraining\\tset.\\tThis\\twill\\treduce\\toverfitting,\\tmaking\\tthis\\ta\\nregularization\\ttechnique.\\tThe\\ttrick\\tis\\tto\\tgenerate\\trealistic\\ttraining\\tinstances;\\tideally,\\ta\\thuman\\tshould\\tnot\\nbe\\table\\tto\\ttell\\twhich\\tinstances\\twere\\tgenerated\\tand\\twhich\\tones\\twere\\tnot.\\tMoreover,\\tsimply\\tadding\\twhite\\nnoise\\twill\\tnot\\thelp;\\tthe\\tmodifications\\tyou\\tapply\\tshould\\tbe\\tlearnable\\t(white\\tnoise\\tis\\tnot).\\nFor\\texample,\\tif\\tyour\\tmodel\\tis\\tmeant\\tto\\tclassify\\tpictures\\tof\\tmushrooms,\\tyou\\tcan\\tslightly\\tshift,\\trotate,\\tand\\nresize\\tevery\\tpicture\\tin\\tthe\\ttraining\\tset\\tby\\tvarious\\tamounts\\tand\\tadd\\tthe\\tresulting\\tpictures\\tto\\tthe\\ttraining\\tset\\n(see\\t\\nFigure\\t11-10\\n).\\tThis\\tforces\\tthe\\tmodel\\tto\\tbe\\tmore\\ttolerant\\tto\\tthe\\tposition,\\torientation,\\tand\\tsize\\tof\\tthe\\nmushrooms\\tin\\tthe\\tpicture.\\tIf\\tyou\\twant\\tthe\\tmodel\\tto\\tbe\\tmore\\ttolerant\\tto\\tlighting\\tconditions,\\tyou\\tcan', 'similarly\\tgenerate\\tmany\\timages\\twith\\tvarious\\tcontrasts.\\tAssuming\\tthe\\tmushrooms\\tare\\tsymmetrical,\\tyou\\ncan\\talso\\tflip\\tthe\\tpictures\\thorizontally.\\tBy\\tcombining\\tthese\\ttransformations\\tyou\\tcan\\tgreatly\\tincrease\\tthe\\nsize\\tof\\tyour\\ttraining\\tset.\\nFigure\\t11-10.\\t\\nGenerating\\tnew\\ttraining\\tinstances\\tfrom\\texisting\\tones\\nIt\\tis\\toften\\tpreferable\\tto\\tgenerate\\ttraining\\tinstances\\ton\\tthe\\tfly\\tduring\\ttraining\\trather\\tthan\\twasting\\tstorage\\nspace\\tand\\tnetwork\\tbandwidth.\\tTensorFlow\\toffers\\tseveral\\timage\\tmanipulation\\toperations\\tsuch\\tas\\ntransposing\\t(shifting),\\trotating,\\tresizing,\\tflipping,\\tand\\tcropping,\\tas\\twell\\tas\\tadjusting\\tthe\\tbrightness,\\ncontrast,\\tsaturation,\\tand\\thue\\t(see\\tthe\\tAPI\\tdocumentation\\tfor\\tmore\\tdetails).\\tThis\\tmakes\\tit\\teasy\\tto\\nimplement\\tdata\\taugmentation\\tfor\\timage\\tdatasets.', 'NOTE\\nAnother\\tpowerful\\ttechnique\\tto\\ttrain\\tvery\\tdeep\\tneural\\tnetworks\\tis\\tto\\tadd\\n\\t\\nskip\\tconnections\\n\\t(a\\tskip\\tconnection\\tis\\twhen\\tyou\\tadd\\nthe\\tinput\\tof\\ta\\tlayer\\tto\\tthe\\toutput\\tof\\ta\\thigher\\tlayer).\\tWe\\twill\\texplore\\tthis\\tidea\\tin\\t\\nChapter\\t13\\n\\twhen\\twe\\ttalk\\tabout\\tdeep\\tresidual\\nnetworks.', 'Practical\\tGuidelines\\nIn\\t\\nthis\\tchapter,\\twe\\thave\\tcovered\\ta\\twide\\trange\\tof\\ttechniques\\tand\\tyou\\tmay\\tbe\\twondering\\twhich\\tones\\tyou\\nshould\\tuse.\\tThe\\tconfiguration\\tin\\t\\nTable\\t11-2\\n\\twill\\twork\\tfine\\tin\\tmost\\tcases.\\nTable\\t11-2.\\t\\nDefault\\tDNN\\tconfiguration\\nInitialization\\nHe\\tinitialization\\nActivation\\tfunction\\nELU\\nNormalization\\nBatch\\tNormalization\\nRegularization\\nDropout\\nOptimizer\\nNesterov\\tAccelerated\\tGradient\\nLearning\\trate\\tschedule\\nNone\\nOf\\tcourse,\\tyou\\tshould\\ttry\\tto\\treuse\\tparts\\tof\\ta\\tpretrained\\tneural\\tnetwork\\tif\\tyou\\tcan\\tfind\\tone\\tthat\\tsolves\\ta\\nsimilar\\tproblem.\\nThis\\tdefault\\tconfiguration\\tmay\\tneed\\tto\\tbe\\ttweaked:\\nIf\\tyou\\tcan’t\\tfind\\ta\\tgood\\tlearning\\trate\\t(convergence\\twas\\ttoo\\tslow,\\tso\\tyou\\tincreased\\tthe\\ttraining\\trate,\\nand\\tnow\\tconvergence\\tis\\tfast\\tbut\\tthe\\tnetwork’s\\taccuracy\\tis\\tsuboptimal),\\tthen\\tyou\\tcan\\ttry\\tadding\\ta\\nlearning\\tschedule\\tsuch\\tas\\texponential\\tdecay.\\nIf\\tyour\\ttraining\\tset\\tis\\ta\\tbit\\ttoo\\tsmall,\\tyou\\tcan\\timplement\\tdata\\taugmentation.\\nIf\\tyou\\tneed\\ta\\tsparse\\tmodel,\\tyou\\tcan\\tadd\\tsome\\tℓ\\n1', '1\\n\\tregularization\\tto\\tthe\\tmix\\t(and\\toptionally\\tzero\\tout\\nthe\\ttiny\\tweights\\tafter\\ttraining).\\tIf\\tyou\\tneed\\tan\\teven\\tsparser\\tmodel,\\tyou\\tcan\\ttry\\tusing\\tFTRL\\tinstead\\tof\\nAdam\\toptimization,\\talong\\twith\\tℓ\\n1\\n\\tregularization.\\nIf\\tyou\\tneed\\ta\\tlightning-fast\\tmodel\\tat\\truntime,\\tyou\\tmay\\twant\\tto\\tdrop\\tBatch\\tNormalization,\\tand\\npossibly\\treplace\\tthe\\tELU\\tactivation\\tfunction\\twith\\tthe\\tleaky\\tReLU.\\tHaving\\ta\\tsparse\\tmodel\\twill\\talso\\nhelp.\\nWith\\tthese\\tguidelines,\\tyou\\tare\\tnow\\tready\\tto\\ttrain\\tvery\\tdeep\\tnets\\t—\\twell,\\tif\\tyou\\tare\\tvery\\tpatient,\\tthat\\tis!\\tIf\\nyou\\tuse\\ta\\tsingle\\tmachine,\\tyou\\tmay\\thave\\tto\\twait\\tfor\\tdays\\tor\\teven\\tmonths\\tfor\\ttraining\\tto\\tcomplete.\\tIn\\tthe\\nnext\\tchapter\\twe\\twill\\tdiscuss\\thow\\tto\\tuse\\tdistributed\\tTensorFlow\\tto\\ttrain\\tand\\trun\\tmodels\\tacross\\tmany\\nservers\\tand\\tGPUs.', 'Exercises\\n1\\n.\\t\\nIs\\tit\\tokay\\tto\\tinitialize\\tall\\tthe\\tweights\\tto\\tthe\\tsame\\tvalue\\tas\\tlong\\tas\\tthat\\tvalue\\tis\\tselected\\trandomly\\nusing\\tHe\\tinitialization?\\n2\\n.\\t\\nIs\\tit\\tokay\\tto\\tinitialize\\tthe\\tbias\\tterms\\tto\\t0?\\n3\\n.\\t\\nName\\tthree\\tadvantages\\tof\\tthe\\tELU\\tactivation\\tfunction\\tover\\tReLU.\\n4\\n.\\t\\nIn\\twhich\\tcases\\twould\\tyou\\twant\\tto\\tuse\\teach\\tof\\tthe\\tfollowing\\tactivation\\tfunctions:\\tELU,\\tleaky\\tReLU\\n(and\\tits\\tvariants),\\tReLU,\\ttanh,\\tlogistic,\\tand\\tsoftmax?\\n5\\n.\\t\\nWhat\\tmay\\thappen\\tif\\tyou\\tset\\tthe\\t\\nmomentum\\n\\thyperparameter\\ttoo\\tclose\\tto\\t1\\t(e.g.,\\t0.99999)\\twhen\\t\\nusing\\na\\t\\nMomentumOptimizer\\n?\\n6\\n.\\t\\nName\\tthree\\tways\\tyou\\tcan\\tproduce\\ta\\tsparse\\tmodel.\\n7\\n.\\t\\nDoes\\tdropout\\tslow\\tdown\\ttraining?\\tDoes\\tit\\tslow\\tdown\\t\\ninference\\t(i.e.,\\tmaking\\tpredictions\\ton\\tnew\\ninstances)?\\n8\\n.\\t\\nDeep\\tLearning.\\na\\n.\\t\\nBuild\\ta\\tDNN\\twith\\tfive\\thidden\\tlayers\\tof\\t100\\tneurons\\teach,\\tHe\\tinitialization,\\tand\\tthe\\tELU\\nactivation\\tfunction.\\nb\\n.\\t\\nUsing\\tAdam\\toptimization\\tand\\tearly\\tstopping,\\ttry\\ttraining\\tit\\ton\\tMNIST\\tbut\\tonly\\ton\\tdigits\\t0\\tto\\t4,', 'as\\twe\\twill\\tuse\\ttransfer\\tlearning\\tfor\\tdigits\\t5\\tto\\t9\\tin\\tthe\\tnext\\texercise.\\tYou\\twill\\tneed\\ta\\tsoftmax\\noutput\\tlayer\\twith\\tfive\\tneurons,\\tand\\tas\\talways\\tmake\\tsure\\tto\\tsave\\tcheckpoints\\tat\\tregular\\tintervals\\nand\\tsave\\tthe\\tfinal\\tmodel\\tso\\tyou\\tcan\\treuse\\tit\\tlater.\\nc\\n.\\t\\nTune\\tthe\\thyperparameters\\tusing\\tcross-validation\\tand\\tsee\\twhat\\tprecision\\tyou\\tcan\\tachieve.\\nd\\n.\\t\\nNow\\ttry\\tadding\\tBatch\\tNormalization\\tand\\tcompare\\tthe\\tlearning\\tcurves:\\tis\\tit\\tconverging\\tfaster\\nthan\\tbefore?\\tDoes\\tit\\tproduce\\ta\\tbetter\\tmodel?\\ne\\n.\\t\\nIs\\tthe\\tmodel\\toverfitting\\tthe\\ttraining\\tset?\\tTry\\tadding\\tdropout\\tto\\tevery\\tlayer\\tand\\ttry\\tagain.\\tDoes\\tit\\nhelp?\\n9\\n.\\t\\nTransfer\\tlearning.\\na\\n.\\t\\nCreate\\ta\\tnew\\tDNN\\tthat\\treuses\\tall\\tthe\\tpretrained\\thidden\\tlayers\\tof\\tthe\\tprevious\\tmodel,\\tfreezes\\nthem,\\tand\\treplaces\\tthe\\tsoftmax\\toutput\\tlayer\\twith\\ta\\tnew\\tone.\\nb\\n.\\t\\nTrain\\tthis\\tnew\\tDNN\\ton\\tdigits\\t5\\tto\\t9,\\tusing\\tonly\\t100\\timages\\tper\\tdigit,\\tand\\ttime\\thow\\tlong\\tit\\ntakes.\\tDespite\\tthis\\tsmall\\tnumber\\tof\\texamples,\\tcan\\tyou\\tachieve\\thigh\\tprecision?\\nc\\n.', 'c\\n.\\t\\nTry\\tcaching\\tthe\\tfrozen\\tlayers,\\tand\\ttrain\\tthe\\tmodel\\tagain:\\thow\\tmuch\\tfaster\\tis\\tit\\tnow?\\nd\\n.\\t\\nTry\\tagain\\treusing\\tjust\\tfour\\thidden\\tlayers\\tinstead\\tof\\tfive.\\tCan\\tyou\\tachieve\\ta\\thigher\\tprecision?', 'e\\n.\\t\\nNow\\tunfreeze\\tthe\\ttop\\ttwo\\thidden\\tlayers\\tand\\tcontinue\\ttraining:\\tcan\\tyou\\tget\\tthe\\tmodel\\tto\\tperform\\neven\\tbetter?\\n10\\n.\\t\\nPretraining\\ton\\tan\\tauxiliary\\ttask.\\na\\n.\\t\\nIn\\tthis\\texercise\\tyou\\twill\\tbuild\\ta\\tDNN\\tthat\\tcompares\\ttwo\\tMNIST\\tdigit\\timages\\tand\\tpredicts\\nwhether\\tthey\\trepresent\\tthe\\tsame\\tdigit\\tor\\tnot.\\tThen\\tyou\\twill\\treuse\\tthe\\tlower\\tlayers\\tof\\tthis\\nnetwork\\tto\\ttrain\\tan\\tMNIST\\tclassifier\\tusing\\tvery\\tlittle\\ttraining\\tdata.\\tStart\\tby\\tbuilding\\ttwo\\tDNNs\\n(let’s\\tcall\\tthem\\tDNN\\tA\\tand\\tB),\\tboth\\tsimilar\\tto\\tthe\\tone\\tyou\\tbuilt\\tearlier\\tbut\\twithout\\tthe\\toutput\\nlayer:\\teach\\tDNN\\tshould\\thave\\tfive\\thidden\\tlayers\\tof\\t100\\tneurons\\teach,\\tHe\\tinitialization,\\tand\\nELU\\tactivation.\\tNext,\\tadd\\tone\\tmore\\thidden\\tlayer\\twith\\t10\\tunits\\ton\\ttop\\tof\\tboth\\tDNNs.\\tTo\\tdo\\tthis,\\nyou\\tshould\\tuse\\tTensorFlow’s\\t\\nconcat()\\n\\t\\nfunction\\twith\\t\\naxis=1\\n\\tto\\tconcatenate\\tthe\\toutputs\\tof\\tboth\\nDNNs\\tfor\\teach\\tinstance,\\tthen\\tfeed\\tthe\\tresult\\tto\\tthe\\thidden\\tlayer.\\tFinally,\\tadd\\tan\\toutput\\tlayer\\nwith\\ta\\tsingle\\tneuron\\tusing\\tthe\\tlogistic\\tactivation\\tfunction.\\nb\\n.', 'b\\n.\\t\\nSplit\\tthe\\tMNIST\\ttraining\\tset\\tin\\ttwo\\tsets:\\tsplit\\t#1\\tshould\\tcontaining\\t55,000\\timages,\\tand\\tsplit\\t#2\\nshould\\tcontain\\tcontain\\t5,000\\timages.\\tCreate\\ta\\tfunction\\tthat\\tgenerates\\ta\\ttraining\\tbatch\\twhere\\neach\\tinstance\\tis\\ta\\tpair\\tof\\tMNIST\\timages\\tpicked\\tfrom\\tsplit\\t#1.\\tHalf\\tof\\tthe\\ttraining\\tinstances\\nshould\\tbe\\tpairs\\tof\\timages\\tthat\\tbelong\\tto\\tthe\\tsame\\tclass,\\twhile\\tthe\\tother\\thalf\\tshould\\tbe\\timages\\nfrom\\tdifferent\\tclasses.\\tFor\\teach\\tpair,\\tthe\\ttraining\\tlabel\\tshould\\tbe\\t0\\tif\\tthe\\timages\\tare\\tfrom\\tthe\\nsame\\tclass,\\tor\\t1\\tif\\tthey\\tare\\tfrom\\tdifferent\\tclasses.\\nc\\n.\\t\\nTrain\\tthe\\tDNN\\ton\\tthis\\ttraining\\tset.\\tFor\\teach\\timage\\tpair,\\tyou\\tcan\\tsimultaneously\\tfeed\\tthe\\tfirst\\nimage\\tto\\tDNN\\tA\\tand\\tthe\\tsecond\\timage\\tto\\tDNN\\tB.\\tThe\\twhole\\tnetwork\\twill\\tgradually\\tlearn\\tto\\ntell\\twhether\\ttwo\\timages\\tbelong\\tto\\tthe\\tsame\\tclass\\tor\\tnot.\\nd\\n.\\t\\nNow\\tcreate\\ta\\tnew\\tDNN\\tby\\treusing\\tand\\tfreezing\\tthe\\thidden\\tlayers\\tof\\tDNN\\tA\\tand\\tadding\\ta\\nsoftmax\\toutput\\tlayer\\ton\\ttop\\twith\\t10\\tneurons.\\tTrain\\tthis\\tnetwork\\ton\\tsplit\\t#2\\tand\\tsee\\tif\\tyou\\tcan', 'achieve\\thigh\\tperformance\\tdespite\\thaving\\tonly\\t500\\timages\\tper\\tclass.\\nSolutions\\tto\\tthese\\texercises\\tare\\t\\navailable\\tin\\t\\nAppendix\\tA\\n.\\n“Understanding\\tthe\\tDifficulty\\tof\\tTraining\\tDeep\\tFeedforward\\tNeural\\tNetworks,”\\tX.\\tGlorot,\\tY\\tBengio\\t(2010).\\nHere’s\\tan\\tanalogy:\\tif\\tyou\\tset\\ta\\tmicrophone\\tamplifier’s\\tknob\\ttoo\\tclose\\tto\\tzero,\\tpeople\\twon’t\\thear\\tyour\\tvoice,\\tbut\\tif\\tyou\\tset\\tit\\ttoo\\tclose\\tto\\nthe\\tmax,\\tyour\\tvoice\\twill\\tbe\\tsaturated\\tand\\tpeople\\twon’t\\tunderstand\\twhat\\tyou\\tare\\tsaying.\\tNow\\timagine\\ta\\tchain\\tof\\tsuch\\tamplifiers:\\tthey\\tall\\nneed\\tto\\tbe\\tset\\tproperly\\tin\\torder\\tfor\\tyour\\tvoice\\tto\\tcome\\tout\\tloud\\tand\\tclear\\tat\\tthe\\tend\\tof\\tthe\\tchain.\\tYour\\tvoice\\thas\\tto\\tcome\\tout\\tof\\teach\\namplifier\\tat\\tthe\\tsame\\tamplitude\\tas\\tit\\tcame\\tin.\\nThis\\tsimplified\\tstrategy\\twas\\tactually\\talready\\tproposed\\tmuch\\tearlier\\t—\\tfor\\texample,\\tin\\tthe\\t1998\\tbook\\t\\nNeural\\tNetworks:\\tTricks\\tof\\tthe\\nTrade\\n\\tby\\tGenevieve\\tOrr\\tand\\tKlaus-Robert\\tMüller\\t(Springer).', 'Such\\tas\\t“Delving\\tDeep\\tinto\\tRectifiers:\\tSurpassing\\tHuman-Level\\tPerformance\\ton\\tImageNet\\tClassification,”\\tK.\\tHe\\tet\\tal.\\t(2015).\\n“Empirical\\tEvaluation\\tof\\tRectified\\tActivations\\tin\\tConvolution\\tNetwork,”\\tB.\\tXu\\tet\\tal.\\t(2015).\\n“Fast\\tand\\tAccurate\\tDeep\\tNetwork\\tLearning\\tby\\tExponential\\tLinear\\tUnits\\t(ELUs),”\\tD.\\tClevert,\\tT.\\tUnterthiner,\\tS.\\tHochreiter\\t(2015).\\n“Batch\\tNormalization:\\tAccelerating\\tDeep\\tNetwork\\tTraining\\tby\\tReducing\\tInternal\\tCovariate\\tShift,”\\tS.\\tIoffe\\tand\\tC.\\tSzegedy\\t(2015).\\nMany\\tresearchers\\targue\\tthat\\tit\\tis\\tjust\\tas\\tgood,\\tor\\teven\\tbetter,\\tto\\tplace\\tthe\\tbatch\\tnormalization\\tlayers\\tafter\\t(rather\\tthan\\tbefore)\\tthe\\nactivations.\\n“On\\tthe\\tdifficulty\\tof\\ttraining\\trecurrent\\tneural\\tnetworks,”\\tR.\\tPascanu\\tet\\tal.\\t(2013).\\nAnother\\toption\\tis\\tto\\tcome\\tup\\twith\\ta\\tsupervised\\ttask\\tfor\\twhich\\tyou\\tcan\\teasily\\tgather\\ta\\tlot\\tof\\tlabeled\\ttraining\\tdata,\\tthen\\tuse\\ttransfer\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10', 'learning,\\tas\\texplained\\tearlier.\\tFor\\texample,\\tif\\tyou\\twant\\tto\\ttrain\\ta\\tmodel\\tto\\tidentify\\tyour\\tfriends\\tin\\tpictures,\\tyou\\tcould\\tdownload\\tmillions\\tof\\nfaces\\ton\\tthe\\tinternet\\tand\\ttrain\\ta\\tclassifier\\tto\\tdetect\\twhether\\ttwo\\tfaces\\tare\\tidentical\\tor\\tnot,\\tthen\\tuse\\tthis\\tclassifier\\tto\\tcompare\\ta\\tnew\\npicture\\twith\\teach\\tpicture\\tof\\tyour\\tfriends.\\n“Some\\tmethods\\tof\\tspeeding\\tup\\tthe\\tconvergence\\tof\\titeration\\tmethods,”\\tB.\\tPolyak\\t(1964).\\n“A\\tMethod\\tfor\\tUnconstrained\\tConvex\\tMinimization\\tProblem\\twith\\tthe\\tRate\\tof\\tConvergence\\tO(1/k\\n),”\\tYurii\\tNesterov\\t(1983).\\n“Adaptive\\tSubgradient\\tMethods\\tfor\\tOnline\\tLearning\\tand\\tStochastic\\tOptimization,”\\tJ.\\tDuchi\\tet\\tal.\\t(2011).\\nThis\\talgorithm\\twas\\tcreated\\tby\\tTijmen\\tTieleman\\tand\\tGeoffrey\\tHinton\\tin\\t2012,\\tand\\tpresented\\tby\\tGeoffrey\\tHinton\\tin\\this\\tCoursera\\tclass\\ton\\nneural\\tnetworks\\t(slides:\\t\\nhttp://goo.gl/RsQeis\\n;\\tvideo:\\t\\nhttps://goo.gl/XUbIyJ\\n).\\tAmusingly,\\tsince\\tthe\\tauthors\\thave\\tnot\\twritten\\ta\\tpaper\\tto\\ndescribe\\tit,\\tresearchers\\toften\\tcite\\t“slide\\t29\\tin\\tlecture\\t6”\\tin\\ttheir\\tpapers.', '“Adam:\\tA\\tMethod\\tfor\\tStochastic\\tOptimization,”\\tD.\\tKingma,\\tJ.\\tBa\\t(2015).\\nThese\\tare\\testimations\\tof\\tthe\\tmean\\tand\\t(uncentered)\\tvariance\\tof\\tthe\\tgradients.\\tThe\\tmean\\tis\\toften\\tcalled\\tthe\\t\\nfirst\\tmoment\\n,\\twhile\\tthe\\nvariance\\tis\\toften\\tcalled\\tthe\\t\\nsecond\\tmoment\\n,\\thence\\tthe\\tname\\tof\\tthe\\talgorithm.\\n“The\\tMarginal\\tValue\\tof\\tAdaptive\\tGradient\\tMethods\\tin\\tMachine\\tLearning,”\\tA.\\tC.\\tWilson\\tet\\tal.\\t(2017).\\n“Primal-Dual\\tSubgradient\\tMethods\\tfor\\tConvex\\tProblems,”\\tYurii\\tNesterov\\t(2005).\\n“Ad\\tClick\\tPrediction:\\ta\\tView\\tfrom\\tthe\\tTrenches,”\\tH.\\tMcMahan\\tet\\tal.\\t(2013).\\n“An\\tEmpirical\\tStudy\\tof\\tLearning\\tRates\\tin\\tDeep\\tNeural\\tNetworks\\tfor\\tSpeech\\tRecognition,”\\tA.\\tSenior\\tet\\tal.\\t(2013).\\n“Improving\\tneural\\tnetworks\\tby\\tpreventing\\tco-adaptation\\tof\\tfeature\\tdetectors,”\\tG.\\tHinton\\tet\\tal.\\t(2012).\\n“Dropout:\\tA\\tSimple\\tWay\\tto\\tPrevent\\tNeural\\tNetworks\\tfrom\\tOverfitting,”\\tN.\\tSrivastava\\tet\\tal.\\t(2014).\\n11\\n12\\n2\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22', 'Chapter\\t12.\\t\\nDistributing\\tTensorFlow\\tAcross\\nDevices\\tand\\tServers\\nIn\\t\\nChapter\\t11\\n\\twe\\t\\ndiscussed\\tseveral\\ttechniques\\tthat\\tcan\\tconsiderably\\tspeed\\tup\\ttraining:\\tbetter\\tweight\\ninitialization,\\tBatch\\tNormalization,\\tsophisticated\\toptimizers,\\tand\\tso\\ton.\\tHowever,\\teven\\twith\\tall\\tof\\tthese\\ntechniques,\\ttraining\\ta\\tlarge\\tneural\\tnetwork\\ton\\ta\\tsingle\\tmachine\\twith\\ta\\tsingle\\tCPU\\tcan\\ttake\\tdays\\tor\\teven\\nweeks.\\nIn\\tthis\\tchapter\\twe\\twill\\tsee\\thow\\tto\\tuse\\tTensorFlow\\tto\\tdistribute\\tcomputations\\tacross\\tmultiple\\tdevices\\n(CPUs\\tand\\tGPUs)\\tand\\trun\\tthem\\tin\\tparallel\\t(see\\t\\nFigure\\t12-1\\n).\\tFirst\\twe\\twill\\tdistribute\\tcomputations\\nacross\\tmultiple\\tdevices\\ton\\tjust\\tone\\tmachine,\\tthen\\ton\\tmultiple\\tdevices\\tacross\\tmultiple\\tmachines.\\nFigure\\t12-1.\\t\\nExecuting\\ta\\tTensorFlow\\tgraph\\tacross\\tmultiple\\tdevices\\tin\\tparallel\\nTensorFlow’s\\tsupport\\tof\\tdistributed\\tcomputing\\tis\\tone\\tof\\tits\\tmain\\thighlights\\tcompared\\tto\\tother\\tneural\\nnetwork\\tframeworks.\\tIt\\tgives\\tyou\\tfull\\tcontrol\\tover\\thow\\tto\\tsplit\\t(or\\treplicate)\\tyour\\tcomputation\\tgraph', 'across\\tdevices\\tand\\tservers,\\tand\\tit\\tlets\\tyou\\tparallelize\\tand\\tsynchronize\\toperations\\tin\\tflexible\\tways\\tso\\tyou\\ncan\\tchoose\\tbetween\\tall\\tsorts\\tof\\tparallelization\\tapproaches.\\nWe\\twill\\tlook\\tat\\tsome\\tof\\tthe\\tmost\\tpopular\\tapproaches\\tto\\tparallelizing\\tthe\\texecution\\tand\\ttraining\\tof\\ta\\nneural\\tnetwork.\\tInstead\\tof\\twaiting\\tfor\\tweeks\\tfor\\ta\\ttraining\\talgorithm\\tto\\tcomplete,\\tyou\\tmay\\tend\\tup\\twaiting\\nfor\\tjust\\ta\\tfew\\thours.\\tNot\\tonly\\tdoes\\tthis\\tsave\\tan\\tenormous\\tamount\\tof\\ttime,\\tit\\talso\\tmeans\\tthat\\tyou\\tcan\\nexperiment\\twith\\tvarious\\tmodels\\tmuch\\tmore\\teasily,\\tand\\tfrequently\\tretrain\\tyour\\tmodels\\ton\\tfresh\\tdata.\\nOther\\tgreat\\tuse\\tcases\\tof\\tparallelization\\tinclude\\texploring\\ta\\tmuch\\tlarger\\thyperparameter\\tspace\\twhen\\tfine-\\ntuning\\tyour\\tmodel,\\tand\\trunning\\tlarge\\tensembles\\tof\\tneural\\tnetworks\\tefficiently.', 'But\\twe\\tmust\\tlearn\\tto\\twalk\\tbefore\\twe\\tcan\\trun.\\tLet’s\\tstart\\tby\\tparallelizing\\tsimple\\tgraphs\\tacross\\tseveral\\nGPUs\\ton\\ta\\tsingle\\tmachine.', 'Multiple\\tDevices\\ton\\ta\\tSingle\\tMachine\\nYou\\t\\ncan\\toften\\tget\\ta\\tmajor\\tperformance\\tboost\\tsimply\\tby\\tadding\\tGPU\\tcards\\tto\\ta\\tsingle\\tmachine.\\tIn\\tfact,\\tin\\nmany\\tcases\\tthis\\twill\\tsuffice;\\tyou\\twon’t\\tneed\\tto\\tuse\\tmultiple\\tmachines\\tat\\tall.\\tFor\\texample,\\tyou\\tcan\\ntypically\\ttrain\\ta\\tneural\\tnetwork\\tjust\\tas\\tfast\\tusing\\t8\\tGPUs\\ton\\ta\\tsingle\\tmachine\\trather\\tthan\\t16\\tGPUs\\tacross\\nmultiple\\tmachines\\t(due\\tto\\tthe\\textra\\tdelay\\timposed\\tby\\tnetwork\\tcommunications\\tin\\ta\\tmultimachine\\tsetup).\\nIn\\tthis\\tsection\\twe\\twill\\tlook\\tat\\thow\\tto\\tset\\tup\\tyour\\tenvironment\\tso\\tthat\\tTensorFlow\\tcan\\tuse\\tmultiple\\tGPU\\ncards\\ton\\tone\\tmachine.\\tThen\\twe\\twill\\tlook\\tat\\thow\\tyou\\tcan\\tdistribute\\toperations\\tacross\\tavailable\\tdevices\\nand\\texecute\\tthem\\tin\\tparallel.', 'Installation\\nIn\\t\\norder\\tto\\trun\\tTensorFlow\\ton\\tmultiple\\tGPU\\tcards,\\tyou\\tfirst\\tneed\\tto\\tmake\\tsure\\tyour\\tGPU\\tcards\\thave\\nNVidia\\tCompute\\tCapability\\t\\n(greater\\tor\\tequal\\tto\\t3.0).\\tThis\\tincludes\\tNvidia’s\\tTitan,\\tTitan\\tX,\\tK20,\\tand\\nK40\\tcards\\t(if\\tyou\\town\\tanother\\tcard,\\tyou\\tcan\\tcheck\\tits\\tcompatibility\\tat\\nhttps://developer.nvidia.com/cuda-gpus\\n).\\nTIP\\nIf\\tyou\\tdon’t\\town\\tany\\tGPU\\tcards,\\tyou\\tcan\\tuse\\ta\\thosting\\tservice\\twith\\tGPU\\tcapability\\tsuch\\tas\\tAmazon\\tAWS.\\tDetailed\\tinstructions\\nto\\tset\\tup\\tTensorFlow\\t0.9\\twith\\tPython\\t3.5\\ton\\tan\\tAmazon\\tAWS\\tGPU\\tinstance\\tare\\tavailable\\tin\\tŽiga\\tAvsec’s\\t\\nhelpful\\tblog\\tpost\\n.\\tIt\\nshould\\tnot\\tbe\\ttoo\\thard\\tto\\tupdate\\tit\\tto\\tthe\\tlatest\\tversion\\tof\\tTensorFlow.\\tGoogle\\talso\\treleased\\ta\\tcloud\\tservice\\tcalled\\t\\nCloud\\nMachine\\tLearning\\n\\tto\\trun\\tTensorFlow\\tgraphs.\\tIn\\tMay\\t2016,\\tthey\\tannounced\\tthat\\ttheir\\tplatform\\tnow\\tincludes\\tservers\\tequipped\\nwith\\t\\ntensor\\tprocessing\\tunits\\n\\t(TPUs),\\t\\nprocessors\\tspecialized\\tfor\\tMachine\\tLearning\\tthat\\tare\\tmuch\\tfaster\\tthan\\tGPUs\\tfor\\tmany', 'ML\\ttasks.\\tOf\\tcourse,\\tanother\\toption\\tis\\tsimply\\tto\\tbuy\\tyour\\town\\tGPU\\tcard.\\tTim\\tDettmers\\twrote\\ta\\t\\ngreat\\tblog\\tpost\\n\\tto\\thelp\\tyou\\nchoose,\\tand\\the\\tupdates\\tit\\tfairly\\tregularly.\\nYou\\tmust\\tthen\\tdownload\\tand\\tinstall\\tthe\\tappropriate\\tversion\\tof\\tthe\\t\\nCUDA\\tand\\tcuDNN\\t\\nlibraries\\t(CUDA\\n8.0\\tand\\tcuDNN\\t5.1\\tif\\tyou\\tare\\tusing\\tthe\\tbinary\\tinstallation\\tof\\tTensorFlow\\t1.0.0),\\tand\\tset\\ta\\tfew\\nenvironment\\tvariables\\tso\\tTensorFlow\\tknows\\twhere\\tto\\tfind\\tCUDA\\tand\\tcuDNN.\\tThe\\tdetailed\\tinstallation\\ninstructions\\tare\\tlikely\\tto\\tchange\\tfairly\\tquickly,\\tso\\tit\\tis\\tbest\\tthat\\tyou\\tfollow\\tthe\\tinstructions\\ton\\nTensorFlow’s\\twebsite.\\nNvidia’s\\t\\nCompute\\tUnified\\tDevice\\tArchitecture\\n\\tlibrary\\t(CUDA)\\tallows\\tdevelopers\\tto\\tuse\\tCUDA-\\nenabled\\tGPUs\\tfor\\tall\\tsorts\\tof\\tcomputations\\t(not\\tjust\\tgraphics\\tacceleration).\\tNvidia’s\\t\\nCUDA\\tDeep\\tNeural\\nNetwork\\n\\tlibrary\\t(cuDNN)\\tis\\ta\\tGPU-accelerated\\tlibrary\\tof\\tprimitives\\tfor\\tDNNs.\\tIt\\tprovides\\toptimized\\nimplementations\\tof\\tcommon\\tDNN\\tcomputations\\tsuch\\tas\\tactivation\\tlayers,\\tnormalization,\\tforward\\tand', 'backward\\tconvolutions,\\tand\\tpooling\\t(see\\t\\nChapter\\t13\\n).\\tIt\\tis\\tpart\\tof\\tNvidia’s\\tDeep\\tLearning\\tSDK\\t(note\\nthat\\tit\\trequires\\tcreating\\tan\\tNvidia\\tdeveloper\\taccount\\tin\\torder\\tto\\tdownload\\tit).\\tTensorFlow\\tuses\\tCUDA\\nand\\tcuDNN\\tto\\tcontrol\\tthe\\tGPU\\tcards\\tand\\taccelerate\\tcomputations\\t(see\\t\\nFigure\\t12-2\\n).', 'Figure\\t12-2.\\t\\nTensorFlow\\tuses\\tCUDA\\tand\\tcuDNN\\tto\\tcontrol\\tGPUs\\tand\\tboost\\tDNNs\\nYou\\tcan\\tuse\\tthe\\t\\nnvidia-smi\\n\\tcommand\\tto\\tcheck\\tthat\\tCUDA\\tis\\tproperly\\tinstalled.\\tIt\\tlists\\tthe\\tavailable\\nGPU\\tcards,\\tas\\twell\\tas\\tprocesses\\trunning\\ton\\teach\\tcard:\\n$\\tnvidia-smi\\nWed\\tSep\\t16\\t09:50:03\\t2016\\n+------------------------------------------------------+\\n|\\tNVIDIA-SMI\\t352.63\\t\\t\\t\\t\\tDriver\\tVersion:\\t352.63\\t\\t\\t\\t\\t\\t\\t\\t\\t|\\n|-------------------------------+----------------------+----------------------+\\n|\\tGPU\\t\\tName\\t\\t\\t\\t\\t\\t\\t\\tPersistence-M|\\tBus-Id\\t\\t\\t\\t\\t\\t\\t\\tDisp.A\\t|\\tVolatile\\tUncorr.\\tECC\\t|\\n|\\tFan\\t\\tTemp\\t\\tPerf\\t\\tPwr:Usage/Cap|\\t\\t\\t\\t\\t\\t\\t\\t\\tMemory-Usage\\t|\\tGPU-Util\\t\\tCompute\\tM.\\t|\\n|===============================+======================+======================|\\n|\\t\\t\\t0\\t\\tGRID\\tK520\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOff\\t\\t|\\t0000:00:03.0\\t\\t\\t\\t\\tOff\\t|\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tN/A\\t|\\n|\\tN/A\\t\\t\\t27C\\t\\t\\t\\tP8\\t\\t\\t\\t17W\\t/\\t125W\\t|\\t\\t\\t\\t\\t11MiB\\t/\\t\\t4095MiB\\t|\\t\\t\\t\\t\\t\\t0%\\t\\t\\t\\t\\t\\tDefault\\t|\\n+-------------------------------+----------------------+----------------------+', '+-----------------------------------------------------------------------------+\\n|\\tProcesses:\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGPU\\tMemory\\t|\\n|\\t\\tGPU\\t\\t\\t\\t\\t\\t\\tPID\\t\\tType\\t\\tProcess\\tname\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tUsage\\t\\t\\t\\t\\t\\t|\\n|=============================================================================|\\n|\\t\\tNo\\trunning\\tprocesses\\tfound\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t|\\n+-----------------------------------------------------------------------------+\\nFinally,\\tyou\\tmust\\tinstall\\tTensorFlow\\twith\\tGPU\\tsupport.\\tIf\\tyou\\tcreated\\tan\\tisolated\\tenvironment\\tusing\\nvirtualenv,\\tyou\\tfirst\\tneed\\tto\\tactivate\\tit:\\n$\\tcd\\t$ML_PATH\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t#\\tYour\\tML\\tworking\\tdirectory\\t(e.g.,\\t$HOME/ml)\\n$\\tsource\\tenv/bin/activate\\nThen\\tinstall\\tthe\\tappropriate\\tGPU-enabled\\tversion\\tof\\tTensorFlow:\\n$\\tpip3\\tinstall\\t--upgrade\\ttensorflow-gpu', 'Now\\tyou\\tcan\\topen\\tup\\ta\\tPython\\tshell\\tand\\tcheck\\tthat\\tTensorFlow\\tdetects\\tand\\tuses\\tCUDA\\tand\\tcuDNN\\nproperly\\tby\\timporting\\tTensorFlow\\tand\\tcreating\\ta\\tsession:\\n>>>\\t\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nI\\t[...]/dso_loader.cc:108]\\tsuccessfully\\topened\\tCUDA\\tlibrary\\tlibcublas.so\\tlocally\\nI\\t[...]/dso_loader.cc:108]\\tsuccessfully\\topened\\tCUDA\\tlibrary\\tlibcudnn.so\\tlocally\\nI\\t[...]/dso_loader.cc:108]\\tsuccessfully\\topened\\tCUDA\\tlibrary\\tlibcufft.so\\tlocally\\nI\\t[...]/dso_loader.cc:108]\\tsuccessfully\\topened\\tCUDA\\tlibrary\\tlibcuda.so.1\\tlocally\\nI\\t[...]/dso_loader.cc:108]\\tsuccessfully\\topened\\tCUDA\\tlibrary\\tlibcurand.so\\tlocally\\n>>>\\t\\nsess\\n\\t\\n=\\n\\t\\ntf\\n.\\nSession\\n()\\n[...]\\nI\\t[...]/gpu_init.cc:102]\\tFound\\tdevice\\t0\\twith\\tproperties:\\nname:\\tGRID\\tK520\\nmajor:\\t3\\tminor:\\t0\\tmemoryClockRate\\t(GHz)\\t0.797\\npciBusID\\t0000:00:03.0\\nTotal\\tmemory:\\t4.00GiB\\nFree\\tmemory:\\t3.95GiB\\nI\\t[...]/gpu_init.cc:126]\\tDMA:\\t0\\nI\\t[...]/gpu_init.cc:136]\\t0:\\t\\t\\tY\\nI\\t[...]/gpu_device.cc:839]\\tCreating\\tTensorFlow\\tdevice\\n(/gpu:0)\\t->\\t(device:\\t0,\\tname:\\tGRID\\tK520,\\tpci\\tbus\\tid:\\t0000:00:03.0)', 'Looks\\tgood!\\tTensorFlow\\tdetected\\tthe\\tCUDA\\tand\\tcuDNN\\tlibraries,\\tand\\tit\\tused\\tthe\\tCUDA\\tlibrary\\tto\\ndetect\\tthe\\tGPU\\t\\ncard\\t(in\\tthis\\tcase\\tan\\tNvidia\\tGrid\\tK520\\tcard).', 'Managing\\tthe\\tGPU\\tRAM\\nBy\\t\\ndefault\\tTensorFlow\\tautomatically\\tgrabs\\tall\\tthe\\tRAM\\tin\\tall\\tavailable\\tGPUs\\tthe\\tfirst\\ttime\\tyou\\trun\\ta\\ngraph,\\tso\\tyou\\twill\\tnot\\tbe\\table\\tto\\tstart\\ta\\tsecond\\tTensorFlow\\tprogram\\twhile\\tthe\\tfirst\\tone\\tis\\tstill\\trunning.\\tIf\\nyou\\ttry,\\tyou\\twill\\tget\\tthe\\tfollowing\\terror:\\nE\\n\\t\\n[\\n...\\n]\\n/\\ncuda_driver\\n.\\ncc\\n:\\n965\\n]\\n\\t\\nfailed\\n\\t\\nto\\n\\t\\nallocate\\n\\t\\n3.66\\nG\\n\\t\\n(\\n3928915968\\n\\t\\nbytes\\n)\\n\\t\\nfrom\\ndevice\\n:\\n\\t\\nCUDA_ERROR_OUT_OF_MEMORY\\nOne\\tsolution\\tis\\tto\\trun\\teach\\tprocess\\ton\\tdifferent\\tGPU\\tcards.\\tTo\\tdo\\tthis,\\tthe\\tsimplest\\toption\\tis\\tto\\tset\\tthe\\nCUDA_VISIBLE_DEVICES\\n\\tenvironment\\tvariable\\tso\\tthat\\teach\\tprocess\\tonly\\tsees\\tthe\\tappropriate\\tGPU\\tcards.\\nFor\\texample,\\tyou\\tcould\\tstart\\ttwo\\tprograms\\tlike\\tthis:\\n$\\tCUDA_VISIBLE_DEVICES=0,1\\tpython3\\tprogram_1.py\\n#\\tand\\tin\\tanother\\tterminal:\\n$\\tCUDA_VISIBLE_DEVICES=3,2\\tpython3\\tprogram_2.py\\nProgram\\t#1\\twill\\tonly\\tsee\\tGPU\\tcards\\t0\\tand\\t1\\t(numbered\\t0\\tand\\t1,\\trespectively),\\tand\\tprogram\\t#2\\twill\\tonly\\nsee\\tGPU\\tcards\\t2\\tand\\t3\\t(numbered\\t1\\tand\\t0,\\trespectively).\\tEverything\\twill\\twork\\tfine\\t(see\\t\\nFigure\\t12-3', ').\\nFigure\\t12-3.\\t\\nEach\\tprogram\\tgets\\ttwo\\tGPUs\\tfor\\titself\\nAnother\\toption\\tis\\tto\\ttell\\t\\nTensorFlow\\tto\\tgrab\\tonly\\ta\\tfraction\\tof\\tthe\\tmemory.\\tFor\\texample,\\tto\\tmake\\nTensorFlow\\tgrab\\tonly\\t40%\\tof\\teach\\tGPU’s\\tmemory,\\tyou\\tmust\\tcreate\\ta\\t\\nConfigProto\\n\\t\\nobject,\\tset\\tits\\ngpu_options.per_process_gpu_memory_fraction\\n\\t\\noption\\tto\\t\\n0.4\\n,\\tand\\tcreate\\tthe\\tsession\\tusing\\tthis\\nconfiguration:\\nconfig\\n\\t\\n=\\n\\t\\ntf\\n.\\nConfigProto\\n()\\nconfig\\n.\\ngpu_options\\n.\\nper_process_gpu_memory_fraction\\n\\t\\n=\\n\\t\\n0.4\\nsession\\n\\t\\n=\\n\\t\\ntf\\n.\\nSession\\n(\\nconfig\\n=\\nconfig\\n)\\nNow\\ttwo\\tprograms\\tlike\\tthis\\tone\\tcan\\trun\\tin\\tparallel\\tusing\\tthe\\tsame\\tGPU\\tcards\\t(but\\tnot\\tthree,\\tsince\\t3\\t×\\n0.4\\n\\t>\\t1).\\tSee\\t\\nFigure\\t12-4\\n.', 'Figure\\t12-4.\\t\\nEach\\tprogram\\tgets\\tall\\tfour\\tGPUs,\\tbut\\twith\\tonly\\t40%\\tof\\tthe\\tRAM\\teach\\nIf\\tyou\\trun\\tthe\\t\\nnvidia-smi\\n\\t\\ncommand\\twhile\\tboth\\tprograms\\tare\\trunning,\\tyou\\tshould\\tsee\\tthat\\teach\\tprocess\\nholds\\troughly\\t40%\\tof\\tthe\\ttotal\\tRAM\\tof\\teach\\tcard:\\n$\\tnvidia-smi\\n[...]\\n+-----------------------------------------------------------------------------+\\n|\\tProcesses:\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGPU\\tMemory\\t|\\n|\\t\\tGPU\\t\\t\\t\\t\\t\\t\\tPID\\t\\tType\\t\\tProcess\\tname\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tUsage\\t\\t\\t\\t\\t\\t|\\n|=============================================================================|\\n|\\t\\t\\t\\t0\\t\\t\\t\\t\\t\\t5231\\t\\t\\t\\tC\\t\\t\\tpython\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1677MiB\\t|\\n|\\t\\t\\t\\t0\\t\\t\\t\\t\\t\\t5262\\t\\t\\t\\tC\\t\\t\\tpython\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1677MiB\\t|\\n|\\t\\t\\t\\t1\\t\\t\\t\\t\\t\\t5231\\t\\t\\t\\tC\\t\\t\\tpython\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1677MiB\\t|\\n|\\t\\t\\t\\t1\\t\\t\\t\\t\\t\\t5262\\t\\t\\t\\tC\\t\\t\\tpython\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1677MiB\\t|\\n[...]\\nYet\\tanother\\toption\\tis\\tto\\ttell\\tTensorFlow\\tto\\tgrab\\tmemory\\tonly\\twhen\\tit\\tneeds\\tit.\\tTo\\tdo\\tthis\\tyou\\tmust', 'set\\nconfig.gpu_options.allow_growth\\n\\tto\\t\\nTrue\\n.\\tHowever,\\tTensorFlow\\tnever\\treleases\\tmemory\\tonce\\tit\\nhas\\tgrabbed\\tit\\t(to\\tavoid\\tmemory\\tfragmentation)\\tso\\tyou\\tmay\\tstill\\trun\\tout\\tof\\tmemory\\tafter\\ta\\twhile.\\tIt\\tmay\\nbe\\tharder\\tto\\tguarantee\\ta\\tdeterministic\\tbehavior\\tusing\\tthis\\toption,\\tso\\tin\\tgeneral\\tyou\\tprobably\\twant\\tto\\tstick\\nwith\\tone\\tof\\tthe\\tprevious\\toptions.\\nOkay,\\tnow\\tyou\\thave\\ta\\tworking\\tGPU-enabled\\tTensorFlow\\tinstallation.\\tLet’s\\tsee\\thow\\tto\\t\\nuse\\tit!', 'Placing\\tOperations\\ton\\tDevices\\nThe\\t\\nTensorFlow\\t\\nwhitepaper\\n1\\n\\tpresents\\ta\\tfriendly\\t\\ndynamic\\tplacer\\n\\t\\nalgorithm\\tthat\\tautomagically\\tdistributes\\noperations\\tacross\\tall\\tavailable\\tdevices,\\ttaking\\tinto\\taccount\\tthings\\tlike\\tthe\\tmeasured\\tcomputation\\ttime\\tin\\nprevious\\truns\\tof\\tthe\\tgraph,\\testimations\\tof\\tthe\\tsize\\tof\\tthe\\tinput\\tand\\toutput\\ttensors\\tto\\teach\\toperation,\\tthe\\namount\\tof\\tRAM\\tavailable\\tin\\teach\\tdevice,\\tcommunication\\tdelay\\twhen\\ttransferring\\tdata\\tin\\tand\\tout\\tof\\ndevices,\\thints\\tand\\tconstraints\\tfrom\\tthe\\tuser,\\tand\\tmore.\\tUnfortunately,\\tthis\\tsophisticated\\talgorithm\\tis\\ninternal\\tto\\tGoogle;\\tit\\twas\\tnot\\treleased\\tin\\tthe\\topen\\tsource\\tversion\\tof\\tTensorFlow.\\tThe\\treason\\tit\\twas\\tleft\\nout\\tseems\\tto\\tbe\\tthat\\tin\\tpractice\\ta\\tsmall\\tset\\tof\\tplacement\\trules\\tspecified\\tby\\tthe\\tuser\\tactually\\tresults\\tin\\nmore\\tefficient\\tplacement\\tthan\\twhat\\tthe\\tdynamic\\tplacer\\tis\\tcapable\\tof.\\tHowever,\\tthe\\tTensorFlow\\tteam\\tis\\nworking\\ton\\timproving\\tthe\\tdynamic\\tplacer,\\tand\\tperhaps\\tit\\twill\\teventually\\tbe\\tgood\\tenough\\tto\\tbe\\treleased.', 'Until\\tthen\\tTensorFlow\\trelies\\ton\\tthe\\t\\nsimple\\tplacer\\n,\\twhich\\t(as\\tits\\tname\\tsuggests)\\tis\\tvery\\tbasic.\\nSimple\\tplacement\\nWhenever\\tyou\\trun\\ta\\tgraph,\\tif\\tTensorFlow\\tneeds\\tto\\tevaluate\\ta\\tnode\\tthat\\tis\\tnot\\tplaced\\ton\\ta\\tdevice\\tyet,\\tit\\nuses\\tthe\\tsimple\\tplacer\\tto\\tplace\\tit,\\talong\\twith\\tall\\tother\\tnodes\\tthat\\tare\\tnot\\tplaced\\tyet.\\tThe\\tsimple\\tplacer\\nrespects\\tthe\\tfollowing\\trules:\\nIf\\ta\\tnode\\twas\\talready\\tplaced\\ton\\ta\\tdevice\\tin\\ta\\tprevious\\trun\\tof\\tthe\\tgraph,\\tit\\tis\\tleft\\ton\\tthat\\tdevice.\\nElse,\\tif\\tthe\\tuser\\t\\npinned\\n\\ta\\tnode\\tto\\ta\\tdevice\\t(described\\tnext),\\tthe\\tplacer\\tplaces\\tit\\ton\\tthat\\tdevice.\\nElse,\\tit\\tdefaults\\tto\\tGPU\\t#0,\\tor\\tthe\\tCPU\\tif\\tthere\\tis\\tno\\tGPU.\\nAs\\tyou\\tcan\\tsee,\\tplacing\\toperations\\ton\\tthe\\tappropriate\\tdevice\\tis\\tmostly\\tup\\tto\\tyou.\\tIf\\tyou\\tdon’t\\tdo\\tanything,\\nthe\\twhole\\tgraph\\twill\\tbe\\tplaced\\ton\\tthe\\tdefault\\tdevice.\\tTo\\tpin\\tnodes\\tonto\\ta\\tdevice,\\tyou\\tmust\\tcreate\\ta\\ndevice\\tblock\\tusing\\tthe\\t\\ndevice()\\n\\t\\nfunction.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tpins\\tthe\\tvariable\\t\\na\\n\\tand\\tthe\\nconstant\\t\\nb\\n\\ton\\tthe\\tCPU,\\tbut\\tthe\\tmultiplication\\tnode\\t\\nc', 'c\\n\\tis\\tnot\\tpinned\\ton\\tany\\tdevice,\\tso\\tit\\twill\\tbe\\tplaced\\ton\\nthe\\t\\ndefault\\tdevice:\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/cpu:0\"\\n):\\n\\t\\t\\t\\t\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3.0\\n)\\n\\t\\t\\t\\t\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n4.0\\n)\\nc\\n\\t\\n=\\n\\t\\na\\n\\t\\n*\\n\\t\\nb\\nNOTE\\nThe\\t\\n\"/cpu:0\"\\n\\tdevice\\taggregates\\tall\\tCPUs\\ton\\ta\\tmulti-CPU\\tsystem.\\tThere\\tis\\tcurrently\\tno\\tway\\tto\\tpin\\tnodes\\ton\\tspecific\\tCPUs\\tor\\nto\\tuse\\tjust\\ta\\tsubset\\tof\\tall\\tCPUs.\\nLogging\\tplacements\\nLet’s\\t\\ncheck\\tthat\\tthe\\tsimple\\tplacer\\trespects\\tthe\\tplacement\\tconstraints\\twe\\thave\\tjust\\tdefined.\\tFor\\tthis\\tyou\\ncan\\tset\\t\\nthe\\t\\nlog_device_placement\\n\\toption\\tto\\t\\nTrue\\n;\\tthis\\ttells\\tthe\\tplacer\\tto\\tlog\\ta\\tmessage\\twhenever\\tit', 'places\\ta\\tnode.\\tFor\\texample:\\n>>>\\t\\nconfig\\n\\t\\n=\\n\\t\\ntf\\n.\\nConfigProto\\n()\\n>>>\\t\\nconfig\\n.\\nlog_device_placement\\n\\t\\n=\\n\\t\\nTrue\\n>>>\\t\\nsess\\n\\t\\n=\\n\\t\\ntf\\n.\\nSession\\n(\\nconfig\\n=\\nconfig\\n)\\nI\\t[...]\\tCreating\\tTensorFlow\\tdevice\\t(/gpu:0)\\t->\\t(device:\\t0,\\tname:\\tGRID\\tK520,\\npci\\tbus\\tid:\\t0000:00:03.0)\\n[...]\\n>>>\\t\\nx\\n.\\ninitializer\\n.\\nrun\\n(\\nsession\\n=\\nsess\\n)\\nI\\t[...]\\ta:\\t/job:localhost/replica:0/task:0/cpu:0\\nI\\t[...]\\ta/read:\\t/job:localhost/replica:0/task:0/cpu:0\\nI\\t[...]\\tmul:\\t/job:localhost/replica:0/task:0/gpu:0\\nI\\t[...]\\ta/Assign:\\t/job:localhost/replica:0/task:0/cpu:0\\nI\\t[...]\\tb:\\t/job:localhost/replica:0/task:0/cpu:0\\nI\\t[...]\\ta/initial_value:\\t/job:localhost/replica:0/task:0/cpu:0\\n>>>\\t\\nsess\\n.\\nrun\\n(\\nc\\n)\\n12\\nThe\\tlines\\tstarting\\twith\\t\\n\"I\"\\n\\tfor\\tInfo\\tare\\tthe\\tlog\\tmessages.\\tWhen\\twe\\tcreate\\ta\\tsession,\\tTensorFlow\\tlogs\\ta\\nmessage\\tto\\ttell\\tus\\tthat\\tit\\thas\\tfound\\ta\\tGPU\\tcard\\t(in\\tthis\\tcase\\tthe\\tGrid\\tK520\\tcard).\\tThen\\tthe\\tfirst\\ttime\\twe\\nrun\\tthe\\tgraph\\t(in\\tthis\\tcase\\twhen\\tinitializing\\tthe\\tvariable\\t\\na\\n),\\tthe\\tsimple\\tplacer\\tis\\trun\\tand\\tplaces\\teach\\tnode', 'on\\tthe\\tdevice\\tit\\twas\\tassigned\\tto.\\tAs\\texpected,\\tthe\\tlog\\tmessages\\tshow\\tthat\\tall\\tnodes\\tare\\tplaced\\ton\\n\"/cpu:0\"\\n\\texcept\\tthe\\tmultiplication\\tnode,\\twhich\\tends\\tup\\ton\\tthe\\tdefault\\tdevice\\t\\n\"/gpu:0\"\\n\\t(you\\tcan\\tsafely\\nignore\\tthe\\tprefix\\t\\n/job:localhost/replica:0/task:0\\n\\tfor\\tnow;\\twe\\twill\\ttalk\\tabout\\tit\\tin\\ta\\tmoment).\\nNotice\\tthat\\tthe\\tsecond\\ttime\\twe\\trun\\tthe\\tgraph\\t(to\\tcompute\\t\\nc\\n),\\tthe\\tplacer\\tis\\tnot\\tused\\tsince\\tall\\tthe\\tnodes\\nTensorFlow\\t\\nneeds\\tto\\tcompute\\t\\nc\\n\\tare\\talready\\tplaced.\\nDynamic\\tplacement\\tfunction\\nWhen\\tyou\\t\\ncreate\\ta\\tdevice\\tblock,\\tyou\\tcan\\tspecify\\ta\\tfunction\\tinstead\\tof\\ta\\tdevice\\tname.\\tTensorFlow\\twill\\ncall\\tthis\\tfunction\\tfor\\teach\\toperation\\tit\\tneeds\\tto\\tplace\\tin\\tthe\\tdevice\\tblock,\\tand\\tthe\\tfunction\\tmust\\treturn\\tthe\\nname\\tof\\tthe\\tdevice\\tto\\tpin\\tthe\\toperation\\ton.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tpins\\tall\\tthe\\t\\nvariable\\tnodes\\tto\\n\"/cpu:0\"\\n\\t(in\\tthis\\tcase\\tjust\\tthe\\tvariable\\t\\na\\n)\\tand\\tall\\tother\\tnodes\\tto\\t\\n\"/gpu:0\"\\n:\\ndef\\n\\t\\nvariables_on_cpu\\n(\\nop\\n):\\n\\t\\t\\t\\t\\nif\\n\\t\\nop\\n.\\ntype\\n\\t\\n==\\n\\t\\n\"Variable\"\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\n\"/cpu:0\"\\n\\t\\t\\t\\t\\nelse\\n:', 'else\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\n\"/gpu:0\"\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\nvariables_on_cpu\\n):\\n\\t\\t\\t\\t\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3.0\\n)\\n\\t\\t\\t\\t\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n4.0\\n)\\n\\t\\t\\t\\t\\nc\\n\\t\\n=\\n\\t\\na\\n\\t\\n*\\n\\t\\nb\\nYou\\tcan\\teasily\\timplement\\tmore\\tcomplex\\talgorithms,\\tsuch\\tas\\tpinning\\tvariables\\tacross\\tGPUs\\tin\\ta\\tround-\\nrobin\\tfashion.\\nOperations\\tand\\tkernels\\nFor\\ta\\tTensorFlow\\toperation\\tto\\trun\\ton\\ta\\tdevice,\\tit\\tneeds\\tto\\thave\\tan\\timplementation\\tfor\\tthat\\tdevice;\\tthis\\tis\\ncalled\\t\\na\\t\\nkernel\\n.\\tMany\\toperations\\thave\\tkernels\\tfor\\tboth\\tCPUs\\tand\\tGPUs,\\tbut\\tnot\\tall\\tof\\tthem.\\tFor\\texample,\\nTensorFlow\\tdoes\\tnot\\thave\\ta\\tGPU\\tkernel\\tfor\\tinteger\\tvariables,\\tso\\tthe\\tfollowing\\tcode\\twill\\tfail\\twhen\\nTensorFlow\\ttries\\tto\\tplace\\tthe\\tvariable\\t\\ni\\n\\ton\\tGPU\\t#0:', '>>>\\t\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/gpu:0\"\\n):\\n...\\t\\n\\t\\t\\t\\t\\ni\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3\\n)\\n[...]\\n>>>\\t\\nsess\\n.\\nrun\\n(\\ni\\n.\\ninitializer\\n)\\nTraceback\\t(most\\trecent\\tcall\\tlast):\\n[...]\\ntensorflow.python.framework.errors.InvalidArgumentError:\\tCannot\\tassign\\ta\\tdevice\\nto\\tnode\\t\\'Variable\\':\\tCould\\tnot\\tsatisfy\\texplicit\\tdevice\\tspecification\\nNote\\tthat\\tTensorFlow\\tinfers\\tthat\\tthe\\tvariable\\tmust\\tbe\\tof\\ttype\\t\\nint32\\n\\t\\nsince\\tthe\\tinitialization\\tvalue\\tis\\tan\\ninteger.\\tIf\\tyou\\tchange\\tthe\\tinitialization\\tvalue\\tto\\t\\n3.0\\n\\tinstead\\tof\\t\\n3\\n,\\tor\\tif\\tyou\\texplicitly\\tset\\ndtype=tf.float32\\n\\twhen\\tcreating\\tthe\\tvariable,\\teverything\\twill\\twork\\tfine.\\nSoft\\tplacement\\nBy\\t\\ndefault,\\tif\\tyou\\ttry\\tto\\tpin\\tan\\toperation\\ton\\ta\\tdevice\\tfor\\twhich\\tthe\\toperation\\thas\\tno\\tkernel,\\tyou\\tget\\tthe\\nexception\\tshown\\tearlier\\twhen\\tTensorFlow\\ttries\\tto\\tplace\\tthe\\toperation\\ton\\tthe\\tdevice.\\tIf\\tyou\\tprefer\\nTensorFlow\\tto\\tfall\\tback\\tto\\tthe\\tCPU\\tinstead,\\tyou\\tcan\\tset\\tthe\\t\\nallow_soft_placement\\n\\tconfiguration\\t\\noption\\nto\\t\\nTrue\\n:\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/gpu:0\"\\n):\\n\\t\\t\\t\\t\\ni\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3\\n)\\nconfig\\n\\t\\n=', '=\\n\\t\\ntf\\n.\\nConfigProto\\n()\\nconfig\\n.\\nallow_soft_placement\\n\\t\\n=\\n\\t\\nTrue\\nsess\\n\\t\\n=\\n\\t\\ntf\\n.\\nSession\\n(\\nconfig\\n=\\nconfig\\n)\\nsess\\n.\\nrun\\n(\\ni\\n.\\ninitializer\\n)\\n\\t\\t\\n#\\tthe\\tplacer\\truns\\tand\\tfalls\\tback\\tto\\t/cpu:0\\nSo\\tfar\\twe\\thave\\tdiscussed\\thow\\tto\\tplace\\tnodes\\ton\\tdifferent\\tdevices.\\tNow\\tlet’s\\tsee\\thow\\tTensorFlow\\twill\\nrun\\tthese\\tnodes\\t\\nin\\tparallel.', 'Parallel\\tExecution\\nWhen\\t\\nTensorFlow\\truns\\ta\\tgraph,\\tit\\tstarts\\tby\\tfinding\\tout\\tthe\\tlist\\tof\\tnodes\\tthat\\tneed\\tto\\tbe\\tevaluated,\\tand\\tit\\ncounts\\thow\\tmany\\tdependencies\\teach\\tof\\tthem\\thas.\\tTensorFlow\\tthen\\tstarts\\tevaluating\\tthe\\tnodes\\twith\\tzero\\ndependencies\\t(i.e.,\\tsource\\tnodes).\\tIf\\tthese\\tnodes\\tare\\tplaced\\ton\\tseparate\\tdevices,\\tthey\\tobviously\\tget\\nevaluated\\tin\\tparallel.\\tIf\\tthey\\tare\\tplaced\\ton\\tthe\\tsame\\tdevice,\\tthey\\tget\\tevaluated\\tin\\tdifferent\\tthreads,\\tso\\tthey\\nmay\\trun\\tin\\tparallel\\ttoo\\t(in\\tseparate\\tGPU\\tthreads\\tor\\tCPU\\tcores).\\nTensorFlow\\tmanages\\ta\\t\\nthread\\tpool\\ton\\teach\\tdevice\\tto\\tparallelize\\toperations\\t(see\\t\\nFigure\\t12-5\\n).\\tThese\\tare\\ncalled\\tthe\\t\\ninter-op\\tthread\\tpools\\n.\\tSome\\toperations\\thave\\tmultithreaded\\tkernels:\\tthey\\tcan\\tuse\\tother\\tthread\\npools\\t(one\\tper\\tdevice)\\tcalled\\tthe\\t\\nintra-op\\tthread\\tpools\\n.\\nFigure\\t12-5.\\t\\nParallelized\\texecution\\tof\\ta\\tTensorFlow\\tgraph\\nFor\\texample,\\tin\\t\\nFigure\\t12-5\\n,\\toperations\\tA,\\tB,\\tand\\tC\\tare\\tsource\\tops,\\t\\nso\\tthey\\tcan\\timmediately\\tbe', 'evaluated.\\tOperations\\tA\\tand\\tB\\tare\\tplaced\\ton\\tGPU\\t#0,\\tso\\tthey\\tare\\tsent\\tto\\tthis\\tdevice’s\\tinter-op\\tthread\\npool,\\tand\\timmediately\\tevaluated\\tin\\tparallel.\\tOperation\\tA\\thappens\\tto\\thave\\ta\\tmultithreaded\\tkernel;\\tits\\ncomputations\\tare\\tsplit\\tin\\tthree\\tparts,\\twhich\\tare\\texecuted\\tin\\tparallel\\tby\\tthe\\tintra-op\\tthread\\tpool.\\tOperation\\nC\\tgoes\\tto\\tGPU\\t#1’s\\tinter-op\\tthread\\tpool.\\nAs\\tsoon\\tas\\toperation\\tC\\tfinishes,\\tthe\\tdependency\\tcounters\\tof\\toperations\\tD\\tand\\tE\\twill\\tbe\\tdecremented\\tand\\nwill\\tboth\\treach\\t0,\\tso\\tboth\\toperations\\twill\\tbe\\tsent\\tto\\tthe\\tinter-op\\tthread\\tpool\\tto\\tbe\\texecuted.', 'TIP\\nYou\\tcan\\tcontrol\\tthe\\tnumber\\tof\\tthreads\\tper\\tinter-op\\tpool\\tby\\tsetting\\tthe\\t\\ninter_op_parallelism_threads\\n\\toption.\\t\\nNote\\tthat\\tthe\\nfirst\\tsession\\tyou\\tstart\\tcreates\\tthe\\tinter-op\\tthread\\tpools.\\tAll\\tother\\tsessions\\twill\\tjust\\treuse\\tthem\\tunless\\tyou\\tset\\tthe\\nuse_per_session_threads\\n\\toption\\tto\\t\\nTrue\\n.\\tYou\\tcan\\tcontrol\\tthe\\tnumber\\tof\\t\\nthreads\\tper\\tintra-op\\tpool\\tby\\tsetting\\tthe\\nintra_op_parallelism_threads\\n\\toption.', 'Control\\tDependencies\\nIn\\t\\nsome\\tcases,\\tit\\tmay\\tbe\\twise\\tto\\tpostpone\\tthe\\tevaluation\\tof\\tan\\toperation\\teven\\tthough\\tall\\tthe\\toperations\\tit\\ndepends\\ton\\thave\\tbeen\\texecuted.\\tFor\\texample,\\tif\\tit\\tuses\\ta\\tlot\\tof\\tmemory\\tbut\\tits\\tvalue\\tis\\tneeded\\tonly\\tmuch\\nfurther\\tin\\tthe\\tgraph,\\tit\\twould\\tbe\\tbest\\tto\\tevaluate\\tit\\tat\\tthe\\tlast\\tmoment\\tto\\tavoid\\tneedlessly\\toccupying\\tRAM\\nthat\\tother\\toperations\\tmay\\tneed.\\tAnother\\texample\\tis\\ta\\tset\\tof\\toperations\\tthat\\tdepend\\ton\\tdata\\tlocated\\toutside\\nof\\tthe\\tdevice.\\tIf\\tthey\\tall\\trun\\tat\\tthe\\tsame\\ttime,\\tthey\\tmay\\tsaturate\\tthe\\tdevice’s\\tcommunication\\tbandwidth,\\nand\\tthey\\twill\\tend\\tup\\tall\\twaiting\\ton\\tI/O.\\tOther\\toperations\\tthat\\tneed\\tto\\tcommunicate\\tdata\\twill\\talso\\tbe\\nblocked.\\tIt\\twould\\tbe\\tpreferable\\tto\\texecute\\tthese\\tcommunication-heavy\\toperations\\tsequentially,\\tallowing\\nthe\\tdevice\\tto\\tperform\\tother\\toperations\\tin\\tparallel.\\nTo\\tpostpone\\tevaluation\\tof\\tsome\\tnodes,\\ta\\tsimple\\tsolution\\tis\\tto\\tadd\\t\\ncontrol\\tdependencies\\n.\\tFor\\texample,\\nthe\\tfollowing\\tcode\\ttells\\tTensorFlow\\tto\\tevaluate\\t\\nx\\n\\tand\\t\\ny\\n\\tonly\\tafter\\t\\na\\n\\tand\\t\\nb', 'a\\n\\tand\\t\\nb\\n\\thave\\tbeen\\t\\nevaluated:\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n1.0\\n)\\nb\\n\\t\\n=\\n\\t\\na\\n\\t\\n+\\n\\t\\n2.0\\nwith\\n\\t\\ntf\\n.\\ncontrol_dependencies\\n([\\na\\n,\\n\\t\\nb\\n]):\\n\\t\\t\\t\\t\\nx\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n3.0\\n)\\n\\t\\t\\t\\t\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n4.0\\n)\\nz\\n\\t\\n=\\n\\t\\nx\\n\\t\\n+\\n\\t\\ny\\nObviously,\\tsince\\t\\nz\\n\\tdepends\\ton\\t\\nx\\n\\tand\\t\\ny\\n,\\tevaluating\\t\\nz\\n\\talso\\timplies\\twaiting\\tfor\\t\\na\\n\\tand\\t\\nb\\n\\tto\\tbe\\tevaluated,\\neven\\tthough\\tit\\tis\\tnot\\texplicitly\\tin\\tthe\\t\\ncontrol_dependencies()\\n\\tblock.\\tAlso,\\tsince\\t\\nb\\n\\tdepends\\ton\\t\\na\\n,\\twe\\ncould\\tsimplify\\tthe\\tpreceding\\tcode\\tby\\tjust\\tcreating\\ta\\tcontrol\\tdependency\\ton\\t\\n[b]\\n\\tinstead\\tof\\t\\n[a,\\tb]\\n,\\tbut\\tin\\nsome\\tcases\\t“explicit\\tis\\tbetter\\tthan\\timplicit.”\\nGreat!\\tNow\\tyou\\tknow:\\nHow\\tto\\tplace\\toperations\\ton\\tmultiple\\tdevices\\tin\\tany\\tway\\tyou\\tplease\\nHow\\tthese\\toperations\\tget\\texecuted\\tin\\tparallel\\nHow\\tto\\tcreate\\tcontrol\\tdependencies\\tto\\toptimize\\t\\nparallel\\texecution\\nIt’s\\ttime\\tto\\tdistribute\\tcomputations\\tacross\\tmultiple\\tservers!', 'Multiple\\tDevices\\tAcross\\tMultiple\\tServers\\nTo\\t\\nrun\\ta\\tgraph\\tacross\\tmultiple\\tservers,\\tyou\\tfirst\\tneed\\tto\\tdefine\\ta\\t\\ncluster\\n.\\tA\\t\\ncluster\\tis\\tcomposed\\tof\\tone\\tor\\nmore\\tTensorFlow\\tservers,\\t\\ncalled\\t\\ntasks\\n,\\ttypically\\tspread\\tacross\\tseveral\\tmachines\\t(see\\t\\nFigure\\t12-6\\n).\\tEach\\ntask\\tbelongs\\tto\\t\\na\\t\\njob\\n.\\tA\\tjob\\tis\\tjust\\ta\\tnamed\\tgroup\\tof\\ttasks\\tthat\\ttypically\\thave\\ta\\tcommon\\trole,\\tsuch\\tas\\nkeeping\\ttrack\\tof\\tthe\\tmodel\\tparameters\\t(such\\ta\\tjob\\tis\\tusually\\t\\nnamed\\t\\n\"ps\"\\n\\tfor\\t\\nparameter\\tserver\\n),\\tor\\nperforming\\tcomputations\\t(such\\ta\\tjob\\tis\\tusually\\t\\nnamed\\t\\n\"worker\"\\n).\\nFigure\\t12-6.\\t\\nTensorFlow\\tcluster\\nThe\\tfollowing\\t\\ncluster\\tspecification\\n\\t\\ndefines\\ttwo\\tjobs,\\t\\n\"ps\"\\n\\tand\\t\\n\"worker\"\\n,\\tcontaining\\tone\\ttask\\tand\\ttwo\\ntasks,\\trespectively.\\tIn\\tthis\\texample,\\tmachine\\tA\\thosts\\ttwo\\tTensorFlow\\tservers\\t(i.e.,\\ttasks),\\tlistening\\ton\\ndifferent\\tports:\\tone\\tis\\tpart\\tof\\tthe\\t\\n\"ps\"\\n\\tjob,\\tand\\tthe\\tother\\tis\\tpart\\tof\\tthe\\t\\n\"worker\"\\n\\tjob.\\tMachine\\tB\\tjust\\thosts\\none\\tTensorFlow\\tserver,\\tpart\\tof\\tthe\\t\\n\"worker\"\\n\\tjob.\\ncluster_spec\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nClusterSpec\\n({', '({\\n\\t\\t\\t\\t\\n\"ps\"\\n:\\n\\t\\n[\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\"machine-a.example.com:2221\"\\n,\\n\\t\\t\\n#\\t/job:ps/task:0\\n\\t\\t\\t\\t\\n],\\n\\t\\t\\t\\t\\n\"worker\"\\n:\\n\\t\\n[\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\"machine-a.example.com:2222\"\\n,\\n\\t\\t\\n#\\t/job:worker/task:0\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\"machine-b.example.com:2222\"\\n,\\n\\t\\t\\n#\\t/job:worker/task:1\\n\\t\\t\\t\\t\\n]})\\nTo\\tstart\\ta\\tTensorFlow\\tserver,\\t\\nyou\\tmust\\tcreate\\ta\\t\\nServer\\n\\tobject,\\tpassing\\tit\\tthe\\tcluster\\tspecification\\t(so\\tit\\ncan\\tcommunicate\\twith\\tother\\tservers)\\tand\\tits\\town\\tjob\\tname\\tand\\ttask\\tnumber.\\tFor\\texample,\\tto\\tstart\\tthe\\tfirst\\nworker\\ttask,\\tyou\\twould\\trun\\tthe\\tfollowing\\tcode\\ton\\tmachine\\tA:\\nserver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nServer\\n(\\ncluster_spec\\n,\\n\\t\\njob_name\\n=\\n\"worker\"\\n,\\n\\t\\ntask_index\\n=\\n0\\n)', 'It\\tis\\tusually\\tsimpler\\tto\\tjust\\trun\\tone\\ttask\\tper\\tmachine,\\tbut\\tthe\\tprevious\\texample\\tdemonstrates\\tthat\\nTensorFlow\\tallows\\tyou\\tto\\trun\\tmultiple\\ttasks\\ton\\tthe\\tsame\\tmachine\\tif\\tyou\\twant.\\n2\\n\\tIf\\tyou\\thave\\tseveral\\nservers\\ton\\tone\\tmachine,\\tyou\\twill\\tneed\\tto\\tensure\\tthat\\tthey\\tdon’t\\tall\\ttry\\tto\\tgrab\\tall\\tthe\\tRAM\\tof\\tevery\\tGPU,\\nas\\texplained\\tearlier.\\tFor\\texample,\\tin\\t\\nFigure\\t12-6\\n\\tthe\\t\\n\"ps\"\\n\\ttask\\tdoes\\tnot\\tsee\\tthe\\tGPU\\tdevices,\\tsince\\npresumably\\tits\\tprocess\\twas\\tlaunched\\twith\\t\\nCUDA_VISIBLE_DEVICES=\"\"\\n.\\tNote\\tthat\\tthe\\tCPU\\tis\\tshared\\tby\\nall\\ttasks\\tlocated\\ton\\tthe\\tsame\\tmachine.\\nIf\\tyou\\twant\\tthe\\tprocess\\tto\\tdo\\tnothing\\tother\\tthan\\trun\\tthe\\tTensorFlow\\tserver,\\tyou\\tcan\\tblock\\tthe\\tmain\\tthread\\nby\\ttelling\\tit\\tto\\twait\\tfor\\tthe\\tserver\\tto\\tfinish\\tusing\\tthe\\t\\njoin()\\n\\t\\nmethod\\t(otherwise\\tthe\\tserver\\twill\\tbe\\tkilled\\nas\\tsoon\\tas\\tyour\\tmain\\tthread\\texits).\\tSince\\tthere\\tis\\tcurrently\\tno\\tway\\tto\\tstop\\tthe\\tserver,\\tthis\\twill\\tactually\\nblock\\tforever:\\nserver\\n.\\njoin\\n()\\n\\t\\t\\n#\\tblocks\\tuntil\\tthe\\tserver\\tstops\\t(i.e.,\\tnever)', 'Opening\\ta\\tSession\\nOnce\\t\\nall\\tthe\\ttasks\\tare\\tup\\tand\\trunning\\t(doing\\tnothing\\tyet),\\tyou\\tcan\\topen\\ta\\tsession\\ton\\tany\\tof\\tthe\\tservers,\\nfrom\\ta\\tclient\\tlocated\\tin\\tany\\tprocess\\ton\\tany\\tmachine\\t(even\\tfrom\\ta\\tprocess\\trunning\\tone\\tof\\tthe\\ttasks),\\tand\\nuse\\tthat\\tsession\\tlike\\ta\\tregular\\tlocal\\tsession.\\t\\nFor\\texample:\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n1.0\\n)\\nb\\n\\t\\n=\\n\\t\\na\\n\\t\\n+\\n\\t\\n2\\nc\\n\\t\\n=\\n\\t\\na\\n\\t\\n*\\n\\t\\n3\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\n\"grpc://machine-b.example.com:2222\"\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nprint\\n(\\nc\\n.\\neval\\n())\\n\\t\\t\\n#\\t9.0\\nThis\\tclient\\tcode\\tfirst\\tcreates\\ta\\tsimple\\tgraph,\\tthen\\topens\\ta\\tsession\\ton\\tthe\\tTensorFlow\\tserver\\tlocated\\ton\\nmachine\\tB\\t(which\\twe\\twill\\tcall\\tthe\\t\\nmaster\\n),\\tand\\tinstructs\\tit\\tto\\tevaluate\\t\\nc\\n.\\tThe\\tmaster\\tstarts\\tby\\tplacing\\tthe\\noperations\\ton\\tthe\\tappropriate\\tdevices.\\tIn\\tthis\\texample,\\tsince\\twe\\tdid\\tnot\\tpin\\tany\\toperation\\ton\\tany\\tdevice,\\nthe\\tmaster\\tsimply\\tplaces\\tthem\\tall\\ton\\tits\\town\\tdefault\\tdevice\\t—\\tin\\tthis\\tcase,\\tmachine\\tB’s\\tGPU\\tdevice.\\nThen\\tit\\tjust\\tevaluates\\t\\nc\\n\\tas\\tinstructed\\tby\\tthe\\tclient,\\tand\\tit\\treturns\\tthe\\tresult.', 'The\\tMaster\\tand\\tWorker\\tServices\\nThe\\t\\nclient\\tuses\\tthe\\t\\ngRPC\\n\\tprotocol\\t(\\nGoogle\\tRemote\\tProcedure\\tCall\\n)\\tto\\tcommunicate\\twith\\tthe\\tserver.\\tThis\\nis\\tan\\tefficient\\topen\\tsource\\tframework\\tto\\tcall\\tremote\\tfunctions\\tand\\tget\\ttheir\\toutputs\\tacross\\ta\\tvariety\\tof\\nplatforms\\tand\\tlanguages.\\n3\\n\\tIt\\tis\\tbased\\ton\\tHTTP2,\\twhich\\topens\\ta\\tconnection\\tand\\tleaves\\tit\\topen\\tduring\\tthe\\nwhole\\tsession,\\tallowing\\tefficient\\tbidirectional\\tcommunication\\tonce\\tthe\\tconnection\\tis\\testablished.\\tData\\tis\\ntransmitted\\tin\\tthe\\tform\\tof\\t\\nprotocol\\tbuffers\\n,\\tanother\\topen\\tsource\\tGoogle\\ttechnology.\\tThis\\tis\\ta\\tlightweight\\nbinary\\tdata\\tinterchange\\tformat.\\nWARNING\\nAll\\tservers\\tin\\ta\\tTensorFlow\\tcluster\\tmay\\tcommunicate\\twith\\tany\\tother\\tserver\\tin\\tthe\\tcluster,\\tso\\tmake\\tsure\\tto\\topen\\tthe\\tappropriate\\nports\\ton\\tyour\\tfirewall.\\nEvery\\tTensorFlow\\tserver\\tprovides\\ttwo\\tservices:\\tthe\\t\\nmaster\\tservice\\n\\tand\\tthe\\t\\nworker\\tservice\\n.\\tThe\\tmaster\\nservice\\tallows\\tclients\\tto\\topen\\tsessions\\tand\\tuse\\tthem\\tto\\trun\\tgraphs.\\tIt\\tcoordinates\\tthe\\tcomputations\\tacross', 'tasks,\\trelying\\ton\\tthe\\tworker\\tservice\\tto\\tactually\\texecute\\tcomputations\\ton\\tother\\ttasks\\tand\\tget\\ttheir\\tresults.\\nThis\\tarchitecture\\tgives\\tyou\\ta\\tlot\\tof\\tflexibility.\\tOne\\tclient\\tcan\\tconnect\\tto\\tmultiple\\tservers\\tby\\topening\\nmultiple\\tsessions\\tin\\tdifferent\\tthreads.\\tOne\\tserver\\tcan\\thandle\\tmultiple\\tsessions\\tsimultaneously\\tfrom\\tone\\tor\\nmore\\tclients.\\tYou\\tcan\\trun\\tone\\tclient\\tper\\ttask\\t(typically\\twithin\\tthe\\tsame\\tprocess),\\tor\\tjust\\tone\\tclient\\tto\\ncontrol\\tall\\ttasks.\\tAll\\toptions\\tare\\topen.', 'Pinning\\tOperations\\tAcross\\tTasks\\nYou\\t\\ncan\\tuse\\tdevice\\tblocks\\tto\\tpin\\toperations\\ton\\tany\\tdevice\\tmanaged\\tby\\tany\\ttask,\\tby\\tspecifying\\tthe\\tjob\\nname,\\ttask\\tindex,\\tdevice\\ttype,\\tand\\tdevice\\tindex.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tpins\\t\\na\\n\\tto\\tthe\\tCPU\\tof\\tthe\\nfirst\\ttask\\tin\\tthe\\t\\n\"ps\"\\n\\tjob\\t(that’s\\tthe\\tCPU\\ton\\tmachine\\tA),\\tand\\tit\\tpins\\t\\nb\\n\\tto\\tthe\\tsecond\\tGPU\\tmanaged\\tby\\tthe\\nfirst\\ttask\\tof\\tthe\\t\\n\"worker\"\\n\\tjob\\t(that’s\\tGPU\\t#1\\ton\\tmachine\\tA).\\tFinally,\\t\\nc\\n\\tis\\tnot\\tpinned\\tto\\tany\\tdevice,\\tso\\tthe\\nmaster\\tplaces\\tit\\ton\\tits\\town\\tdefault\\tdevice\\t\\n(machine\\tB’s\\tGPU\\t#0\\tdevice).\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/job:ps/task:0/cpu:0\"\\n)\\n\\t\\t\\t\\t\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nconstant\\n(\\n1.0\\n)\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/job:worker/task:0/gpu:1\"\\n)\\n\\t\\t\\t\\t\\nb\\n\\t\\n=\\n\\t\\na\\n\\t\\n+\\n\\t\\n2\\nc\\n\\t\\n=\\n\\t\\na\\n\\t\\n+\\n\\t\\nb\\nAs\\tearlier,\\tif\\tyou\\tomit\\tthe\\tdevice\\ttype\\tand\\tindex,\\tTensorFlow\\twill\\tdefault\\tto\\tthe\\ttask’s\\tdefault\\tdevice;\\tfor\\nexample,\\tpinning\\tan\\toperation\\tto\\t\\n\"/job:ps/task:0\"\\n\\twill\\tplace\\tit\\ton\\tthe\\tdefault\\tdevice\\tof\\tthe\\tfirst\\ttask\\tof\\nthe\\t\\n\"ps\"\\n\\tjob\\t(machine\\tA’s\\tCPU).\\tIf\\tyou\\talso\\tomit\\tthe\\ttask\\tindex\\t(e.g.,', '\"/job:ps\"\\n),\\tTensorFlow\\tdefaults\\nto\\t\\n\"/task:0\"\\n.\\tIf\\tyou\\tomit\\tthe\\tjob\\tname\\tand\\tthe\\ttask\\tindex,\\tTensorFlow\\tdefaults\\tto\\tthe\\tsession’s\\tmaster\\ntask.', 'Sharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nAs\\twe\\t\\nwill\\tsee\\tshortly,\\ta\\tcommon\\tpattern\\twhen\\ttraining\\ta\\tneural\\tnetwork\\ton\\ta\\tdistributed\\tsetup\\tis\\tto\\tstore\\nthe\\tmodel\\tparameters\\ton\\ta\\tset\\tof\\tparameter\\tservers\\t(i.e.,\\tthe\\ttasks\\tin\\tthe\\t\\n\"ps\"\\n\\tjob)\\twhile\\tother\\ttasks\\tfocus\\non\\tcomputations\\t(i.e.,\\tthe\\ttasks\\tin\\tthe\\t\\n\"worker\"\\n\\tjob).\\tFor\\tlarge\\tmodels\\twith\\tmillions\\tof\\tparameters,\\tit\\tis\\nuseful\\tto\\tshard\\tthese\\tparameters\\tacross\\tmultiple\\tparameter\\tservers,\\tto\\treduce\\tthe\\trisk\\tof\\tsaturating\\ta\\nsingle\\tparameter\\tserver’s\\tnetwork\\tcard.\\tIf\\tyou\\twere\\tto\\tmanually\\tpin\\tevery\\tvariable\\tto\\ta\\tdifferent\\nparameter\\tserver,\\tit\\twould\\tbe\\tquite\\ttedious.\\tFortunately,\\tTensorFlow\\tprovides\\tthe\\nreplica_device_setter()\\n\\tfunction,\\t\\nwhich\\tdistributes\\tvariables\\tacross\\tall\\tthe\\t\\n\"ps\"\\n\\ttasks\\tin\\ta\\tround-\\nrobin\\tfashion.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tpins\\tfive\\tvariables\\tto\\ttwo\\tparameter\\tservers:\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\ntf\\n.\\ntrain\\n.\\nreplica_device_setter\\n(\\nps_tasks\\n=\\n2\\n):\\n\\t\\t\\t\\t\\nv1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n1.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:0', 'v2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n2.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:1\\n\\t\\t\\t\\t\\nv3\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:0\\n\\t\\t\\t\\t\\nv4\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n4.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:1\\n\\t\\t\\t\\t\\nv5\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n5.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:0\\nInstead\\tof\\tpassing\\tthe\\tnumber\\tof\\t\\nps_tasks\\n,\\tyou\\tcan\\tpass\\tthe\\tcluster\\tspec\\t\\ncluster=cluster_spec\\n\\tand\\nTensorFlow\\twill\\tsimply\\tcount\\tthe\\tnumber\\tof\\ttasks\\tin\\tthe\\t\\n\"ps\"\\n\\tjob.\\nIf\\tyou\\tcreate\\tother\\toperations\\tin\\tthe\\tblock,\\tbeyond\\tjust\\tvariables,\\tTensorFlow\\tautomatically\\tpins\\tthem\\tto\\n\"/job:worker\"\\n,\\twhich\\twill\\tdefault\\tto\\tthe\\tfirst\\tdevice\\tmanaged\\tby\\tthe\\tfirst\\ttask\\tin\\tthe\\t\\n\"worker\"\\n\\tjob.\\tYou\\ncan\\tpin\\tthem\\tto\\tanother\\tdevice\\tby\\tsetting\\tthe\\t\\nworker_device\\n\\t\\nparameter,\\tbut\\ta\\tbetter\\tapproach\\tis\\tto\\tuse\\nembedded\\tdevice\\tblocks.\\tAn\\tinner\\tdevice\\tblock\\tcan\\toverride\\tthe\\tjob,\\ttask,\\tor\\tdevice\\tdefined\\tin\\tan\\touter\\nblock.\\tFor\\texample:\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\ntf\\n.\\ntrain\\n.\\nreplica_device_setter\\n(\\nps_tasks\\n=\\n2\\n)):\\n\\t\\t\\t\\t\\nv1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n1.0\\n)', '1.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:0\\t(+\\tdefaults\\tto\\t/cpu:0)\\n\\t\\t\\t\\t\\nv2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n2.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:1\\t(+\\tdefaults\\tto\\t/cpu:0)\\n\\t\\t\\t\\t\\nv3\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n3.0\\n)\\n\\t\\t\\n#\\tpinned\\tto\\t/job:ps/task:0\\t(+\\tdefaults\\tto\\t/cpu:0)\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\ns\\n\\t\\n=\\n\\t\\nv1\\n\\t\\n+\\n\\t\\nv2\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tpinned\\tto\\t/job:worker\\t(+\\tdefaults\\tto\\ttask:0/gpu:0)\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/gpu:1\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\np1\\n\\t\\n=\\n\\t\\n2\\n\\t\\n*\\n\\t\\ns\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tpinned\\tto\\t/job:worker/gpu:1\\t(+\\tdefaults\\tto\\t/task:0)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/task:1\"\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\np2\\n\\t\\n=\\n\\t\\n3\\n\\t\\n*\\n\\t\\ns\\n\\t\\t\\t\\t\\t\\n#\\tpinned\\tto\\t/job:worker/task:1/gpu:1\\nNOTE\\nThis\\texample\\tassumes\\tthat\\tthe\\tparameter\\tservers\\tare\\tCPU-only,\\twhich\\tis\\ttypically\\tthe\\tcase\\tsince\\tthey\\tonly\\tneed\\tto\\tstore\\tand\\ncommunicate\\tparameters,\\tnot\\tperform\\tintensive\\tcomputations.', 'Sharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\nWhen\\t\\nyou\\tare\\tusing\\ta\\tplain\\t\\nlocal\\tsession\\n\\t\\n(not\\tthe\\tdistributed\\tkind),\\teach\\tvariable’s\\tstate\\tis\\tmanaged\\tby\\nthe\\tsession\\titself;\\tas\\tsoon\\tas\\tit\\tends,\\tall\\tvariable\\tvalues\\tare\\tlost.\\tMoreover,\\tmultiple\\tlocal\\tsessions\\tcannot\\nshare\\tany\\tstate,\\teven\\tif\\tthey\\tboth\\trun\\tthe\\tsame\\tgraph;\\teach\\tsession\\thas\\tits\\town\\tcopy\\tof\\tevery\\tvariable\\t(as\\nwe\\tdiscussed\\tin\\t\\nChapter\\t9\\n).\\tIn\\tcontrast,\\twhen\\tyou\\tare\\tusing\\t\\ndistributed\\tsessions\\n,\\t\\nvariable\\tstate\\tis\\nmanaged\\tby\\t\\nresource\\tcontainers\\n\\tlocated\\ton\\tthe\\tcluster\\titself,\\tnot\\tby\\tthe\\tsessions.\\tSo\\tif\\tyou\\tcreate\\ta\\nvariable\\tnamed\\t\\nx\\n\\tusing\\tone\\tclient\\tsession,\\tit\\twill\\tautomatically\\tbe\\tavailable\\tto\\tany\\tother\\tsession\\ton\\tthe\\nsame\\tcluster\\t(even\\tif\\tboth\\tsessions\\tare\\tconnected\\tto\\ta\\tdifferent\\tserver).\\tFor\\texample,\\tconsider\\tthe\\nfollowing\\t\\nclient\\tcode:\\n#\\tsimple_client.py\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nimport\\n\\t\\nsys\\nx\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0.0\\n,\\n\\t\\nname\\n=\\n\"x\"\\n)\\nincrement_x\\n\\t\\n=\\n\\t\\ntf\\n.\\nassign\\n(\\nx\\n,\\n\\t\\nx\\n\\t\\n+\\n\\t\\n1\\n)\\nwith\\n\\t\\ntf\\n.\\nSession', '.\\nSession\\n(\\nsys\\n.\\nargv\\n[\\n1\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nif\\n\\t\\nsys\\n.\\nargv\\n[\\n2\\n:]\\n==\\n[\\n\"init\"\\n]:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nx\\n.\\ninitializer\\n)\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nincrement_x\\n)\\n\\t\\t\\t\\t\\nprint\\n(\\nx\\n.\\neval\\n())\\nLet’s\\tsuppose\\tyou\\thave\\ta\\tTensorFlow\\tcluster\\tup\\tand\\trunning\\ton\\tmachines\\tA\\tand\\tB,\\tport\\t2222.\\tYou\\tcould\\nlaunch\\tthe\\tclient,\\thave\\tit\\topen\\ta\\tsession\\twith\\tthe\\tserver\\ton\\tmachine\\tA,\\tand\\ttell\\tit\\tto\\tinitialize\\tthe\\tvariable,\\nincrement\\tit,\\tand\\tprint\\tits\\tvalue\\tby\\tlaunching\\tthe\\tfollowing\\tcommand:\\n$\\tpython3\\tsimple_client.py\\tgrpc://machine-a.example.com:2222\\tinit\\n1.0\\nNow\\tif\\tyou\\tlaunch\\tthe\\tclient\\twith\\tthe\\tfollowing\\tcommand,\\tit\\twill\\tconnect\\tto\\tthe\\tserver\\ton\\tmachine\\tB\\tand\\nmagically\\treuse\\tthe\\tsame\\tvariable\\t\\nx\\n\\t(this\\ttime\\twe\\tdon’t\\task\\tthe\\tserver\\tto\\tinitialize\\tthe\\tvariable):\\n$\\tpython3\\tsimple_client.py\\tgrpc://machine-b.example.com:2222\\n2.0\\nThis\\tfeature\\tcuts\\tboth\\tways:\\tit’s\\tgreat\\tif\\tyou\\twant\\tto\\tshare\\tvariables\\tacross\\tmultiple\\tsessions,\\tbut\\tif\\tyou', 'want\\tto\\trun\\tcompletely\\tindependent\\tcomputations\\ton\\tthe\\tsame\\tcluster\\tyou\\twill\\thave\\tto\\tbe\\tcareful\\tnot\\tto\\nuse\\tthe\\tsame\\tvariable\\tnames\\tby\\taccident.\\tOne\\tway\\tto\\tensure\\tthat\\tyou\\twon’t\\thave\\tname\\tclashes\\tis\\tto\\twrap\\nall\\tof\\tyour\\tconstruction\\tphase\\tinside\\ta\\tvariable\\tscope\\twith\\ta\\tunique\\tname\\tfor\\teach\\t\\ncomputation,\\tfor\\nexample:\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\n\"my_problem_1\"\\n):\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tConstruction\\tphase\\tof\\tproblem\\t1\\nA\\tbetter\\toption\\tis\\tto\\tuse\\ta\\tcontainer\\tblock:\\nwith\\n\\t\\ntf\\n.\\ncontainer\\n(\\n\"my_problem_1\"\\n):\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tConstruction\\tphase\\tof\\tproblem\\t1', 'This\\twill\\tuse\\ta\\tcontainer\\tdedicated\\tto\\tproblem\\t#1,\\tinstead\\tof\\tthe\\tdefault\\tone\\t(whose\\tname\\tis\\tan\\tempty\\nstring\\t\\n\"\"\\n).\\tOne\\tadvantage\\tis\\tthat\\tvariable\\tnames\\tremain\\tnice\\tand\\tshort.\\tAnother\\tadvantage\\tis\\tthat\\tyou\\tcan\\neasily\\treset\\ta\\tnamed\\tcontainer.\\tFor\\texample,\\tthe\\tfollowing\\tcommand\\twill\\tconnect\\tto\\tthe\\tserver\\ton\\nmachine\\tA\\tand\\task\\tit\\tto\\treset\\tthe\\tcontainer\\tnamed\\t\\n\"my_problem_1\"\\n,\\twhich\\twill\\tfree\\tall\\tthe\\tresources\\tthis\\ncontainer\\tused\\t(and\\talso\\tclose\\tall\\tsessions\\topen\\ton\\tthe\\tserver).\\tAny\\tvariable\\tmanaged\\tby\\tthis\\tcontainer\\nmust\\tbe\\tinitialized\\tbefore\\tyou\\tcan\\tuse\\tit\\tagain:\\ntf\\n.\\nSession\\n.\\nreset\\n(\\n\"grpc://machine-a.example.com:2222\"\\n,\\n\\t\\n[\\n\"my_problem_1\"\\n])\\nResource\\tcontainers\\tmake\\tit\\teasy\\tto\\tshare\\tvariables\\tacross\\tsessions\\tin\\tflexible\\tways.\\tFor\\texample,\\nFigure\\t12-7\\n\\tshows\\tfour\\tclients\\trunning\\tdifferent\\tgraphs\\ton\\tthe\\tsame\\tcluster,\\tbut\\tsharing\\tsome\\tvariables.\\nClients\\tA\\tand\\tB\\tshare\\tthe\\tsame\\tvariable\\t\\nx\\n\\tmanaged\\tby\\tthe\\tdefault\\tcontainer,\\twhile\\tclients\\tC\\tand\\tD\\tshare\\nanother\\tvariable\\tnamed\\t\\nx', 'x\\n\\tmanaged\\tby\\tthe\\tcontainer\\tnamed\\t\\n\"my_problem_1\"\\n.\\tNote\\tthat\\tclient\\tC\\teven\\tuses\\nvariables\\tfrom\\tboth\\tcontainers.\\nFigure\\t12-7.\\t\\nResource\\tcontainers\\nResource\\tcontainers\\talso\\ttake\\tcare\\tof\\tpreserving\\tthe\\tstate\\tof\\tother\\tstateful\\toperations,\\tnamely\\tqueues\\tand\\nreaders.\\tLet’s\\ttake\\ta\\tlook\\tat\\tqueues\\t\\nfirst.', 'Asynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\nQueues\\t\\nare\\tanother\\tgreat\\tway\\tto\\texchange\\tdata\\tbetween\\tmultiple\\tsessions;\\tfor\\texample,\\tone\\tcommon\\tuse\\ncase\\tis\\tto\\thave\\ta\\tclient\\tcreate\\ta\\tgraph\\tthat\\tloads\\tthe\\ttraining\\tdata\\tand\\tpushes\\tit\\tinto\\ta\\tqueue,\\twhile\\tanother\\nclient\\tcreates\\ta\\tgraph\\tthat\\tpulls\\tthe\\tdata\\tfrom\\tthe\\tqueue\\tand\\ttrains\\ta\\tmodel\\t(see\\t\\nFigure\\t12-8\\n).\\tThis\\tcan\\nspeed\\tup\\ttraining\\tconsiderably\\tbecause\\tthe\\ttraining\\toperations\\tdon’t\\thave\\tto\\twait\\tfor\\tthe\\tnext\\tmini-batch\\nat\\tevery\\tstep.\\nFigure\\t12-8.\\t\\nUsing\\tqueues\\tto\\tload\\tthe\\ttraining\\tdata\\tasynchronously\\nTensorFlow\\tprovides\\tvarious\\tkinds\\tof\\t\\nqueues.\\tThe\\tsimplest\\tkind\\tis\\tthe\\t\\nfirst-in\\tfirst-out\\n\\t(\\nFIFO\\n)\\tqueue.\\nFor\\texample,\\tthe\\tfollowing\\tcode\\tcreates\\ta\\tFIFO\\tqueue\\tthat\\tcan\\tstore\\tup\\tto\\t10\\ttensors\\tcontaining\\ttwo\\tfloat\\nvalues\\teach:\\nq\\n\\t\\n=\\n\\t\\ntf\\n.\\nFIFOQueue\\n(\\ncapacity\\n=\\n10\\n,\\n\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nfloat32\\n],\\n\\t\\nshapes\\n=\\n[[\\n2\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"q\"\\n,\\n\\t\\nshared_name\\n=\\n\"shared_q\"\\n)\\nWARNING', ')\\nWARNING\\nTo\\tshare\\tvariables\\tacross\\tsessions,\\tall\\tyou\\thad\\tto\\tdo\\twas\\tto\\tspecify\\tthe\\t\\nsame\\tname\\tand\\tcontainer\\ton\\tboth\\tends.\\tWith\\tqueues\\nTensorFlow\\tdoes\\tnot\\tuse\\tthe\\t\\nname\\n\\tattribute\\tbut\\tinstead\\tuses\\t\\nshared_name\\n,\\tso\\tit\\tis\\timportant\\tto\\tspecify\\tit\\t(even\\tif\\tit\\tis\\tthe\\tsame\\tas\\nthe\\t\\nname\\n).\\tAnd,\\tof\\tcourse,\\tuse\\tthe\\tsame\\tcontainer.\\nEnqueuing\\tdata\\nTo\\t\\npush\\tdata\\tto\\ta\\tqueue,\\tyou\\tmust\\tcreate\\tan\\t\\nenqueue\\n\\toperation.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\tpushes\\nthree\\ttraining\\tinstances\\tto\\tthe\\tqueue:\\n#\\ttraining_data_loader.py\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nq\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\ntraining_instance\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\n2\\n))\\nenqueue\\n\\t\\n=\\n\\t\\nq\\n.\\nenqueue\\n([\\ntraining_instance\\n])\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\n\"grpc://machine-a.example.com:2222\"\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\ntraining_instance\\n:\\n\\t\\n[\\n1.\\n,\\n\\t\\n2.\\n]})', 'sess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\ntraining_instance\\n:\\n\\t\\n[\\n3.\\n,\\n\\t\\n4.\\n]})\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\ntraining_instance\\n:\\n\\t\\n[\\n5.\\n,\\n\\t\\n6.\\n]})\\nInstead\\tof\\tenqueuing\\tinstances\\tone\\tby\\tone,\\tyou\\tcan\\tenqueue\\tseveral\\tat\\ta\\ttime\\tusing\\tan\\t\\nenqueue_many\\noperation:\\n[\\n...\\n]\\ntraining_instances\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\n2\\n))\\nenqueue_many\\n\\t\\n=\\n\\t\\nq\\n.\\nenqueue\\n([\\ntraining_instances\\n])\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\n\"grpc://machine-a.example.com:2222\"\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue_many\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n=\\n{\\ntraining_instances\\n:\\n\\t\\n[[\\n1.\\n,\\n\\t\\n2.\\n],\\n\\t\\n[\\n3.\\n,\\n\\t\\n4.\\n],\\n\\t\\n[\\n5.\\n,\\n\\t\\n6.\\n]]})\\nBoth\\texamples\\tenqueue\\tthe\\tsame\\tthree\\ttensors\\tto\\tthe\\tqueue.\\nDequeuing\\tdata\\nTo\\t\\npull\\tthe\\tinstances\\tout\\tof\\tthe\\tqueue,\\ton\\tthe\\tother\\tend,\\tyou\\tneed\\tto\\tuse\\ta\\t\\ndequeue\\n\\toperation:\\n#\\ttrainer.py\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nq\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\ndequeue\\n\\t\\n=\\n\\t\\nq\\n.\\ndequeue\\n()\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\n\"grpc://machine-a.example.com:2222\"\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))', '))\\n\\t\\t\\n#\\t[1.,\\t2.]\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\t\\n#\\t[3.,\\t4.]\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\t\\n#\\t[5.,\\t6.]\\nIn\\tgeneral\\tyou\\twill\\twant\\tto\\tpull\\ta\\twhole\\tmini-batch\\tat\\tonce,\\tinstead\\tof\\tpulling\\tjust\\tone\\tinstance\\tat\\ta\\ttime.\\nTo\\tdo\\tso,\\tyou\\tmust\\tuse\\ta\\t\\ndequeue_many\\n\\toperation,\\tspecifying\\tthe\\tmini-batch\\tsize:\\n[\\n...\\n]\\nbatch_size\\n\\t\\n=\\n\\t\\n2\\ndequeue_mini_batch\\n=\\n\\t\\nq\\n.\\ndequeue_many\\n(\\nbatch_size\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n(\\n\"grpc://machine-a.example.com:2222\"\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue_mini_batch\\n))\\n\\t\\t\\n#\\t[[1.,\\t2.],\\t[4.,\\t5.]]\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue_mini_batch\\n))\\n\\t\\t\\n#\\tblocked\\twaiting\\tfor\\tanother\\tinstance\\nWhen\\ta\\tqueue\\tis\\tfull,\\tthe\\tenqueue\\toperation\\twill\\tblock\\tuntil\\titems\\tare\\tpulled\\tout\\tby\\ta\\tdequeue\\toperation.\\nSimilarly,\\twhen\\ta\\tqueue\\tis\\tempty\\t(or\\tyou\\tare\\tusing\\t\\ndequeue_many()\\n\\tand\\tthere\\tare\\tfewer\\titems\\tthan\\tthe\\nmini-batch\\tsize),\\tthe\\tdequeue\\toperation\\twill\\tblock\\tuntil\\tenough\\titems\\tare\\tpushed\\tinto\\tthe\\tqueue\\tusing\\tan\\nenqueue\\toperation.\\nQueues\\tof\\ttuples\\nEach', 'Each\\t\\nitem\\tin\\ta\\tqueue\\tcan\\tbe\\ta\\ttuple\\tof\\ttensors\\t(of\\tvarious\\ttypes\\tand\\tshapes)\\tinstead\\tof\\tjust\\ta\\tsingle\\ttensor.\\nFor\\texample,\\tthe\\tfollowing\\tqueue\\tstores\\tpairs\\tof\\ttensors,\\tone\\tof\\ttype\\t\\nint32\\n\\tand\\tshape\\t\\n()\\n,\\tand\\tthe\\tother\\tof\\ntype\\t\\nfloat32\\n\\tand\\tshape\\t\\n[3,2]\\n:\\nq\\n\\t\\n=\\n\\t\\ntf\\n.\\nFIFOQueue\\n(\\ncapacity\\n=\\n10\\n,\\n\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nint32\\n,\\n\\t\\ntf\\n.\\nfloat32\\n],\\n\\t\\nshapes\\n=\\n[[],[\\n3\\n,\\n2\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"q\"\\n,\\n\\t\\nshared_name\\n=\\n\"shared_q\"\\n)\\nThe\\tenqueue\\toperation\\tmust\\tbe\\tgiven\\tpairs\\tof\\ttensors\\t(note\\tthat\\teach\\tpair\\trepresents\\tonly\\tone\\t\\nitem\\tin\\tthe', 'queue):\\na\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint32\\n,\\n\\t\\nshape\\n=\\n())\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\n3\\n,\\n\\t\\n2\\n))\\nenqueue\\n\\t\\n=\\n\\t\\nq\\n.\\nenqueue\\n((\\na\\n,\\n\\t\\nb\\n))\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\na\\n:\\n\\t\\n10\\n,\\n\\t\\nb\\n:[[\\n1.\\n,\\n\\t\\n2.\\n],\\n\\t\\n[\\n3.\\n,\\n\\t\\n4.\\n],\\n\\t\\n[\\n5.\\n,\\n\\t\\n6.\\n]]})\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\na\\n:\\n\\t\\n11\\n,\\n\\t\\nb\\n:[[\\n2.\\n,\\n\\t\\n4.\\n],\\n\\t\\n[\\n6.\\n,\\n\\t\\n8.\\n],\\n\\t\\n[\\n0.\\n,\\n\\t\\n2.\\n]]})\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\na\\n:\\n\\t\\n12\\n,\\n\\t\\nb\\n:[[\\n3.\\n,\\n\\t\\n6.\\n],\\n\\t\\n[\\n9.\\n,\\n\\t\\n2.\\n],\\n\\t\\n[\\n5.\\n,\\n\\t\\n8.\\n]]})\\nOn\\tthe\\tother\\tend,\\tthe\\t\\ndequeue()\\n\\t\\nfunction\\tnow\\tcreates\\ta\\tpair\\tof\\tdequeue\\toperations:\\ndequeue_a\\n,\\n\\t\\ndequeue_b\\n\\t\\n=\\n\\t\\nq\\n.\\ndequeue\\n()\\nIn\\tgeneral,\\tyou\\tshould\\trun\\tthese\\toperations\\ttogether:\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\na_val\\n,\\n\\t\\nb_val\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n([\\ndequeue_a\\n,\\n\\t\\ndequeue_b\\n])\\n\\t\\t\\t\\t\\nprint\\n(\\na_val\\n)\\n\\t\\n#\\t10\\n\\t\\t\\t\\t\\nprint\\n(\\nb_val\\n)\\n\\t\\n#\\t[[1.,\\t2.],\\t[3.,\\t4.],\\t[5.,\\t6.]]\\nWARNING\\nIf\\tyou\\trun\\t\\ndequeue_a', 'dequeue_a\\n\\ton\\tits\\town,\\tit\\twill\\tdequeue\\ta\\tpair\\tand\\treturn\\tonly\\tthe\\tfirst\\telement;\\tthe\\tsecond\\telement\\twill\\tbe\\tlost\\t(and\\nsimilarly,\\tif\\tyou\\trun\\t\\ndequeue_b\\n\\ton\\tits\\town,\\tthe\\tfirst\\telement\\twill\\tbe\\tlost).\\nThe\\t\\ndequeue_many()\\n\\t\\nfunction\\talso\\treturns\\ta\\tpair\\tof\\toperations:\\nbatch_size\\n\\t\\n=\\n\\t\\n2\\ndequeue_as\\n,\\n\\t\\ndequeue_bs\\n\\t\\n=\\n\\t\\nq\\n.\\ndequeue_many\\n(\\nbatch_size\\n)\\nYou\\tcan\\tuse\\tit\\tas\\tyou\\twould\\texpect:\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\na\\n,\\n\\t\\nb\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n([\\ndequeue_a\\n,\\n\\t\\ndequeue_b\\n])\\n\\t\\t\\t\\t\\nprint\\n(\\na\\n)\\n\\t\\n#\\t[10,\\t11]\\n\\t\\t\\t\\t\\nprint\\n(\\nb\\n)\\n\\t\\n#\\t[[[1.,\\t2.],\\t[3.,\\t4.],\\t[5.,\\t6.]],\\t[[2.,\\t4.],\\t[6.,\\t8.],\\t[0.,\\t2.]]]\\n\\t\\t\\t\\t\\na\\n,\\n\\t\\nb\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n([\\ndequeue_a\\n,\\n\\t\\ndequeue_b\\n])\\n\\t\\t\\n#\\tblocked\\twaiting\\tfor\\tanother\\tpair\\nClosing\\ta\\tqueue\\nIt\\t\\nis\\tpossible\\tto\\tclose\\ta\\tqueue\\tto\\tsignal\\tto\\tthe\\tother\\tsessions\\tthat\\tno\\tmore\\tdata\\twill\\tbe\\tenqueued:\\nclose_q\\n\\t\\n=\\n\\t\\nq\\n.\\nclose\\n()\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nclose_q\\n)\\nSubsequent\\texecutions\\tof\\t\\nenqueue\\n\\tor\\t\\nenqueue_many', 'operations\\twill\\traise\\tan\\texception.\\tBy\\tdefault,\\tany\\npending\\tenqueue\\trequest\\twill\\tbe\\thonored,\\tunless\\tyou\\tcall\\t\\nq.close(cancel_pending_enqueues=True)\\n.\\nSubsequent\\texecutions\\tof\\t\\ndequeue\\n\\tor\\t\\ndequeue_many\\n\\toperations\\twill\\tcontinue\\tto\\tsucceed\\tas\\tlong\\tas\\tthere', 'are\\titems\\tin\\tthe\\tqueue,\\tbut\\tthey\\twill\\tfail\\twhen\\tthere\\tare\\tnot\\tenough\\titems\\tleft\\tin\\tthe\\tqueue.\\tIf\\tyou\\tare\\tusing\\na\\t\\ndequeue_many\\n\\toperation\\tand\\tthere\\tare\\ta\\tfew\\tinstances\\tleft\\tin\\tthe\\tqueue,\\tbut\\tfewer\\tthan\\tthe\\tmini-batch\\nsize,\\tthey\\twill\\tbe\\tlost.\\tYou\\tmay\\tprefer\\tto\\tuse\\ta\\t\\ndequeue_up_to\\n\\t\\noperation\\tinstead;\\tit\\tbehaves\\texactly\\tlike\\ndequeue_many\\n\\texcept\\twhen\\ta\\tqueue\\tis\\tclosed\\tand\\tthere\\tare\\tfewer\\tthan\\t\\nbatch_size\\n\\tinstances\\tleft\\tin\\tthe\\nqueue,\\tin\\twhich\\tcase\\tit\\tjust\\treturns\\tthem.\\nRandomShuffleQueue\\nTensorFlow\\talso\\t\\nsupports\\ta\\tcouple\\tmore\\ttypes\\tof\\tqueues,\\tincluding\\t\\nRandomShuffleQueue\\n,\\twhich\\tcan\\tbe\\nused\\tjust\\tlike\\ta\\t\\nFIFOQueue\\n\\t\\nexcept\\tthat\\titems\\tare\\tdequeued\\tin\\ta\\trandom\\torder.\\tThis\\tcan\\tbe\\tuseful\\tto\\tshuffle\\ntraining\\tinstances\\tat\\teach\\tepoch\\tduring\\ttraining.\\tFirst,\\tlet’s\\tcreate\\tthe\\tqueue:\\nq\\n\\t\\n=\\n\\t\\ntf\\n.\\nRandomShuffleQueue\\n(\\ncapacity\\n=\\n50\\n,\\n\\t\\nmin_after_dequeue\\n=\\n10\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nfloat32\\n],\\n\\t\\nshapes\\n=\\n[()],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"q\"\\n,\\n\\t\\nshared_name\\n=\\n\"shared_q\"', ')\\nThe\\t\\nmin_after_dequeue\\n\\t\\nspecifies\\tthe\\tminimum\\tnumber\\tof\\titems\\tthat\\tmust\\tremain\\tin\\tthe\\tqueue\\tafter\\ta\\ndequeue\\toperation.\\tThis\\tensures\\tthat\\tthere\\twill\\tbe\\tenough\\tinstances\\tin\\tthe\\tqueue\\tto\\thave\\tenough\\nrandomness\\t(once\\tthe\\tqueue\\tis\\tclosed,\\tthe\\t\\nmin_after_dequeue\\n\\tlimit\\tis\\tignored).\\tNow\\tsuppose\\tthat\\tyou\\nenqueued\\t22\\titems\\tin\\tthis\\tqueue\\t(floats\\t\\n1.\\n\\tto\\t\\n22.\\n).\\tHere\\tis\\thow\\tyou\\tcould\\tdequeue\\tthem:\\ndequeue\\n\\t\\n=\\n\\t\\nq\\n.\\ndequeue_many\\n(\\n5\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\n#\\t[\\t20.\\t\\t15.\\t\\t11.\\t\\t12.\\t\\t\\t4.]\\t\\t\\t(17\\titems\\tleft)\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\n#\\t[\\t\\t5.\\t\\t13.\\t\\t\\t6.\\t\\t\\t0.\\t\\t17.]\\t\\t\\t(12\\titems\\tleft)\\n\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n\\t\\n#\\t12\\t-\\t5\\t<\\t10:\\tblocked\\twaiting\\tfor\\t3\\tmore\\tinstances\\nPaddingFifoQueue\\nA\\t\\nPaddingFIFOQueue\\n\\t\\ncan\\talso\\tbe\\tused\\tjust\\tlike\\ta\\t\\nFIFOQueue\\n\\texcept\\tthat\\tit\\taccepts\\ttensors\\tof\\tvariable\\nsizes\\talong\\tany\\tdimension\\t(but\\twith\\ta\\tfixed\\trank).\\tWhen\\tyou\\tare\\tdequeuing\\tthem\\twith\\ta\\t\\ndequeue_many\\n\\tor\\ndequeue_up_to', 'operation,\\teach\\ttensor\\tis\\tpadded\\twith\\tzeros\\talong\\tevery\\tvariable\\tdimension\\tto\\tmake\\tit\\nthe\\tsame\\tsize\\tas\\tthe\\tlargest\\ttensor\\tin\\tthe\\tmini-batch.\\tFor\\texample,\\tyou\\tcould\\tenqueue\\t2D\\ttensors\\n(matrices)\\tof\\tarbitrary\\tsizes:\\nq\\n\\t\\n=\\n\\t\\ntf\\n.\\nPaddingFIFOQueue\\n(\\ncapacity\\n=\\n50\\n,\\n\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nfloat32\\n],\\n\\t\\nshapes\\n=\\n[(\\nNone\\n,\\n\\t\\nNone\\n)]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"q\"\\n,\\n\\t\\nshared_name\\n=\\n\"shared_q\"\\n)\\nv\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nNone\\n))\\nenqueue\\n\\t\\n=\\n\\t\\nq\\n.\\nenqueue\\n([\\nv\\n])\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nv\\n:\\n\\t\\n[[\\n1.\\n,\\n\\t\\n2.\\n],\\n\\t\\n[\\n3.\\n,\\n\\t\\n4.\\n],\\n\\t\\n[\\n5.\\n,\\n\\t\\n6.\\n]]})\\n\\t\\t\\t\\t\\t\\t\\t\\n#\\t3x2\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nv\\n:\\n\\t\\n[[\\n1.\\n]]})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\t1x1\\n\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nv\\n:\\n\\t\\n[[\\n7.\\n,\\n\\t\\n8.\\n,\\n\\t\\n9.\\n,\\n\\t\\n5.\\n],\\n\\t\\n[\\n6.\\n,\\n\\t\\n7.\\n,\\n\\t\\n8.\\n,\\n\\t\\n9.\\n]]})\\n\\t\\n#\\t2x4\\nIf\\twe\\tjust\\tdequeue\\tone\\titem\\tat\\ta\\ttime,\\twe\\tget\\tthe\\texact\\tsame\\ttensors\\tthat\\twere\\tenqueued.\\tBut\\tif\\twe', 'dequeue\\tseveral\\titems\\tat\\ta\\ttime\\t(using\\t\\ndequeue_many()\\n\\tor\\t\\ndequeue_up_to()\\n),\\t\\nthe\\tqueue\\tautomatically\\npads\\tthe\\ttensors\\t\\nappropriately.\\tFor\\texample,\\tif\\twe\\tdequeue\\tall\\tthree\\titems\\tat\\tonce,\\tall\\ttensors\\twill\\tbe\\npadded\\twith\\tzeros\\tto\\tbecome\\t3\\t×\\t4\\ttensors,\\tsince\\tthe\\tmaximum\\tsize\\tfor\\tthe\\tfirst\\tdimension\\tis\\t3\\t(first\\titem)\\nand\\tthe\\tmaximum\\tsize\\tfor\\tthe\\tsecond\\tdimension\\tis\\t4\\t(third\\titem):', '>>>\\t\\nq\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n>>>\\t\\ndequeue\\n\\t\\n=\\n\\t\\nq\\n.\\ndequeue_many\\n(\\n3\\n)\\n>>>\\t\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n...\\t\\n\\t\\t\\t\\t\\nprint\\n(\\nsess\\n.\\nrun\\n(\\ndequeue\\n))\\n[[[\\t1.\\t\\t2.\\t\\t0.\\t\\t0.]\\n\\t\\t[\\t3.\\t\\t4.\\t\\t0.\\t\\t0.]\\n\\t\\t[\\t5.\\t\\t6.\\t\\t0.\\t\\t0.]]\\n\\t[[\\t1.\\t\\t0.\\t\\t0.\\t\\t0.]\\n\\t\\t[\\t0.\\t\\t0.\\t\\t0.\\t\\t0.]\\n\\t\\t[\\t0.\\t\\t0.\\t\\t0.\\t\\t0.]]\\n\\t[[\\t7.\\t\\t8.\\t\\t9.\\t\\t5.]\\n\\t\\t[\\t6.\\t\\t7.\\t\\t8.\\t\\t9.]\\n\\t\\t[\\t0.\\t\\t0.\\t\\t0.\\t\\t0.]]]\\nThis\\ttype\\tof\\tqueue\\tcan\\tbe\\tuseful\\twhen\\tyou\\tare\\tdealing\\twith\\tvariable\\tlength\\tinputs,\\tsuch\\tas\\tsequences\\tof\\nwords\\t(see\\t\\nChapter\\t14\\n).\\nOkay,\\tnow\\tlet’s\\tpause\\tfor\\ta\\tsecond:\\tso\\tfar\\tyou\\thave\\tlearned\\tto\\tdistribute\\tcomputations\\tacross\\tmultiple\\ndevices\\tand\\tservers,\\tshare\\tvariables\\tacross\\tsessions,\\tand\\tcommunicate\\tasynchronously\\tusing\\tqueues.\\nBefore\\tyou\\tstart\\ttraining\\tneural\\tnetworks,\\tthough,\\tthere’s\\tone\\tlast\\ttopic\\twe\\tneed\\tto\\tdiscuss:\\thow\\tto\\nefficiently\\tload\\ttraining\\tdata.', 'Loading\\tData\\tDirectly\\tfrom\\tthe\\tGraph\\nSo\\t\\nfar\\twe\\thave\\tassumed\\tthat\\tthe\\tclients\\twould\\tload\\tthe\\ttraining\\tdata\\tand\\tfeed\\tit\\tto\\tthe\\tcluster\\tusing\\nplaceholders.\\tThis\\tis\\tsimple\\tand\\tworks\\tquite\\twell\\tfor\\tsimple\\tsetups,\\tbut\\tit\\tis\\trather\\tinefficient\\tsince\\tit\\ntransfers\\tthe\\ttraining\\tdata\\tseveral\\ttimes:\\n1\\n.\\t\\nFrom\\tthe\\tfilesystem\\tto\\tthe\\tclient\\n2\\n.\\t\\nFrom\\tthe\\tclient\\tto\\tthe\\tmaster\\ttask\\n3\\n.\\t\\nPossibly\\tfrom\\tthe\\tmaster\\ttask\\tto\\tother\\ttasks\\twhere\\tthe\\tdata\\tis\\tneeded\\nIt\\tgets\\tworse\\tif\\tyou\\thave\\tseveral\\tclients\\ttraining\\tvarious\\tneural\\tnetworks\\tusing\\tthe\\tsame\\ttraining\\tdata\\t(for\\nexample,\\tfor\\thyperparameter\\ttuning):\\tif\\tevery\\tclient\\tloads\\tthe\\tdata\\tsimultaneously,\\tyou\\tmay\\tend\\tup\\teven\\nsaturating\\tyour\\tfile\\tserver\\tor\\tthe\\tnetwork’s\\tbandwidth.\\nPreload\\tthe\\tdata\\tinto\\ta\\tvariable\\nFor\\tdatasets\\tthat\\tcan\\tfit\\tin\\tmemory,\\ta\\tbetter\\toption\\tis\\tto\\tload\\tthe\\ttraining\\tdata\\tonce\\tand\\tassign\\tit\\tto\\ta\\nvariable,\\tthen\\tjust\\tuse\\tthat\\tvariable\\tin\\tyour\\tgraph.\\tThis\\tis\\t\\ncalled\\t\\npreloading\\n\\tthe\\ttraining\\tset.\\tThis\\tway\\tthe', 'data\\twill\\tbe\\ttransferred\\tonly\\tonce\\tfrom\\tthe\\tclient\\tto\\tthe\\tcluster\\t(but\\tit\\tmay\\tstill\\tneed\\tto\\tbe\\tmoved\\taround\\nfrom\\ttask\\tto\\ttask\\tdepending\\ton\\twhich\\toperations\\tneed\\tit).\\tThe\\tfollowing\\tcode\\tshows\\thow\\tto\\tload\\tthe\\tfull\\ntraining\\tset\\tinto\\ta\\tvariable:\\ntraining_set_init\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nn_features\\n))\\ntraining_set\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntraining_set_init\\n,\\n\\t\\ntrainable\\n=\\nFalse\\n,\\n\\t\\ncollections\\n=\\n[],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"training_set\"\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ndata\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\t\\n#\\tload\\tthe\\ttraining\\tdata\\tfrom\\tthe\\tdatastore\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_set\\n.\\ninitializer\\n,\\n\\t\\nfeed_dict\\n=\\n{\\ntraining_set_init\\n:\\n\\t\\ndata\\n})\\nYou\\tmust\\tset\\t\\ntrainable=False\\n\\tso\\tthe\\toptimizers\\tdon’t\\ttry\\tto\\ttweak\\tthis\\tvariable.\\tYou\\tshould\\talso\\tset\\ncollections=[]\\n\\tto\\tensure\\tthat\\tthis\\tvariable\\twon’t\\tget\\tadded\\tto\\tthe\\t\\nGraphKeys.GLOBAL_VARIABLES\\ncollection,\\twhich\\tis\\tused\\tfor\\tsaving\\tand\\trestoring\\tcheckpoints.\\nNOTE', 'NOTE\\nThis\\texample\\tassumes\\tthat\\tall\\tof\\tyour\\ttraining\\tset\\t(including\\tthe\\tlabels)\\tconsists\\tonly\\tof\\t\\nfloat32\\n\\tvalues.\\tIf\\tthat’s\\tnot\\tthe\\tcase,\\tyou\\nwill\\tneed\\tone\\tvariable\\tper\\ttype.\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\nIf\\tthe\\ttraining\\tset\\tdoes\\tnot\\tfit\\tin\\tmemory,\\ta\\tgood\\tsolution\\tis\\tto\\tuse\\t\\nreader\\toperations\\n:\\t\\nthese\\tare\\toperations\\ncapable\\tof\\treading\\tdata\\tdirectly\\tfrom\\tthe\\tfilesystem.\\tThis\\tway\\tthe\\ttraining\\tdata\\tnever\\tneeds\\tto\\tflow\\nthrough\\tthe\\tclients\\tat\\tall.\\tTensorFlow\\tprovides\\treaders\\tfor\\tvarious\\tfile\\tformats:\\nCSV', 'Fixed-length\\tbinary\\trecords\\nTensorFlow’s\\town\\t\\nTFRecords\\n\\tformat,\\tbased\\ton\\tprotocol\\tbuffers\\nLet’s\\tlook\\tat\\ta\\tsimple\\texample\\treading\\tfrom\\ta\\tCSV\\tfile\\t(for\\tother\\tformats,\\tplease\\tcheck\\tout\\tthe\\tAPI\\ndocumentation).\\tSuppose\\tyou\\thave\\tfile\\tnamed\\t\\nmy_test.csv\\n\\tthat\\tcontains\\ttraining\\tinstances,\\tand\\tyou\\twant\\nto\\tcreate\\toperations\\tto\\tread\\tit.\\tSuppose\\tit\\thas\\tthe\\tfollowing\\tcontent,\\twith\\ttwo\\tfloat\\tfeatures\\t\\nx1\\n\\tand\\t\\nx2\\n\\tand\\none\\tinteger\\t\\ntarget\\n\\trepresenting\\ta\\tbinary\\tclass:\\nx1\\n,\\n\\t\\t\\nx2\\n,\\n\\t\\t\\ntarget\\n1.\\n\\t\\n,\\n\\t\\n2.\\n\\t\\n,\\n\\t\\n0\\n4.\\n\\t\\n,\\n\\t\\n5\\n\\t\\t\\n,\\n\\t\\n1\\n7.\\n\\t\\n,\\n\\t\\t\\t\\t\\n,\\n\\t\\n0\\nFirst,\\tlet’s\\tcreate\\ta\\t\\nTextLineReader\\n\\tto\\t\\nread\\tthis\\tfile.\\tA\\t\\nTextLineReader\\n\\topens\\ta\\tfile\\t(once\\twe\\ttell\\tit\\nwhich\\tone\\tto\\topen)\\tand\\treads\\tlines\\tone\\tby\\tone.\\tIt\\tis\\ta\\tstateful\\toperation,\\tlike\\tvariables\\tand\\tqueues:\\tit\\npreserves\\tits\\tstate\\tacross\\tmultiple\\truns\\tof\\tthe\\tgraph,\\tkeeping\\ttrack\\tof\\twhich\\tfile\\tit\\tis\\tcurrently\\treading\\tand\\nwhat\\tits\\tcurrent\\tposition\\tis\\tin\\tthis\\tfile.\\nreader\\n\\t\\n=\\n\\t\\ntf\\n.\\nTextLineReader\\n(\\nskip_header_lines\\n=\\n1\\n)', '=\\n1\\n)\\nNext,\\twe\\tcreate\\ta\\tqueue\\tthat\\tthe\\treader\\twill\\tpull\\tfrom\\tto\\tknow\\twhich\\tfile\\tto\\tread\\tnext.\\tWe\\talso\\tcreate\\tan\\nenqueue\\toperation\\tand\\ta\\tplaceholder\\tto\\tpush\\tany\\tfilename\\twe\\twant\\tto\\tthe\\tqueue,\\tand\\twe\\tcreate\\tan\\noperation\\tto\\tclose\\tthe\\tqueue\\tonce\\twe\\thave\\tno\\tmore\\t\\nfiles\\tto\\tread:\\nfilename_queue\\n\\t\\n=\\n\\t\\ntf\\n.\\nFIFOQueue\\n(\\ncapacity\\n=\\n10\\n,\\n\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nstring\\n],\\n\\t\\nshapes\\n=\\n[()])\\nfilename\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nstring\\n)\\nenqueue_filename\\n\\t\\n=\\n\\t\\nfilename_queue\\n.\\nenqueue\\n([\\nfilename\\n])\\nclose_filename_queue\\n\\t\\n=\\n\\t\\nfilename_queue\\n.\\nclose\\n()\\nNow\\twe\\tare\\tready\\tto\\tcreate\\ta\\t\\nread\\n\\toperation\\tthat\\twill\\tread\\tone\\trecord\\t(i.e.,\\ta\\tline)\\tat\\ta\\ttime\\tand\\treturn\\ta\\nkey/value\\tpair.\\tThe\\tkey\\tis\\tthe\\trecord’s\\tunique\\tidentifier\\t—\\ta\\tstring\\tcomposed\\tof\\tthe\\tfilename,\\ta\\tcolon\\t(\\n:\\n),\\nand\\tthe\\tline\\tnumber\\t—\\tand\\tthe\\tvalue\\tis\\tsimply\\ta\\tstring\\tcontaining\\tthe\\tcontent\\tof\\tthe\\tline:\\nkey\\n,\\n\\t\\nvalue\\n\\t\\n=\\n\\t\\nreader\\n.\\nread\\n(\\nfilename_queue\\n)', ')\\nWe\\thave\\tall\\twe\\tneed\\tto\\tread\\tthe\\tfile\\tline\\tby\\tline!\\tBut\\twe\\tare\\tnot\\tquite\\tdone\\tyet\\t—\\twe\\tneed\\tto\\tparse\\tthis\\nstring\\tto\\tget\\tthe\\t\\nfeatures\\tand\\ttarget:\\nx1\\n,\\n\\t\\nx2\\n,\\n\\t\\ntarget\\n\\t\\n=\\n\\t\\ntf\\n.\\ndecode_csv\\n(\\nvalue\\n,\\n\\t\\nrecord_defaults\\n=\\n[[\\n-\\n1.\\n],\\n\\t\\n[\\n-\\n1.\\n],\\n\\t\\n[\\n-\\n1\\n]])\\nfeatures\\n\\t\\n=\\n\\t\\ntf\\n.\\nstack\\n([\\nx1\\n,\\n\\t\\nx2\\n])\\nThe\\tfirst\\tline\\tuses\\tTensorFlow’s\\tCSV\\tparser\\tto\\textract\\tthe\\tvalues\\tfrom\\tthe\\tcurrent\\tline.\\tThe\\tdefault\\tvalues\\nare\\tused\\twhen\\ta\\tfield\\tis\\tmissing\\t(in\\tthis\\texample\\tthe\\tthird\\ttraining\\tinstance’s\\t\\nx2\\n\\tfeature),\\tand\\tthey\\tare\\talso\\nused\\tto\\tdetermine\\tthe\\ttype\\tof\\teach\\tfield\\t(in\\tthis\\tcase\\ttwo\\tfloats\\tand\\tone\\tinteger).\\nFinally,\\twe\\tcan\\tpush\\tthis\\ttraining\\tinstance\\tand\\tits\\ttarget\\tto\\ta\\t\\nRandomShuffleQueue\\n\\tthat\\twe\\twill\\t\\nshare\\nwith\\tthe\\ttraining\\tgraph\\t(so\\tit\\tcan\\tpull\\tmini-batches\\tfrom\\tit),\\tand\\twe\\tcreate\\tan\\toperation\\tto\\tclose\\tthat\\tqueue\\nwhen\\twe\\tare\\tdone\\tpushing\\tinstances\\tto\\tit:', 'instance_queue\\n\\t\\n=\\n\\t\\ntf\\n.\\nRandomShuffleQueue\\n(\\n\\t\\t\\t\\t\\ncapacity\\n=\\n10\\n,\\n\\t\\nmin_after_dequeue\\n=\\n2\\n,\\n\\t\\t\\t\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nfloat32\\n,\\n\\t\\ntf\\n.\\nint32\\n],\\n\\t\\nshapes\\n=\\n[[\\n2\\n],[]],\\n\\t\\t\\t\\t\\nname\\n=\\n\"instance_q\"\\n,\\n\\t\\nshared_name\\n=\\n\"shared_instance_q\"\\n)\\nenqueue_instance\\n\\t\\n=\\n\\t\\ninstance_queue\\n.\\nenqueue\\n([\\nfeatures\\n,\\n\\t\\ntarget\\n])\\nclose_instance_queue\\n\\t\\n=\\n\\t\\ninstance_queue\\n.\\nclose\\n()\\nWow!\\tThat\\twas\\ta\\tlot\\tof\\twork\\tjust\\tto\\tread\\ta\\tfile.\\tPlus\\twe\\tonly\\tcreated\\tthe\\tgraph,\\tso\\tnow\\twe\\tneed\\tto\\trun\\tit:\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue_filename\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nfilename\\n:\\n\\t\\n\"my_test.csv\"\\n})\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nclose_filename_queue\\n)\\n\\t\\t\\t\\t\\ntry\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nwhile\\n\\t\\nTrue\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nenqueue_instance\\n)\\n\\t\\t\\t\\t\\nexcept\\n\\t\\ntf\\n.\\nerrors\\n.\\nOutOfRangeError\\n\\t\\nas\\n\\t\\nex\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npass\\n\\t\\n#\\tno\\tmore\\trecords\\tin\\tthe\\tcurrent\\tfile\\tand\\tno\\tmore\\tfiles\\tto\\tread\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nclose_instance_queue\\n)\\nFirst\\twe\\topen\\tthe\\tsession,\\tand\\tthen\\twe\\tenqueue\\tthe\\tfilename\\t\\n\"my_test.csv\"\\n\\tand\\timmediately\\tclose\\tthat', 'queue\\tsince\\twe\\twill\\tnot\\tenqueue\\tany\\tmore\\tfilenames.\\tThen\\twe\\trun\\tan\\tinfinite\\tloop\\tto\\tenqueue\\tinstances\\none\\tby\\tone.\\tThe\\t\\nenqueue_instance\\n\\tdepends\\ton\\tthe\\treader\\treading\\tthe\\tnext\\tline,\\tso\\tat\\tevery\\titeration\\ta\\nnew\\trecord\\tis\\tread\\tuntil\\tit\\treaches\\tthe\\tend\\tof\\tthe\\tfile.\\tAt\\tthat\\tpoint\\tit\\ttries\\tto\\tread\\tthe\\tfilename\\tqueue\\tto\\nknow\\twhich\\tfile\\tto\\tread\\tnext,\\tand\\tsince\\tthe\\tqueue\\tis\\tclosed\\tit\\tthrows\\t\\nan\\t\\nOutOfRangeError\\n\\texception\\t(if\\nwe\\tdid\\tnot\\tclose\\tthe\\tqueue,\\tit\\twould\\tjust\\tremain\\tblocked\\tuntil\\twe\\tpushed\\tanother\\tfilename\\tor\\tclosed\\tthe\\nqueue).\\tLastly,\\twe\\tclose\\tthe\\tinstance\\tqueue\\tso\\tthat\\tthe\\ttraining\\toperations\\tpulling\\tfrom\\tit\\twon’t\\tget\\nblocked\\tforever.\\t\\nFigure\\t12-9\\n\\tsummarizes\\twhat\\twe\\thave\\tlearned;\\tit\\trepresents\\ta\\ttypical\\tgraph\\tfor\\treading\\ntraining\\tinstances\\tfrom\\ta\\tset\\tof\\tCSV\\tfiles.\\nFigure\\t12-9.\\t\\nA\\tgraph\\tdedicated\\tto\\treading\\ttraining\\tinstances\\tfrom\\tCSV\\tfiles\\nIn\\tthe\\ttraining\\tgraph,\\tyou\\tneed\\tto\\tcreate\\tthe\\tshared\\tinstance\\tqueue\\tand\\tsimply\\tdequeue\\tmini-batches\\t\\nfrom\\nit:\\ninstance_queue\\n\\t\\n=\\n\\t\\ntf\\n.', '=\\n\\t\\ntf\\n.\\nRandomShuffleQueue\\n([\\n...\\n],\\n\\t\\nshared_name\\n=\\n\"shared_instance_q\"\\n)\\nmini_batch_instances\\n,\\n\\t\\nmini_batch_targets\\n\\t\\n=\\n\\t\\ninstance_queue\\n.\\ndequeue_up_to\\n(\\n2\\n)\\n[\\n...\\n]\\n\\t\\n#\\tuse\\tthe\\tmini_batch\\tinstances\\tand\\ttargets\\tto\\tbuild\\tthe\\ttraining\\tgraph\\ntraining_op\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ntry\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nstep\\n\\t\\nin\\n\\t\\nrange\\n(\\nmax_steps\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n)\\n\\t\\t\\t\\t\\nexcept\\n\\t\\ntf\\n.\\nerrors\\n.\\nOutOfRangeError\\n\\t\\nas\\n\\t\\nex\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npass\\n\\t\\n#\\tno\\tmore\\ttraining\\tinstances', 'In\\tthis\\texample,\\tthe\\tfirst\\tmini-batch\\twill\\tcontain\\tthe\\tfirst\\ttwo\\tinstances\\tof\\tthe\\tCSV\\tfile,\\tand\\tthe\\tsecond\\nmini-batch\\twill\\tcontain\\tthe\\tlast\\tinstance.\\nWARNING\\nTensorFlow\\tqueues\\tdon’t\\thandle\\tsparse\\ttensors\\twell,\\tso\\tif\\tyour\\ttraining\\tinstances\\tare\\tsparse\\tyou\\tshould\\tparse\\tthe\\trecords\\tafter\\nthe\\tinstance\\tqueue.\\nThis\\tarchitecture\\twill\\tonly\\tuse\\tone\\tthread\\tto\\tread\\trecords\\tand\\tpush\\tthem\\tto\\tthe\\tinstance\\tqueue.\\tYou\\tcan\\nget\\ta\\tmuch\\thigher\\tthroughput\\tby\\thaving\\tmultiple\\tthreads\\tread\\tsimultaneously\\tfrom\\tmultiple\\tfiles\\tusing\\nmultiple\\treaders.\\tLet’s\\tsee\\thow.\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\nTo\\thave\\tmultiple\\tthreads\\tread\\tinstances\\tsimultaneously,\\tyou\\tcould\\tcreate\\t\\nPython\\tthreads\\t(using\\tthe\\nthreading\\n\\tmodule)\\tand\\tmanage\\tthem\\tyourself.\\tHowever,\\tTensorFlow\\tprovides\\tsome\\ttools\\tto\\tmake\\tthis\\nsimpler:\\tthe\\t\\nCoordinator\\n\\t\\nclass\\tand\\t\\nthe\\t\\nQueueRunner\\n\\tclass.\\nA\\t\\nCoordinator\\n\\tis\\ta\\tvery\\tsimple\\tobject\\twhose\\tsole\\tpurpose\\tis\\tto\\tcoordinate\\tstopping\\tmultiple\\tthreads.\\nFirst\\tyou\\tcreate\\ta', 'Coordinator\\n:\\ncoord\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nCoordinator\\n()\\nThen\\tyou\\tgive\\tit\\tto\\tall\\tthreads\\tthat\\tneed\\tto\\tstop\\tjointly,\\tand\\ttheir\\tmain\\tloop\\tlooks\\tlike\\tthis:\\nwhile\\n\\t\\nnot\\n\\t\\ncoord\\n.\\nshould_stop\\n():\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tdo\\tsomething\\nAny\\tthread\\tcan\\trequest\\tthat\\tevery\\tthread\\tstop\\tby\\tcalling\\tthe\\t\\nCoordinator\\n’s\\t\\nrequest_stop()\\n\\t\\nmethod:\\ncoord\\n.\\nrequest_stop\\n()\\nEvery\\tthread\\twill\\tstop\\tas\\tsoon\\tas\\tit\\tfinishes\\tits\\tcurrent\\titeration.\\tYou\\tcan\\twait\\tfor\\tall\\tof\\tthe\\tthreads\\tto\\nfinish\\tby\\tcalling\\tthe\\t\\nCoordinator\\n’s\\t\\njoin()\\n\\tmethod,\\t\\npassing\\tit\\tthe\\tlist\\tof\\tthreads:\\ncoord\\n.\\njoin\\n(\\nlist_of_threads\\n)\\nA\\t\\nQueueRunner\\n\\truns\\tmultiple\\tthreads\\tthat\\teach\\trun\\tan\\tenqueue\\toperation\\trepeatedly,\\tfilling\\tup\\ta\\tqueue\\tas\\nfast\\tas\\tpossible.\\tAs\\tsoon\\tas\\tthe\\tqueue\\tis\\tclosed,\\tthe\\tnext\\tthread\\tthat\\ttries\\tto\\tpush\\tan\\titem\\tto\\tthe\\tqueue\\twill\\nget\\tan\\t\\nOutOfRangeError\\n;\\t\\nthis\\tthread\\tcatches\\tthe\\terror\\tand\\timmediately\\ttells\\tother\\tthreads\\tto\\tstop\\tusing\\ta\\nCoordinator\\n.\\tThe\\tfollowing\\tcode\\tshows\\thow\\tyou\\tcan\\tuse\\ta\\t\\nQueueRunner', 'to\\thave\\tfive\\tthreads\\treading\\ninstances\\tsimultaneously\\tand\\tpushing\\tthem\\tto\\tan\\tinstance\\tqueue:\\n[\\n...\\n]\\n\\t\\n#\\tsame\\tconstruction\\tphase\\tas\\tearlier\\nqueue_runner\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nQueueRunner\\n(\\ninstance_queue\\n,\\n\\t\\n[\\nenqueue_instance\\n]\\n\\t\\n*\\n\\t\\n5\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:', 'sess\\n.\\nrun\\n(\\nenqueue_filename\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nfilename\\n:\\n\\t\\n\"my_test.csv\"\\n})\\n\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\nclose_filename_queue\\n)\\n\\t\\t\\t\\t\\ncoord\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nCoordinator\\n()\\n\\t\\t\\t\\t\\nenqueue_threads\\n\\t\\n=\\n\\t\\nqueue_runner\\n.\\ncreate_threads\\n(\\nsess\\n,\\n\\t\\ncoord\\n=\\ncoord\\n,\\n\\t\\nstart\\n=\\nTrue\\n)\\nThe\\tfirst\\tline\\tcreates\\tthe\\t\\nQueueRunner\\n\\tand\\ttells\\tit\\tto\\trun\\tfive\\tthreads,\\tall\\trunning\\tthe\\tsame\\nenqueue_instance\\n\\toperation\\trepeatedly.\\tThen\\twe\\tstart\\ta\\tsession\\tand\\twe\\tenqueue\\tthe\\tname\\tof\\tthe\\tfiles\\tto\\nread\\t(in\\tthis\\tcase\\tjust\\t\\n\"my_test.csv\"\\n).\\tNext\\twe\\tcreate\\ta\\t\\nCoordinator\\n\\tthat\\tthe\\t\\nQueueRunner\\n\\twill\\tuse\\tto\\nstop\\tgracefully,\\tas\\tjust\\texplained.\\tFinally,\\twe\\ttell\\tthe\\t\\nQueueRunner\\n\\tto\\tcreate\\tthe\\tthreads\\tand\\tstart\\tthem.\\nThe\\tthreads\\twill\\tread\\tall\\ttraining\\tinstances\\tand\\tpush\\tthem\\tto\\tthe\\tinstance\\tqueue,\\tand\\tthen\\tthey\\twill\\tall\\tstop\\ngracefully.\\nThis\\twill\\tbe\\ta\\tbit\\tmore\\tefficient\\tthan\\tearlier,\\tbut\\twe\\tcan\\tdo\\tbetter.\\tCurrently\\tall\\tthreads\\tare\\treading\\tfrom', 'the\\tsame\\tfile.\\tWe\\tcan\\tmake\\tthem\\tread\\tsimultaneously\\tfrom\\tseparate\\tfiles\\tinstead\\t(assuming\\tthe\\ttraining\\ndata\\tis\\tsharded\\tacross\\tmultiple\\tCSV\\tfiles)\\tby\\tcreating\\tmultiple\\treaders\\t(see\\t\\nFigure\\t12-10\\n).\\nFigure\\t12-10.\\t\\nReading\\tsimultaneously\\tfrom\\tmultiple\\tfiles\\nFor\\tthis\\twe\\tneed\\tto\\twrite\\ta\\tsmall\\tfunction\\tto\\tcreate\\ta\\treader\\tand\\tthe\\tnodes\\tthat\\twill\\tread\\tand\\tpush\\tone\\ninstance\\t\\nto\\tthe\\tinstance\\tqueue:\\ndef\\n\\t\\nread_and_push_instance\\n(\\nfilename_queue\\n,\\n\\t\\ninstance_queue\\n):\\n\\t\\t\\t\\t\\nreader\\n\\t\\n=\\n\\t\\ntf\\n.\\nTextLineReader\\n(\\nskip_header_lines\\n=\\n1\\n)\\n\\t\\t\\t\\t\\nkey\\n,\\n\\t\\nvalue\\n\\t\\n=\\n\\t\\nreader\\n.\\nread\\n(\\nfilename_queue\\n)\\n\\t\\t\\t\\t\\nx1\\n,\\n\\t\\nx2\\n,\\n\\t\\ntarget\\n\\t\\n=\\n\\t\\ntf\\n.\\ndecode_csv\\n(\\nvalue\\n,\\n\\t\\nrecord_defaults\\n=\\n[[\\n-\\n1.\\n],\\n\\t\\n[\\n-\\n1.\\n],\\n\\t\\n[\\n-\\n1\\n]])\\n\\t\\t\\t\\t\\nfeatures\\n\\t\\n=\\n\\t\\ntf\\n.\\nstack\\n([\\nx1\\n,\\n\\t\\nx2\\n])\\n\\t\\t\\t\\t\\nenqueue_instance\\n\\t\\n=\\n\\t\\ninstance_queue\\n.\\nenqueue\\n([\\nfeatures\\n,\\n\\t\\ntarget\\n])\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nenqueue_instance\\nNext\\twe\\tdefine\\tthe\\t\\nqueues:\\nfilename_queue\\n\\t\\n=\\n\\t\\ntf\\n.\\nFIFOQueue\\n(\\ncapacity\\n=\\n10\\n,\\n\\t\\ndtypes\\n=\\n[\\ntf\\n.\\nstring\\n],\\n\\t\\nshapes\\n=\\n[()])\\nfilename\\n\\t\\n=\\n\\t\\ntf\\n.', '=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nstring\\n)\\nenqueue_filename\\n\\t\\n=\\n\\t\\nfilename_queue\\n.\\nenqueue\\n([\\nfilename\\n])\\nclose_filename_queue\\n\\t\\n=\\n\\t\\nfilename_queue\\n.\\nclose\\n()\\ninstance_queue\\n\\t\\n=\\n\\t\\ntf\\n.\\nRandomShuffleQueue\\n([\\n...\\n])\\nAnd\\tfinally\\twe\\tcreate\\tthe\\t\\nQueueRunner\\n,\\tbut\\tthis\\ttime\\twe\\tgive\\tit\\ta\\tlist\\tof\\tdifferent\\tenqueue\\toperations.\\nEach\\toperation\\twill\\tuse\\ta\\tdifferent\\treader,\\tso\\tthe\\tthreads\\twill\\tsimultaneously\\tread\\tfrom\\tdifferent\\tfiles:\\nread_and_enqueue_ops\\n\\t\\n=\\n\\t\\n[\\n\\t\\t\\t\\t\\nread_and_push_instance\\n(\\nfilename_queue\\n,\\n\\t\\ninstance_queue\\n)', 'for\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n)]\\nqueue_runner\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nQueueRunner\\n(\\ninstance_queue\\n,\\n\\t\\nread_and_enqueue_ops\\n)\\nThe\\texecution\\tphase\\tis\\tthen\\tthe\\tsame\\tas\\tbefore:\\tfirst\\tpush\\tthe\\tnames\\tof\\tthe\\tfiles\\tto\\tread,\\tthen\\tcreate\\t\\na\\nCoordinator\\n\\tand\\tcreate\\tand\\tstart\\tthe\\t\\nQueueRunner\\n\\tthreads.\\tThis\\ttime\\tall\\tthreads\\twill\\tread\\tfrom\\ndifferent\\tfiles\\tsimultaneously\\tuntil\\tall\\tfiles\\tare\\tread\\tentirely,\\tand\\tthen\\tthe\\t\\nQueueRunner\\n\\twill\\tclose\\tthe\\ninstance\\tqueue\\tso\\tthat\\tother\\tops\\tpulling\\tfrom\\tit\\tdon’t\\tget\\t\\nblocked.\\nOther\\tconvenience\\tfunctions\\nTensorFlow\\t\\nalso\\toffers\\ta\\tfew\\tconvenience\\tfunctions\\tto\\tsimplify\\tsome\\tcommon\\ttasks\\twhen\\treading\\ntraining\\tinstances.\\tWe\\twill\\tgo\\tover\\tjust\\ta\\tfew\\t(see\\tthe\\tAPI\\tdocumentation\\tfor\\tthe\\tfull\\tlist).\\nThe\\t\\nstring_input_producer()\\n\\t\\ntakes\\ta\\t1D\\ttensor\\tcontaining\\ta\\tlist\\tof\\tfilenames,\\tcreates\\ta\\tthread\\tthat\\npushes\\tone\\tfilename\\tat\\ta\\ttime\\tto\\tthe\\tfilename\\tqueue,\\tand\\tthen\\tcloses\\tthe\\tqueue.\\tIf\\tyou\\tspecify\\ta\\tnumber\\tof', 'epochs,\\tit\\twill\\tcycle\\tthrough\\tthe\\tfilenames\\tonce\\tper\\tepoch\\tbefore\\tclosing\\tthe\\tqueue.\\tBy\\tdefault,\\tit\\tshuffles\\nthe\\tfilenames\\tat\\teach\\tepoch.\\tIt\\tcreates\\ta\\t\\nQueueRunner\\n\\tto\\tmanage\\tits\\tthread,\\tand\\tadds\\tit\\tto\\tthe\\nGraphKeys.QUEUE_RUNNERS\\n\\tcollection.\\tTo\\tstart\\tevery\\t\\nQueueRunner\\n\\tin\\tthat\\tcollection,\\tyou\\tcan\\tcall\\tthe\\ntf.train.start_queue_runners()\\n\\tfunction.\\tNote\\tthat\\tif\\tyou\\tforget\\tto\\tstart\\tthe\\t\\nQueueRunner\\n,\\tthe\\nfilename\\tqueue\\twill\\tbe\\topen\\tand\\tempty,\\tand\\tyour\\treaders\\twill\\tbe\\tblocked\\tforever.\\nThere\\tare\\ta\\tfew\\tother\\t\\nproducer\\n\\t\\nfunctions\\tthat\\tsimilarly\\tcreate\\ta\\tqueue\\tand\\ta\\tcorresponding\\t\\nQueueRunner\\nfor\\trunning\\tan\\tenqueue\\toperation\\t(e.g.,\\t\\ninput_producer()\\n,\\t\\nrange_input_producer()\\n,\\tand\\nslice_input_producer()\\n).\\nThe\\t\\nshuffle_batch()\\n\\tfunction\\t\\ntakes\\ta\\tlist\\tof\\ttensors\\t(e.g.,\\t\\n[features,\\ttarget]\\n)\\tand\\tcreates:\\nA\\t\\nRandomShuffleQueue\\nA\\t\\nQueueRunner\\n\\tto\\t\\nenqueue\\tthe\\ttensors\\tto\\tthe\\tqueue\\t(added\\tto\\tthe\\t\\nGraphKeys.QUEUE_RUNNERS\\ncollection)\\nA\\t\\ndequeue_many', 'operation\\tto\\textract\\ta\\tmini-batch\\tfrom\\tthe\\tqueue\\nThis\\tmakes\\tit\\teasy\\tto\\tmanage\\tin\\ta\\tsingle\\tprocess\\ta\\tmultithreaded\\tinput\\tpipeline\\tfeeding\\ta\\tqueue\\tand\\ta\\ntraining\\tpipeline\\treading\\tmini-batches\\tfrom\\tthat\\tqueue.\\tAlso\\tcheck\\tout\\tthe\\t\\nbatch()\\n,\\t\\nbatch_join()\\n,\\tand\\nshuffle_batch_join()\\n\\tfunctions\\tthat\\t\\nprovide\\tsimilar\\tfunctionality.\\nOkay!\\tYou\\tnow\\thave\\tall\\tthe\\ttools\\tyou\\tneed\\tto\\tstart\\ttraining\\tand\\trunning\\tneural\\tnetworks\\tefficiently\\tacross\\nmultiple\\tdevices\\tand\\tservers\\ton\\ta\\tTensorFlow\\tcluster.\\tLet’s\\treview\\twhat\\tyou\\thave\\tlearned:\\nUsing\\tmultiple\\tGPU\\tdevices\\nSetting\\tup\\tand\\tstarting\\ta\\tTensorFlow\\tcluster\\nDistributing\\tcomputations\\tacross\\tmultiple\\tdevices\\tand\\tservers\\nSharing\\tvariables\\t(and\\tother\\tstateful\\tops\\tsuch\\tas\\tqueues\\tand\\treaders)\\tacross\\tsessions\\tusing\\ncontainers', 'Coordinating\\tmultiple\\tgraphs\\tworking\\tasynchronously\\tusing\\tqueues\\nReading\\tinputs\\tefficiently\\tusing\\treaders,\\tqueue\\trunners,\\tand\\tcoordinators\\nNow\\tlet’s\\tuse\\tall\\tof\\tthis\\tto\\tparallelize\\tneural\\t\\nnetworks!', 'Parallelizing\\tNeural\\tNetworks\\ton\\ta\\tTensorFlow\\tCluster\\nIn\\tthis\\tsection,\\tfirst\\twe\\twill\\tlook\\tat\\thow\\tto\\tparallelize\\tseveral\\tneural\\tnetworks\\tby\\tsimply\\tplacing\\teach\\none\\ton\\ta\\tdifferent\\tdevice.\\tThen\\twe\\twill\\tlook\\tat\\tthe\\tmuch\\ttrickier\\tproblem\\tof\\ttraining\\ta\\tsingle\\tneural\\nnetwork\\tacross\\tmultiple\\tdevices\\tand\\tservers.', 'One\\tNeural\\tNetwork\\tper\\tDevice\\nThe\\t\\nmost\\ttrivial\\tway\\tto\\ttrain\\tand\\trun\\tneural\\tnetworks\\ton\\ta\\tTensorFlow\\tcluster\\tis\\tto\\ttake\\tthe\\texact\\tsame\\ncode\\tyou\\twould\\tuse\\tfor\\ta\\tsingle\\tdevice\\ton\\ta\\tsingle\\tmachine,\\tand\\tspecify\\tthe\\tmaster\\tserver’s\\taddress\\nwhen\\tcreating\\tthe\\tsession.\\tThat’s\\tit\\t—\\tyou’re\\tdone!\\tYour\\tcode\\twill\\tbe\\trunning\\ton\\tthe\\tserver’s\\tdefault\\ndevice.\\tYou\\tcan\\tchange\\tthe\\tdevice\\tthat\\twill\\trun\\tyour\\tgraph\\tsimply\\tby\\tputting\\tyour\\tcode’s\\tconstruction\\nphase\\twithin\\ta\\tdevice\\tblock.\\nBy\\trunning\\tseveral\\tclient\\tsessions\\tin\\tparallel\\t(in\\tdifferent\\tthreads\\tor\\tdifferent\\tprocesses),\\tconnecting\\tthem\\nto\\tdifferent\\tservers,\\tand\\tconfiguring\\tthem\\tto\\tuse\\tdifferent\\tdevices,\\tyou\\tcan\\tquite\\teasily\\ttrain\\tor\\trun\\tmany\\nneural\\tnetworks\\tin\\tparallel,\\tacross\\tall\\tdevices\\tand\\tall\\tmachines\\tin\\tyour\\tcluster\\t(see\\t\\nFigure\\t12-11\\n).\\tThe\\nspeedup\\tis\\talmost\\tlinear.\\n4\\n\\tTraining\\t100\\tneural\\tnetworks\\tacross\\t50\\tservers\\twith\\t2\\tGPUs\\teach\\twill\\tnot\\ttake\\nmuch\\tlonger\\tthan\\ttraining\\tjust\\t1\\tneural\\tnetwork\\ton\\t1\\tGPU.\\nFigure\\t12-11.', 'Training\\tone\\tneural\\tnetwork\\tper\\tdevice\\nThis\\tsolution\\tis\\tperfect\\tfor\\thyperparameter\\ttuning:\\teach\\tdevice\\tin\\tthe\\tcluster\\twill\\ttrain\\ta\\tdifferent\\tmodel\\nwith\\tits\\town\\tset\\tof\\thyperparameters.\\tThe\\tmore\\tcomputing\\tpower\\tyou\\thave,\\tthe\\tlarger\\tthe\\thyperparameter\\nspace\\tyou\\tcan\\texplore.\\nIt\\talso\\tworks\\tperfectly\\tif\\tyou\\thost\\ta\\tweb\\tservice\\tthat\\treceives\\ta\\tlarge\\tnumber\\t\\nof\\t\\nqueries\\tper\\tsecond\\n(QPS)\\tand\\tyou\\tneed\\tyour\\tneural\\tnetwork\\tto\\tmake\\ta\\tprediction\\tfor\\teach\\tquery.\\tSimply\\treplicate\\tthe\\tneural\\nnetwork\\tacross\\tall\\tdevices\\ton\\tthe\\tcluster\\tand\\tdispatch\\tqueries\\tacross\\tall\\tdevices.\\tBy\\tadding\\tmore\\tservers\\nyou\\tcan\\thandle\\tan\\tunlimited\\tnumber\\tof\\tQPS\\t(however,\\tthis\\twill\\tnot\\treduce\\tthe\\ttime\\tit\\ttakes\\tto\\tprocess\\ta\\nsingle\\trequest\\tsince\\tit\\twill\\tstill\\thave\\tto\\twait\\tfor\\ta\\tneural\\tnetwork\\tto\\tmake\\ta\\tprediction).', 'NOTE\\nAnother\\toption\\tis\\tto\\tserve\\tyour\\tneural\\tnetworks\\tusing\\t\\nTensorFlow\\tServing\\n.\\tIt\\t\\nis\\tan\\topen\\tsource\\tsystem,\\treleased\\tby\\tGoogle\\tin\\nFebruary\\t2016,\\tdesigned\\tto\\tserve\\ta\\thigh\\tvolume\\tof\\tqueries\\tto\\tMachine\\tLearning\\tmodels\\t(typically\\tbuilt\\twith\\tTensorFlow).\\tIt\\nhandles\\tmodel\\tversioning,\\tso\\tyou\\tcan\\teasily\\tdeploy\\ta\\tnew\\tversion\\tof\\tyour\\tnetwork\\tto\\tproduction,\\tor\\texperiment\\twith\\tvarious\\nalgorithms\\twithout\\tinterrupting\\tyour\\tservice,\\tand\\tit\\tcan\\tsustain\\ta\\theavy\\tload\\tby\\tadding\\tmore\\tservers.\\tFor\\tmore\\tdetails,\\t\\ncheck\\tout\\nhttps://tensorflow.github.io/serving/\\n.', 'In-Graph\\tVersus\\tBetween-Graph\\tReplication\\nYou\\t\\ncan\\talso\\tparallelize\\tthe\\ttraining\\tof\\ta\\tlarge\\tensemble\\tof\\tneural\\tnetworks\\tby\\tsimply\\tplacing\\tevery\\nneural\\tnetwork\\ton\\ta\\tdifferent\\tdevice\\t(ensembles\\twere\\tintroduced\\tin\\t\\nChapter\\t7\\n).\\tHowever,\\tonce\\tyou\\twant\\nto\\t\\nrun\\n\\tthe\\tensemble,\\tyou\\twill\\tneed\\tto\\taggregate\\tthe\\tindividual\\tpredictions\\tmade\\tby\\teach\\tneural\\tnetwork\\tto\\nproduce\\tthe\\t\\nensemble’s\\tprediction,\\tand\\tthis\\trequires\\ta\\tbit\\tof\\tcoordination.\\nThere\\tare\\ttwo\\tmajor\\tapproaches\\tto\\thandling\\ta\\tneural\\tnetwork\\tensemble\\t(or\\tany\\tother\\tgraph\\tthat\\tcontains\\nlarge\\tchunks\\tof\\tindependent\\tcomputations):\\nYou\\tcan\\tcreate\\tone\\tbig\\tgraph,\\tcontaining\\tevery\\tneural\\tnetwork,\\teach\\tpinned\\tto\\ta\\tdifferent\\tdevice,\\nplus\\tthe\\tcomputations\\tneeded\\tto\\taggregate\\tthe\\tindividual\\tpredictions\\tfrom\\tall\\tthe\\tneural\\tnetworks\\n(see\\t\\nFigure\\t12-12\\n).\\tThen\\tyou\\tjust\\tcreate\\tone\\tsession\\tto\\tany\\tserver\\tin\\tthe\\tcluster\\tand\\tlet\\tit\\ttake\\tcare\\tof\\neverything\\t(including\\twaiting\\tfor\\tall\\tindividual\\tpredictions\\tto\\tbe\\tavailable\\tbefore\\taggregating\\tthem).', 'This\\tapproach\\tis\\t\\ncalled\\t\\nin-graph\\treplication\\n.\\nFigure\\t12-12.\\t\\nIn-graph\\treplication\\nAlternatively,\\tyou\\tcan\\tcreate\\tone\\tseparate\\tgraph\\tfor\\teach\\tneural\\tnetwork\\tand\\thandle\\tsynchronization\\nbetween\\tthese\\tgraphs\\tyourself.\\tThis\\tapproach\\tis\\t\\ncalled\\t\\nbetween-graph\\treplication\\n.\\tOne\\ttypical\\nimplementation\\tis\\tto\\tcoordinate\\tthe\\texecution\\tof\\tthese\\tgraphs\\tusing\\tqueues\\t(see\\t\\nFigure\\t12-13\\n).\\tA\\tset\\nof\\tclients\\thandles\\tone\\tneural\\tnetwork\\teach,\\treading\\tfrom\\tits\\tdedicated\\tinput\\tqueue,\\tand\\twriting\\tto\\tits\\ndedicated\\tprediction\\tqueue.\\tAnother\\tclient\\tis\\tin\\tcharge\\tof\\treading\\tthe\\tinputs\\tand\\tpushing\\tthem\\tto\\tall\\nthe\\tinput\\tqueues\\t(copying\\tall\\tinputs\\tto\\tevery\\tqueue).\\tFinally,\\tone\\tlast\\tclient\\tis\\tin\\tcharge\\tof\\treading\\none\\tprediction\\tfrom\\teach\\tprediction\\tqueue\\tand\\taggregating\\tthem\\tto\\tproduce\\tthe\\tensemble’s\\nprediction.', 'Figure\\t12-13.\\t\\nBetween-graph\\treplication\\nThese\\tsolutions\\thave\\ttheir\\tpros\\tand\\tcons.\\tIn-graph\\treplication\\tis\\tsomewhat\\tsimpler\\tto\\timplement\\tsince\\nyou\\tdon’t\\thave\\tto\\tmanage\\tmultiple\\tclients\\tand\\tmultiple\\tqueues.\\tHowever,\\tbetween-graph\\treplication\\tis\\ta\\nbit\\teasier\\tto\\torganize\\tinto\\twell-bounded\\tand\\teasy-to-test\\tmodules.\\tMoreover,\\tit\\tgives\\tyou\\tmore\\nflexibility.\\tFor\\texample,\\tyou\\tcould\\tadd\\ta\\tdequeue\\ttimeout\\tin\\tthe\\taggregator\\tclient\\tso\\tthat\\tthe\\tensemble\\nwould\\tnot\\tfail\\teven\\tif\\tone\\tof\\tthe\\tneural\\tnetwork\\tclients\\tcrashes\\tor\\tif\\tone\\tneural\\tnetwork\\ttakes\\ttoo\\tlong\\tto\\nproduce\\tits\\tprediction.\\tTensorFlow\\tlets\\tyou\\tspecify\\ta\\ttimeout\\twhen\\tcalling\\t\\nthe\\t\\nrun()\\n\\tfunction\\t\\nby\\tpassing\\na\\t\\nRunOptions\\n\\twith\\t\\ntimeout_in_ms\\n:\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n])\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\nrun_options\\n\\t\\n=\\n\\t\\ntf\\n.\\nRunOptions\\n()\\n\\t\\t\\t\\t\\nrun_options\\n.\\ntimeout_in_ms\\n\\t\\n=\\n\\t\\n1000\\n\\t\\t\\n#\\t1s\\ttimeout\\n\\t\\t\\t\\t\\ntry\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npred\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\ndequeue_prediction\\n,\\n\\t\\noptions\\n=\\nrun_options\\n)\\n\\t\\t\\t\\t\\nexcept\\n\\t\\ntf\\n.\\nerrors\\n.\\nDeadlineExceededError', 'as\\n\\t\\nex\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tthe\\tdequeue\\toperation\\ttimed\\tout\\tafter\\t1s\\nAnother\\tway\\tyou\\tcan\\tspecify\\ta\\ttimeout\\tis\\tto\\tset\\tthe\\tsession’s\\t\\noperation_timeout_in_ms\\n\\t\\nconfiguration\\noption,\\tbut\\tin\\tthis\\tcase\\tthe\\t\\nrun()\\n\\tfunction\\ttimes\\t\\nout\\tif\\t\\nany\\n\\toperation\\ttakes\\tlonger\\tthan\\tthe\\ttimeout\\t\\ndelay:\\nconfig\\n\\t\\n=\\n\\t\\ntf\\n.\\nConfigProto\\n()\\nconfig\\n.\\noperation_timeout_in_ms\\n\\t\\n=\\n\\t\\n1000\\n\\t\\t\\n#\\t1s\\ttimeout\\tfor\\tevery\\toperation\\nwith\\n\\t\\ntf\\n.\\nSession\\n([\\n...\\n],\\n\\t\\nconfig\\n=\\nconfig\\n)\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\t\\t\\ntry\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\npred\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\ndequeue_prediction\\n)', 'except\\n\\t\\ntf\\n.\\nerrors\\n.\\nDeadlineExceededError\\n\\t\\nas\\n\\t\\nex\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\t\\n#\\tthe\\tdequeue\\toperation\\ttimed\\tout\\tafter\\t1s', 'Model\\tParallelism\\nSo\\t\\nfar\\twe\\thave\\trun\\teach\\tneural\\tnetwork\\ton\\ta\\tsingle\\tdevice.\\tWhat\\tif\\twe\\twant\\tto\\trun\\ta\\tsingle\\tneural\\nnetwork\\tacross\\tmultiple\\tdevices?\\tThis\\trequires\\tchopping\\tyour\\tmodel\\tinto\\tseparate\\tchunks\\tand\\trunning\\neach\\tchunk\\ton\\ta\\tdifferent\\tdevice.\\tThis\\tis\\tcalled\\t\\nmodel\\tparallelism\\n.\\tUnfortunately,\\tmodel\\tparallelism\\tturns\\nout\\tto\\tbe\\tpretty\\ttricky,\\tand\\tit\\treally\\tdepends\\ton\\tthe\\tarchitecture\\tof\\tyour\\tneural\\tnetwork.\\tFor\\tfully\\nconnected\\tnetworks,\\tthere\\tis\\tgenerally\\tnot\\tmuch\\tto\\tbe\\tgained\\tfrom\\tthis\\tapproach\\t(see\\t\\nFigure\\t12-14\\n).\\nIntuitively,\\tit\\tmay\\tseem\\tthat\\tan\\teasy\\tway\\tto\\tsplit\\tthe\\tmodel\\tis\\tto\\tplace\\teach\\tlayer\\ton\\ta\\tdifferent\\tdevice,\\tbut\\nthis\\tdoes\\tnot\\twork\\tsince\\teach\\tlayer\\tneeds\\tto\\twait\\tfor\\tthe\\toutput\\tof\\tthe\\tprevious\\tlayer\\tbefore\\tit\\tcan\\tdo\\nanything.\\tSo\\tperhaps\\tyou\\tcan\\tslice\\tit\\tvertically\\t—\\tfor\\texample,\\twith\\tthe\\tleft\\thalf\\tof\\teach\\tlayer\\ton\\tone\\ndevice,\\tand\\tthe\\tright\\tpart\\ton\\tanother\\tdevice?\\tThis\\tis\\tslightly\\tbetter,\\tsince\\tboth\\thalves\\tof\\teach\\tlayer\\tcan', 'indeed\\twork\\tin\\tparallel,\\tbut\\tthe\\tproblem\\tis\\tthat\\teach\\thalf\\tof\\tthe\\tnext\\tlayer\\trequires\\tthe\\toutput\\tof\\tboth\\nhalves,\\tso\\tthere\\twill\\tbe\\ta\\tlot\\tof\\tcross-device\\tcommunication\\t(represented\\tby\\tthe\\tdashed\\tarrows).\\tThis\\tis\\nlikely\\tto\\tcompletely\\tcancel\\tout\\tthe\\tbenefit\\tof\\tthe\\tparallel\\tcomputation,\\tsince\\tcross-device\\tcommunication\\nis\\tslow\\t(especially\\tif\\tit\\tis\\tacross\\tseparate\\tmachines).\\nFigure\\t12-14.\\t\\nSplitting\\ta\\tfully\\tconnected\\tneural\\tnetwork\\nHowever,\\tas\\twe\\twill\\tsee\\tin\\t\\nChapter\\t13\\n,\\tsome\\tneural\\tnetwork\\tarchitectures,\\tsuch\\tas\\tconvolutional\\tneural\\nnetworks,\\tcontain\\tlayers\\tthat\\tare\\tonly\\tpartially\\tconnected\\tto\\tthe\\tlower\\tlayers,\\tso\\tit\\tis\\tmuch\\teasier\\tto\\ndistribute\\tchunks\\tacross\\tdevices\\tin\\tan\\tefficient\\tway.', 'Figure\\t12-15.\\t\\nSplitting\\ta\\tpartially\\tconnected\\tneural\\tnetwork\\nMoreover,\\tas\\twe\\twill\\tsee\\tin\\t\\nChapter\\t14\\n,\\tsome\\tdeep\\trecurrent\\tneural\\tnetworks\\tare\\tcomposed\\tof\\tseveral\\nlayers\\tof\\t\\nmemory\\tcells\\n\\t\\n(see\\tthe\\tleft\\tside\\tof\\t\\nFigure\\t12-16\\n).\\tA\\tcell’s\\toutput\\tat\\ttime\\t\\nt\\n\\tis\\tfed\\tback\\tto\\tits\\tinput\\nat\\ttime\\t\\nt\\n\\t+\\t1\\t(as\\tyou\\tcan\\tsee\\tmore\\tclearly\\ton\\tthe\\tright\\tside\\tof\\t\\nFigure\\t12-16\\n).\\tIf\\tyou\\tsplit\\tsuch\\ta\\tnetwork\\nhorizontally,\\tplacing\\teach\\tlayer\\ton\\ta\\tdifferent\\tdevice,\\tthen\\tat\\tthe\\tfirst\\tstep\\tonly\\tone\\tdevice\\twill\\tbe\\tactive,\\nat\\tthe\\tsecond\\tstep\\ttwo\\twill\\tbe\\tactive,\\tand\\tby\\tthe\\ttime\\tthe\\tsignal\\tpropagates\\tto\\tthe\\toutput\\tlayer\\tall\\tdevices\\nwill\\tbe\\tactive\\tsimultaneously.\\tThere\\tis\\tstill\\ta\\tlot\\tof\\tcross-device\\tcommunication\\tgoing\\ton,\\tbut\\tsince\\teach\\ncell\\tmay\\tbe\\tfairly\\tcomplex,\\tthe\\tbenefit\\tof\\trunning\\tmultiple\\tcells\\tin\\tparallel\\toften\\toutweighs\\tthe\\ncommunication\\tpenalty.\\nFigure\\t12-16.\\t\\nSplitting\\ta\\tdeep\\trecurrent\\tneural\\tnetwork', 'In\\tshort,\\tmodel\\tparallelism\\tcan\\tspeed\\tup\\trunning\\tor\\ttraining\\tsome\\ttypes\\tof\\tneural\\tnetworks,\\tbut\\tnot\\tall,\\nand\\tit\\trequires\\tspecial\\tcare\\tand\\ttuning,\\tsuch\\tas\\tmaking\\tsure\\tthat\\tdevices\\tthat\\tneed\\tto\\tcommunicate\\tthe\\nmost\\trun\\ton\\tthe\\tsame\\t\\nmachine.', 'Data\\tParallelism\\nAnother\\t\\nway\\tto\\tparallelize\\tthe\\ttraining\\tof\\ta\\tneural\\tnetwork\\tis\\tto\\treplicate\\tit\\ton\\teach\\tdevice,\\trun\\ta\\ttraining\\nstep\\tsimultaneously\\ton\\tall\\treplicas\\tusing\\ta\\tdifferent\\tmini-batch\\tfor\\teach,\\tand\\tthen\\taggregate\\tthe\\tgradients\\nto\\tupdate\\tthe\\tmodel\\tparameters.\\tThis\\tis\\tcalled\\t\\ndata\\tparallelism\\n\\t(see\\t\\nFigure\\t12-17\\n).\\nFigure\\t12-17.\\t\\nData\\tparallelism\\nThere\\tare\\ttwo\\tvariants\\tof\\tthis\\tapproach:\\t\\nsynchronous\\tupdates\\n\\tand\\t\\nasynchronous\\tupdates\\n.\\nSynchronous\\tupdates\\nWith\\t\\nsynchronous\\tupdates\\n,\\t\\nthe\\taggregator\\twaits\\tfor\\tall\\tgradients\\tto\\tbe\\tavailable\\tbefore\\tcomputing\\tthe\\naverage\\tand\\tapplying\\tthe\\tresult\\t(i.e.,\\tusing\\tthe\\taggregated\\tgradients\\tto\\tupdate\\tthe\\tmodel\\tparameters).\\nOnce\\ta\\treplica\\thas\\tfinished\\tcomputing\\tits\\tgradients,\\tit\\tmust\\twait\\tfor\\tthe\\tparameters\\tto\\tbe\\tupdated\\tbefore\\tit\\ncan\\tproceed\\tto\\tthe\\tnext\\tmini-batch.\\tThe\\tdownside\\tis\\tthat\\tsome\\tdevices\\tmay\\tbe\\tslower\\tthan\\tothers,\\tso\\tall\\nother\\tdevices\\twill\\thave\\tto\\twait\\tfor\\tthem\\tat\\tevery\\tstep.\\tMoreover,\\tthe\\tparameters\\twill\\tbe\\tcopied\\tto\\tevery', 'device\\talmost\\tat\\tthe\\tsame\\ttime\\t(immediately\\tafter\\tthe\\tgradients\\tare\\tapplied),\\twhich\\tmay\\tsaturate\\tthe\\nparameter\\tservers’\\tbandwidth.\\nTIP\\nTo\\treduce\\tthe\\twaiting\\ttime\\tat\\teach\\tstep,\\tyou\\tcould\\tignore\\tthe\\tgradients\\tfrom\\tthe\\tslowest\\tfew\\treplicas\\t(typically\\t~10%).\\tFor\\nexample,\\tyou\\tcould\\trun\\t20\\treplicas,\\tbut\\tonly\\taggregate\\tthe\\tgradients\\tfrom\\tthe\\tfastest\\t18\\treplicas\\tat\\teach\\tstep,\\tand\\tjust\\tignore\\tthe\\ngradients\\tfrom\\tthe\\tlast\\t2.\\tAs\\tsoon\\tas\\tthe\\tparameters\\tare\\tupdated,\\tthe\\tfirst\\t18\\treplicas\\tcan\\tstart\\tworking\\tagain\\timmediately,\\nwithout\\thaving\\tto\\twait\\tfor\\tthe\\t2\\tslowest\\treplicas.\\tThis\\tsetup\\tis\\tgenerally\\tdescribed\\tas\\thaving\\t18\\treplicas\\tplus\\t2\\t\\nspare\\treplicas\\n.\\n5', 'Asynchronous\\tupdates\\nWith\\t\\nasynchronous\\tupdates,\\twhenever\\ta\\treplica\\thas\\tfinished\\tcomputing\\tthe\\tgradients,\\tit\\timmediately\\tuses\\nthem\\tto\\tupdate\\tthe\\tmodel\\tparameters.\\tThere\\tis\\tno\\taggregation\\t(remove\\tthe\\t“mean”\\tstep\\tin\\t\\nFigure\\t12-17\\n),\\nand\\tno\\tsynchronization.\\tReplicas\\tjust\\twork\\tindependently\\tof\\tthe\\tother\\treplicas.\\tSince\\tthere\\tis\\tno\\twaiting\\nfor\\tthe\\tother\\treplicas,\\tthis\\tapproach\\truns\\tmore\\ttraining\\tsteps\\tper\\tminute.\\tMoreover,\\talthough\\tthe\\nparameters\\tstill\\tneed\\tto\\tbe\\tcopied\\tto\\tevery\\tdevice\\tat\\tevery\\tstep,\\tthis\\thappens\\tat\\tdifferent\\ttimes\\tfor\\teach\\nreplica\\tso\\tthe\\trisk\\tof\\tbandwidth\\tsaturation\\tis\\treduced.\\nData\\tparallelism\\twith\\tasynchronous\\tupdates\\tis\\tan\\tattractive\\tchoice,\\tbecause\\tof\\tits\\tsimplicity,\\tthe\\tabsence\\nof\\tsynchronization\\tdelay,\\tand\\ta\\tbetter\\tuse\\tof\\tthe\\tbandwidth.\\tHowever,\\talthough\\tit\\tworks\\treasonably\\twell\\nin\\tpractice,\\tit\\tis\\talmost\\tsurprising\\tthat\\tit\\tworks\\tat\\tall!\\tIndeed,\\tby\\tthe\\ttime\\ta\\treplica\\thas\\tfinished\\tcomputing', 'the\\tgradients\\tbased\\ton\\tsome\\tparameter\\tvalues,\\tthese\\tparameters\\twill\\thave\\tbeen\\tupdated\\tseveral\\ttimes\\tby\\nother\\treplicas\\t(on\\taverage\\t\\nN\\n\\t–\\t1\\ttimes\\tif\\tthere\\tare\\t\\nN\\n\\treplicas)\\tand\\tthere\\tis\\tno\\tguarantee\\tthat\\tthe\\tcomputed\\ngradients\\twill\\tstill\\tbe\\tpointing\\tin\\tthe\\tright\\tdirection\\t(see\\t\\nFigure\\t12-18\\n).\\tWhen\\tgradients\\tare\\tseverely\\tout-\\nof-date,\\tthey\\tare\\tcalled\\t\\nstale\\tgradients\\n:\\t\\nthey\\tcan\\tslow\\tdown\\tconvergence,\\tintroducing\\tnoise\\tand\\twobble\\neffects\\t(the\\tlearning\\tcurve\\tmay\\tcontain\\ttemporary\\toscillations),\\tor\\tthey\\tcan\\teven\\tmake\\tthe\\ttraining\\nalgorithm\\tdiverge.\\nFigure\\t12-18.\\t\\nStale\\tgradients\\twhen\\tusing\\tasynchronous\\tupdates\\nThere\\tare\\ta\\tfew\\tways\\tto\\treduce\\tthe\\teffect\\tof\\tstale\\tgradients:\\nReduce\\tthe\\tlearning\\trate.\\nDrop\\tstale\\tgradients\\tor\\tscale\\tthem\\tdown.\\nAdjust\\tthe\\tmini-batch\\tsize.\\nStart\\tthe\\tfirst\\tfew\\tepochs\\tusing\\tjust\\tone\\t\\nreplica\\t(this\\tis\\tcalled\\tthe\\t\\nwarmup\\tphase\\n).\\tStale\\tgradients\\ntend\\tto\\tbe\\tmore\\tdamaging\\tat\\tthe\\tbeginning\\tof\\ttraining,\\twhen\\tgradients\\tare\\ttypically\\tlarge\\tand\\tthe', 'parameters\\thave\\tnot\\tsettled\\tinto\\ta\\tvalley\\tof\\tthe\\t\\ncost\\tfunction\\tyet,\\tso\\tdifferent\\treplicas\\tmay\\tpush\\tthe\\nparameters\\tin\\tquite\\tdifferent\\tdirections.', 'A\\t\\npaper\\tpublished\\tby\\tthe\\tGoogle\\tBrain\\tteam\\tin\\tApril\\t2016\\n\\tbenchmarked\\tvarious\\tapproaches\\tand\\tfound\\nthat\\tdata\\tparallelism\\twith\\tsynchronous\\tupdates\\tusing\\ta\\tfew\\tspare\\treplicas\\twas\\tthe\\tmost\\tefficient,\\tnot\\tonly\\nconverging\\tfaster\\tbut\\talso\\tproducing\\ta\\tbetter\\tmodel.\\tHowever,\\tthis\\tis\\tstill\\tan\\tactive\\tarea\\tof\\tresearch,\\tso\\nyou\\tshould\\tnot\\trule\\tout\\tasynchronous\\tupdates\\t\\nquite\\tyet.\\nBandwidth\\tsaturation\\nWhether\\t\\nyou\\tuse\\tsynchronous\\tor\\tasynchronous\\tupdates,\\tdata\\tparallelism\\tstill\\trequires\\tcommunicating\\tthe\\nmodel\\tparameters\\tfrom\\tthe\\tparameter\\tservers\\tto\\tevery\\treplica\\tat\\tthe\\tbeginning\\tof\\tevery\\ttraining\\tstep,\\tand\\nthe\\tgradients\\tin\\tthe\\tother\\tdirection\\tat\\tthe\\tend\\tof\\teach\\ttraining\\tstep.\\tUnfortunately,\\tthis\\tmeans\\tthat\\tthere\\nalways\\tcomes\\ta\\tpoint\\twhere\\tadding\\tan\\textra\\tGPU\\twill\\tnot\\timprove\\tperformance\\tat\\tall\\tbecause\\tthe\\ttime\\nspent\\tmoving\\tthe\\tdata\\tin\\tand\\tout\\tof\\tGPU\\tRAM\\t(and\\tpossibly\\tacross\\tthe\\tnetwork)\\twill\\toutweigh\\tthe', 'speedup\\tobtained\\tby\\tsplitting\\tthe\\tcomputation\\tload.\\tAt\\tthat\\tpoint,\\tadding\\tmore\\tGPUs\\twill\\tjust\\tincrease\\nsaturation\\tand\\tslow\\tdown\\ttraining.\\nTIP\\nFor\\tsome\\tmodels,\\ttypically\\trelatively\\tsmall\\tand\\ttrained\\ton\\ta\\tvery\\tlarge\\ttraining\\tset,\\tyou\\tare\\toften\\tbetter\\toff\\ttraining\\tthe\\tmodel\\ton\\ta\\nsingle\\tmachine\\twith\\ta\\tsingle\\tGPU.\\nSaturation\\tis\\tmore\\tsevere\\tfor\\tlarge\\tdense\\tmodels,\\tsince\\tthey\\thave\\ta\\tlot\\tof\\tparameters\\tand\\tgradients\\tto\\ntransfer.\\tIt\\tis\\tless\\tsevere\\tfor\\tsmall\\tmodels\\t(but\\tthe\\tparallelization\\tgain\\tis\\tsmall)\\tand\\talso\\tfor\\tlarge\\tsparse\\nmodels\\tsince\\tthe\\tgradients\\tare\\ttypically\\tmostly\\tzeros,\\tso\\tthey\\tcan\\tbe\\tcommunicated\\tefficiently.\\tJeff\\tDean,\\ninitiator\\tand\\tlead\\tof\\tthe\\tGoogle\\tBrain\\tproject,\\t\\nreported\\n\\ttypical\\tspeedups\\tof\\t25–40x\\twhen\\tdistributing\\ncomputations\\tacross\\t50\\tGPUs\\tfor\\tdense\\tmodels,\\tand\\t300x\\tspeedup\\tfor\\tsparser\\tmodels\\ttrained\\tacross\\t500\\nGPUs.\\tAs\\tyou\\tcan\\tsee,\\tsparse\\tmodels\\treally\\tdo\\tscale\\tbetter.\\tHere\\tare\\ta\\tfew\\tconcrete\\texamples:\\nNeural\\tMachine\\tTranslation:\\t6x\\tspeedup\\ton\\t8\\tGPUs', 'Inception/ImageNet:\\t32x\\tspeedup\\ton\\t50\\tGPUs\\nRankBrain:\\t300x\\tspeedup\\ton\\t500\\tGPUs\\nThese\\tnumbers\\trepresent\\tthe\\tstate\\tof\\tthe\\tart\\tin\\tQ1\\t2016.\\tBeyond\\ta\\tfew\\tdozen\\tGPUs\\tfor\\ta\\tdense\\tmodel\\tor\\nfew\\thundred\\tGPUs\\tfor\\ta\\tsparse\\tmodel,\\tsaturation\\tkicks\\tin\\tand\\tperformance\\tdegrades.\\tThere\\tis\\tplenty\\tof\\nresearch\\tgoing\\ton\\tto\\tsolve\\tthis\\tproblem\\t(exploring\\tpeer-to-peer\\tarchitectures\\trather\\tthan\\tcentralized\\nparameter\\tservers,\\tusing\\tlossy\\tmodel\\tcompression,\\toptimizing\\twhen\\tand\\twhat\\tthe\\treplicas\\tneed\\tto\\ncommunicate,\\tand\\tso\\ton),\\tso\\tthere\\twill\\tlikely\\tbe\\ta\\tlot\\tof\\tprogress\\tin\\tparallelizing\\tneural\\tnetworks\\tin\\tthe\\nnext\\tfew\\tyears.\\nIn\\tthe\\tmeantime,\\there\\tare\\ta\\tfew\\tsimple\\tsteps\\tyou\\tcan\\ttake\\tto\\treduce\\tthe\\tsaturation\\tproblem:\\nGroup\\tyour\\tGPUs\\ton\\ta\\tfew\\tservers\\trather\\tthan\\tscattering\\tthem\\tacross\\tmany\\tservers.\\tThis\\twill\\tavoid\\nunnecessary\\tnetwork\\thops.\\nShard\\tthe\\tparameters\\tacross\\tmultiple\\tparameter\\tservers\\t(as\\tdiscussed\\tearlier).', 'Drop\\tthe\\tmodel\\tparameters’\\tfloat\\tprecision\\tfrom\\t32\\tbits\\t(\\ntf.float32\\n)\\tto\\t16\\tbits\\t(\\ntf.bfloat16\\n).\\nThis\\twill\\tcut\\tin\\thalf\\tthe\\tamount\\tof\\tdata\\tto\\ttransfer,\\twithout\\tmuch\\timpact\\ton\\tthe\\tconvergence\\trate\\tor\\nthe\\tmodel’s\\tperformance.\\nTIP\\nAlthough\\t16-bit\\tprecision\\tis\\tthe\\tminimum\\tfor\\ttraining\\tneural\\tnetwork,\\tyou\\tcan\\tactually\\tdrop\\tdown\\tto\\t8-bit\\tprecision\\tafter\\ttraining\\nto\\treduce\\tthe\\tsize\\tof\\tthe\\tmodel\\tand\\tspeed\\tup\\tcomputations.\\tThis\\tis\\tcalled\\t\\nquantizing\\n\\tthe\\t\\nneural\\tnetwork.\\tIt\\tis\\tparticularly\\tuseful\\nfor\\tdeploying\\tand\\trunning\\tpretrained\\tmodels\\ton\\tmobile\\tphones.\\tSee\\tPete\\tWarden’s\\t\\ngreat\\tpost\\n\\ton\\tthe\\t\\nsubject.\\nTensorFlow\\timplementation\\nTo\\t\\nimplement\\tdata\\tparallelism\\tusing\\tTensorFlow,\\tyou\\tfirst\\tneed\\tto\\tchoose\\twhether\\tyou\\twant\\tin-graph\\nreplication\\tor\\tbetween-graph\\treplication,\\tand\\twhether\\tyou\\twant\\tsynchronous\\tupdates\\tor\\tasynchronous\\nupdates.\\tLet’s\\tlook\\tat\\thow\\tyou\\twould\\timplement\\teach\\tcombination\\t(see\\tthe\\texercises\\tand\\tthe\\tJupyter\\nnotebooks\\tfor\\tcomplete\\tcode\\texamples).', 'With\\tin-graph\\treplication\\t+\\tsynchronous\\tupdates,\\tyou\\tbuild\\tone\\tbig\\tgraph\\tcontaining\\tall\\tthe\\tmodel\\nreplicas\\t(placed\\ton\\tdifferent\\tdevices),\\tand\\ta\\tfew\\tnodes\\tto\\taggregate\\tall\\ttheir\\tgradients\\tand\\tfeed\\tthem\\tto\\nan\\toptimizer.\\tYour\\tcode\\topens\\ta\\tsession\\tto\\tthe\\tcluster\\tand\\tsimply\\truns\\tthe\\ttraining\\toperation\\trepeatedly.\\nWith\\tin-graph\\treplication\\t+\\tasynchronous\\tupdates,\\tyou\\talso\\tcreate\\tone\\tbig\\tgraph,\\tbut\\twith\\tone\\toptimizer\\nper\\treplica,\\tand\\tyou\\trun\\tone\\tthread\\tper\\treplica,\\trepeatedly\\trunning\\tthe\\treplica’s\\toptimizer.\\nWith\\tbetween-graph\\treplication\\t+\\tasynchronous\\t\\nupdates,\\tyou\\trun\\tmultiple\\tindependent\\tclients\\t(typically\\nin\\tseparate\\tprocesses),\\teach\\ttraining\\tthe\\tmodel\\treplica\\tas\\tif\\tit\\twere\\talone\\tin\\tthe\\tworld,\\tbut\\tthe\\tparameters\\nare\\tactually\\tshared\\twith\\tother\\treplicas\\t(using\\ta\\tresource\\tcontainer).\\nWith\\tbetween-graph\\treplication\\t+\\tsynchronous\\tupdates,\\tonce\\tagain\\tyou\\trun\\tmultiple\\tclients,\\teach\\ttraining\\na\\tmodel\\treplica\\tbased\\ton\\tshared\\tparameters,\\tbut\\tthis\\ttime\\tyou\\twrap\\tthe\\toptimizer\\t(e.g.,\\ta', 'MomentumOptimizer\\n)\\twithin\\ta\\t\\nSyncReplicasOptimizer\\n.\\tEach\\treplica\\tuses\\tthis\\toptimizer\\tas\\tit\\twould\\nuse\\tany\\tother\\t\\noptimizer,\\tbut\\tunder\\tthe\\thood\\tthis\\toptimizer\\tsends\\tthe\\tgradients\\tto\\ta\\tset\\tof\\tqueues\\t(one\\tper\\nvariable),\\twhich\\tis\\tread\\tby\\tone\\tof\\tthe\\treplica’s\\t\\nSyncReplicasOptimizer\\n,\\tcalled\\tthe\\t\\nchief\\n.\\tThe\\tchief\\naggregates\\tthe\\tgradients\\tand\\tapplies\\tthem,\\tthen\\twrites\\ta\\ttoken\\tto\\ta\\t\\ntoken\\tqueue\\n\\tfor\\teach\\treplica,\\tsignaling\\nit\\tthat\\tit\\tcan\\tgo\\tahead\\tand\\tcompute\\tthe\\tnext\\tgradients.\\tThis\\tapproach\\tsupports\\thaving\\t\\nspare\\treplicas\\n.\\nIf\\tyou\\tgo\\tthrough\\tthe\\texercises,\\tyou\\twill\\timplement\\teach\\tof\\tthese\\tfour\\tsolutions.\\tYou\\twill\\teasily\\tbe\\table\\tto\\napply\\twhat\\tyou\\thave\\tlearned\\tto\\ttrain\\tlarge\\tdeep\\tneural\\tnetworks\\tacross\\tdozens\\tof\\tservers\\tand\\tGPUs!\\tIn\\nthe\\tfollowing\\tchapters\\twe\\twill\\tgo\\tthrough\\ta\\tfew\\tmore\\timportant\\tneural\\tnetwork\\tarchitectures\\tbefore\\twe\\ntackle\\t\\nReinforcement\\tLearning.', 'Exercises\\n1\\n.\\t\\nIf\\tyou\\tget\\ta\\t\\nCUDA_ERROR_OUT_OF_MEMORY\\n\\twhen\\tstarting\\tyour\\tTensorFlow\\tprogram,\\twhat\\tis\\nprobably\\tgoing\\ton?\\tWhat\\tcan\\tyou\\tdo\\tabout\\tit?\\n2\\n.\\t\\nWhat\\tis\\tthe\\tdifference\\tbetween\\tpinning\\tan\\toperation\\ton\\ta\\tdevice\\tand\\tplacing\\tan\\toperation\\ton\\ta\\ndevice?\\n3\\n.\\t\\nIf\\tyou\\tare\\trunning\\ton\\ta\\tGPU-enabled\\tTensorFlow\\tinstallation,\\tand\\tyou\\tjust\\tuse\\tthe\\tdefault\\tplacement,\\nwill\\tall\\toperations\\tbe\\tplaced\\ton\\tthe\\tfirst\\tGPU?\\n4\\n.\\t\\nIf\\tyou\\tpin\\ta\\tvariable\\tto\\t\\n\"/gpu:0\"\\n,\\tcan\\tit\\tbe\\tused\\tby\\toperations\\tplaced\\ton\\t\\n/gpu:1\\n?\\tOr\\tby\\toperations\\nplaced\\ton\\t\\n\"/cpu:0\"\\n?\\tOr\\tby\\toperations\\tpinned\\tto\\tdevices\\tlocated\\ton\\tother\\tservers?\\n5\\n.\\t\\nCan\\ttwo\\toperations\\tplaced\\ton\\tthe\\tsame\\tdevice\\trun\\tin\\tparallel?\\n6\\n.\\t\\nWhat\\tis\\ta\\tcontrol\\tdependency\\tand\\twhen\\twould\\tyou\\twant\\tto\\tuse\\tone?\\n7\\n.\\t\\nSuppose\\tyou\\ttrain\\ta\\tDNN\\tfor\\tdays\\ton\\ta\\tTensorFlow\\tcluster,\\tand\\timmediately\\tafter\\tyour\\ttraining\\nprogram\\tends\\tyou\\trealize\\tthat\\tyou\\tforgot\\tto\\tsave\\tthe\\tmodel\\tusing\\ta\\t\\nSaver\\n.\\tIs\\tyour\\ttrained\\tmodel\\tlost?\\n8\\n.', '8\\n.\\t\\nTrain\\tseveral\\tDNNs\\tin\\tparallel\\ton\\ta\\tTensorFlow\\tcluster,\\tusing\\tdifferent\\thyperparameter\\tvalues.\\tThis\\ncould\\tbe\\tDNNs\\tfor\\tMNIST\\tclassification\\tor\\tany\\tother\\ttask\\tyou\\tare\\tinterested\\tin.\\tThe\\tsimplest\\toption\\nis\\tto\\twrite\\ta\\tsingle\\tclient\\tprogram\\tthat\\ttrains\\tonly\\tone\\tDNN,\\tthen\\trun\\tthis\\tprogram\\tin\\tmultiple\\nprocesses\\tin\\tparallel,\\twith\\tdifferent\\thyperparameter\\tvalues\\tfor\\teach\\tclient.\\tThe\\tprogram\\tshould\\thave\\ncommand-line\\toptions\\tto\\tcontrol\\twhat\\tserver\\tand\\tdevice\\tthe\\tDNN\\tshould\\tbe\\tplaced\\ton,\\tand\\twhat\\nresource\\tcontainer\\t\\nand\\thyperparameter\\tvalues\\tto\\tuse\\t(make\\tsure\\tto\\tuse\\ta\\tdifferent\\tresource\\tcontainer\\nfor\\teach\\tDNN).\\tUse\\ta\\tvalidation\\tset\\tor\\tcross-validation\\tto\\tselect\\tthe\\ttop\\tthree\\tmodels.\\n9\\n.\\t\\nCreate\\tan\\tensemble\\tusing\\tthe\\ttop\\tthree\\tmodels\\tfrom\\tthe\\tprevious\\texercise.\\tDefine\\tit\\tin\\ta\\tsingle\\ngraph,\\tensuring\\tthat\\teach\\tDNN\\truns\\ton\\ta\\tdifferent\\tdevice.\\tEvaluate\\tit\\ton\\tthe\\tvalidation\\tset:\\tdoes\\tthe\\nensemble\\tperform\\tbetter\\tthan\\tthe\\tindividual\\tDNNs?\\n10\\n.', '10\\n.\\t\\nTrain\\ta\\tDNN\\tusing\\tbetween-graph\\treplication\\tand\\tdata\\tparallelism\\twith\\tasynchronous\\tupdates,\\ntiming\\thow\\tlong\\tit\\ttakes\\tto\\treach\\ta\\tsatisfying\\tperformance.\\tNext,\\ttry\\tagain\\tusing\\tsynchronous\\nupdates.\\tDo\\tsynchronous\\tupdates\\tproduce\\ta\\tbetter\\tmodel?\\tIs\\ttraining\\tfaster?\\tSplit\\tthe\\tDNN\\nvertically\\tand\\tplace\\teach\\tvertical\\tslice\\ton\\ta\\tdifferent\\tdevice,\\tand\\ttrain\\tthe\\tmodel\\tagain.\\tIs\\ttraining\\nany\\tfaster?\\tIs\\t\\nthe\\tperformance\\tany\\tdifferent?\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\n“TensorFlow:\\tLarge-Scale\\tMachine\\tLearning\\ton\\tHeterogeneous\\tDistributed\\tSystems,”\\tGoogle\\tResearch\\t(2015).\\nYou\\tcan\\teven\\tstart\\tmultiple\\ttasks\\tin\\tthe\\tsame\\tprocess.\\tIt\\tmay\\tbe\\tuseful\\tfor\\ttests,\\tbut\\tit\\tis\\tnot\\trecommended\\tin\\tproduction.\\nIt\\tis\\tthe\\tnext\\tversion\\tof\\tGoogle’s\\tinternal\\t\\nStubby\\n\\tservice,\\twhich\\tGoogle\\thas\\tused\\tsuccessfully\\tfor\\tover\\ta\\tdecade.\\tSee\\t\\nhttp://grpc.io/\\n\\tfor\\nmore\\tdetails.\\nNot\\t100%\\tlinear\\tif\\tyou\\twait\\tfor\\tall\\tdevices\\tto\\tfinish,\\tsince\\tthe\\ttotal\\ttime\\twill\\tbe\\tthe\\ttime\\ttaken\\tby\\tthe\\tslowest\\tdevice.', '1\\n2\\n3\\n4', 'This\\tname\\tis\\tslightly\\tconfusing\\tsince\\tit\\tsounds\\tlike\\tsome\\treplicas\\tare\\tspecial,\\tdoing\\tnothing.\\tIn\\treality,\\tall\\treplicas\\tare\\tequivalent:\\tthey\\tall\\nwork\\thard\\tto\\tbe\\tamong\\tthe\\tfastest\\tat\\teach\\ttraining\\tstep,\\tand\\tthe\\tlosers\\tvary\\tat\\tevery\\tstep\\t(unless\\tsome\\tdevices\\tare\\treally\\tslower\\tthan\\nothers).\\n5', 'Chapter\\t13.\\t\\nConvolutional\\tNeural\\tNetworks\\nAlthough\\tIBM’s\\tDeep\\tBlue\\tsupercomputer\\tbeat\\tthe\\tchess\\tworld\\tchampion\\tGarry\\tKasparov\\tback\\tin\\t1996,\\nuntil\\tquite\\trecently\\tcomputers\\twere\\tunable\\tto\\treliably\\tperform\\tseemingly\\ttrivial\\ttasks\\tsuch\\tas\\tdetecting\\ta\\npuppy\\tin\\ta\\tpicture\\tor\\trecognizing\\tspoken\\twords.\\tWhy\\tare\\tthese\\ttasks\\tso\\teffortless\\tto\\tus\\thumans?\\tThe\\nanswer\\tlies\\tin\\tthe\\tfact\\tthat\\tperception\\tlargely\\ttakes\\tplace\\toutside\\tthe\\trealm\\tof\\tour\\tconsciousness,\\twithin\\nspecialized\\tvisual,\\tauditory,\\tand\\tother\\tsensory\\tmodules\\tin\\tour\\tbrains.\\tBy\\tthe\\ttime\\tsensory\\tinformation\\nreaches\\tour\\tconsciousness,\\tit\\tis\\talready\\tadorned\\twith\\thigh-level\\tfeatures;\\tfor\\texample,\\twhen\\tyou\\tlook\\tat\\ta\\npicture\\tof\\ta\\tcute\\tpuppy,\\tyou\\tcannot\\tchoose\\t\\nnot\\n\\tto\\tsee\\tthe\\tpuppy,\\tor\\t\\nnot\\n\\tto\\tnotice\\tits\\tcuteness.\\tNor\\tcan\\tyou\\nexplain\\t\\nhow\\n\\tyou\\trecognize\\ta\\tcute\\tpuppy;\\tit’s\\tjust\\tobvious\\tto\\tyou.\\tThus,\\twe\\tcannot\\ttrust\\tour\\tsubjective\\nexperience:\\tperception\\tis\\tnot\\ttrivial\\tat\\tall,\\tand\\tto\\tunderstand\\tit\\twe\\tmust\\tlook\\tat\\thow\\tthe\\tsensory\\tmodules\\nwork.', 'work.\\nConvolutional\\tneural\\tnetworks\\t(CNNs)\\t\\nemerged\\tfrom\\tthe\\tstudy\\tof\\tthe\\tbrain’s\\tvisual\\tcortex,\\tand\\tthey\\thave\\nbeen\\tused\\tin\\timage\\trecognition\\tsince\\tthe\\t1980s.\\tIn\\tthe\\tlast\\tfew\\tyears,\\tthanks\\tto\\tthe\\tincrease\\tin\\ncomputational\\tpower,\\tthe\\tamount\\tof\\tavailable\\ttraining\\tdata,\\tand\\tthe\\ttricks\\tpresented\\tin\\t\\nChapter\\t11\\n\\tfor\\ntraining\\tdeep\\tnets,\\tCNNs\\thave\\tmanaged\\tto\\tachieve\\tsuperhuman\\tperformance\\ton\\tsome\\tcomplex\\tvisual\\ntasks.\\tThey\\tpower\\timage\\tsearch\\tservices,\\tself-driving\\tcars,\\tautomatic\\tvideo\\tclassification\\tsystems,\\tand\\nmore.\\tMoreover,\\tCNNs\\tare\\tnot\\trestricted\\tto\\tvisual\\tperception:\\tthey\\tare\\talso\\tsuccessful\\tat\\tother\\ttasks,\\nsuch\\t\\nas\\t\\nvoice\\trecognition\\n\\tor\\t\\nnatural\\tlanguage\\tprocessing\\n\\t(NLP);\\thowever,\\twe\\twill\\tfocus\\ton\\tvisual\\napplications\\tfor\\tnow.\\nIn\\tthis\\tchapter\\twe\\twill\\tpresent\\twhere\\tCNNs\\tcame\\tfrom,\\twhat\\ttheir\\tbuilding\\tblocks\\tlook\\tlike,\\tand\\thow\\tto\\nimplement\\tthem\\tusing\\tTensorFlow.\\tThen\\twe\\twill\\tpresent\\tsome\\tof\\tthe\\tbest\\tCNN\\tarchitectures.', 'The\\tArchitecture\\tof\\tthe\\tVisual\\tCortex\\nDavid\\tH.\\tHubel\\tand\\tTorsten\\tWiesel\\t\\nperformed\\ta\\tseries\\tof\\texperiments\\ton\\tcats\\tin\\t\\n1958\\n1\\n\\tand\\t\\n1959\\n2\\n\\t(and\\ta\\nfew\\tyears\\tlater\\ton\\tmonkeys\\n3\\n),\\tgiving\\tcrucial\\tinsights\\ton\\tthe\\tstructure\\tof\\tthe\\tvisual\\tcortex\\t(the\\tauthors\\nreceived\\tthe\\tNobel\\tPrize\\tin\\tPhysiology\\tor\\tMedicine\\tin\\t1981\\tfor\\ttheir\\twork).\\tIn\\tparticular,\\tthey\\tshowed\\nthat\\tmany\\tneurons\\tin\\tthe\\tvisual\\tcortex\\thave\\ta\\t\\nsmall\\t\\nlocal\\treceptive\\tfield\\n,\\tmeaning\\tthey\\treact\\tonly\\tto\\tvisual\\nstimuli\\tlocated\\tin\\ta\\tlimited\\tregion\\tof\\tthe\\tvisual\\tfield\\t(see\\t\\nFigure\\t13-1\\n,\\tin\\twhich\\tthe\\tlocal\\treceptive\\tfields\\nof\\tfive\\tneurons\\tare\\trepresented\\tby\\tdashed\\tcircles).\\tThe\\treceptive\\tfields\\tof\\tdifferent\\tneurons\\tmay\\toverlap,\\nand\\ttogether\\tthey\\ttile\\tthe\\twhole\\tvisual\\tfield.\\tMoreover,\\tthe\\tauthors\\tshowed\\tthat\\tsome\\tneurons\\treact\\tonly\\nto\\timages\\tof\\thorizontal\\tlines,\\twhile\\tothers\\treact\\tonly\\tto\\tlines\\twith\\tdifferent\\torientations\\t(two\\tneurons\\tmay', 'have\\tthe\\tsame\\treceptive\\tfield\\tbut\\treact\\tto\\tdifferent\\tline\\torientations).\\tThey\\talso\\tnoticed\\tthat\\tsome\\tneurons\\nhave\\tlarger\\treceptive\\tfields,\\tand\\tthey\\treact\\tto\\tmore\\tcomplex\\tpatterns\\tthat\\tare\\tcombinations\\tof\\tthe\\tlower-\\nlevel\\tpatterns.\\tThese\\tobservations\\tled\\tto\\tthe\\tidea\\tthat\\tthe\\thigher-level\\tneurons\\tare\\tbased\\ton\\tthe\\toutputs\\tof\\nneighboring\\tlower-level\\tneurons\\t(in\\t\\nFigure\\t13-1\\n,\\tnotice\\tthat\\teach\\tneuron\\tis\\tconnected\\tonly\\tto\\ta\\tfew\\nneurons\\tfrom\\tthe\\tprevious\\tlayer).\\tThis\\tpowerful\\tarchitecture\\tis\\table\\tto\\tdetect\\tall\\tsorts\\tof\\tcomplex\\tpatterns\\nin\\tany\\tarea\\tof\\tthe\\tvisual\\tfield.\\nFigure\\t13-1.\\t\\nLocal\\treceptive\\tfields\\tin\\tthe\\tvisual\\tcortex\\nThese\\tstudies\\tof\\tthe\\tvisual\\tcortex\\tinspired\\tthe\\t\\nneocognitron,\\tintroduced\\tin\\t1980\\n,\\n4\\n\\twhich\\tgradually\\nevolved\\tinto\\twhat\\twe\\tnow\\tcall\\t\\nconvolutional\\tneural\\tnetworks\\n.\\tAn\\timportant\\tmilestone\\twas\\ta\\t\\n1998\\npaper\\n5\\n\\tby\\tYann\\tLeCun,\\tLéon\\tBottou,\\tYoshua\\tBengio,\\tand\\tPatrick\\tHaffner,\\twhich\\tintroduced\\tthe\\tfamous\\nLeNet-5\\n\\tarchitecture,', 'widely\\tused\\tto\\trecognize\\thandwritten\\tcheck\\tnumbers.\\tThis\\tarchitecture\\thas\\tsome\\nbuilding\\tblocks\\tthat\\tyou\\talready\\tknow,\\tsuch\\tas\\tfully\\tconnected\\tlayers\\tand\\tsigmoid\\tactivation\\tfunctions,\\nbut\\tit\\talso\\tintroduces\\ttwo\\tnew\\tbuilding\\tblocks:\\t\\nconvolutional\\tlayers\\n\\tand\\t\\npooling\\tlayers\\n.\\tLet’s\\tlook\\tat\\nthem\\tnow.\\nNOTE\\nWhy\\tnot\\tsimply\\tuse\\ta\\tregular\\tdeep\\tneural\\tnetwork\\twith\\tfully\\tconnected\\tlayers\\tfor\\timage\\trecognition\\ttasks?\\tUnfortunately,\\nalthough\\tthis\\tworks\\tfine\\tfor\\tsmall\\timages\\t(e.g.,\\tMNIST),\\tit\\tbreaks\\tdown\\tfor\\tlarger\\timages\\tbecause\\tof\\tthe\\thuge\\tnumber\\tof\\nparameters\\tit\\trequires.\\tFor\\texample,\\ta\\t100\\t×\\t100\\timage\\thas\\t10,000\\tpixels,\\tand\\tif\\tthe\\tfirst\\tlayer\\thas\\tjust\\t1,000\\tneurons\\t(which\\nalready\\tseverely\\trestricts\\tthe\\tamount\\tof\\tinformation\\ttransmitted\\tto\\tthe\\tnext\\tlayer),\\tthis\\tmeans\\ta\\ttotal\\tof\\t10\\tmillion\\tconnections.\\nAnd\\tthat’s\\tjust\\tthe\\tfirst\\tlayer.\\tCNNs\\tsolve\\tthis\\tproblem\\tusing\\tpartially\\tconnected\\tlayers.', 'Convolutional\\tLayer\\nThe\\tmost\\t\\nimportant\\tbuilding\\tblock\\tof\\ta\\tCNN\\tis\\tthe\\t\\nconvolutional\\tlayer\\n:\\n6\\n\\tneurons\\tin\\tthe\\tfirst\\nconvolutional\\tlayer\\tare\\tnot\\tconnected\\tto\\tevery\\tsingle\\tpixel\\tin\\tthe\\tinput\\timage\\t(like\\tthey\\twere\\tin\\tprevious\\nchapters),\\tbut\\tonly\\tto\\tpixels\\tin\\ttheir\\treceptive\\tfields\\t(see\\t\\nFigure\\t13-2\\n).\\tIn\\tturn,\\teach\\tneuron\\tin\\tthe\\tsecond\\nconvolutional\\tlayer\\tis\\tconnected\\tonly\\tto\\tneurons\\tlocated\\twithin\\ta\\tsmall\\trectangle\\tin\\tthe\\tfirst\\tlayer.\\tThis\\narchitecture\\tallows\\tthe\\tnetwork\\tto\\tconcentrate\\ton\\tlow-level\\tfeatures\\tin\\tthe\\tfirst\\thidden\\tlayer,\\tthen\\nassemble\\tthem\\tinto\\thigher-level\\tfeatures\\tin\\tthe\\tnext\\thidden\\tlayer,\\tand\\tso\\ton.\\tThis\\thierarchical\\tstructure\\tis\\ncommon\\tin\\treal-world\\timages,\\twhich\\tis\\tone\\tof\\tthe\\treasons\\twhy\\tCNNs\\twork\\tso\\twell\\tfor\\timage\\nrecognition.\\nFigure\\t13-2.\\t\\nCNN\\tlayers\\twith\\trectangular\\tlocal\\treceptive\\tfields\\nNOTE\\nUntil\\tnow,\\tall\\tmultilayer\\tneural\\tnetworks\\twe\\tlooked\\tat\\thad\\tlayers\\tcomposed\\tof\\ta\\tlong\\tline\\tof\\tneurons,\\tand\\twe\\thad\\tto\\tflatten\\tinput', 'images\\tto\\t1D\\tbefore\\tfeeding\\tthem\\tto\\tthe\\tneural\\tnetwork.\\tNow\\teach\\tlayer\\tis\\trepresented\\tin\\t2D,\\twhich\\tmakes\\tit\\teasier\\tto\\tmatch\\nneurons\\twith\\ttheir\\tcorresponding\\tinputs.\\nA\\tneuron\\tlocated\\tin\\trow\\t\\ni\\n,\\tcolumn\\t\\nj\\n\\tof\\ta\\tgiven\\tlayer\\tis\\tconnected\\tto\\tthe\\toutputs\\tof\\tthe\\tneurons\\tin\\tthe\\nprevious\\tlayer\\tlocated\\tin\\trows\\t\\ni\\n\\tto\\t\\ni\\n\\t+\\t\\nf\\nh\\n\\t–\\t1,\\tcolumns\\t\\nj\\n\\tto\\t\\nj\\n\\t+\\t\\nf\\nw\\n\\t–\\t1,\\twhere\\t\\nf\\nh\\n\\tand\\t\\nf\\nw\\n\\tare\\tthe\\theight\\tand\\nwidth\\tof\\tthe\\treceptive\\tfield\\t(see\\t\\nFigure\\t13-3\\n).\\tIn\\torder\\tfor\\ta\\tlayer\\tto\\thave\\tthe\\tsame\\theight\\tand\\twidth\\tas\\nthe\\tprevious\\tlayer,\\tit\\tis\\tcommon\\tto\\tadd\\tzeros\\taround\\tthe\\tinputs,\\tas\\tshown\\tin\\tthe\\tdiagram.\\tThis\\tis\\t\\ncalled\\nzero\\tpadding\\n.', 'Figure\\t13-3.\\t\\nConnections\\tbetween\\tlayers\\tand\\tzero\\tpadding\\nIt\\tis\\talso\\tpossible\\tto\\tconnect\\ta\\tlarge\\tinput\\tlayer\\tto\\ta\\tmuch\\tsmaller\\tlayer\\tby\\tspacing\\tout\\tthe\\treceptive\\nfields,\\tas\\tshown\\tin\\t\\nFigure\\t13-4\\n.\\tThe\\tdistance\\tbetween\\ttwo\\tconsecutive\\treceptive\\tfields\\tis\\tcalled\\t\\nthe\\nstride\\n.\\tIn\\tthe\\tdiagram,\\ta\\t5\\t×\\t7\\tinput\\tlayer\\t(plus\\tzero\\tpadding)\\tis\\tconnected\\tto\\ta\\t3\\t×\\t4\\tlayer,\\tusing\\t3\\t×\\t3\\nreceptive\\tfields\\tand\\ta\\tstride\\tof\\t2\\t(in\\tthis\\texample\\tthe\\tstride\\tis\\tthe\\tsame\\tin\\tboth\\tdirections,\\tbut\\tit\\tdoes\\tnot\\nhave\\tto\\tbe\\tso).\\tA\\tneuron\\tlocated\\tin\\trow\\t\\ni\\n,\\tcolumn\\t\\nj\\n\\tin\\tthe\\tupper\\tlayer\\tis\\tconnected\\tto\\tthe\\toutputs\\tof\\tthe\\nneurons\\tin\\tthe\\tprevious\\tlayer\\tlocated\\tin\\trows\\t\\ni\\n\\t×\\t\\ns\\nh\\n\\tto\\t\\ni\\n\\t×\\t\\ns\\nh\\n\\t+\\t\\nf\\nh\\n\\t–\\t1,\\tcolumns\\t\\nj\\n\\t×\\t\\ns\\nw\\n\\t+\\t\\nf\\nw\\n\\t–\\t1,\\twhere\\t\\ns\\nh\\nand\\t\\ns\\nw\\n\\tare\\tthe\\tvertical\\tand\\thorizontal\\tstrides.\\nFigure\\t13-4.\\t\\nReducing\\tdimensionality\\tusing\\ta\\tstride', 'Filters\\nA\\tneuron’s\\t\\nweights\\tcan\\tbe\\trepresented\\tas\\ta\\tsmall\\timage\\tthe\\tsize\\tof\\tthe\\treceptive\\tfield.\\tFor\\texample,\\nFigure\\t13-5\\n\\tshows\\ttwo\\tpossible\\tsets\\tof\\tweights,\\tcalled\\t\\nfilters\\n\\t\\n(or\\t\\nconvolution\\tkernels\\n).\\tThe\\tfirst\\tone\\tis\\nrepresented\\tas\\ta\\tblack\\tsquare\\twith\\ta\\tvertical\\twhite\\tline\\tin\\tthe\\tmiddle\\t(it\\tis\\ta\\t7\\t×\\t7\\tmatrix\\tfull\\tof\\t0s\\texcept\\nfor\\tthe\\tcentral\\tcolumn,\\twhich\\tis\\tfull\\tof\\t1s);\\tneurons\\tusing\\tthese\\tweights\\twill\\tignore\\teverything\\tin\\ttheir\\nreceptive\\tfield\\texcept\\tfor\\tthe\\tcentral\\tvertical\\tline\\t(since\\tall\\tinputs\\twill\\tget\\tmultiplied\\tby\\t0,\\texcept\\tfor\\tthe\\nones\\tlocated\\tin\\tthe\\tcentral\\tvertical\\tline).\\tThe\\tsecond\\tfilter\\tis\\ta\\tblack\\tsquare\\twith\\ta\\thorizontal\\twhite\\tline\\nin\\tthe\\tmiddle.\\tOnce\\tagain,\\tneurons\\tusing\\tthese\\tweights\\twill\\tignore\\teverything\\tin\\ttheir\\treceptive\\tfield\\nexcept\\tfor\\tthe\\tcentral\\thorizontal\\tline.\\nNow\\tif\\tall\\tneurons\\tin\\ta\\tlayer\\tuse\\tthe\\tsame\\tvertical\\tline\\tfilter\\t(and\\tthe\\tsame\\tbias\\tterm),\\tand\\tyou\\tfeed\\tthe\\nnetwork\\tthe\\tinput\\timage\\tshown\\tin\\t\\nFigure\\t13-5', '(bottom\\timage),\\tthe\\tlayer\\twill\\toutput\\tthe\\ttop-left\\timage.\\nNotice\\tthat\\tthe\\tvertical\\twhite\\tlines\\tget\\tenhanced\\twhile\\tthe\\trest\\tgets\\tblurred.\\tSimilarly,\\tthe\\tupper-right\\nimage\\tis\\twhat\\tyou\\tget\\tif\\tall\\tneurons\\tuse\\tthe\\thorizontal\\tline\\tfilter;\\tnotice\\tthat\\tthe\\thorizontal\\twhite\\tlines\\tget\\nenhanced\\twhile\\tthe\\trest\\tis\\tblurred\\tout.\\tThus,\\ta\\tlayer\\tfull\\tof\\tneurons\\tusing\\tthe\\tsame\\tfilter\\tgives\\t\\nyou\\ta\\nfeature\\tmap\\n,\\twhich\\thighlights\\tthe\\tareas\\tin\\tan\\timage\\tthat\\tare\\tmost\\tsimilar\\tto\\tthe\\tfilter.\\tDuring\\ttraining,\\ta\\nCNN\\tfinds\\tthe\\tmost\\tuseful\\tfilters\\tfor\\tits\\ttask,\\tand\\tit\\tlearns\\tto\\tcombine\\tthem\\tinto\\tmore\\tcomplex\\tpatterns\\n(e.g.,\\ta\\tcross\\tis\\tan\\tarea\\tin\\tan\\timage\\twhere\\tboth\\tthe\\tvertical\\tfilter\\tand\\tthe\\thorizontal\\tfilter\\tare\\tactive).\\nFigure\\t13-5.\\t\\nApplying\\ttwo\\tdifferent\\tfilters\\tto\\tget\\ttwo\\tfeature\\tmaps', 'Stacking\\tMultiple\\tFeature\\tMaps\\nUp\\t\\nto\\tnow,\\tfor\\tsimplicity,\\twe\\thave\\trepresented\\teach\\tconvolutional\\tlayer\\tas\\ta\\tthin\\t2D\\tlayer,\\tbut\\tin\\treality\\nit\\tis\\tcomposed\\tof\\tseveral\\tfeature\\tmaps\\tof\\tequal\\tsizes,\\tso\\tit\\tis\\tmore\\taccurately\\trepresented\\tin\\t3D\\t(see\\nFigure\\t13-6\\n).\\tWithin\\tone\\tfeature\\tmap,\\tall\\tneurons\\tshare\\tthe\\tsame\\tparameters\\t(weights\\tand\\tbias\\tterm),\\tbut\\ndifferent\\tfeature\\tmaps\\tmay\\thave\\tdifferent\\tparameters.\\tA\\tneuron’s\\treceptive\\tfield\\tis\\tthe\\tsame\\tas\\tdescribed\\nearlier,\\tbut\\tit\\textends\\tacross\\tall\\tthe\\tprevious\\tlayers’\\tfeature\\tmaps.\\tIn\\tshort,\\ta\\tconvolutional\\tlayer\\nsimultaneously\\tapplies\\tmultiple\\tfilters\\tto\\tits\\tinputs,\\tmaking\\tit\\tcapable\\tof\\tdetecting\\tmultiple\\tfeatures\\nanywhere\\tin\\tits\\tinputs.\\nNOTE\\nThe\\tfact\\tthat\\tall\\tneurons\\tin\\ta\\tfeature\\tmap\\tshare\\tthe\\tsame\\tparameters\\tdramatically\\treduces\\tthe\\tnumber\\tof\\tparameters\\tin\\tthe\\nmodel,\\tbut\\tmost\\timportantly\\tit\\tmeans\\tthat\\tonce\\tthe\\tCNN\\thas\\tlearned\\tto\\trecognize\\ta\\tpattern\\tin\\tone\\tlocation,\\tit\\tcan\\trecognize\\tit\\tin', 'any\\tother\\tlocation.\\tIn\\tcontrast,\\tonce\\ta\\tregular\\tDNN\\thas\\tlearned\\tto\\trecognize\\ta\\tpattern\\tin\\tone\\tlocation,\\tit\\tcan\\trecognize\\tit\\tonly\\tin\\nthat\\tparticular\\tlocation.\\nMoreover,\\tinput\\timages\\tare\\talso\\tcomposed\\tof\\tmultiple\\tsublayers:\\tone\\tper\\t\\ncolor\\tchannel\\n.\\tThere\\tare\\ntypically\\tthree:\\tred,\\tgreen,\\tand\\tblue\\t(RGB).\\tGrayscale\\timages\\thave\\tjust\\tone\\tchannel,\\tbut\\tsome\\timages\\nmay\\thave\\tmuch\\tmore\\t—\\tfor\\texample,\\tsatellite\\timages\\tthat\\tcapture\\textra\\tlight\\tfrequencies\\t(such\\tas\\ninfrared).', 'Figure\\t13-6.\\t\\nConvolution\\tlayers\\twith\\tmultiple\\tfeature\\tmaps,\\tand\\timages\\twith\\tthree\\tchannels\\nSpecifically,\\ta\\tneuron\\tlocated\\tin\\trow\\t\\ni\\n,\\tcolumn\\t\\nj\\n\\tof\\tthe\\tfeature\\tmap\\t\\nk\\n\\tin\\ta\\tgiven\\tconvolutional\\tlayer\\t\\nl\\n\\tis\\nconnected\\tto\\tthe\\toutputs\\tof\\tthe\\tneurons\\tin\\tthe\\tprevious\\tlayer\\t\\nl\\n\\t–\\t1,\\tlocated\\tin\\trows\\t\\ni\\n\\t×\\t\\ns\\nh\\n\\tto\\t\\ni\\n\\t×\\t\\ns\\nh\\n\\t+\\t\\nf\\nh\\n\\t–\\t1\\nand\\tcolumns\\t\\nj\\n\\t×\\t\\ns\\nw\\n\\tto\\t\\nj\\n\\t×\\t\\ns\\nw\\n\\t+\\t\\nf\\nw\\n\\t–\\t1,\\tacross\\tall\\tfeature\\tmaps\\t(in\\tlayer\\t\\nl\\n\\t–\\t\\n1\\n).\\tNote\\tthat\\tall\\tneurons\\tlocated\\nin\\tthe\\tsame\\trow\\t\\ni\\n\\tand\\tcolumn\\t\\nj\\n\\tbut\\tin\\tdifferent\\tfeature\\tmaps\\tare\\tconnected\\tto\\tthe\\toutputs\\tof\\tthe\\texact\\tsame\\nneurons\\tin\\tthe\\tprevious\\tlayer.\\nEquation\\t13-1\\n\\tsummarizes\\tthe\\tpreceding\\texplanations\\tin\\tone\\tbig\\tmathematical\\tequation:\\tit\\tshows\\thow\\tto\\ncompute\\tthe\\toutput\\tof\\ta\\tgiven\\tneuron\\tin\\ta\\tconvolutional\\tlayer.\\tIt\\tis\\ta\\tbit\\tugly\\tdue\\tto\\tall\\tthe\\tdifferent\\nindices,\\tbut\\tall\\tit\\tdoes\\tis\\tcalculate\\tthe\\tweighted\\tsum\\tof\\tall\\tthe\\tinputs,\\tplus\\tthe\\tbias\\tterm.\\nEquation\\t13-1.\\t\\nComputing\\tthe\\toutput\\tof\\ta\\tneuron\\tin\\ta\\tconvolutional\\tlayer\\nz\\ni,\\tj,\\tk', 'z\\ni,\\tj,\\tk\\n\\tis\\tthe\\toutput\\tof\\tthe\\tneuron\\tlocated\\tin\\trow\\t\\ni\\n,\\tcolumn\\t\\nj\\n\\tin\\tfeature\\tmap\\t\\nk\\n\\tof\\tthe\\tconvolutional\\tlayer\\n(layer\\t\\nl\\n).\\nAs\\texplained\\tearlier,\\t\\ns\\nh\\n\\tand\\t\\ns\\nw\\n\\tare\\tthe\\tvertical\\tand\\thorizontal\\tstrides,\\t\\nf\\nh\\n\\tand\\t\\nf\\nw\\n\\tare\\tthe\\theight\\tand\\nwidth\\tof\\tthe\\treceptive\\tfield,\\tand\\t\\nf\\nn\\n′\\n\\tis\\tthe\\tnumber\\tof\\tfeature\\tmaps\\tin\\tthe\\tprevious\\tlayer\\t(layer\\t\\nl\\n\\t–\\t1).', 'x\\ni\\n′,\\t\\nj\\n′,\\t\\nk\\n′\\n\\tis\\tthe\\toutput\\tof\\tthe\\tneuron\\tlocated\\tin\\tlayer\\t\\nl\\n\\t–\\t1,\\trow\\t\\ni\\n′,\\tcolumn\\t\\nj\\n′,\\tfeature\\tmap\\t\\nk\\n′\\t(or\\tchannel\\t\\nk\\n′\\nif\\tthe\\tprevious\\tlayer\\tis\\tthe\\tinput\\tlayer).\\nb\\nk\\n\\tis\\tthe\\tbias\\tterm\\tfor\\tfeature\\tmap\\t\\nk\\n\\t(in\\tlayer\\t\\nl\\n).\\tYou\\tcan\\tthink\\tof\\tit\\tas\\ta\\tknob\\tthat\\ttweaks\\tthe\\toverall\\nbrightness\\tof\\tthe\\tfeature\\tmap\\t\\nk\\n.\\nw\\nu\\n,\\t\\nv\\n,\\t\\nk\\n′\\t,\\nk\\n\\tis\\tthe\\tconnection\\tweight\\tbetween\\tany\\tneuron\\tin\\tfeature\\tmap\\t\\nk\\n\\tof\\tthe\\tlayer\\t\\nl\\n\\tand\\tits\\tinput\\nlocated\\tat\\trow\\t\\nu\\n,\\tcolumn\\t\\nv\\n\\t(relative\\tto\\tthe\\tneuron’s\\treceptive\\tfield),\\tand\\tfeature\\tmap\\t\\nk\\n′.', 'TensorFlow\\tImplementation\\nIn\\t\\nTensorFlow,\\teach\\tinput\\timage\\tis\\ttypically\\trepresented\\tas\\ta\\t3D\\ttensor\\tof\\t\\nshape\\t[height,\\twidth,\\nchannels]\\n.\\tA\\tmini-batch\\tis\\trepresented\\tas\\ta\\t4D\\ttensor\\tof\\t\\nshape\\t[mini-batch\\tsize,\\theight,\\nwidth,\\tchannels]\\n.\\tThe\\tweights\\tof\\ta\\tconvolutional\\tlayer\\tare\\trepresented\\tas\\ta\\t4D\\ttensor\\tof\\tshape\\t[\\nf\\nh\\n,\\t\\nf\\nw\\n,\\nf\\nn\\n′\\n,\\t\\nf\\nn\\n].\\tThe\\tbias\\tterms\\tof\\ta\\tconvolutional\\tlayer\\tare\\tsimply\\trepresented\\tas\\ta\\t1D\\ttensor\\tof\\t\\nshape\\t[f\\nn\\n]\\n.\\nLet’s\\tlook\\tat\\ta\\tsimple\\texample.\\tThe\\tfollowing\\tcode\\tloads\\ttwo\\tsample\\timages,\\tusing\\tScikit-Learn’s\\nload_sample_images()\\n\\t\\n(which\\tloads\\ttwo\\tcolor\\timages,\\tone\\tof\\ta\\tChinese\\ttemple,\\tand\\tthe\\tother\\tof\\ta\\nflower).\\tThen\\tit\\tcreates\\ttwo\\t7\\t×\\t7\\tfilters\\t(one\\twith\\ta\\tvertical\\twhite\\tline\\tin\\tthe\\tmiddle,\\tand\\tthe\\tother\\twith\\ta\\nhorizontal\\twhite\\tline\\tin\\tthe\\tmiddle),\\tand\\tapplies\\tthem\\tto\\tboth\\timages\\tusing\\ta\\tconvolutional\\tlayer\\tbuilt\\nusing\\tTensorFlow’s\\t\\ntf.nn.conv2d()\\n\\tfunction\\t(with\\tzero\\tpadding\\tand\\ta\\tstride\\tof\\t\\n2\\n).\\t\\nFinally,\\tit\\tplots\\tone', 'of\\tthe\\tresulting\\tfeature\\tmaps\\t(similar\\tto\\tthe\\ttop-right\\timage\\tin\\t\\nFigure\\t13-5\\n).\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nfrom\\n\\t\\nsklearn.datasets\\n\\t\\nimport\\n\\t\\nload_sample_images\\n#\\tLoad\\tsample\\timages\\nchina\\n\\t\\n=\\n\\t\\nload_sample_image\\n(\\n\"china.jpg\"\\n)\\nflower\\n\\t\\n=\\n\\t\\nload_sample_image\\n(\\n\"flower.jpg\"\\n)\\ndataset\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\nchina\\n,\\n\\t\\nflower\\n],\\n\\t\\ndtype\\n=\\nnp\\n.\\nfloat32\\n)\\nbatch_size\\n,\\n\\t\\nheight\\n,\\n\\t\\nwidth\\n,\\n\\t\\nchannels\\n\\t\\n=\\n\\t\\ndataset\\n.\\nshape\\n#\\tCreate\\t2\\tfilters\\nfilters\\n\\t\\n=\\n\\t\\nnp\\n.\\nzeros\\n(\\nshape\\n=\\n(\\n7\\n,\\n\\t\\n7\\n,\\n\\t\\nchannels\\n,\\n\\t\\n2\\n),\\n\\t\\ndtype\\n=\\nnp\\n.\\nfloat32\\n)\\nfilters\\n[:,\\n\\t\\n3\\n,\\n\\t\\n:,\\n\\t\\n0\\n]\\n\\t\\n=\\n\\t\\n1\\n\\t\\t\\n#\\tvertical\\tline\\nfilters\\n[\\n3\\n,\\n\\t\\n:,\\n\\t\\n:,\\n\\t\\n1\\n]\\n\\t\\n=\\n\\t\\n1\\n\\t\\t\\n#\\thorizontal\\tline\\n#\\tCreate\\ta\\tgraph\\twith\\tinput\\tX\\tplus\\ta\\tconvolutional\\tlayer\\tapplying\\tthe\\t2\\tfilters\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nheight\\n,\\n\\t\\nwidth\\n,\\n\\t\\nchannels\\n))\\nconvolution\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nconv2d\\n(\\nX\\n,\\n\\t\\nfilters\\n,\\n\\t\\nstrides\\n=\\n[\\n1\\n,\\n2\\n,\\n2\\n,\\n1\\n],\\n\\t\\npadding\\n=\\n\"SAME\"\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\noutput\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(', '.\\nrun\\n(\\nconvolution\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\ndataset\\n})\\nplt\\n.\\nimshow\\n(\\noutput\\n[\\n0\\n,\\n\\t\\n:,\\n\\t\\n:,\\n\\t\\n1\\n],\\n\\t\\ncmap\\n=\\n\"gray\"\\n)\\n\\t\\n#\\tplot\\t1st\\timage\\'s\\t2nd\\tfeature\\tmap\\nplt\\n.\\nshow\\n()\\nMost\\tof\\tthis\\tcode\\tis\\tself-explanatory,\\t\\nbut\\tthe\\t\\ntf.nn.conv2d()\\n\\tline\\tdeserves\\ta\\tbit\\tof\\texplanation:\\nX\\n\\tis\\tthe\\tinput\\tmini-batch\\t(a\\t4D\\ttensor,\\tas\\texplained\\tearlier).\\nfilters\\n\\tis\\tthe\\tset\\tof\\tfilters\\tto\\tapply\\t(also\\ta\\t4D\\ttensor,\\tas\\texplained\\tearlier).\\nstrides\\n\\tis\\ta\\tfour-element\\t1D\\tarray,\\twhere\\tthe\\ttwo\\tcentral\\telements\\tare\\tthe\\tvertical\\tand\\thorizontal\\nstrides\\t(\\ns\\nh\\n\\tand\\t\\ns\\nw\\n).\\tThe\\tfirst\\tand\\tlast\\telements\\tmust\\tcurrently\\tbe\\tequal\\tto\\t1.\\tThey\\tmay\\tone\\tday\\tbe\\nused\\tto\\tspecify\\ta\\tbatch\\tstride\\t(to\\tskip\\tsome\\tinstances)\\tand\\ta\\tchannel\\tstride\\t(to\\tskip\\tsome\\tof\\tthe\\nprevious\\tlayer’s\\tfeature\\tmaps\\tor\\tchannels).\\npadding\\n\\tmust\\tbe\\teither\\t\\n\"VALID\"\\n\\tor\\t\\n\"SAME\"\\n:\\nIf\\tset\\tto\\t\\n\"VALID\"\\n,\\tthe\\tconvolutional\\tlayer\\tdoes\\t\\nnot\\n\\tuse\\tzero\\tpadding,\\tand\\tmay\\tignore\\tsome\\nrows\\tand\\tcolumns\\tat\\tthe\\tbottom\\tand\\tright\\tof\\tthe\\tinput\\timage,\\tdepending\\ton\\tthe\\tstride,\\tas\\tshown', 'in\\t\\nFigure\\t13-7\\n\\t(for\\tsimplicity,\\tonly\\tthe\\thorizontal\\tdimension\\tis\\tshown\\there,\\tbut\\tof\\tcourse\\tthe\\nsame\\tlogic\\tapplies\\tto\\tthe\\tvertical\\tdimension).', 'If\\tset\\tto\\t\\n\"SAME\"\\n,\\tthe\\tconvolutional\\tlayer\\tuses\\t\\nzero\\tpadding\\tif\\tnecessary.\\tIn\\tthis\\tcase,\\tthe\\tnumber\\nof\\toutput\\tneurons\\tis\\tequal\\tto\\tthe\\tnumber\\tof\\tinput\\tneurons\\tdivided\\tby\\tthe\\tstride,\\trounded\\tup\\t(in\\nthis\\texample,\\tceil\\t(13\\t/\\t5)\\t=\\t3).\\tThen\\tzeros\\tare\\tadded\\tas\\tevenly\\tas\\tpossible\\taround\\tthe\\tinputs.\\nFigure\\t13-7.\\t\\nPadding\\toptions\\t—\\tinput\\twidth:\\t13,\\tfilter\\twidth:\\t6,\\tstride:\\t5\\nIn\\tthis\\tsimple\\texample,\\twe\\tmanually\\tcreated\\tthe\\tfilters,\\tbut\\tin\\ta\\treal\\tCNN\\tyou\\twould\\tlet\\tthe\\ttraining\\nalgorithm\\tdiscover\\tthe\\tbest\\tfilters\\tautomatically.\\tTensorFlow\\thas\\ta\\t\\ntf.layers.conv2d()\\n\\tfunction\\twhich\\ncreates\\tthe\\tfilters\\tvariable\\tfor\\tyou\\t(called\\t\\nkernel\\n),\\tand\\tinitializes\\tit\\trandomly.\\tFor\\texample,\\tthe\\nfollowing\\tcode\\tcreates\\tan\\tinput\\tplaceholder\\tfollowed\\tby\\ta\\tconvolutional\\tlayer\\twith\\ttwo\\t7\\t×\\t7\\tfeature\\nmaps,\\tusing\\t2\\t×\\t2\\tstrides\\t(note\\tthat\\tthis\\tfunction\\tonly\\texpects\\tthe\\tvertical\\tand\\thorizontal\\tstrides),\\tand\\n\"SAME\"\\n\\tpadding:\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nheight\\n,\\n\\t\\nwidth\\n,\\n\\t\\nchannels\\n),\\n\\t\\ndtype\\n=\\ntf\\n.', '=\\ntf\\n.\\nfloat32\\n)\\nconv\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\nconv2d\\n(\\nX\\n,\\n\\t\\nfilters\\n=\\n2\\n,\\n\\t\\nkernel_size\\n=\\n7\\n,\\n\\t\\nstrides\\n=\\n[\\n2\\n,\\n2\\n],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\npadding\\n=\\n\"SAME\"\\n)\\nUnfortunately,\\tconvolutional\\tlayers\\thave\\tquite\\ta\\tfew\\thyperparameters:\\tyou\\tmust\\tchoose\\tthe\\tnumber\\tof\\nfilters,\\ttheir\\theight\\tand\\twidth,\\tthe\\tstrides,\\tand\\tthe\\tpadding\\ttype.\\tAs\\talways,\\tyou\\tcan\\tuse\\tcross-validation\\nto\\tfind\\tthe\\tright\\thyperparameter\\tvalues,\\tbut\\tthis\\tis\\tvery\\ttime-consuming.\\tWe\\twill\\tdiscuss\\tcommon\\tCNN\\narchitectures\\tlater,\\tto\\tgive\\tyou\\tsome\\tidea\\tof\\twhat\\thyperparameter\\tvalues\\twork\\tbest\\tin\\t\\npractice.', 'Memory\\tRequirements\\nAnother\\t\\nproblem\\twith\\tCNNs\\tis\\tthat\\tthe\\tconvolutional\\tlayers\\trequire\\ta\\thuge\\tamount\\tof\\tRAM,\\tespecially\\nduring\\ttraining,\\tbecause\\tthe\\treverse\\tpass\\tof\\tbackpropagation\\trequires\\tall\\tthe\\tintermediate\\tvalues\\ncomputed\\tduring\\tthe\\tforward\\tpass.\\nFor\\texample,\\tconsider\\ta\\tconvolutional\\tlayer\\twith\\t5\\t×\\t5\\tfilters,\\toutputting\\t200\\tfeature\\tmaps\\tof\\tsize\\t150\\t×\\n100,\\twith\\tstride\\t1\\tand\\tSAME\\tpadding.\\tIf\\tthe\\tinput\\tis\\ta\\t150\\t×\\t100\\tRGB\\timage\\t(three\\tchannels),\\tthen\\tthe\\nnumber\\tof\\tparameters\\tis\\t(5\\t×\\t5\\t×\\t3\\t+\\t1)\\t×\\t200\\t=\\t15,200\\t(the\\t+1\\tcorresponds\\tto\\tthe\\tbias\\tterms),\\twhich\\tis\\nfairly\\tsmall\\tcompared\\tto\\ta\\tfully\\tconnected\\tlayer.\\n7\\n\\tHowever,\\teach\\tof\\tthe\\t200\\tfeature\\tmaps\\tcontains\\t150\\t×\\n100\\tneurons,\\tand\\teach\\tof\\tthese\\tneurons\\tneeds\\tto\\tcompute\\ta\\tweighted\\tsum\\tof\\tits\\t5\\t×\\t5\\t×\\t3\\t=\\t75\\t\\ninputs:\\nthat’s\\ta\\ttotal\\tof\\t225\\tmillion\\tfloat\\tmultiplications.\\tNot\\tas\\tbad\\tas\\ta\\tfully\\tconnected\\tlayer,\\tbut\\tstill\\tquite\\ncomputationally\\tintensive.\\tMoreover,\\tif\\tthe\\tfeature\\tmaps\\tare\\trepresented\\tusing\\t32-bit\\tfloats,\\tthen\\tthe', 'convolutional\\tlayer’s\\toutput\\twill\\toccupy\\t200\\t×\\t150\\t×\\t100\\t×\\t32\\t=\\t96\\tmillion\\tbits\\t(about\\t11.4\\tMB)\\tof\\nRAM.\\n8\\n\\tAnd\\tthat’s\\tjust\\tfor\\tone\\tinstance!\\tIf\\ta\\ttraining\\tbatch\\tcontains\\t100\\tinstances,\\tthen\\tthis\\tlayer\\twill\\tuse\\nup\\tover\\t1\\tGB\\tof\\tRAM!\\nDuring\\t\\ninference\\t(i.e.,\\twhen\\tmaking\\ta\\tprediction\\tfor\\ta\\tnew\\tinstance)\\tthe\\tRAM\\toccupied\\tby\\tone\\tlayer\\tcan\\nbe\\treleased\\tas\\tsoon\\tas\\tthe\\tnext\\tlayer\\thas\\tbeen\\tcomputed,\\tso\\tyou\\tonly\\tneed\\tas\\tmuch\\tRAM\\tas\\trequired\\tby\\ntwo\\tconsecutive\\tlayers.\\tBut\\tduring\\ttraining\\teverything\\tcomputed\\tduring\\tthe\\tforward\\tpass\\tneeds\\tto\\tbe\\npreserved\\tfor\\tthe\\treverse\\tpass,\\tso\\tthe\\tamount\\tof\\tRAM\\tneeded\\tis\\t(at\\tleast)\\tthe\\ttotal\\tamount\\tof\\tRAM\\nrequired\\tby\\tall\\tlayers.\\nTIP\\nIf\\ttraining\\tcrashes\\tbecause\\tof\\tan\\tout-of-memory\\terror,\\tyou\\tcan\\ttry\\treducing\\tthe\\tmini-batch\\tsize.\\tAlternatively,\\tyou\\tcan\\ttry\\nreducing\\tdimensionality\\tusing\\ta\\tstride,\\tor\\tremoving\\ta\\tfew\\tlayers.\\tOr\\tyou\\tcan\\ttry\\tusing\\t16-bit\\tfloats\\tinstead\\tof\\t32-bit\\tfloats.\\tOr\\tyou\\ncould\\tdistribute\\tthe\\tCNN\\tacross\\tmultiple\\tdevices.\\nNow', 'Now\\t\\nlet’s\\tlook\\tat\\tthe\\tsecond\\tcommon\\tbuilding\\tblock\\t\\nof\\tCNNs:\\tthe\\t\\npooling\\tlayer\\n.', 'Pooling\\tLayer\\nOnce\\t\\nyou\\tunderstand\\thow\\tconvolutional\\tlayers\\twork,\\tthe\\tpooling\\tlayers\\tare\\tquite\\teasy\\tto\\tgrasp.\\tTheir\\ngoal\\tis\\t\\nto\\t\\nsubsample\\n\\t(i.e.,\\tshrink)\\tthe\\tinput\\timage\\tin\\torder\\tto\\treduce\\tthe\\tcomputational\\tload,\\tthe\\tmemory\\nusage,\\tand\\tthe\\tnumber\\tof\\tparameters\\t(thereby\\tlimiting\\tthe\\trisk\\tof\\toverfitting).\\tReducing\\tthe\\tinput\\timage\\nsize\\talso\\tmakes\\tthe\\tneural\\tnetwork\\ttolerate\\ta\\tlittle\\tbit\\tof\\t\\nimage\\tshift\\t(\\nlocation\\tinvariance\\n).\\nJust\\tlike\\tin\\tconvolutional\\tlayers,\\teach\\tneuron\\tin\\ta\\tpooling\\tlayer\\tis\\tconnected\\tto\\tthe\\toutputs\\tof\\ta\\tlimited\\nnumber\\tof\\tneurons\\tin\\tthe\\tprevious\\tlayer,\\tlocated\\twithin\\ta\\tsmall\\trectangular\\treceptive\\tfield.\\tYou\\tmust\\ndefine\\tits\\tsize,\\tthe\\tstride,\\tand\\tthe\\tpadding\\ttype,\\tjust\\tlike\\tbefore.\\tHowever,\\ta\\tpooling\\tneuron\\thas\\tno\\nweights;\\tall\\tit\\tdoes\\tis\\taggregate\\tthe\\tinputs\\tusing\\tan\\taggregation\\tfunction\\tsuch\\tas\\tthe\\tmax\\tor\\tmean.\\nFigure\\t13-8\\n\\tshows\\ta\\t\\nmax\\tpooling\\tlayer\\n,\\t\\nwhich\\tis\\tthe\\tmost\\tcommon\\ttype\\tof\\tpooling\\tlayer.\\tIn\\tthis\\texample,\\nwe\\tuse\\ta\\t2\\t×\\t2\\t\\npooling\\tkernel\\n,', ',\\t\\na\\tstride\\tof\\t2,\\tand\\tno\\tpadding.\\tNote\\tthat\\tonly\\tthe\\tmax\\tinput\\tvalue\\tin\\teach\\nkernel\\tmakes\\tit\\tto\\tthe\\tnext\\tlayer.\\tThe\\tother\\tinputs\\tare\\tdropped.\\nFigure\\t13-8.\\t\\nMax\\tpooling\\tlayer\\t(2\\t×\\t2\\tpooling\\tkernel,\\tstride\\t2,\\tno\\tpadding)\\nThis\\tis\\tobviously\\ta\\tvery\\tdestructive\\tkind\\tof\\tlayer:\\teven\\twith\\ta\\ttiny\\t2\\t×\\t2\\tkernel\\tand\\ta\\tstride\\tof\\t2,\\tthe\\noutput\\twill\\tbe\\ttwo\\ttimes\\tsmaller\\tin\\tboth\\tdirections\\t(so\\tits\\tarea\\twill\\tbe\\tfour\\ttimes\\tsmaller),\\tsimply\\ndropping\\t75%\\tof\\tthe\\tinput\\tvalues.\\nA\\tpooling\\tlayer\\ttypically\\tworks\\ton\\tevery\\tinput\\tchannel\\tindependently,\\tso\\tthe\\toutput\\tdepth\\tis\\tthe\\tsame\\tas\\nthe\\tinput\\tdepth.\\tYou\\tmay\\talternatively\\tpool\\tover\\tthe\\tdepth\\tdimension,\\tas\\twe\\twill\\tsee\\tnext,\\tin\\twhich\\tcase\\nthe\\timage’s\\tspatial\\tdimensions\\t(height\\tand\\twidth)\\tremain\\tunchanged,\\tbut\\tthe\\tnumber\\tof\\tchannels\\tis\\nreduced.\\nImplementing\\ta\\tmax\\tpooling\\tlayer\\tin\\t\\nTensorFlow\\tis\\tquite\\teasy.\\tThe\\tfollowing\\tcode\\tcreates\\ta\\tmax\\tpooling\\nlayer\\tusing\\ta\\t2\\t×\\t2\\tkernel,\\tstride\\t2,\\tand\\tno\\tpadding,\\tthen\\tapplies\\tit\\tto\\tall\\tthe\\timages\\tin\\t\\nthe\\tdataset:\\n[\\n...\\n]', '[\\n...\\n]\\n\\t\\n#\\tload\\tthe\\timage\\tdataset,\\tjust\\tlike\\tabove\\n#\\tCreate\\ta\\tgraph\\twith\\tinput\\tX\\tplus\\ta\\tmax\\tpooling\\tlayer\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n(\\nNone\\n,\\n\\t\\nheight\\n,\\n\\t\\nwidth\\n,\\n\\t\\nchannels\\n))\\nmax_pool\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nmax_pool\\n(\\nX\\n,\\n\\t\\nksize\\n=\\n[\\n1\\n,\\n2\\n,\\n2\\n,\\n1\\n],\\n\\t\\nstrides\\n=\\n[\\n1\\n,\\n2\\n,\\n2\\n,\\n1\\n],\\npadding\\n=\\n\"VALID\"\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\noutput\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\nmax_pool\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\ndataset\\n})', 'plt\\n.\\nimshow\\n(\\noutput\\n[\\n0\\n]\\n.\\nastype\\n(\\nnp\\n.\\nuint8\\n))\\n\\t\\t\\n#\\tplot\\tthe\\toutput\\tfor\\tthe\\t1st\\timage\\nplt\\n.\\nshow\\n()\\nThe\\t\\nksize\\n\\targument\\tcontains\\tthe\\tkernel\\tshape\\talong\\tall\\tfour\\tdimensions\\tof\\tthe\\tinput\\ttensor:\\t\\n[batch\\nsize,\\theight,\\twidth,\\tchannels]\\n.\\tTensorFlow\\tcurrently\\tdoes\\tnot\\tsupport\\tpooling\\tover\\tmultiple\\ninstances,\\tso\\tthe\\tfirst\\telement\\tof\\t\\nksize\\n\\tmust\\tbe\\tequal\\tto\\t1.\\tMoreover,\\tit\\tdoes\\tnot\\tsupport\\tpooling\\tover\\nboth\\tthe\\tspatial\\tdimensions\\t(height\\tand\\twidth)\\tand\\tthe\\tdepth\\tdimension,\\tso\\teither\\t\\nksize[1]\\n\\tand\\nksize[2]\\n\\tmust\\tboth\\tbe\\tequal\\tto\\t1,\\tor\\t\\nksize[3]\\n\\tmust\\tbe\\tequal\\tto\\t1.\\nTo\\tcreate\\tan\\t\\naverage\\tpooling\\tlayer\\n,\\t\\njust\\tuse\\tthe\\t\\navg_pool()\\n\\t\\nfunction\\tinstead\\t\\nof\\t\\nmax_pool()\\n.\\nNow\\tyou\\tknow\\tall\\tthe\\tbuilding\\tblocks\\tto\\tcreate\\ta\\tconvolutional\\tneural\\tnetwork.\\tLet’s\\tsee\\t\\nhow\\tto\\nassemble\\tthem.', 'CNN\\tArchitectures\\nTypical\\t\\nCNN\\tarchitectures\\tstack\\ta\\tfew\\tconvolutional\\tlayers\\t(each\\tone\\tgenerally\\tfollowed\\tby\\ta\\tReLU\\nlayer),\\tthen\\ta\\tpooling\\tlayer,\\tthen\\tanother\\tfew\\tconvolutional\\tlayers\\t(+ReLU),\\tthen\\tanother\\tpooling\\tlayer,\\nand\\tso\\ton.\\tThe\\timage\\tgets\\tsmaller\\tand\\tsmaller\\tas\\tit\\tprogresses\\tthrough\\tthe\\tnetwork,\\tbut\\tit\\talso\\ttypically\\ngets\\tdeeper\\tand\\tdeeper\\t(i.e.,\\twith\\tmore\\tfeature\\tmaps)\\tthanks\\tto\\tthe\\tconvolutional\\tlayers\\t(see\\t\\nFigure\\t13-\\n9\\n).\\tAt\\tthe\\ttop\\tof\\tthe\\tstack,\\ta\\tregular\\tfeedforward\\tneural\\tnetwork\\tis\\tadded,\\tcomposed\\tof\\ta\\tfew\\tfully\\nconnected\\tlayers\\t(+ReLUs),\\tand\\tthe\\tfinal\\tlayer\\toutputs\\tthe\\tprediction\\t(e.g.,\\ta\\tsoftmax\\tlayer\\tthat\\toutputs\\nestimated\\tclass\\tprobabilities).\\nFigure\\t13-9.\\t\\nTypical\\tCNN\\tarchitecture\\nTIP\\nA\\tcommon\\tmistake\\tis\\tto\\tuse\\t\\nconvolution\\tkernels\\tthat\\tare\\ttoo\\tlarge.\\tYou\\tcan\\toften\\tget\\tthe\\tsame\\teffect\\tas\\ta\\t9\\t×\\t9\\tkernel\\tby\\nstacking\\ttwo\\t3\\t×\\t3\\tkernels\\ton\\ttop\\tof\\teach\\tother,\\tfor\\ta\\tlot\\tless\\tcompute.', 'Over\\tthe\\tyears,\\tvariants\\tof\\tthis\\tfundamental\\tarchitecture\\thave\\tbeen\\tdeveloped,\\tleading\\tto\\tamazing\\nadvances\\tin\\tthe\\tfield.\\tA\\tgood\\tmeasure\\tof\\tthis\\tprogress\\tis\\tthe\\terror\\trate\\tin\\tcompetitions\\tsuch\\tas\\t\\nthe\\nILSVRC\\t\\nImageNet\\tchallenge\\n.\\tIn\\tthis\\tcompetition\\tthe\\ttop-5\\terror\\trate\\tfor\\t\\nimage\\tclassification\\tfell\\tfrom\\nover\\t26%\\tto\\tbarely\\tover\\t3%\\tin\\tjust\\tfive\\tyears.\\tThe\\ttop-five\\terror\\trate\\tis\\tthe\\tnumber\\tof\\ttest\\timages\\tfor\\nwhich\\tthe\\tsystem’s\\ttop\\t5\\tpredictions\\tdid\\tnot\\tinclude\\tthe\\tcorrect\\tanswer.\\tThe\\timages\\tare\\tlarge\\t(256\\tpixels\\nhigh)\\tand\\tthere\\tare\\t1,000\\tclasses,\\tsome\\tof\\twhich\\tare\\treally\\tsubtle\\t(try\\tdistinguishing\\t120\\tdog\\tbreeds).\\nLooking\\tat\\tthe\\tevolution\\tof\\tthe\\twinning\\tentries\\tis\\ta\\tgood\\tway\\tto\\tunderstand\\thow\\tCNNs\\twork.\\nWe\\twill\\tfirst\\tlook\\tat\\tthe\\tclassical\\tLeNet-5\\tarchitecture\\t(1998),\\tthen\\tthree\\tof\\tthe\\twinners\\tof\\tthe\\tILSVRC\\nchallenge:\\tAlexNet\\t(2012),\\tGoogLeNet\\t(2014),\\tand\\tResNet\\t(2015).\\nOTHER\\tVISUAL\\tTASKS', 'There\\twas\\tstunning\\tprogress\\tas\\twell\\tin\\tother\\tvisual\\ttasks\\tsuch\\tas\\tobject\\tdetection\\tand\\tlocalization,\\tand\\timage\\tsegmentation.\\tIn\\tobject\\ndetection\\tand\\tlocalization,\\tthe\\tneural\\tnetwork\\ttypically\\toutputs\\ta\\tsequence\\tof\\tbounding\\tboxes\\taround\\tvarious\\tobjects\\tin\\tthe\\timage.\\tFor\\nexample,\\tsee\\tMaxine\\tOquab\\tet\\tal.’s\\t2015\\t\\npaper\\n\\tthat\\toutputs\\ta\\theat\\tmap\\tfor\\teach\\tobject\\tclass,\\tor\\tRussell\\tStewart\\tet\\tal.’s\\t2015\\t\\npaper\\n\\tthat\\nuses\\ta\\tcombination\\tof\\ta\\tCNN\\tto\\tdetect\\tfaces\\tand\\ta\\trecurrent\\tneural\\tnetwork\\tto\\toutput\\ta\\tsequence\\tof\\tbounding\\tboxes\\taround\\tthem.\\tIn\\nimage\\tsegmentation,\\tthe\\tnet\\toutputs\\tan\\timage\\t(usually\\tof\\tthe\\tsame\\tsize\\tas\\tthe\\tinput)\\twhere\\teach\\tpixel\\tindicates\\tthe\\tclass\\tof\\tthe\\tobject\\tto\\nwhich\\tthe\\tcorresponding\\tinput\\tpixel\\tbelongs.\\tFor\\texample,\\tcheck\\tout\\tEvan\\tShelhamer\\tet\\tal.’s\\t2016\\t\\npaper\\n.', 'LeNet-5\\nThe\\t\\nLeNet-5\\tarchitecture\\tis\\tperhaps\\tthe\\tmost\\twidely\\tknown\\tCNN\\tarchitecture.\\tAs\\tmentioned\\tearlier,\\tit\\nwas\\tcreated\\tby\\tYann\\tLeCun\\tin\\t1998\\tand\\twidely\\tused\\tfor\\thandwritten\\tdigit\\trecognition\\t(MNIST).\\tIt\\tis\\ncomposed\\tof\\tthe\\tlayers\\tshown\\tin\\t\\nTable\\t13-1\\n.\\nTable\\t13-1.\\t\\nLeNet-5\\tarchitecture\\nLayer\\nType\\nMaps\\nSize\\nKernel\\tsize\\nStride\\nActivation\\nOut\\nFully\\tConnected\\n–\\n10\\n–\\n–\\nRBF\\nF6\\nFully\\tConnected\\n–\\n84\\n–\\n–\\ntanh\\nC5\\nConvolution\\n120\\n1\\t×\\t1\\n5\\t×\\t5\\n1\\ntanh\\nS4\\nAvg\\tPooling\\n16\\n5\\t×\\t5\\n2\\t×\\t2\\n2\\ntanh\\nC3\\nConvolution\\n16\\n10\\t×\\t10\\n5\\t×\\t5\\n1\\ntanh\\nS2\\nAvg\\tPooling\\n6\\n14\\t×\\t14\\n2\\t×\\t2\\n2\\ntanh\\nC1\\nConvolution\\n6\\n28\\t×\\t28\\n5\\t×\\t5\\n1\\ntanh\\nIn\\nInput\\n1\\n32\\t×\\t32\\n–\\n–\\n–\\nThere\\tare\\ta\\tfew\\textra\\tdetails\\tto\\tbe\\tnoted:\\nMNIST\\timages\\tare\\t28\\t×\\t28\\tpixels,\\tbut\\tthey\\tare\\tzero-padded\\tto\\t32\\t×\\t32\\tpixels\\tand\\tnormalized\\tbefore\\nbeing\\tfed\\tto\\tthe\\tnetwork.\\tThe\\trest\\tof\\tthe\\tnetwork\\tdoes\\tnot\\tuse\\tany\\tpadding,\\twhich\\tis\\twhy\\tthe\\tsize\\nkeeps\\tshrinking\\tas\\tthe\\timage\\tprogresses\\tthrough\\tthe\\tnetwork.', 'The\\taverage\\tpooling\\tlayers\\tare\\tslightly\\tmore\\tcomplex\\tthan\\tusual:\\teach\\tneuron\\tcomputes\\tthe\\tmean\\tof\\nits\\tinputs,\\tthen\\tmultiplies\\tthe\\tresult\\tby\\ta\\tlearnable\\tcoefficient\\t(one\\tper\\tmap)\\tand\\tadds\\ta\\tlearnable\\nbias\\tterm\\t(again,\\tone\\tper\\tmap),\\tthen\\tfinally\\tapplies\\tthe\\tactivation\\tfunction.\\nMost\\tneurons\\tin\\tC3\\tmaps\\tare\\tconnected\\tto\\tneurons\\tin\\tonly\\tthree\\tor\\tfour\\tS2\\tmaps\\t(instead\\tof\\tall\\tsix\\nS2\\tmaps).\\tSee\\ttable\\t1\\tin\\tthe\\toriginal\\tpaper\\tfor\\tdetails.\\nThe\\toutput\\tlayer\\tis\\ta\\tbit\\tspecial:\\tinstead\\tof\\tcomputing\\tthe\\tdot\\tproduct\\tof\\tthe\\tinputs\\tand\\tthe\\tweight\\nvector,\\teach\\tneuron\\toutputs\\tthe\\tsquare\\tof\\tthe\\tEuclidian\\tdistance\\tbetween\\tits\\tinput\\tvector\\tand\\tits\\nweight\\tvector.\\tEach\\toutput\\tmeasures\\thow\\tmuch\\tthe\\timage\\tbelongs\\tto\\ta\\tparticular\\tdigit\\tclass.\\tThe\\ncross\\tentropy\\t\\ncost\\tfunction\\tis\\tnow\\tpreferred,\\tas\\tit\\tpenalizes\\tbad\\tpredictions\\tmuch\\tmore,\\tproducing\\nlarger\\tgradients\\tand\\tthus\\tconverging\\tfaster.\\nYann\\tLeCun’s\\t\\nwebsite\\n\\t(“LENET”\\tsection)\\tfeatures\\tgreat\\tdemos\\tof\\tLeNet-5\\tclassifying\\t\\ndigits.', 'AlexNet\\nThe\\n\\t\\nAlexNet\\n\\tCNN\\tarchitecture\\n9\\n\\twon\\tthe\\t2012\\tImageNet\\tILSVRC\\tchallenge\\tby\\ta\\tlarge\\tmargin:\\tit\\tachieved\\n17%\\ttop-5\\terror\\trate\\twhile\\tthe\\tsecond\\tbest\\tachieved\\tonly\\t26%!\\tIt\\twas\\tdeveloped\\tby\\tAlex\\tKrizhevsky\\n(hence\\tthe\\tname),\\tIlya\\tSutskever,\\tand\\tGeoffrey\\tHinton.\\tIt\\tis\\tquite\\tsimilar\\tto\\tLeNet-5,\\tonly\\tmuch\\tlarger\\tand\\ndeeper,\\tand\\tit\\twas\\tthe\\tfirst\\tto\\tstack\\tconvolutional\\tlayers\\tdirectly\\ton\\ttop\\tof\\teach\\tother,\\tinstead\\tof\\tstacking\\ta\\npooling\\tlayer\\ton\\ttop\\tof\\teach\\tconvolutional\\tlayer.\\t\\nTable\\t13-2\\n\\tpresents\\tthis\\tarchitecture.\\nTable\\t13-2.\\t\\nAlexNet\\tarchitecture\\nLayer\\nType\\nMaps\\nSize\\nKernel\\tsize\\nStride\\nPadding\\nActivation\\nOut\\nFully\\tConnected\\n–\\n1,000\\n–\\n–\\n–\\nSoftmax\\nF9\\nFully\\tConnected\\n–\\n4,096\\n–\\n–\\n–\\nReLU\\nF8\\nFully\\tConnected\\n–\\n4,096\\n–\\n–\\n–\\nReLU\\nC7\\nConvolution\\n256\\n13\\t×\\t13\\n3\\t×\\t3\\n1\\nSAME\\nReLU\\nC6\\nConvolution\\n384\\n13\\t×\\t13\\n3\\t×\\t3\\n1\\nSAME\\nReLU\\nC5\\nConvolution\\n384\\n13\\t×\\t13\\n3\\t×\\t3\\n1\\nSAME\\nReLU\\nS4\\nMax\\tPooling\\n256\\n13\\t×\\t13\\n3\\t×\\t3\\n2\\nVALID\\n–\\nC3\\nConvolution\\n256\\n27\\t×\\t27\\n5\\t×\\t5\\n1\\nSAME\\nReLU\\nS2\\nMax\\tPooling\\n96\\n27\\t×\\t27\\n3\\t×\\t3\\n2\\nVALID\\n–\\nC1', '–\\nC1\\nConvolution\\n96\\n55\\t×\\t55\\n11\\t×\\t11\\n4\\nSAME\\nReLU\\nIn\\nInput\\n3\\t(RGB)\\n224\\t×\\t224\\n–\\n–\\n–\\n–\\nTo\\treduce\\toverfitting,\\tthe\\tauthors\\tused\\ttwo\\tregularization\\ttechniques\\twe\\tdiscussed\\tin\\tprevious\\tchapters:\\nfirst\\tthey\\tapplied\\tdropout\\t(with\\ta\\t50%\\tdropout\\trate)\\tduring\\ttraining\\tto\\tthe\\toutputs\\tof\\tlayers\\tF8\\tand\\tF9.\\nSecond,\\tthey\\tperformed\\tdata\\taugmentation\\tby\\trandomly\\tshifting\\tthe\\ttraining\\timages\\tby\\tvarious\\toffsets,\\nflipping\\tthem\\thorizontally,\\tand\\tchanging\\tthe\\tlighting\\tconditions.\\nAlexNet\\talso\\tuses\\ta\\tcompetitive\\tnormalization\\tstep\\timmediately\\tafter\\tthe\\tReLU\\tstep\\tof\\tlayers\\tC1\\tand\\tC3,\\ncalled\\t\\nlocal\\tresponse\\tnormalization\\n.\\tThis\\tform\\tof\\tnormalization\\tmakes\\tthe\\tneurons\\tthat\\tmost\\tstrongly\\nactivate\\tinhibit\\tneurons\\tat\\tthe\\tsame\\tlocation\\tbut\\tin\\tneighboring\\tfeature\\tmaps\\t(such\\tcompetitive\\tactivation\\nhas\\tbeen\\tobserved\\tin\\tbiological\\tneurons).\\tThis\\tencourages\\tdifferent\\tfeature\\tmaps\\tto\\tspecialize,\\tpushing\\nthem\\tapart\\tand\\tforcing\\tthem\\tto\\texplore\\ta\\twider\\trange\\tof\\tfeatures,\\tultimately\\timproving\\tgeneralization.\\nEquation\\t13-2', 'shows\\thow\\tto\\tapply\\tLRN.\\nEquation\\t13-2.\\t\\nLocal\\tresponse\\tnormalization', 'b\\ni\\n\\tis\\tthe\\tnormalized\\toutput\\tof\\tthe\\tneuron\\tlocated\\tin\\tfeature\\tmap\\t\\ni\\n,\\tat\\tsome\\trow\\t\\nu\\n\\tand\\tcolumn\\t\\nv\\n\\t(note\\nthat\\tin\\tthis\\tequation\\twe\\tconsider\\tonly\\tneurons\\tlocated\\tat\\tthis\\trow\\tand\\tcolumn,\\tso\\t\\nu\\n\\tand\\t\\nv\\n\\tare\\tnot\\nshown).\\na\\ni\\n\\tis\\tthe\\tactivation\\tof\\tthat\\tneuron\\tafter\\tthe\\tReLU\\tstep,\\tbut\\tbefore\\tnormalization.\\nk\\n,\\t\\nα\\n,\\t\\nβ\\n,\\tand\\t\\nr\\n\\tare\\thyperparameters.\\t\\nk\\n\\tis\\tcalled\\tthe\\t\\nbias\\n,\\tand\\t\\nr\\n\\tis\\tcalled\\t\\nthe\\t\\ndepth\\tradius\\n.\\nf\\nn\\n\\tis\\tthe\\tnumber\\tof\\tfeature\\tmaps.\\nFor\\texample,\\tif\\t\\nr\\n\\t=\\t2\\tand\\ta\\tneuron\\thas\\ta\\tstrong\\tactivation,\\tit\\twill\\tinhibit\\tthe\\tactivation\\tof\\tthe\\tneurons\\nlocated\\tin\\tthe\\tfeature\\tmaps\\timmediately\\tabove\\tand\\tbelow\\tits\\town.\\nIn\\tAlexNet,\\tthe\\thyperparameters\\tare\\tset\\tas\\tfollows:\\t\\nr\\n\\t=\\t2,\\t\\nα\\n\\t=\\t0.00002,\\t\\nβ\\n\\t=\\t0.75,\\tand\\t\\nk\\n\\t=\\t1.\\tThis\\tstep\\tcan\\nbe\\timplemented\\tusing\\tTensorFlow’s\\t\\ntf.nn.local_response_normalization()\\n\\toperation.\\nA\\tvariant\\tof\\tAlexNet\\tcalled\\t\\nZF\\tNet\\n\\twas\\tdeveloped\\tby\\tMatthew\\tZeiler\\tand\\tRob\\tFergus\\tand\\twon\\tthe\\t2013\\nILSVRC\\tchallenge.\\tIt\\tis\\tessentially\\tAlexNet\\twith\\ta\\tfew\\ttweaked', 'hyperparameters\\t(number\\tof\\tfeature\\nmaps,\\tkernel\\tsize,\\tstride,\\tetc.).', 'GoogLeNet\\nThe\\t\\nGoogLeNet\\tarchitecture\\n\\twas\\tdeveloped\\tby\\tChristian\\tSzegedy\\tet\\tal.\\tfrom\\tGoogle\\tResearch,\\n10\\n\\tand\\tit\\nwon\\tthe\\tILSVRC\\t2014\\tchallenge\\tby\\tpushing\\tthe\\ttop-5\\terror\\trate\\tbelow\\t7%.\\tThis\\tgreat\\tperformance\\tcame\\nin\\tlarge\\tpart\\tfrom\\tthe\\tfact\\tthat\\tthe\\tnetwork\\twas\\tmuch\\tdeeper\\tthan\\tprevious\\tCNNs\\t(see\\t\\nFigure\\t13-11\\n).\\tThis\\nwas\\tmade\\tpossible\\tby\\tsub-networks\\tcalled\\t\\ninception\\tmodules\\n,\\n11\\n\\twhich\\tallow\\tGoogLeNet\\tto\\tuse\\nparameters\\tmuch\\tmore\\tefficiently\\tthan\\tprevious\\tarchitectures:\\tGoogLeNet\\tactually\\thas\\t10\\ttimes\\tfewer\\nparameters\\tthan\\tAlexNet\\t(roughly\\t6\\tmillion\\tinstead\\tof\\t60\\tmillion).\\nFigure\\t13-10\\n\\tshows\\tthe\\tarchitecture\\tof\\tan\\t\\ninception\\tmodule.\\tThe\\tnotation\\t“3\\t×\\t3\\t+\\t2(S)”\\tmeans\\tthat\\tthe\\nlayer\\tuses\\ta\\t3\\t×\\t3\\tkernel,\\tstride\\t2,\\tand\\tSAME\\tpadding.\\tThe\\tinput\\tsignal\\tis\\tfirst\\tcopied\\tand\\tfed\\tto\\tfour\\ndifferent\\tlayers.\\tAll\\tconvolutional\\tlayers\\tuse\\tthe\\tReLU\\tactivation\\tfunction.\\tNote\\tthat\\tthe\\tsecond\\tset\\tof\\nconvolutional\\tlayers\\tuses\\tdifferent\\tkernel\\tsizes\\t(1\\t×\\t1,\\t3\\t×\\t3,\\tand\\t5\\t×\\t5),\\tallowing\\tthem\\tto\\tcapture', 'patterns\\tat\\tdifferent\\tscales.\\tAlso\\tnote\\tthat\\tevery\\tsingle\\tlayer\\tuses\\ta\\tstride\\tof\\t1\\tand\\tSAME\\tpadding\\t(even\\nthe\\tmax\\tpooling\\tlayer),\\tso\\ttheir\\toutputs\\tall\\thave\\tthe\\tsame\\theight\\tand\\twidth\\tas\\ttheir\\tinputs.\\tThis\\tmakes\\tit\\npossible\\tto\\tconcatenate\\tall\\tthe\\toutputs\\talong\\tthe\\tdepth\\tdimension\\tin\\tthe\\t\\nfinal\\t\\ndepth\\tconcat\\tlayer\\n\\t\\n(i.e.,\\nstack\\tthe\\tfeature\\tmaps\\tfrom\\tall\\tfour\\ttop\\tconvolutional\\tlayers).\\tThis\\tconcatenation\\tlayer\\tcan\\tbe\\nimplemented\\tin\\tTensorFlow\\tusing\\tthe\\t\\ntf.concat()\\n\\t\\noperation,\\twith\\t\\naxis=3\\n\\t(axis\\t3\\tis\\tthe\\tdepth).\\nFigure\\t13-10.\\t\\nInception\\tmodule\\nYou\\tmay\\twonder\\twhy\\tinception\\tmodules\\thave\\tconvolutional\\tlayers\\twith\\t1\\t×\\t1\\tkernels.\\tSurely\\tthese\\tlayers\\ncannot\\tcapture\\tany\\tfeatures\\tsince\\tthey\\tlook\\tat\\tonly\\tone\\tpixel\\tat\\ta\\ttime?\\tIn\\tfact,\\tthese\\tlayers\\tserve\\ttwo\\npurposes:\\nFirst,\\tthey\\tare\\tconfigured\\tto\\toutput\\tmany\\tfewer\\tfeature\\tmaps\\tthan\\ttheir\\tinputs,\\tso\\tthey\\tserve\\t\\nas\\nbottleneck\\tlayers\\n,\\tmeaning\\tthey\\treduce\\tdimensionality.\\tThis\\tis\\tparticularly\\tuseful\\tbefore\\tthe\\t3\\t×\\t3', 'and\\t5\\t×\\t5\\tconvolutions,\\tsince\\tthese\\tare\\tvery\\tcomputationally\\texpensive\\tlayers.\\nSecond,\\teach\\tpair\\tof\\t\\nconvolutional\\tlayers\\t([1\\t×\\t1,\\t3\\t×\\t3]\\tand\\t[1\\t×\\t1,\\t5\\t×\\t5])\\tacts\\tlike\\ta\\tsingle,', 'powerful\\tconvolutional\\tlayer,\\tcapable\\tof\\tcapturing\\tmore\\tcomplex\\tpatterns.\\tIndeed,\\tinstead\\tof\\nsweeping\\ta\\tsimple\\tlinear\\tclassifier\\tacross\\tthe\\timage\\t(as\\ta\\tsingle\\tconvolutional\\tlayer\\tdoes),\\tthis\\tpair\\nof\\tconvolutional\\tlayers\\tsweeps\\ta\\ttwo-layer\\tneural\\tnetwork\\tacross\\tthe\\timage.\\nIn\\tshort,\\tyou\\tcan\\tthink\\tof\\tthe\\twhole\\tinception\\tmodule\\tas\\ta\\tconvolutional\\tlayer\\ton\\tsteroids,\\table\\tto\\toutput\\nfeature\\tmaps\\tthat\\tcapture\\tcomplex\\tpatterns\\tat\\tvarious\\tscales.\\nWARNING\\nThe\\tnumber\\tof\\t\\nconvolutional\\tkernels\\tfor\\teach\\tconvolutional\\tlayer\\tis\\ta\\thyperparameter.\\tUnfortunately,\\tthis\\tmeans\\tthat\\tyou\\thave\\nsix\\tmore\\thyperparameters\\tto\\ttweak\\tfor\\tevery\\tinception\\tlayer\\tyou\\tadd.\\nNow\\tlet’s\\tlook\\tat\\tthe\\tarchitecture\\tof\\tthe\\tGoogLeNet\\tCNN\\t(see\\t\\nFigure\\t13-11\\n).\\tIt\\tis\\tso\\tdeep\\tthat\\twe\\thad\\tto\\nrepresent\\tit\\tin\\tthree\\tcolumns,\\tbut\\tGoogLeNet\\tis\\tactually\\tone\\ttall\\tstack,\\tincluding\\tnine\\tinception\\tmodules\\n(the\\tboxes\\twith\\tthe\\tspinning\\ttops)\\tthat\\tactually\\tcontain\\tthree\\tlayers\\teach.\\tThe\\tnumber\\tof\\tfeature\\tmaps', 'output\\tby\\teach\\tconvolutional\\tlayer\\tand\\teach\\tpooling\\tlayer\\tis\\tshown\\tbefore\\tthe\\tkernel\\tsize.\\tThe\\tsix\\nnumbers\\tin\\tthe\\tinception\\tmodules\\trepresent\\tthe\\tnumber\\tof\\tfeature\\tmaps\\toutput\\tby\\teach\\tconvolutional\\tlayer\\nin\\tthe\\tmodule\\t(in\\tthe\\tsame\\torder\\tas\\tin\\t\\nFigure\\t13-10\\n).\\tNote\\tthat\\tall\\tthe\\tconvolutional\\tlayers\\tuse\\tthe\\tReLU\\nactivation\\tfunction.\\nFigure\\t13-11.\\t\\nGoogLeNet\\tarchitecture', 'Let’s\\tgo\\tthrough\\tthis\\tnetwork:\\nThe\\tfirst\\ttwo\\tlayers\\tdivide\\tthe\\timage’s\\theight\\tand\\twidth\\tby\\t4\\t(so\\tits\\tarea\\tis\\tdivided\\tby\\t16),\\tto\\treduce\\nthe\\tcomputational\\tload.\\nThen\\tthe\\tlocal\\tresponse\\tnormalization\\tlayer\\tensures\\tthat\\tthe\\tprevious\\tlayers\\tlearn\\ta\\twide\\tvariety\\tof\\nfeatures\\t(as\\tdiscussed\\tearlier).\\nTwo\\tconvolutional\\tlayers\\tfollow,\\twhere\\tthe\\tfirst\\tacts\\tlike\\ta\\t\\nbottleneck\\tlayer\\n.\\tAs\\texplained\\tearlier,\\nyou\\tcan\\tthink\\tof\\tthis\\tpair\\tas\\ta\\tsingle\\tsmarter\\tconvolutional\\tlayer.\\nAgain,\\ta\\tlocal\\tresponse\\tnormalization\\tlayer\\tensures\\tthat\\tthe\\tprevious\\tlayers\\tcapture\\ta\\twide\\tvariety\\nof\\tpatterns.\\nNext\\ta\\tmax\\tpooling\\tlayer\\treduces\\tthe\\timage\\theight\\tand\\twidth\\tby\\t2,\\tagain\\tto\\tspeed\\tup\\tcomputations.\\nThen\\tcomes\\tthe\\ttall\\tstack\\tof\\tnine\\tinception\\tmodules,\\tinterleaved\\twith\\ta\\tcouple\\tmax\\tpooling\\tlayers\\tto\\nreduce\\tdimensionality\\tand\\tspeed\\tup\\tthe\\tnet.\\nNext,\\tthe\\taverage\\tpooling\\tlayer\\tuses\\ta\\tkernel\\tthe\\tsize\\tof\\tthe\\tfeature\\tmaps\\twith\\tVALID\\tpadding,\\noutputting\\t1\\t×\\t1\\tfeature\\tmaps:\\tthis\\tsurprising\\tstrategy\\tis\\t\\ncalled', 'called\\t\\nglobal\\taverage\\tpooling\\n.\\tIt\\teffectively\\nforces\\tthe\\tprevious\\tlayers\\tto\\tproduce\\tfeature\\tmaps\\tthat\\tare\\tactually\\tconfidence\\tmaps\\tfor\\teach\\ttarget\\nclass\\t(since\\tother\\tkinds\\tof\\tfeatures\\twould\\tbe\\tdestroyed\\tby\\tthe\\taveraging\\tstep).\\tThis\\tmakes\\tit\\nunnecessary\\tto\\thave\\tseveral\\tfully\\tconnected\\tlayers\\tat\\tthe\\ttop\\tof\\tthe\\tCNN\\t(like\\tin\\tAlexNet),\\nconsiderably\\treducing\\tthe\\tnumber\\tof\\tparameters\\tin\\tthe\\tnetwork\\tand\\tlimiting\\tthe\\trisk\\tof\\toverfitting.\\nThe\\tlast\\tlayers\\tare\\tself-explanatory:\\tdropout\\tfor\\tregularization,\\tthen\\ta\\tfully\\tconnected\\tlayer\\twith\\ta\\nsoftmax\\tactivation\\tfunction\\tto\\toutput\\testimated\\tclass\\tprobabilities.\\nThis\\tdiagram\\tis\\tslightly\\tsimplified:\\tthe\\toriginal\\tGoogLeNet\\tarchitecture\\talso\\tincluded\\ttwo\\tauxiliary\\nclassifiers\\tplugged\\ton\\ttop\\tof\\tthe\\tthird\\tand\\tsixth\\tinception\\tmodules.\\tThey\\twere\\tboth\\tcomposed\\tof\\tone\\naverage\\tpooling\\tlayer,\\tone\\tconvolutional\\tlayer,\\ttwo\\tfully\\tconnected\\tlayers,\\tand\\ta\\tsoftmax\\tactivation\\tlayer.', 'During\\ttraining,\\ttheir\\tloss\\t(scaled\\tdown\\tby\\t70%)\\twas\\tadded\\tto\\tthe\\toverall\\tloss.\\tThe\\tgoal\\twas\\tto\\tfight\\tthe\\nvanishing\\tgradients\\tproblem\\tand\\tregularize\\tthe\\tnetwork.\\tHowever,\\tit\\twas\\tshown\\tthat\\ttheir\\teffect\\twas\\nrelatively\\t\\nminor.', 'ResNet\\nLast\\t\\nbut\\tnot\\tleast,\\tthe\\twinner\\tof\\tthe\\tILSVRC\\t2015\\tchallenge\\twas\\tthe\\t\\nResidual\\tNetwork\\n\\t(or\\t\\nResNet\\n),\\ndeveloped\\tby\\tKaiming\\tHe\\tet\\tal.,\\n12\\n\\twhich\\tdelivered\\tan\\tastounding\\ttop-5\\terror\\trate\\tunder\\t3.6%,\\tusing\\tan\\nextremely\\tdeep\\tCNN\\tcomposed\\tof\\t152\\tlayers.\\tThe\\tkey\\tto\\tbeing\\table\\tto\\ttrain\\tsuch\\ta\\tdeep\\tnetwork\\tis\\tto\\t\\nuse\\nskip\\tconnections\\n\\t(also\\tcalled\\t\\nshortcut\\tconnections\\n):\\tthe\\tsignal\\tfeeding\\tinto\\ta\\tlayer\\tis\\talso\\tadded\\tto\\tthe\\noutput\\tof\\ta\\tlayer\\tlocated\\ta\\tbit\\thigher\\tup\\tthe\\tstack.\\tLet’s\\tsee\\twhy\\tthis\\tis\\tuseful.\\nWhen\\ttraining\\ta\\tneural\\tnetwork,\\tthe\\tgoal\\tis\\tto\\tmake\\tit\\tmodel\\ta\\ttarget\\tfunction\\t\\nh\\n(\\nx\\n).\\tIf\\tyou\\tadd\\tthe\\tinput\\t\\nx\\nto\\tthe\\toutput\\tof\\tthe\\tnetwork\\t(i.e.,\\tyou\\tadd\\ta\\tskip\\tconnection),\\tthen\\tthe\\tnetwork\\twill\\tbe\\tforced\\tto\\tmodel\\nf\\n(\\nx\\n)\\t=\\t\\nh\\n(\\nx\\n)\\t–\\t\\nx\\n\\trather\\tthan\\t\\nh\\n(\\nx\\n).\\tThis\\tis\\tcalled\\t\\nresidual\\tlearning\\n\\t\\n(see\\t\\nFigure\\t13-12\\n).\\nFigure\\t13-12.\\t\\nResidual\\tlearning\\nWhen\\tyou\\tinitialize\\ta\\tregular\\tneural\\tnetwork,\\tits\\tweights\\tare\\tclose\\tto\\tzero,\\tso\\tthe\\tnetwork\\tjust\\toutputs', 'values\\tclose\\tto\\tzero.\\tIf\\tyou\\tadd\\ta\\tskip\\tconnection,\\tthe\\tresulting\\tnetwork\\tjust\\toutputs\\ta\\tcopy\\tof\\tits\\tinputs;\\tin\\nother\\twords,\\tit\\tinitially\\tmodels\\tthe\\tidentity\\tfunction.\\tIf\\tthe\\ttarget\\tfunction\\tis\\tfairly\\tclose\\tto\\tthe\\tidentity\\nfunction\\t(which\\tis\\toften\\tthe\\tcase),\\tthis\\twill\\tspeed\\tup\\ttraining\\tconsiderably.\\nMoreover,\\tif\\tyou\\tadd\\tmany\\tskip\\tconnections,\\tthe\\tnetwork\\tcan\\tstart\\tmaking\\tprogress\\teven\\tif\\tseveral\\tlayers\\nhave\\tnot\\tstarted\\tlearning\\tyet\\t(see\\t\\nFigure\\t13-13\\n).\\tThanks\\tto\\tskip\\tconnections,\\tthe\\tsignal\\tcan\\teasily\\tmake\\tits\\nway\\tacross\\tthe\\twhole\\tnetwork.\\tThe\\tdeep\\tresidual\\tnetwork\\tcan\\tbe\\tseen\\tas\\ta\\tstack\\tof\\t\\nresidual\\tunits\\n,\\t\\nwhere\\neach\\tresidual\\tunit\\tis\\ta\\tsmall\\tneural\\tnetwork\\twith\\ta\\tskip\\tconnection.', 'Figure\\t13-13.\\t\\nRegular\\tdeep\\tneural\\tnetwork\\t(left)\\tand\\tdeep\\tresidual\\tnetwork\\t(right)\\nNow\\tlet’s\\tlook\\tat\\tResNet’s\\tarchitecture\\t(see\\t\\nFigure\\t13-14\\n).\\tIt\\tis\\tactually\\tsurprisingly\\tsimple.\\tIt\\tstarts\\tand\\nends\\texactly\\tlike\\tGoogLeNet\\t(except\\twithout\\ta\\tdropout\\tlayer),\\tand\\tin\\tbetween\\tis\\tjust\\ta\\tvery\\tdeep\\tstack\\tof\\nsimple\\tresidual\\tunits.\\tEach\\tresidual\\tunit\\tis\\tcomposed\\tof\\ttwo\\tconvolutional\\tlayers,\\twith\\t\\nBatch\\nNormalization\\t(BN)\\tand\\tReLU\\tactivation,\\tusing\\t3\\t×\\t3\\tkernels\\tand\\tpreserving\\tspatial\\tdimensions\\t(stride\\t1,\\nSAME\\tpadding).\\nFigure\\t13-14.\\t\\nResNet\\tarchitecture\\nNote\\tthat\\tthe\\tnumber\\tof\\t\\nfeature\\tmaps\\tis\\tdoubled\\tevery\\tfew\\tresidual\\tunits,\\tat\\tthe\\tsame\\ttime\\tas\\ttheir\\theight\\nand\\twidth\\tare\\thalved\\t(using\\ta\\tconvolutional\\tlayer\\twith\\tstride\\t2).\\tWhen\\tthis\\thappens\\tthe\\tinputs\\tcannot\\tbe\\nadded\\tdirectly\\tto\\tthe\\toutputs\\tof\\tthe\\tresidual\\tunit\\tsince\\tthey\\tdon’t\\thave\\tthe\\tsame\\tshape\\t(for\\texample,\\tthis', 'problem\\taffects\\tthe\\tskip\\tconnection\\trepresented\\tby\\tthe\\tdashed\\tarrow\\tin\\t\\nFigure\\t13-14\\n).\\tTo\\tsolve\\tthis\\nproblem,\\tthe\\tinputs\\tare\\tpassed\\tthrough\\ta\\t1\\t×\\t1\\tconvolutional\\tlayer\\twith\\tstride\\t2\\tand\\tthe\\tright\\tnumber\\tof\\noutput\\tfeature\\tmaps\\t(see\\t\\nFigure\\t13-15\\n).\\nFigure\\t13-15.\\t\\nSkip\\tconnection\\twhen\\tchanging\\tfeature\\tmap\\tsize\\tand\\tdepth\\nResNet-34\\tis\\tthe\\tResNet\\twith\\t34\\tlayers\\t(only\\tcounting\\tthe\\tconvolutional\\tlayers\\tand\\tthe\\tfully\\tconnected\\nlayer)\\tcontaining\\tthree\\tresidual\\tunits\\tthat\\toutput\\t64\\tfeature\\tmaps,\\t4\\tRUs\\twith\\t128\\tmaps,\\t6\\tRUs\\twith\\t256\\nmaps,\\tand\\t3\\tRUs\\twith\\t512\\tmaps.\\nResNets\\tdeeper\\tthan\\tthat,\\tsuch\\tas\\tResNet-152,\\tuse\\tslightly\\tdifferent\\tresidual\\tunits.\\tInstead\\tof\\ttwo\\t3\\t×\\t3\\nconvolutional\\tlayers\\twith\\t(say)\\t256\\tfeature\\tmaps,\\tthey\\tuse\\tthree\\tconvolutional\\tlayers:\\tfirst\\ta\\t1\\t×\\t1\\nconvolutional\\tlayer\\twith\\tjust\\t64\\tfeature\\tmaps\\t(4\\ttimes\\tless),\\twhich\\tacts\\ta\\ta\\tbottleneck\\tlayer\\t(as\\tdiscussed\\nalready),\\tthen\\ta\\t3\\t×\\t3\\tlayer\\twith\\t64\\tfeature\\tmaps,\\tand\\tfinally\\tanother\\t1\\t×\\t1\\tconvolutional\\tlayer\\twith\\t256', 'feature\\tmaps\\t(4\\ttimes\\t64)\\tthat\\trestores\\tthe\\toriginal\\tdepth.\\tResNet-152\\tcontains\\tthree\\tsuch\\tRUs\\tthat\\toutput\\n256\\tmaps,\\tthen\\t8\\tRUs\\twith\\t512\\tmaps,\\ta\\twhopping\\t36\\tRUs\\twith\\t1,024\\tmaps,\\tand\\tfinally\\t3\\tRUs\\twith\\t2,048\\nmaps.\\nAs\\tyou\\tcan\\tsee,\\tthe\\tfield\\tis\\tmoving\\trapidly,\\twith\\tall\\tsorts\\tof\\tarchitectures\\tpopping\\tout\\tevery\\tyear.\\tOne\\nclear\\ttrend\\tis\\tthat\\tCNNs\\tkeep\\tgetting\\tdeeper\\tand\\tdeeper.\\tThey\\tare\\talso\\tgetting\\tlighter,\\trequiring\\tfewer\\tand\\nfewer\\tparameters.\\tAt\\tpresent,\\tthe\\tResNet\\tarchitecture\\tis\\tboth\\tthe\\tmost\\tpowerful\\tand\\targuably\\tthe\\nsimplest,\\tso\\tit\\tis\\treally\\tthe\\tone\\tyou\\tshould\\tprobably\\tuse\\tfor\\tnow,\\tbut\\tkeep\\tlooking\\tat\\tthe\\tILSVRC\\nchallenge\\tevery\\tyear.\\tThe\\t2016\\twinners\\twere\\tthe\\tTrimps-Soushen\\tteam\\tfrom\\tChina\\twith\\tan\\tastounding\\n2.99%\\terror\\trate.\\tTo\\tachieve\\tthis\\tthey\\ttrained\\tcombinations\\tof\\tthe\\tprevious\\tmodels\\tand\\tjoined\\tthem\\tinto\\nan\\tensemble.\\tDepending\\ton\\tthe\\ttask,\\tthe\\treduced\\terror\\trate\\tmay\\tor\\tmay\\tnot\\tbe\\tworth\\tthe\\textra\\tc\\nomplexity.', 'There\\tare\\ta\\tfew\\tother\\tarchitectures\\tthat\\tyou\\tmay\\twant\\tto\\tlook\\tat,\\tin\\tparticular\\n\\t\\nVGGNet\\n13\\n\\t(runner-up\\tof\\tthe\\nILSVRC\\t2014\\tchallenge)\\tand\\t\\nInception-v4\\n14\\n\\t(which\\tmerges\\t\\nthe\\tideas\\tof\\tGoogLeNet\\tand\\tResNet\\tand\\nachieves\\tclose\\tto\\t3%\\ttop-5\\terror\\trate\\ton\\tImageNet\\tclassification).', 'NOTE\\nThere\\tis\\treally\\tnothing\\tspecial\\tabout\\timplementing\\tthe\\tvarious\\tCNN\\tarchitectures\\twe\\tjust\\tdiscussed.\\tWe\\tsaw\\tearlier\\thow\\tto\\tbuild\\nall\\tthe\\tindividual\\tbuilding\\tblocks,\\tso\\tnow\\tall\\tyou\\tneed\\tis\\tto\\tassemble\\tthem\\tto\\tcreate\\tthe\\tdesired\\tarchitecture.\\tWe\\twill\\tbuild\\ta\\ncomplete\\tCNN\\tin\\tthe\\tupcoming\\texercises\\tand\\tyou\\twill\\tfind\\tfull\\tworking\\tcode\\tin\\tthe\\tJupyter\\tnotebooks.\\nTENSORFLOW\\tCONVOLUTION\\tOPERATIONS\\nTensorFlow\\talso\\t\\noffers\\ta\\tfew\\tother\\tkinds\\tof\\tconvolutional\\tlayers:\\ntf.layers.conv1d()\\n\\t\\ncreates\\ta\\tconvolutional\\tlayer\\tfor\\t1D\\tinputs.\\tThis\\tis\\tuseful,\\tfor\\texample,\\tin\\tnatural\\tlanguage\\tprocessing,\\nwhere\\ta\\tsentence\\tmay\\tbe\\trepresented\\tas\\ta\\t1D\\tarray\\tof\\twords,\\tand\\tthe\\treceptive\\tfield\\tcovers\\ta\\tfew\\tneighboring\\twords.\\ntf.layers.conv3d()\\n\\t\\ncreates\\ta\\tconvolutional\\tlayer\\tfor\\t3D\\tinputs,\\tsuch\\tas\\t3D\\tPET\\tscan.\\ntf.nn.atrous_conv2d()\\n\\t\\ncreates\\tan\\t\\natrous\\tconvolutional\\tlayer\\n\\t(“à\\ttrous”\\tis\\tFrench\\tfor\\t“with\\tholes”).\\tThis\\tis\\tequivalent\\tto\\tusing', 'a\\tregular\\tconvolutional\\tlayer\\twith\\ta\\tfilter\\tdilated\\tby\\tinserting\\trows\\tand\\tcolumns\\tof\\tzeros\\t(i.e.,\\tholes).\\tFor\\texample,\\ta\\t1\\t×\\t3\\tfilter\\nequal\\tto\\t\\n[[1,2,3]]\\n\\tmay\\tbe\\tdilated\\twith\\ta\\t\\ndilation\\trate\\n\\tof\\t4,\\tresulting\\tin\\ta\\t\\ndilated\\tfilter\\n\\t\\n[[1,\\t0,\\t0,\\t0,\\t2,\\t0,\\t0,\\t0,\\t3]]\\n.\\tThis\\nallows\\tthe\\tconvolutional\\tlayer\\tto\\thave\\ta\\tlarger\\treceptive\\tfield\\tat\\tno\\tcomputational\\tprice\\tand\\tusing\\tno\\textra\\tparameters.\\ntf.layers.conv2d_transpose()\\n\\t\\ncreates\\ta\\t\\ntranspose\\tconvolutional\\tlayer\\n,\\tsometimes\\t\\ncalled\\ta\\t\\ndeconvolutional\\tlayer\\n,\\n15\\n\\twhich\\nupsamples\\n\\tan\\timage.\\tIt\\tdoes\\tso\\tby\\tinserting\\tzeros\\tbetween\\tthe\\tinputs,\\tso\\tyou\\tcan\\tthink\\tof\\tthis\\tas\\ta\\tregular\\tconvolutional\\tlayer\\nusing\\ta\\tfractional\\tstride.\\tUpsampling\\t\\nis\\tuseful,\\tfor\\texample,\\tin\\timage\\tsegmentation:\\tin\\ta\\ttypical\\tCNN,\\tfeature\\tmaps\\tget\\tsmaller\\tand\\nsmaller\\tas\\tyou\\tprogress\\tthrough\\tthe\\tnetwork,\\tso\\tif\\tyou\\twant\\tto\\toutput\\tan\\timage\\tof\\tthe\\tsame\\tsize\\tas\\tthe\\tinput,\\tyou\\tneed\\tan\\nupsampling\\tlayer.\\ntf.nn.depthwise_conv2d()\\n\\t\\ncreates\\ta\\t\\ndepthwise\\tconvolutional\\tlayer', 'that\\tapplies\\tevery\\tfilter\\tto\\tevery\\tindividual\\tinput\\tchannel\\nindependently.\\tThus,\\tif\\tthere\\tare\\t\\nf\\nn\\n\\tfilters\\tand\\t\\nf\\nn\\n′\\n\\t\\ninput\\tchannels,\\tthen\\tthis\\twill\\toutput\\t\\nf\\nn\\n\\t×\\t\\nf\\nn\\n′\\n\\tfeature\\tmaps.\\ntf.layers.separable_conv2d()\\n\\t\\ncreates\\ta\\t\\nseparable\\tconvolutional\\tlayer\\n\\tthat\\tfirst\\tacts\\tlike\\ta\\tdepthwise\\tconvolutional\\tlayer,\\nthen\\tapplies\\ta\\t1\\t×\\t1\\tconvolutional\\tlayer\\tto\\tthe\\tresulting\\tfeature\\tmaps.\\tThis\\tmakes\\tit\\tpossible\\tto\\tapply\\tfilters\\tto\\tarbitrary\\tsets\\tof\\ninputs\\t\\nchannels.', 'Exercises\\n1\\n.\\t\\nWhat\\tare\\tthe\\tadvantages\\tof\\ta\\tCNN\\tover\\ta\\tfully\\tconnected\\tDNN\\tfor\\timage\\tclassification?\\n2\\n.\\t\\nConsider\\ta\\tCNN\\tcomposed\\tof\\tthree\\tconvolutional\\tlayers,\\teach\\twith\\t3\\t×\\t3\\tkernels,\\ta\\tstride\\tof\\t2,\\tand\\nSAME\\tpadding.\\tThe\\tlowest\\tlayer\\toutputs\\t100\\tfeature\\tmaps,\\tthe\\tmiddle\\tone\\toutputs\\t200,\\tand\\tthe\\ttop\\none\\toutputs\\t400.\\tThe\\tinput\\timages\\tare\\tRGB\\timages\\tof\\t200\\t×\\t300\\tpixels.\\tWhat\\tis\\tthe\\ttotal\\tnumber\\tof\\nparameters\\tin\\tthe\\tCNN?\\tIf\\twe\\tare\\tusing\\t32-bit\\tfloats,\\tat\\tleast\\thow\\tmuch\\tRAM\\twill\\tthis\\tnetwork\\nrequire\\twhen\\tmaking\\ta\\tprediction\\tfor\\ta\\tsingle\\tinstance?\\tWhat\\tabout\\twhen\\ttraining\\ton\\ta\\tmini-batch\\tof\\n50\\timages?\\n3\\n.\\t\\nIf\\tyour\\tGPU\\truns\\tout\\tof\\tmemory\\twhile\\ttraining\\ta\\tCNN,\\twhat\\tare\\tfive\\tthings\\tyou\\tcould\\ttry\\tto\\tsolve\\tthe\\nproblem?\\n4\\n.\\t\\nWhy\\twould\\tyou\\twant\\tto\\tadd\\ta\\tmax\\tpooling\\tlayer\\trather\\tthan\\ta\\tconvolutional\\tlayer\\twith\\tthe\\tsame\\nstride?\\n5\\n.\\t\\nWhen\\twould\\tyou\\twant\\tto\\tadd\\ta\\t\\nlocal\\tresponse\\tnormalization\\n\\tlayer?\\n6\\n.\\t\\nCan\\tyou\\tname\\tthe\\tmain\\tinnovations\\tin\\tAlexNet,\\tcompared\\tto\\tLeNet-5?\\tWhat\\tabout\\tthe\\tmain', 'innovations\\tin\\tGoogLeNet\\tand\\tResNet?\\n7\\n.\\t\\nBuild\\tyour\\town\\tCNN\\tand\\ttry\\tto\\tachieve\\tthe\\thighest\\tpossible\\taccuracy\\ton\\tMNIST.\\n8\\n.\\t\\nClassifying\\tlarge\\timages\\tusing\\tInception\\tv3.\\na\\n.\\t\\nDownload\\tsome\\timages\\tof\\tvarious\\tanimals.\\tLoad\\tthem\\tin\\tPython,\\tfor\\texample\\tusing\\tthe\\nmatplotlib.image.mpimg.imread()\\n\\tfunction\\tor\\tthe\\t\\nscipy.misc.imread()\\n\\tfunction.\\tResize\\nand/or\\tcrop\\tthem\\tto\\t299\\t×\\t299\\tpixels,\\tand\\tensure\\tthat\\tthey\\thave\\tjust\\tthree\\tchannels\\t(RGB),\\twith\\nno\\ttransparency\\tchannel.\\nb\\n.\\t\\nDownload\\tthe\\tlatest\\tpretrained\\tInception\\tv3\\tmodel:\\tthe\\tcheckpoint\\tis\\tavailable\\tat\\nhttps://goo.gl/nxSQvl\\n.\\nc\\n.\\t\\nCreate\\tthe\\tInception\\tv3\\tmodel\\tby\\tcalling\\tthe\\t\\ninception_v3()\\n\\tfunction,\\tas\\tshown\\tbelow.\\tThis\\nmust\\tbe\\tdone\\twithin\\tan\\targument\\tscope\\tcreated\\tby\\tthe\\t\\ninception_v3_arg_scope()\\n\\tfunction.\\nAlso,\\tyou\\tmust\\tset\\t\\nis_training=False\\n\\tand\\t\\nnum_classes=1001\\n\\t\\nlike\\tso:\\nfrom\\n\\t\\ntensorflow.contrib.slim.nets\\n\\t\\nimport\\n\\t\\ninception\\nimport\\n\\t\\ntensorflow.contrib.slim\\n\\t\\nas\\n\\t\\nslim\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone', '=\\n[\\nNone\\n,\\n\\t\\n299\\n,\\n\\t\\n299\\n,\\n\\t\\n3\\n],\\n\\t\\nname\\n=\\n\"X\"\\n)\\nwith\\n\\t\\nslim\\n.\\narg_scope\\n(\\ninception\\n.\\ninception_v3_arg_scope\\n()):\\n\\t\\t\\t\\t\\nlogits\\n,\\n\\t\\nend_points\\n\\t\\n=\\n\\t\\ninception\\n.\\ninception_v3\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nX\\n,\\n\\t\\nnum_classes\\n=\\n1001\\n,\\n\\t\\nis_training\\n=\\nFalse\\n)\\npredictions\\n\\t\\n=\\n\\t\\nend_points\\n[\\n\"Predictions\"\\n]\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\nd\\n.\\t\\nOpen\\ta\\tsession\\tand\\tuse\\tthe\\t\\nSaver\\n\\tto\\trestore\\tthe\\tpretrained\\tmodel\\tcheckpoint\\tyou\\tdownloaded\\nearlier.', 'e\\n.\\t\\nRun\\tthe\\tmodel\\tto\\tclassify\\tthe\\timages\\tyou\\tprepared.\\tDisplay\\tthe\\ttop\\tfive\\tpredictions\\tfor\\teach\\nimage,\\talong\\twith\\tthe\\testimated\\tprobability\\t(the\\tlist\\tof\\tclass\\tnames\\tis\\tavailable\\tat\\nhttps://goo.gl/brXRtZ\\n).\\tHow\\taccurate\\tis\\tthe\\tmodel?\\n9\\n.\\t\\nTransfer\\tlearning\\tfor\\tlarge\\timage\\tclassification.\\na\\n.\\t\\nCreate\\ta\\ttraining\\tset\\tcontaining\\tat\\tleast\\t100\\timages\\tper\\tclass.\\tFor\\texample,\\tyou\\tcould\\tclassify\\nyour\\town\\tpictures\\tbased\\ton\\tthe\\tlocation\\t(beach,\\tmountain,\\tcity,\\tetc.),\\tor\\talternatively\\tyou\\tcan\\njust\\tuse\\tan\\texisting\\tdataset,\\tsuch\\tas\\tthe\\t\\nflowers\\tdataset\\n\\tor\\tMIT’s\\t\\nplaces\\tdataset\\n\\t(requires\\nregistration,\\tand\\tit\\tis\\thuge).\\nb\\n.\\t\\nWrite\\ta\\tpreprocessing\\tstep\\tthat\\twill\\tresize\\tand\\tcrop\\tthe\\timage\\tto\\t299\\t×\\t299,\\twith\\tsome\\nrandomness\\tfor\\tdata\\taugmentation.\\nc\\n.\\t\\nUsing\\tthe\\tpretrained\\tInception\\tv3\\tmodel\\tfrom\\tthe\\tprevious\\texercise,\\tfreeze\\tall\\tlayers\\tup\\tto\\tthe\\nbottleneck\\tlayer\\t(i.e.,\\tthe\\tlast\\tlayer\\tbefore\\tthe\\toutput\\tlayer),\\tand\\treplace\\tthe\\toutput\\tlayer\\twith', 'the\\tappropriate\\tnumber\\tof\\toutputs\\tfor\\tyour\\tnew\\tclassification\\ttask\\t(e.g.,\\tthe\\tflowers\\tdataset\\thas\\nfive\\tmutually\\texclusive\\tclasses\\tso\\tthe\\toutput\\tlayer\\tmust\\thave\\tfive\\tneurons\\tand\\tuse\\tthe\\tsoftmax\\nactivation\\tfunction).\\nd\\n.\\t\\nSplit\\tyour\\tdataset\\tinto\\ta\\ttraining\\tset\\tand\\ta\\ttest\\tset.\\tTrain\\tthe\\tmodel\\ton\\tthe\\ttraining\\tset\\tand\\nevaluate\\tit\\ton\\tthe\\ttest\\tset.\\n10\\n.\\t\\nGo\\tthrough\\tTensorFlow’s\\t\\nDeepDream\\ttutorial\\n.\\tIt\\tis\\ta\\tfun\\tway\\tto\\tfamiliarize\\tyourself\\twith\\tvarious\\nways\\tof\\tvisualizing\\tthe\\tpatterns\\tlearned\\tby\\ta\\tCNN,\\tand\\tto\\tgenerate\\tart\\tusing\\tDeep\\tLearning.\\nSolutions\\tto\\t\\nthese\\texercises\\tare\\tavailable\\tin\\t\\nAppendix\\tA\\n.\\n“Single\\tUnit\\tActivity\\tin\\tStriate\\tCortex\\tof\\tUnrestrained\\tCats,”\\tD.\\tHubel\\tand\\tT.\\tWiesel\\t(1958).\\n“Receptive\\tFields\\tof\\tSingle\\tNeurones\\tin\\tthe\\tCat’s\\tStriate\\tCortex,”\\tD.\\tHubel\\tand\\tT.\\tWiesel\\t(1959).\\n“Receptive\\tFields\\tand\\tFunctional\\tArchitecture\\tof\\tMonkey\\tStriate\\tCortex,”\\tD.\\tHubel\\tand\\tT.\\tWiesel\\t(1968).', '“Neocognitron:\\tA\\tSelf-organizing\\tNeural\\tNetwork\\tModel\\tfor\\ta\\tMechanism\\tof\\tPattern\\tRecognition\\tUnaffected\\tby\\tShift\\tin\\tPosition,”\\tK.\\nFukushima\\t(1980).\\n“Gradient-Based\\tLearning\\tApplied\\tto\\tDocument\\tRecognition,”\\tY.\\tLeCun\\tet\\tal.\\t(1998).\\nA\\tconvolution\\tis\\ta\\tmathematical\\toperation\\tthat\\tslides\\tone\\tfunction\\tover\\tanother\\tand\\tmeasures\\tthe\\tintegral\\tof\\ttheir\\tpointwise\\tmultiplication.\\nIt\\thas\\tdeep\\tconnections\\twith\\tthe\\tFourier\\ttransform\\tand\\tthe\\tLaplace\\ttransform,\\tand\\tis\\theavily\\tused\\tin\\tsignal\\tprocessing.\\tConvolutional\\nlayers\\tactually\\tuse\\tcross-correlations,\\twhich\\tare\\tvery\\tsimilar\\tto\\tconvolutions\\t(see\\t\\nhttp://goo.gl/HAfxXd\\n\\tfor\\tmore\\tdetails).\\nA\\tfully\\tconnected\\tlayer\\twith\\t150\\t×\\t100\\tneurons,\\teach\\tconnected\\tto\\tall\\t150\\t×\\t100\\t×\\t3\\tinputs,\\twould\\thave\\t150\\n×\\t100\\n×\\t3\\t=\\t675\\tmillion\\tparameters!\\n1\\tMB\\t=\\t1,024\\tkB\\t=\\t1,024\\t×\\t1,024\\tbytes\\t=\\t1,024\\t×\\t1,024\\t×\\t8\\tbits.\\n“ImageNet\\tClassification\\twith\\tDeep\\tConvolutional\\tNeural\\tNetworks,”\\tA.\\tKrizhevsky\\tet\\tal.\\t(2012).\\n“Going\\tDeeper\\twith\\tConvolutions,”\\tC.\\tSzegedy\\tet\\tal.\\t(2015).', 'In\\tthe\\t2010\\tmovie\\t\\nInception\\n,\\tthe\\tcharacters\\tkeep\\tgoing\\tdeeper\\tand\\tdeeper\\tinto\\tmultiple\\tlayers\\tof\\tdreams,\\thence\\tthe\\tname\\tof\\tthese\\nmodules.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n2\\n2\\n8\\n9\\n10\\n11\\n12', '“Deep\\tResidual\\tLearning\\tfor\\tImage\\tRecognition,”\\tK.\\tHe\\t(2015).\\n“Very\\tDeep\\tConvolutional\\tNetworks\\tfor\\tLarge-Scale\\tImage\\tRecognition,”\\tK.\\tSimonyan\\tand\\tA.\\tZisserman\\t(2015).\\n“Inception-v4,\\tInception-ResNet\\tand\\tthe\\tImpact\\tof\\tResidual\\tConnections\\ton\\tLearning,”\\tC.\\tSzegedy\\tet\\tal.\\t(2016).\\nThis\\tname\\tis\\tquite\\tmisleading\\tsince\\tthis\\tlayer\\tdoes\\t\\nnot\\n\\tperform\\ta\\tdeconvolution,\\twhich\\tis\\ta\\twell-defined\\tmathematical\\toperation\\t(the\\ninverse\\tof\\ta\\tconvolution).\\n12\\n13\\n14\\n15', 'Chapter\\t14.\\t\\nRecurrent\\tNeural\\tNetworks\\nThe\\t\\nbatter\\thits\\tthe\\tball.\\tYou\\timmediately\\tstart\\trunning,\\tanticipating\\tthe\\tball’s\\ttrajectory.\\tYou\\ttrack\\tit\\tand\\nadapt\\tyour\\tmovements,\\tand\\tfinally\\tcatch\\tit\\t(under\\ta\\tthunder\\tof\\tapplause).\\tPredicting\\tthe\\tfuture\\tis\\twhat\\tyou\\ndo\\tall\\tthe\\ttime,\\twhether\\tyou\\tare\\tfinishing\\ta\\tfriend’s\\tsentence\\tor\\tanticipating\\tthe\\tsmell\\tof\\tcoffee\\tat\\nbreakfast.\\tIn\\tthis\\tchapter,\\twe\\tare\\tgoing\\tto\\tdiscuss\\t\\nrecurrent\\tneural\\tnetworks\\n\\t(RNN),\\ta\\tclass\\tof\\tnets\\tthat\\ncan\\tpredict\\tthe\\tfuture\\t(well,\\tup\\tto\\ta\\tpoint,\\tof\\tcourse).\\tThey\\tcan\\tanalyze\\t\\ntime\\tseries\\n\\t\\ndata\\tsuch\\tas\\tstock\\nprices,\\tand\\ttell\\tyou\\twhen\\tto\\tbuy\\tor\\tsell.\\tIn\\t\\nautonomous\\tdriving\\tsystems,\\tthey\\tcan\\tanticipate\\tcar\\ntrajectories\\tand\\thelp\\tavoid\\taccidents.\\tMore\\tgenerally,\\tthey\\tcan\\twork\\ton\\t\\nsequences\\n\\t\\nof\\tarbitrary\\tlengths,\\nrather\\tthan\\ton\\tfixed-sized\\tinputs\\tlike\\tall\\tthe\\tnets\\twe\\thave\\tdiscussed\\tso\\tfar.\\tFor\\texample,\\tthey\\tcan\\ttake\\nsentences,\\tdocuments,\\tor\\taudio\\tsamples\\tas\\tinput,\\tmaking\\tthem\\textremely\\tuseful\\tfor\\t\\nnatural\\tlanguage', 'processing\\t(NLP)\\tsystems\\tsuch\\tas\\tautomatic\\ttranslation,\\tspeech-to-text,\\tor\\t\\nsentiment\\tanalysis\\n\\t\\n(e.g.,\\nreading\\tmovie\\treviews\\tand\\textracting\\tthe\\trater’s\\tfeeling\\tabout\\tthe\\tmovie).\\nMoreover,\\tRNNs’\\tability\\tto\\tanticipate\\talso\\tmakes\\tthem\\tcapable\\tof\\tsurprising\\tcreativity.\\tYou\\tcan\\task\\tthem\\nto\\tpredict\\twhich\\tare\\tthe\\tmost\\tlikely\\tnext\\tnotes\\tin\\ta\\tmelody,\\tthen\\trandomly\\tpick\\tone\\tof\\tthese\\tnotes\\tand\\tplay\\nit.\\tThen\\task\\tthe\\tnet\\tfor\\tthe\\tnext\\tmost\\tlikely\\tnotes,\\tplay\\tit,\\tand\\trepeat\\tthe\\tprocess\\tagain\\tand\\tagain.\\tBefore\\nyou\\tknow\\tit,\\tyour\\tnet\\twill\\tcompose\\ta\\tmelody\\tsuch\\tas\\t\\nthe\\tone\\n\\tproduced\\tby\\tGoogle’s\\t\\nMagenta\\tproject\\n.\\nSimilarly,\\tRNNs\\tcan\\t\\ngenerate\\tsentences\\n,\\t\\nimage\\tcaptions\\n,\\tand\\tmuch\\tmore.\\tThe\\tresult\\tis\\tnot\\texactly\\nShakespeare\\tor\\tMozart\\tyet,\\tbut\\twho\\tknows\\twhat\\tthey\\twill\\tproduce\\ta\\tfew\\tyears\\tfrom\\tnow?\\nIn\\tthis\\tchapter,\\twe\\twill\\tlook\\tat\\tthe\\tfundamental\\tconcepts\\tunderlying\\tRNNs,\\tthe\\tmain\\tproblem\\tthey\\tface\\n(namely,\\tvanishing/exploding\\tgradients,\\tdiscussed\\tin\\t\\nChapter\\t11\\n),\\tand\\tthe\\tsolutions\\twidely\\tused\\tto\\tfight', 'it:\\tLSTM\\tand\\tGRU\\tcells.\\tAlong\\tthe\\tway,\\tas\\talways,\\twe\\twill\\tshow\\thow\\tto\\timplement\\tRNNs\\tusing\\nTensorFlow.\\tFinally,\\twe\\twill\\ttake\\ta\\tlook\\tat\\tthe\\tarchitecture\\tof\\ta\\tmachine\\ttranslation\\tsystem.', 'Recurrent\\tNeurons\\nUp\\t\\nto\\tnow\\twe\\thave\\tmostly\\tlooked\\tat\\tfeedforward\\tneural\\tnetworks,\\twhere\\tthe\\tactivations\\tflow\\tonly\\tin\\tone\\ndirection,\\tfrom\\tthe\\tinput\\tlayer\\tto\\tthe\\toutput\\tlayer\\t(except\\tfor\\ta\\tfew\\tnetworks\\tin\\t\\nAppendix\\tE\\n).\\tA\\trecurrent\\nneural\\tnetwork\\tlooks\\tvery\\tmuch\\tlike\\ta\\tfeedforward\\tneural\\tnetwork,\\texcept\\tit\\talso\\thas\\tconnections\\npointing\\tbackward.\\tLet’s\\tlook\\tat\\tthe\\tsimplest\\tpossible\\tRNN,\\tcomposed\\tof\\tjust\\tone\\tneuron\\treceiving\\ninputs,\\tproducing\\tan\\toutput,\\tand\\tsending\\tthat\\toutput\\tback\\tto\\titself,\\tas\\tshown\\tin\\t\\nFigure\\t14-1\\n\\t(left).\\tAt\\teach\\ntime\\tstep\\n\\t\\nt\\n\\t(also\\tcalled\\ta\\t\\nframe\\n),\\tthis\\t\\nrecurrent\\tneuron\\n\\treceives\\tthe\\tinputs\\t\\nx\\n(\\nt\\n)\\n\\tas\\twell\\tas\\tits\\town\\toutput\\nfrom\\tthe\\tprevious\\ttime\\tstep,\\t\\ny\\n(\\nt\\n–1)\\n.\\tWe\\tcan\\trepresent\\tthis\\ttiny\\tnetwork\\tagainst\\tthe\\ttime\\taxis,\\tas\\tshown\\tin\\nFigure\\t14-1\\n\\t(right).\\tThis\\tis\\tcalled\\t\\nunrolling\\tthe\\tnetwork\\tthrough\\ttime\\n.\\nFigure\\t14-1.\\t\\nA\\trecurrent\\tneuron\\t(left),\\tunrolled\\tthrough\\ttime\\t(right)\\nYou\\tcan\\teasily\\tcreate\\ta\\tlayer\\tof\\trecurrent\\tneurons.\\tAt\\teach\\ttime\\tstep\\t\\nt', 't\\n,\\tevery\\tneuron\\treceives\\tboth\\tthe\\ninput\\tvector\\t\\nx\\n(\\nt\\n)\\n\\tand\\tthe\\toutput\\tvector\\tfrom\\tthe\\tprevious\\ttime\\tstep\\t\\ny\\n(\\nt\\n–1)\\n,\\tas\\tshown\\tin\\t\\nFigure\\t14-2\\n.\\tNote\\nthat\\tboth\\tthe\\tinputs\\tand\\toutputs\\tare\\tvectors\\tnow\\t(when\\tthere\\twas\\tjust\\ta\\tsingle\\tneuron,\\tthe\\toutput\\twas\\ta\\nscalar).\\nFigure\\t14-2.\\t\\nA\\tlayer\\tof\\trecurrent\\tneurons\\t(left),\\tunrolled\\tthrough\\ttime\\t(right)\\nEach\\trecurrent\\tneuron\\thas\\ttwo\\tsets\\tof\\tweights:\\tone\\tfor\\tthe\\tinputs\\t\\nx\\n(\\nt\\n)\\n\\tand\\tthe\\tother\\tfor\\tthe\\toutputs\\tof\\tthe\\nprevious\\ttime\\tstep,\\t\\ny\\n(\\nt\\n–1)\\n.\\tLet’s\\tcall\\tthese\\tweight\\tvectors\\t\\nw\\nx\\n\\tand\\t\\nw\\ny\\n.\\tThe\\toutput\\tof\\ta\\trecurrent\\tlayer\\tcan\\tbe', 'computed\\tpretty\\tmuch\\tas\\tyou\\tmight\\texpect,\\tas\\tshown\\tin\\t\\nEquation\\t14-1\\n\\t(\\nb\\n\\tis\\tthe\\tbias\\tterm\\tand\\tϕ(·)\\tis\\tthe\\nactivation\\tfunction,\\te.g.,\\tReLU\\n1\\n).\\nEquation\\t14-1.\\t\\nOutput\\tof\\ta\\trecurrent\\tlayer\\tfor\\ta\\tsingle\\tinstance\\nJust\\tlike\\tfor\\tfeedforward\\tneural\\tnetworks,\\twe\\tcan\\tcompute\\ta\\trecurrent\\tlayer’s\\toutput\\tin\\tone\\tshot\\tfor\\ta\\nwhole\\tmini-batch\\tusing\\ta\\tvectorized\\tform\\tof\\tthe\\tprevious\\tequation\\t(see\\t\\nEquation\\t14-2\\n).\\nEquation\\t14-2.\\t\\nOutputs\\tof\\ta\\tlayer\\tof\\trecurrent\\tneurons\\tfor\\tall\\tinstances\\tin\\ta\\tmini-batch\\nY\\n(\\nt\\n)\\n\\tis\\tan\\t\\nm\\n\\t×\\t\\nn\\nneurons\\n\\tmatrix\\tcontaining\\tthe\\tlayer’s\\toutputs\\tat\\ttime\\tstep\\t\\nt\\n\\tfor\\teach\\tinstance\\tin\\tthe\\tmini-\\nbatch\\t(\\nm\\n\\tis\\tthe\\tnumber\\tof\\tinstances\\tin\\tthe\\tmini-batch\\tand\\t\\nn\\nneurons\\n\\tis\\tthe\\tnumber\\tof\\tneurons).\\nX\\n(\\nt\\n)\\n\\tis\\tan\\t\\nm\\n\\t×\\t\\nn\\ninputs\\n\\tmatrix\\tcontaining\\tthe\\tinputs\\tfor\\tall\\tinstances\\t(\\nn\\ninputs\\n\\tis\\tthe\\tnumber\\tof\\tinput\\nfeatures).\\nW\\nx\\n\\tis\\tan\\t\\nn\\ninputs\\n\\t×\\t\\nn\\nneurons\\n\\tmatrix\\tcontaining\\tthe\\tconnection\\tweights\\tfor\\tthe\\tinputs\\tof\\tthe\\tcurrent\\ttime\\nstep.\\nW\\ny\\n\\tis\\tan\\t\\nn\\nneurons\\n\\t×\\t\\nn\\nneurons', 'n\\nneurons\\n\\tmatrix\\tcontaining\\tthe\\tconnection\\tweights\\tfor\\tthe\\toutputs\\tof\\tthe\\tprevious\\ntime\\tstep.\\nThe\\tweight\\tmatrices\\t\\nW\\nx\\n\\tand\\t\\nW\\ny\\n\\tare\\toften\\tconcatenated\\tinto\\ta\\tsingle\\tweight\\tmatrix\\t\\nW\\n\\tof\\tshape\\n(\\nn\\ninputs\\n\\t+\\t\\nn\\nneurons\\n)\\t×\\t\\nn\\nneurons\\n\\t(see\\tthe\\tsecond\\tline\\tof\\t\\nEquation\\t14-2\\n).\\nb\\n\\tis\\ta\\tvector\\tof\\tsize\\t\\nn\\nneurons\\n\\tcontaining\\teach\\tneuron’s\\tbias\\tterm.\\nNotice\\tthat\\t\\nY\\n(\\nt\\n)\\n\\tis\\ta\\tfunction\\tof\\t\\nX\\n(\\nt\\n)\\n\\tand\\t\\nY\\n(\\nt\\n–1)\\n,\\twhich\\tis\\ta\\tfunction\\tof\\t\\nX\\n(\\nt\\n–1)\\n\\tand\\t\\nY\\n(\\nt\\n–2)\\n,\\twhich\\tis\\ta\\tfunction\\nof\\t\\nX\\n(\\nt\\n–2)\\n\\tand\\t\\nY\\n(\\nt\\n–3)\\n,\\tand\\tso\\ton.\\tThis\\tmakes\\t\\nY\\n(\\nt\\n)\\n\\ta\\tfunction\\tof\\tall\\tthe\\tinputs\\tsince\\ttime\\t\\nt\\n\\t=\\t0\\t(that\\tis,\\t\\nX\\n(0)\\n,\\nX\\n(1)\\n,\\t…,\\t\\nX\\n(\\nt\\n)\\n).\\tAt\\tthe\\tfirst\\ttime\\tstep,\\t\\nt\\n\\t=\\t0,\\tthere\\tare\\tno\\tprevious\\toutputs,\\tso\\tthey\\tare\\ttypically\\tassumed\\tto\\nbe\\tall\\tzeros.', 'Memory\\tCells\\nSince\\t\\nthe\\toutput\\tof\\ta\\trecurrent\\tneuron\\tat\\ttime\\tstep\\t\\nt\\n\\tis\\ta\\tfunction\\tof\\tall\\tthe\\tinputs\\tfrom\\tprevious\\ttime\\tsteps,\\nyou\\tcould\\tsay\\tit\\thas\\ta\\tform\\tof\\t\\nmemory\\n.\\tA\\tpart\\tof\\ta\\tneural\\tnetwork\\tthat\\tpreserves\\tsome\\tstate\\tacross\\ttime\\nsteps\\tis\\tcalled\\ta\\t\\nmemory\\tcell\\n\\t(or\\tsimply\\ta\\t\\ncell\\n).\\tA\\tsingle\\trecurrent\\tneuron,\\tor\\ta\\tlayer\\tof\\trecurrent\\nneurons,\\tis\\ta\\tvery\\t\\nbasic\\tcell\\n,\\tbut\\tlater\\tin\\tthis\\tchapter\\twe\\twill\\tlook\\tat\\tsome\\tmore\\tcomplex\\tand\\tpowerful\\ntypes\\tof\\tcells.\\nIn\\tgeneral\\ta\\tcell’s\\tstate\\tat\\ttime\\tstep\\t\\nt\\n,\\tdenoted\\t\\nh\\n(\\nt\\n)\\n\\t(the\\t“h”\\tstands\\tfor\\t“hidden”),\\tis\\ta\\tfunction\\tof\\tsome\\ninputs\\tat\\tthat\\ttime\\tstep\\tand\\tits\\tstate\\tat\\tthe\\tprevious\\ttime\\tstep:\\t\\nh\\n(\\nt\\n)\\n\\t=\\t\\nf\\n(\\nh\\n(\\nt\\n–1)\\n,\\t\\nx\\n(\\nt\\n)\\n).\\tIts\\toutput\\tat\\ttime\\tstep\\t\\nt\\n,\\ndenoted\\t\\ny\\n(\\nt\\n)\\n,\\tis\\talso\\ta\\tfunction\\tof\\tthe\\tprevious\\tstate\\tand\\tthe\\tcurrent\\tinputs.\\tIn\\tthe\\tcase\\tof\\tthe\\tbasic\\tcells\\twe\\nhave\\tdiscussed\\tso\\tfar,\\tthe\\toutput\\tis\\tsimply\\tequal\\tto\\tthe\\tstate,\\tbut\\tin\\tmore\\tcomplex\\tcells\\tthis\\tis\\tnot\\talways\\nthe\\tcase,\\tas\\tshown\\tin\\t\\nFigure\\t14-3\\n.\\nFigure\\t14-3.', 'A\\tcell’s\\thidden\\tstate\\tand\\tits\\toutput\\tmay\\tbe\\tdifferent', 'Input\\tand\\tOutput\\tSequences\\nAn\\t\\nRNN\\tcan\\tsimultaneously\\ttake\\ta\\tsequence\\tof\\tinputs\\tand\\tproduce\\ta\\tsequence\\tof\\toutputs\\t(see\\t\\nFigure\\t14-\\n4\\n,\\ttop-left\\tnetwork).\\tFor\\texample,\\tthis\\ttype\\tof\\tnetwork\\tis\\tuseful\\tfor\\tpredicting\\ttime\\tseries\\tsuch\\tas\\tstock\\nprices:\\tyou\\tfeed\\tit\\tthe\\tprices\\tover\\tthe\\tlast\\t\\nN\\n\\tdays,\\tand\\tit\\tmust\\toutput\\tthe\\tprices\\tshifted\\tby\\tone\\tday\\tinto\\tthe\\nfuture\\t(i.e.,\\tfrom\\t\\nN\\n\\t–\\t1\\tdays\\tago\\tto\\ttomorrow).\\nAlternatively,\\tyou\\tcould\\tfeed\\tthe\\tnetwork\\ta\\tsequence\\tof\\tinputs,\\tand\\tignore\\tall\\toutputs\\texcept\\tfor\\tthe\\tlast\\none\\t(see\\tthe\\ttop-right\\tnetwork).\\tIn\\tother\\twords,\\tthis\\tis\\ta\\tsequence-to-vector\\tnetwork.\\tFor\\texample,\\tyou\\ncould\\tfeed\\tthe\\tnetwork\\ta\\tsequence\\tof\\twords\\tcorresponding\\tto\\ta\\tmovie\\treview,\\tand\\tthe\\tnetwork\\twould\\noutput\\ta\\tsentiment\\tscore\\t(e.g.,\\tfrom\\t–1\\t[hate]\\tto\\t+1\\t[love]).\\nConversely,\\tyou\\tcould\\tfeed\\tthe\\tnetwork\\ta\\tsingle\\tinput\\tat\\tthe\\tfirst\\ttime\\tstep\\t(and\\tzeros\\tfor\\tall\\tother\\ttime\\nsteps),\\tand\\tlet\\tit\\toutput\\ta\\tsequence\\t(see\\tthe\\tbottom-left\\tnetwork).\\tThis\\tis\\ta\\tvector-to-sequence\\tnetwork.', 'For\\texample,\\tthe\\tinput\\tcould\\tbe\\tan\\timage,\\tand\\tthe\\toutput\\tcould\\tbe\\ta\\tcaption\\tfor\\tthat\\timage.\\nLastly,\\tyou\\tcould\\thave\\ta\\tsequence-to-vector\\tnetwork,\\tcalled\\t\\nan\\t\\nencoder\\n,\\tfollowed\\tby\\ta\\tvector-to-\\nsequence\\tnetwork,\\tcalled\\ta\\t\\ndecoder\\n\\t(see\\tthe\\tbottom-right\\tnetwork).\\tFor\\texample,\\tthis\\tcan\\tbe\\tused\\tfor\\ntranslating\\ta\\tsentence\\tfrom\\tone\\tlanguage\\tto\\tanother.\\tYou\\twould\\tfeed\\tthe\\tnetwork\\ta\\tsentence\\tin\\tone\\nlanguage,\\tthe\\tencoder\\twould\\tconvert\\tthis\\tsentence\\tinto\\ta\\tsingle\\tvector\\trepresentation,\\tand\\tthen\\tthe\\ndecoder\\twould\\tdecode\\tthis\\tvector\\tinto\\ta\\tsentence\\tin\\tanother\\tlanguage.\\tThis\\ttwo-step\\tmodel,\\tcalled\\tan\\nEncoder–Decoder,\\tworks\\tmuch\\tbetter\\tthan\\ttrying\\tto\\ttranslate\\ton\\tthe\\tfly\\twith\\ta\\tsingle\\tsequence-to-\\nsequence\\tRNN\\t(like\\tthe\\tone\\trepresented\\ton\\tthe\\ttop\\tleft),\\tsince\\tthe\\tlast\\twords\\tof\\ta\\tsentence\\tcan\\taffect\\tthe\\nfirst\\twords\\tof\\tthe\\ttranslation,\\tso\\tyou\\tneed\\tto\\twait\\tuntil\\tyou\\thave\\theard\\tthe\\twhole\\tsentence\\tbefore\\ntranslating\\tit.\\nFigure\\t14-4.', 'Seq\\tto\\tseq\\t(top\\tleft),\\tseq\\tto\\tvector\\t(top\\tright),\\tvector\\tto\\tseq\\t(bottom\\tleft),\\tdelayed\\tseq\\tto\\tseq\\t(bottom\\tright)\\nSounds\\tpromising,\\tso\\tlet’s\\t\\nstart\\tcoding!', 'Basic\\tRNNs\\tin\\tTensorFlow\\nFirst,\\t\\nlet’s\\timplement\\ta\\tvery\\tsimple\\tRNN\\tmodel,\\twithout\\tusing\\tany\\tof\\tTensorFlow’s\\tRNN\\toperations,\\tto\\nbetter\\tunderstand\\twhat\\tgoes\\ton\\tunder\\tthe\\thood.\\tWe\\twill\\tcreate\\tan\\tRNN\\tcomposed\\tof\\ta\\tlayer\\tof\\tfive\\nrecurrent\\tneurons\\t(like\\tthe\\tRNN\\trepresented\\tin\\t\\nFigure\\t14-2\\n),\\tusing\\tthe\\ttanh\\tactivation\\tfunction.\\tWe\\twill\\nassume\\tthat\\tthe\\tRNN\\truns\\tover\\tonly\\ttwo\\ttime\\tsteps,\\ttaking\\tinput\\tvectors\\tof\\tsize\\t3\\tat\\teach\\ttime\\tstep.\\tThe\\nfollowing\\tcode\\tbuilds\\tthis\\tRNN,\\tunrolled\\tthrough\\ttwo\\ttime\\tsteps:\\nn_inputs\\n\\t\\n=\\n\\t\\n3\\nn_neurons\\n\\t\\n=\\n\\t\\n5\\nX0\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nX1\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nWx\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_normal\\n(\\nshape\\n=\\n[\\nn_inputs\\n,\\n\\t\\nn_neurons\\n],\\ndtype\\n=\\ntf\\n.\\nfloat32\\n))\\nWy\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nrandom_normal\\n(\\nshape\\n=\\n[\\nn_neurons\\n,\\nn_neurons\\n],\\ndtype\\n=\\ntf\\n.\\nfloat32\\n))\\nb\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n([\\n1\\n,\\n\\t\\nn_neurons\\n],\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n))\\nY0\\n\\t\\n=\\n\\t\\ntf\\n.\\ntanh\\n(\\ntf\\n.\\nmatmul\\n(', 'matmul\\n(\\nX0\\n,\\n\\t\\nWx\\n)\\n\\t\\n+\\n\\t\\nb\\n)\\nY1\\n\\t\\n=\\n\\t\\ntf\\n.\\ntanh\\n(\\ntf\\n.\\nmatmul\\n(\\nY0\\n,\\n\\t\\nWy\\n)\\n\\t\\n+\\n\\t\\ntf\\n.\\nmatmul\\n(\\nX1\\n,\\n\\t\\nWx\\n)\\n\\t\\n+\\n\\t\\nb\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nThis\\tnetwork\\tlooks\\tmuch\\tlike\\ta\\ttwo-layer\\tfeedforward\\tneural\\tnetwork,\\twith\\ta\\tfew\\ttwists:\\tfirst,\\tthe\\tsame\\nweights\\tand\\tbias\\tterms\\tare\\tshared\\tby\\tboth\\tlayers,\\tand\\tsecond,\\twe\\tfeed\\tinputs\\tat\\teach\\tlayer,\\tand\\twe\\tget\\noutputs\\tfrom\\teach\\tlayer.\\tTo\\trun\\tthe\\tmodel,\\twe\\tneed\\tto\\tfeed\\tit\\tthe\\tinputs\\tat\\tboth\\ttime\\tsteps,\\tlike\\tso:\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\n#\\tMini-batch:\\t\\t\\t\\t\\t\\t\\t\\tinstance\\t0,instance\\t1,instance\\t2,instance\\t3\\nX0_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([[\\n0\\n,\\n\\t\\n1\\n,\\n\\t\\n2\\n],\\n\\t\\n[\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n5\\n],\\n\\t\\n[\\n6\\n,\\n\\t\\n7\\n,\\n\\t\\n8\\n],\\n\\t\\n[\\n9\\n,\\n\\t\\n0\\n,\\n\\t\\n1\\n]])\\n\\t\\n#\\tt\\t=\\t0\\nX1_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([[\\n9\\n,\\n\\t\\n8\\n,\\n\\t\\n7\\n],\\n\\t\\n[\\n0\\n,\\n\\t\\n0\\n,\\n\\t\\n0\\n],\\n\\t\\n[\\n6\\n,\\n\\t\\n5\\n,\\n\\t\\n4\\n],\\n\\t\\n[\\n3\\n,\\n\\t\\n2\\n,\\n\\t\\n1\\n]])\\n\\t\\n#\\tt\\t=\\t1\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nY0_val\\n,\\n\\t\\nY1_val\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n([\\nY0\\n,\\n\\t\\nY1\\n],\\n\\t\\nfeed_dict\\n=\\n{\\nX0\\n:\\n\\t\\nX0_batch\\n,\\n\\t\\nX1\\n:\\n\\t\\nX1_batch\\n})', '})\\nThis\\tmini-batch\\tcontains\\tfour\\tinstances,\\teach\\twith\\tan\\tinput\\tsequence\\tcomposed\\tof\\texactly\\ttwo\\tinputs.\\tAt\\nthe\\tend,\\t\\nY0_val\\n\\tand\\t\\nY1_val\\n\\tcontain\\tthe\\toutputs\\tof\\tthe\\tnetwork\\tat\\tboth\\ttime\\tsteps\\tfor\\tall\\tneurons\\tand\\tall\\ninstances\\tin\\tthe\\tmini-batch:\\n>>>\\t\\nprint\\n(\\nY0_val\\n)\\n\\t\\t\\n#\\toutput\\tat\\tt\\t=\\t0\\n[[-0.0664006\\t\\t\\t0.96257669\\t\\t0.68105787\\t\\t0.70918542\\t-0.89821595]\\t\\t#\\tinstance\\t0\\n\\t[\\t0.9977755\\t\\t-0.71978885\\t-0.99657625\\t\\t0.9673925\\t\\t-0.99989718]\\t\\t#\\tinstance\\t1\\n\\t[\\t0.99999774\\t-0.99898815\\t-0.99999893\\t\\t0.99677622\\t-0.99999988]\\t\\t#\\tinstance\\t2\\n\\t[\\t1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.99818915\\t\\t0.99950868]]\\t#\\tinstance\\t3\\n>>>\\t\\nprint\\n(\\nY1_val\\n)\\n\\t\\t\\n#\\toutput\\tat\\tt\\t=\\t1\\n[[\\t1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.40200216\\t-1.\\t\\t\\t\\t\\t\\t\\t\\t]\\t\\t#\\tinstance\\t0\\n\\t[-0.12210433\\t\\t0.62805319\\t\\t0.96718419\\t-0.99371207\\t-0.25839335]\\t\\t#\\tinstance\\t1\\n\\t[\\t0.99999827\\t-0.9999994\\t\\t-0.9999975\\t\\t-0.85943311\\t-0.9999879\\t]\\t\\t#\\tinstance\\t2\\n\\t[\\t0.99928284\\t-0.99999815\\t-0.99990582\\t\\t0.98579615\\t-0.92205751]]\\t#\\tinstance\\t3', 'That\\twasn’t\\ttoo\\thard,\\tbut\\tof\\tcourse\\tif\\tyou\\twant\\tto\\tbe\\table\\tto\\trun\\tan\\tRNN\\tover\\t100\\ttime\\tsteps,\\tthe\\tgraph\\tis\\ngoing\\tto\\tbe\\tpretty\\tbig.\\tNow\\tlet’s\\tlook\\tat\\thow\\tto\\tcreate\\tthe\\tsame\\tmodel\\tusing\\tTensorFlow’s\\tRNN\\noperations.', 'Static\\tUnrolling\\tThrough\\tTime\\nThe\\t\\nstatic_rnn()\\n\\t\\nfunction\\tcreates\\tan\\tunrolled\\tRNN\\tnetwork\\tby\\tchaining\\tcells.\\tThe\\tfollowing\\tcode\\ncreates\\tthe\\texact\\tsame\\tmodel\\tas\\tthe\\tprevious\\tone:\\nX0\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nX1\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nbasic_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\noutput_seqs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nstatic_rnn\\n(\\nbasic_cell\\n,\\n\\t\\n[\\nX0\\n,\\n\\t\\nX1\\n],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nY0\\n,\\n\\t\\nY1\\n\\t\\n=\\n\\t\\noutput_seqs\\nFirst\\twe\\tcreate\\tthe\\tinput\\tplaceholders,\\tas\\tbefore.\\tThen\\twe\\tcreate\\ta\\t\\nBasicRNNCell\\n,\\twhich\\tyou\\tcan\\tthink\\nof\\tas\\ta\\tfactory\\tthat\\tcreates\\tcopies\\tof\\tthe\\tcell\\tto\\tbuild\\tthe\\tunrolled\\tRNN\\t\\n(one\\tfor\\teach\\ttime\\tstep).\\tThen\\twe\\ncall\\t\\nstatic_rnn()\\n,\\tgiving\\tit\\tthe\\tcell\\tfactory\\tand\\tthe\\tinput\\ttensors,\\tand\\ttelling\\tit\\tthe\\tdata\\ttype\\tof\\tthe\\tinputs', '(this\\tis\\tused\\tto\\tcreate\\tthe\\tinitial\\tstate\\tmatrix,\\twhich\\tby\\tdefault\\tis\\tfull\\tof\\tzeros).\\tThe\\t\\nstatic_rnn()\\nfunction\\tcalls\\tthe\\tcell\\tfactory’s\\t\\n__call__()\\n\\t\\nfunction\\tonce\\tper\\tinput,\\tcreating\\ttwo\\tcopies\\tof\\tthe\\tcell\\t(each\\ncontaining\\ta\\tlayer\\tof\\tfive\\trecurrent\\tneurons),\\twith\\tshared\\tweights\\tand\\tbias\\tterms,\\tand\\tit\\tchains\\tthem\\tjust\\nlike\\twe\\tdid\\tearlier.\\tThe\\t\\nstatic_rnn()\\n\\tfunction\\treturns\\ttwo\\tobjects.\\tThe\\tfirst\\tis\\ta\\tPython\\tlist\\tcontaining\\nthe\\toutput\\ttensors\\tfor\\teach\\ttime\\tstep.\\tThe\\tsecond\\tis\\ta\\ttensor\\tcontaining\\tthe\\tfinal\\tstates\\tof\\tthe\\tnetwork.\\nWhen\\tyou\\tare\\tusing\\tbasic\\tcells,\\tthe\\tfinal\\tstate\\tis\\tsimply\\tequal\\tto\\tthe\\tlast\\toutput.\\nIf\\tthere\\twere\\t50\\ttime\\tsteps,\\tit\\twould\\tnot\\tbe\\tvery\\tconvenient\\tto\\thave\\tto\\tdefine\\t50\\tinput\\tplaceholders\\tand\\n50\\toutput\\ttensors.\\tMoreover,\\tat\\texecution\\ttime\\tyou\\twould\\thave\\tto\\tfeed\\teach\\tof\\tthe\\t50\\tplaceholders\\tand\\nmanipulate\\tthe\\t50\\toutputs.\\tLet’s\\tsimplify\\tthis.\\tThe\\tfollowing\\tcode\\tbuilds\\tthe\\tsame\\tRNN\\tagain,\\tbut\\tthis\\ntime\\tit\\ttakes\\ta\\tsingle\\tinput\\tplaceholder\\tof\\tshape', '[None,\\tn_steps,\\tn_inputs]\\n\\twhere\\tthe\\tfirst\\tdimension\\nis\\tthe\\tmini-batch\\tsize.\\tThen\\tit\\textracts\\tthe\\tlist\\tof\\tinput\\tsequences\\tfor\\teach\\ttime\\tstep.\\t\\nX_seqs\\n\\tis\\ta\\tPython\\nlist\\tof\\t\\nn_steps\\n\\ttensors\\tof\\tshape\\t\\n[None,\\tn_inputs]\\n,\\twhere\\tonce\\tagain\\tthe\\tfirst\\tdimension\\tis\\tthe\\tmini-\\nbatch\\tsize.\\tTo\\tdo\\tthis,\\twe\\tfirst\\tswap\\tthe\\tfirst\\ttwo\\tdimensions\\tusing\\tthe\\t\\ntranspose()\\n\\t\\nfunction,\\tso\\tthat\\tthe\\ntime\\tsteps\\tare\\tnow\\tthe\\tfirst\\tdimension.\\tThen\\twe\\textract\\ta\\tPython\\tlist\\tof\\ttensors\\talong\\tthe\\tfirst\\tdimension\\n(i.e.,\\tone\\ttensor\\tper\\ttime\\tstep)\\tusing\\tthe\\t\\nunstack()\\n\\t\\nfunction.\\tThe\\tnext\\ttwo\\tlines\\tare\\tthe\\tsame\\tas\\tbefore.\\nFinally,\\twe\\tmerge\\tall\\tthe\\toutput\\ttensors\\tinto\\ta\\tsingle\\ttensor\\tusing\\tthe\\t\\nstack()\\n\\t\\nfunction,\\tand\\twe\\tswap\\tthe\\nfirst\\ttwo\\tdimensions\\tto\\tget\\ta\\tfinal\\t\\noutputs\\n\\ttensor\\tof\\tshape\\t\\n[None,\\tn_steps,\\tn_neurons]\\n\\t(again\\tthe\\nfirst\\tdimension\\tis\\tthe\\tmini-batch\\tsize).\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n])\\nX_seqs\\n\\t\\n=\\n\\t\\ntf\\n.\\nunstack\\n(\\ntf\\n.\\ntranspose\\n(\\nX\\n,\\n\\t\\nperm\\n=\\n[\\n1\\n,\\n\\t\\n0\\n,\\n\\t\\n2', '0\\n,\\n\\t\\n2\\n]))\\nbasic_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\noutput_seqs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nstatic_rnn\\n(\\nbasic_cell\\n,\\n\\t\\nX_seqs\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\ntranspose\\n(\\ntf\\n.\\nstack\\n(\\noutput_seqs\\n),\\n\\t\\nperm\\n=\\n[\\n1\\n,\\n\\t\\n0\\n,\\n\\t\\n2\\n])\\nNow\\twe\\t\\ncan\\trun\\tthe\\tnetwork\\tby\\tfeeding\\tit\\ta\\tsingle\\ttensor\\tthat\\tcontains\\tall\\tthe\\tmini-batch\\tsequences:\\nX_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tt\\t=\\t0\\t\\t\\t\\t\\tt\\t=\\t1\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n0\\n,\\n\\t\\n1\\n,\\n\\t\\n2\\n],\\n\\t\\n[\\n9\\n,\\n\\t\\n8\\n,\\n\\t\\n7\\n]],\\n\\t\\n#\\tinstance\\t0\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n5\\n],\\n\\t\\n[\\n0\\n,\\n\\t\\n0\\n,\\n\\t\\n0\\n]],\\n\\t\\n#\\tinstance\\t1', '[[\\n6\\n,\\n\\t\\n7\\n,\\n\\t\\n8\\n],\\n\\t\\n[\\n6\\n,\\n\\t\\n5\\n,\\n\\t\\n4\\n]],\\n\\t\\n#\\tinstance\\t2\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n9\\n,\\n\\t\\n0\\n,\\n\\t\\n1\\n],\\n\\t\\n[\\n3\\n,\\n\\t\\n2\\n,\\n\\t\\n1\\n]],\\n\\t\\n#\\tinstance\\t3\\n\\t\\t\\t\\t\\n])\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\noutputs_val\\n\\t\\n=\\n\\t\\noutputs\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n})\\nAnd\\twe\\tget\\ta\\tsingle\\t\\noutputs_val\\n\\ttensor\\tfor\\tall\\tinstances,\\tall\\ttime\\tsteps,\\tand\\tall\\tneurons:\\n>>>\\t\\nprint\\n(\\noutputs_val\\n)\\n[[[-0.91279727\\t\\t0.83698678\\t-0.89277941\\t\\t0.80308062\\t-0.5283336\\t]\\n\\t\\t[-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1.\\t\\t\\t\\t\\t\\t\\t\\t\\t-0.99794829\\t\\t0.99985468\\t-0.99273592]]\\n\\t[[-0.99994391\\t\\t0.99951613\\t-0.9946925\\t\\t\\t0.99030769\\t-0.94413054]\\n\\t\\t[\\t0.48733309\\t\\t0.93389565\\t-0.31362072\\t\\t0.88573611\\t\\t0.2424476\\t]]\\n\\t[[-1.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.99999875\\t-0.99975014\\t\\t0.99956584\\t-0.99466234]\\n\\t\\t[-0.99994856\\t\\t0.99999434\\t-0.96058172\\t\\t0.99784708\\t-0.9099462\\t]]\\n\\t[[-0.95972425\\t\\t0.99951482\\t\\t0.96938795\\t-0.969908\\t\\t\\t-0.67668229]\\n\\t\\t[-0.84596014\\t\\t0.96288228\\t\\t0.96856463\\t-0.14777924\\t-0.9119423\\t]]]', 'However,\\tthis\\tapproach\\tstill\\tbuilds\\ta\\tgraph\\tcontaining\\tone\\tcell\\tper\\ttime\\tstep.\\tIf\\tthere\\twere\\t50\\ttime\\tsteps,\\nthe\\tgraph\\twould\\tlook\\tpretty\\tugly.\\tIt\\tis\\ta\\tbit\\tlike\\twriting\\ta\\tprogram\\twithout\\tever\\tusing\\tloops\\t(e.g.,\\t\\nY0=f(0,\\nX0);\\tY1=f(Y0,\\tX1);\\tY2=f(Y1,\\tX2);\\t...;\\tY50=f(Y49,\\tX50)\\n).\\tWith\\tsuch\\tas\\tlarge\\tgraph,\\tyou\\tmay\\neven\\tget\\tout-of-memory\\t(OOM)\\terrors\\t\\nduring\\tbackpropagation\\t(especially\\twith\\tthe\\tlimited\\tmemory\\tof\\nGPU\\tcards),\\tsince\\tit\\tmust\\tstore\\tall\\ttensor\\tvalues\\tduring\\tthe\\tforward\\tpass\\tso\\tit\\tcan\\tuse\\tthem\\tto\\tcompute\\ngradients\\tduring\\tthe\\t\\nreverse\\tpass.\\nFortunately,\\tthere\\tis\\ta\\t\\nbetter\\tsolution:\\tthe\\t\\ndynamic_rnn()\\n\\tfunction.', 'Dynamic\\tUnrolling\\tThrough\\tTime\\nThe\\t\\ndynamic_rnn()\\n\\t\\nfunction\\tuses\\ta\\t\\nwhile_loop()\\n\\t\\noperation\\tto\\trun\\tover\\tthe\\tcell\\tthe\\tappropriate\\tnumber\\nof\\ttimes,\\tand\\tyou\\tcan\\tset\\t\\nswap_memory=True\\n\\tif\\tyou\\twant\\tit\\tto\\tswap\\tthe\\tGPU’s\\tmemory\\tto\\tthe\\tCPU’s\\nmemory\\tduring\\tbackpropagation\\tto\\tavoid\\tOOM\\terrors.\\tConveniently,\\tit\\talso\\taccepts\\ta\\tsingle\\ttensor\\tfor\\tall\\ninputs\\tat\\tevery\\ttime\\tstep\\t(shape\\t\\n[None,\\tn_steps,\\tn_inputs]\\n)\\tand\\tit\\toutputs\\ta\\tsingle\\ttensor\\tfor\\tall\\noutputs\\tat\\tevery\\ttime\\tstep\\t(shape\\t\\n[None,\\tn_steps,\\tn_neurons]\\n);\\tthere\\tis\\tno\\tneed\\tto\\tstack,\\tunstack,\\tor\\ntranspose.\\tThe\\tfollowing\\tcode\\tcreates\\tthe\\tsame\\tRNN\\tas\\tearlier\\tusing\\tthe\\t\\ndynamic_rnn()\\n\\tfunction.\\t\\nIt’s\\tso\\nmuch\\tnicer!\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n])\\nbasic_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nbasic_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nNOTE\\nDuring\\tbackpropagation,\\tthe\\t\\nwhile_loop()', 'operation\\tdoes\\tthe\\tappropriate\\tmagic:\\tit\\tstores\\tthe\\ttensor\\tvalues\\tfor\\teach\\titeration\\nduring\\tthe\\tforward\\tpass\\tso\\tit\\tcan\\tuse\\tthem\\tto\\tcompute\\tgradients\\tduring\\tthe\\treverse\\tpass.', 'Handling\\tVariable\\tLength\\tInput\\tSequences\\nSo\\tfar\\t\\nwe\\thave\\tused\\tonly\\tfixed-size\\tinput\\tsequences\\t(all\\texactly\\ttwo\\tsteps\\tlong).\\tWhat\\tif\\tthe\\tinput\\nsequences\\thave\\tvariable\\tlengths\\t(e.g.,\\tlike\\tsentences)?\\tIn\\tthis\\tcase\\tyou\\tshould\\tset\\tthe\\t\\nsequence_length\\nargument\\twhen\\tcalling\\tthe\\t\\ndynamic_rnn()\\n\\t(or\\t\\nstatic_rnn()\\n)\\tfunction;\\tit\\tmust\\tbe\\ta\\t1D\\ttensor\\tindicating\\nthe\\tlength\\tof\\tthe\\tinput\\tsequence\\tfor\\teach\\tinstance.\\tFor\\texample:\\nseq_length\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint32\\n,\\n\\t\\n[\\nNone\\n])\\n[\\n...\\n]\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nbasic_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsequence_length\\n=\\nseq_length\\n)\\nFor\\texample,\\tsuppose\\tthe\\tsecond\\tinput\\tsequence\\tcontains\\tonly\\tone\\tinput\\tinstead\\tof\\ttwo.\\tIt\\tmust\\tbe\\tpadded\\nwith\\ta\\tzero\\tvector\\tin\\torder\\tto\\tfit\\tin\\tthe\\tinput\\ttensor\\t\\nX\\n\\t(because\\tthe\\tinput\\ttensor’s\\tsecond\\tdimension\\tis\\tthe\\nsize\\tof\\tthe\\tlongest\\tsequence\\t—\\ti.e.,\\t2).\\nX_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tstep\\t0\\t\\t\\t\\t\\tstep\\t1\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n0\\n,\\n\\t\\n1\\n,\\n\\t\\n2\\n],\\n\\t\\n[\\n9\\n,', '[\\n9\\n,\\n\\t\\n8\\n,\\n\\t\\n7\\n]],\\n\\t\\n#\\tinstance\\t0\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n5\\n],\\n\\t\\n[\\n0\\n,\\n\\t\\n0\\n,\\n\\t\\n0\\n]],\\n\\t\\n#\\tinstance\\t1\\t(padded\\twith\\ta\\tzero\\tvector)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n6\\n,\\n\\t\\n7\\n,\\n\\t\\n8\\n],\\n\\t\\n[\\n6\\n,\\n\\t\\n5\\n,\\n\\t\\n4\\n]],\\n\\t\\n#\\tinstance\\t2\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n9\\n,\\n\\t\\n0\\n,\\n\\t\\n1\\n],\\n\\t\\n[\\n3\\n,\\n\\t\\n2\\n,\\n\\t\\n1\\n]],\\n\\t\\n#\\tinstance\\t3\\n\\t\\t\\t\\t\\n])\\nseq_length_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n2\\n,\\n\\t\\n1\\n,\\n\\t\\n2\\n,\\n\\t\\n2\\n])\\nOf\\tcourse,\\tyou\\tnow\\tneed\\tto\\tfeed\\tvalues\\tfor\\tboth\\tplaceholders\\t\\nX\\n\\tand\\t\\nseq_length\\n:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\noutputs_val\\n,\\n\\t\\nstates_val\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\noutputs\\n,\\n\\t\\nstates\\n],\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\nseq_length\\n:\\n\\t\\nseq_length_batch\\n})\\nNow\\tthe\\tRNN\\toutputs\\tzero\\tvectors\\tfor\\tevery\\ttime\\tstep\\tpast\\tthe\\tinput\\tsequence\\tlength\\t(look\\tat\\tthe\\tsecond\\ninstance’s\\toutput\\tfor\\tthe\\tsecond\\ttime\\tstep):\\n>>>\\t\\nprint\\n(\\noutputs_val\\n)\\n[[[-0.68579948\\t-0.25901747\\t-0.80249101\\t-0.18141513\\t-0.37491536]\\n\\t\\t[-0.99996698\\t-0.94501185\\t\\t0.98072106\\t-0.9689762\\t\\t\\t0.99966913]]\\t\\t#\\tfinal\\tstate', '[[-0.99099374\\t-0.64768541\\t-0.67801034\\t-0.7415446\\t\\t\\t0.7719509\\t]\\t\\t\\t#\\tfinal\\tstate\\n\\t\\t[\\t0.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t0.\\t\\t\\t\\t\\t\\t\\t\\t]]\\t\\t#\\tzero\\tvector\\n\\t[[-0.99978048\\t-0.85583007\\t-0.49696958\\t-0.93838578\\t\\t0.98505187]\\n\\t\\t[-0.99951065\\t-0.89148796\\t\\t0.94170523\\t-0.38407657\\t\\t0.97499216]]\\t\\t#\\tfinal\\tstate\\n\\t[[-0.02052618\\t-0.94588047\\t\\t0.99935204\\t\\t0.37283331\\t\\t0.9998163\\t]\\n\\t\\t[-0.91052347\\t\\t0.05769409\\t\\t0.47446665\\t-0.44611037\\t\\t0.89394671]]]\\t#\\tfinal\\tstate\\nMoreover,\\tthe\\t\\nstates\\n\\t\\ntensor\\tcontains\\tthe\\tfinal\\tstate\\tof\\teach\\tcell\\t(excluding\\tthe\\tzero\\tvectors):\\n>>>\\t\\nprint\\n(\\nstates_val\\n)\\n[[-0.99996698\\t-0.94501185\\t\\t0.98072106\\t-0.9689762\\t\\t\\t0.99966913]\\t\\t#\\tt\\t=\\t1\\n\\t[-0.99099374\\t-0.64768541\\t-0.67801034\\t-0.7415446\\t\\t\\t0.7719509\\t]\\t\\t#\\tt\\t=\\t0\\t!!!\\n\\t[-0.99951065\\t-0.89148796\\t\\t0.94170523\\t-0.38407657\\t\\t0.97499216]\\t\\t#\\tt\\t=\\t1\\n\\t[-0.91052347\\t\\t0.05769409\\t\\t0.47446665\\t-0.44611037\\t\\t0.89394671]]\\t#\\tt\\t=\\t1', 'Handling\\tVariable-Length\\tOutput\\tSequences\\nWhat\\tif\\t\\nthe\\toutput\\tsequences\\thave\\tvariable\\tlengths\\tas\\twell?\\tIf\\tyou\\tknow\\tin\\tadvance\\twhat\\tlength\\teach\\nsequence\\twill\\thave\\t(for\\texample\\tif\\tyou\\tknow\\tthat\\tit\\twill\\tbe\\tthe\\tsame\\tlength\\tas\\tthe\\tinput\\tsequence),\\tthen\\nyou\\tcan\\tset\\tthe\\t\\nsequence_length\\n\\t\\nparameter\\tas\\tdescribed\\tabove.\\tUnfortunately,\\tin\\tgeneral\\tthis\\twill\\tnot\\nbe\\tpossible:\\tfor\\texample,\\tthe\\tlength\\tof\\ta\\ttranslated\\tsentence\\tis\\tgenerally\\tdifferent\\tfrom\\tthe\\tlength\\tof\\tthe\\ninput\\tsentence.\\tIn\\tthis\\tcase,\\tthe\\tmost\\tcommon\\tsolution\\tis\\tto\\tdefine\\ta\\tspecial\\toutput\\tcalled\\t\\nan\\t\\nend-of-\\nsequence\\ttoken\\n\\t(EOS\\ttoken).\\tAny\\toutput\\tpast\\tthe\\tEOS\\tshould\\tbe\\tignored\\t(we\\twill\\tdiscuss\\tthis\\tlater\\tin\\nthis\\tchapter).\\nOkay,\\tnow\\tyou\\tknow\\thow\\tto\\tbuild\\tan\\tRNN\\tnetwork\\t(or\\tmore\\tprecisely\\tan\\tRNN\\tnetwork\\tunrolled\\tthrough\\ntime).\\t\\nBut\\thow\\tdo\\tyou\\ttrain\\tit?', 'Training\\tRNNs\\nTo\\t\\ntrain\\tan\\tRNN,\\tthe\\ttrick\\tis\\tto\\tunroll\\tit\\tthrough\\ttime\\t(like\\twe\\tjust\\tdid)\\tand\\tthen\\tsimply\\tuse\\tregular\\nbackpropagation\\t(see\\t\\nFigure\\t14-5\\n).\\tThis\\t\\nstrategy\\tis\\tcalled\\t\\nbackpropagation\\tthrough\\ttime\\n\\t(BPTT).\\nFigure\\t14-5.\\t\\nBackpropagation\\tthrough\\ttime\\nJust\\tlike\\tin\\tregular\\tbackpropagation,\\tthere\\tis\\ta\\tfirst\\tforward\\tpass\\tthrough\\tthe\\tunrolled\\tnetwork\\n(represented\\tby\\tthe\\tdashed\\tarrows);\\tthen\\tthe\\toutput\\tsequence\\tis\\tevaluated\\tusing\\ta\\t\\ncost\\tfunction\\t\\n\\t(where\\t\\nt\\nmin\\n\\tand\\t\\nt\\nmax\\n\\tare\\tthe\\tfirst\\tand\\tlast\\toutput\\ttime\\tsteps,\\tnot\\tcounting\\nthe\\tignored\\toutputs),\\tand\\tthe\\tgradients\\tof\\tthat\\tcost\\tfunction\\tare\\tpropagated\\tbackward\\tthrough\\tthe\\tunrolled\\nnetwork\\t(represented\\tby\\tthe\\tsolid\\tarrows);\\tand\\tfinally\\tthe\\t\\nmodel\\tparameters\\tare\\tupdated\\tusing\\tthe\\ngradients\\tcomputed\\tduring\\tBPTT.\\tNote\\tthat\\tthe\\tgradients\\tflow\\tbackward\\tthrough\\tall\\tthe\\toutputs\\tused\\tby\\nthe\\tcost\\tfunction,\\tnot\\tjust\\tthrough\\tthe\\tfinal\\toutput\\t(for\\texample,\\tin\\t\\nFigure\\t14-5\\n\\tthe\\tcost\\tfunction\\tis', 'computed\\tusing\\tthe\\tlast\\tthree\\toutputs\\tof\\tthe\\tnetwork,\\t\\nY\\n(2)\\n,\\t\\nY\\n(3)\\n,\\tand\\t\\nY\\n(4)\\n,\\tso\\tgradients\\tflow\\tthrough\\tthese\\nthree\\toutputs,\\tbut\\tnot\\tthrough\\t\\nY\\n(0)\\n\\tand\\t\\nY\\n(1)\\n).\\tMoreover,\\tsince\\tthe\\tsame\\tparameters\\t\\nW\\n\\tand\\t\\nb\\n\\tare\\tused\\tat\\neach\\ttime\\tstep,\\tbackpropagation\\twill\\tdo\\tthe\\tright\\tthing\\tand\\tsum\\tover\\tall\\ttime\\tsteps.', 'Training\\ta\\tSequence\\tClassifier\\nLet’s\\t\\ntrain\\tan\\tRNN\\tto\\tclassify\\tMNIST\\timages.\\tA\\tconvolutional\\tneural\\tnetwork\\twould\\tbe\\tbetter\\tsuited\\tfor\\nimage\\tclassification\\t(see\\t\\nChapter\\t13\\n),\\tbut\\tthis\\tmakes\\tfor\\ta\\tsimple\\texample\\tthat\\tyou\\tare\\talready\\tfamiliar\\nwith.\\tWe\\twill\\ttreat\\teach\\timage\\tas\\ta\\tsequence\\tof\\t28\\trows\\tof\\t28\\tpixels\\teach\\t(since\\teach\\tMNIST\\timage\\tis\\n28\\t×\\t28\\tpixels).\\tWe\\twill\\tuse\\tcells\\tof\\t150\\trecurrent\\tneurons,\\tplus\\ta\\tfully\\tconnected\\tlayer\\tcontaining\\t10\\nneurons\\t(one\\tper\\tclass)\\tconnected\\tto\\tthe\\toutput\\tof\\tthe\\tlast\\ttime\\tstep,\\tfollowed\\tby\\ta\\tsoftmax\\tlayer\\t(see\\nFigure\\t14-6\\n).\\nFigure\\t14-6.\\t\\nSequence\\tclassifier\\nThe\\tconstruction\\tphase\\tis\\tquite\\tstraightforward;\\tit’s\\tpretty\\tmuch\\tthe\\tsame\\tas\\tthe\\tMNIST\\tclassifier\\twe\\nbuilt\\tin\\t\\nChapter\\t10\\n\\texcept\\tthat\\tan\\tunrolled\\tRNN\\treplaces\\tthe\\thidden\\tlayers.\\tNote\\tthat\\tthe\\tfully\\tconnected\\nlayer\\tis\\tconnected\\tto\\tthe\\t\\nstates\\n\\ttensor,\\twhich\\tcontains\\tonly\\tthe\\tfinal\\tstate\\tof\\tthe\\tRNN\\t(i.e.,\\tthe\\t28\\nth\\noutput).\\tAlso\\tnote\\tthat\\t\\ny\\n\\tis\\ta\\tplaceholder\\tfor\\t\\nthe\\ttarget\\tclasses.\\nn_steps\\n\\t\\n=', '=\\n\\t\\n28\\nn_inputs\\n\\t\\n=\\n\\t\\n28\\nn_neurons\\n\\t\\n=\\n\\t\\n150\\nn_outputs\\n\\t\\n=\\n\\t\\n10\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.001\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n])\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint32\\n,\\n\\t\\n[\\nNone\\n])\\nbasic_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nbasic_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nstates\\n,\\n\\t\\nn_outputs\\n)\\nxentropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsparse_softmax_cross_entropy_with_logits\\n(\\nlabels\\n=\\ny\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlogits\\n=\\nlogits\\n)\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\nxentropy\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\ncorrect\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nin_top_k\\n(\\nlogits\\n,\\n\\t\\ny\\n,\\n\\t\\n1\\n)\\naccuracy\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\ncast\\n(\\ncorrect\\n,\\n\\t\\ntf\\n.\\nfloat32\\n))', 'init\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nNow\\tlet’s\\t\\nload\\tthe\\tMNIST\\tdata\\tand\\treshape\\tthe\\ttest\\tdata\\tto\\t\\n[batch_size,\\tn_steps,\\tn_inputs]\\n\\tas\\tis\\nexpected\\tby\\tthe\\tnetwork.\\tWe\\twill\\ttake\\tcare\\tof\\treshaping\\tthe\\ttraining\\tdata\\tin\\ta\\tmoment.\\nfrom\\n\\t\\ntensorflow.examples.tutorials.mnist\\n\\t\\nimport\\n\\t\\ninput_data\\nmnist\\n\\t\\n=\\n\\t\\ninput_data\\n.\\nread_data_sets\\n(\\n\"/tmp/data/\"\\n)\\nX_test\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntest\\n.\\nimages\\n.\\nreshape\\n((\\n-\\n1\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n))\\ny_test\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntest\\n.\\nlabels\\nNow\\twe\\tare\\tready\\tto\\ttrain\\tthe\\tRNN.\\tThe\\texecution\\tphase\\tis\\texactly\\tthe\\tsame\\tas\\tfor\\tthe\\tMNIST\\tclassifier\\nin\\t\\nChapter\\t10\\n,\\texcept\\tthat\\twe\\treshape\\teach\\ttraining\\tbatch\\tbefore\\tfeeding\\tit\\tto\\tthe\\tnetwork.\\nn_epochs\\n\\t\\n=\\n\\t\\n100\\nbatch_size\\n\\t\\n=\\n\\t\\n150\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch', 'X_batch\\n\\t\\n=\\n\\t\\nX_batch\\n.\\nreshape\\n((\\n-\\n1\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nacc_train\\n\\t\\n=\\n\\t\\naccuracy\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nacc_test\\n\\t\\n=\\n\\t\\naccuracy\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_test\\n,\\n\\t\\ny\\n:\\n\\t\\ny_test\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nprint\\n(\\nepoch\\n,\\n\\t\\n\"Train\\taccuracy:\"\\n,\\n\\t\\nacc_train\\n,\\n\\t\\n\"Test\\taccuracy:\"\\n,\\n\\t\\nacc_test\\n)\\nThe\\toutput\\tshould\\tlook\\tlike\\tthis:\\n0\\tTrain\\taccuracy:\\t0.94\\tTest\\taccuracy:\\t0.9308\\n1\\tTrain\\taccuracy:\\t0.933333\\tTest\\taccuracy:\\t0.9431\\n[...]\\n98\\tTrain\\taccuracy:\\t0.98\\tTest\\taccuracy:\\t0.9794\\n99\\tTrain\\taccuracy:\\t1.0\\tTest\\taccuracy:\\t0.9804\\nWe\\tget\\tover\\t98%\\taccuracy\\t—\\tnot\\tbad!\\tPlus\\tyou\\twould\\tcertainly\\tget\\ta\\tbetter\\tresult\\tby\\ttuning\\tthe\\nhyperparameters,\\tinitializing\\tthe\\tRNN\\tweights\\tusing\\tHe\\tinitialization,\\ttraining\\tlonger,\\tor\\tadding\\ta\\tbit\\tof\\nregularization\\t(e.g.,\\tdropout).\\nTIP\\nYou\\t\\ncan\\tspecify\\tan\\tinitializer\\tfor\\tthe\\tRNN\\tby\\twrapping\\tits\\t\\nconstruction', 'code\\tin\\ta\\tvariable\\tscope\\t(e.g.,\\tuse\\nvariable_scope\\n(\"rnn\",\\tinitializer=variance_scaling_initializer())\\n\\tto\\tuse\\tHe\\tinitialization).', 'Training\\tto\\tPredict\\tTime\\tSeries\\nNow\\t\\nlet’s\\ttake\\ta\\tlook\\tat\\thow\\tto\\thandle\\ttime\\tseries,\\tsuch\\tas\\tstock\\tprices,\\tair\\ttemperature,\\tbrain\\twave\\npatterns,\\tand\\tso\\ton.\\tIn\\tthis\\tsection\\twe\\twill\\ttrain\\tan\\tRNN\\tto\\tpredict\\tthe\\tnext\\tvalue\\tin\\ta\\tgenerated\\ttime\\nseries.\\tEach\\ttraining\\tinstance\\tis\\ta\\trandomly\\tselected\\tsequence\\tof\\t20\\tconsecutive\\tvalues\\tfrom\\tthe\\ttime\\nseries,\\tand\\tthe\\ttarget\\tsequence\\tis\\tthe\\tsame\\tas\\tthe\\tinput\\tsequence,\\texcept\\tit\\tis\\tshifted\\tby\\tone\\ttime\\tstep\\tinto\\nthe\\tfuture\\t(see\\t\\nFigure\\t14-7\\n).\\nFigure\\t14-7.\\t\\nTime\\tseries\\t(left),\\tand\\ta\\ttraining\\tinstance\\tfrom\\tthat\\tseries\\t(right)\\nFirst,\\tlet’s\\tcreate\\tthe\\tRNN.\\tIt\\twill\\tcontain\\t100\\trecurrent\\tneurons\\tand\\twe\\twill\\tunroll\\tit\\tover\\t20\\ttime\\tsteps\\nsince\\teach\\ttraining\\tinstance\\twill\\tbe\\t20\\tinputs\\tlong.\\tEach\\tinput\\twill\\tcontain\\tonly\\tone\\tfeature\\t(the\\tvalue\\tat\\nthat\\ttime).\\tThe\\ttargets\\t\\nare\\talso\\tsequences\\tof\\t20\\tinputs,\\teach\\tcontaining\\ta\\tsingle\\tvalue.\\tThe\\tcode\\tis\\talmost\\nthe\\tsame\\tas\\tearlier:\\nn_steps\\n\\t\\n=\\n\\t\\n20\\nn_inputs\\n\\t\\n=\\n\\t\\n1\\nn_neurons\\n\\t\\n=\\n\\t\\n100\\nn_outputs\\n\\t\\n=\\n\\t\\n1\\nX\\n\\t\\n=\\n\\t\\ntf\\n.', '=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n])\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_outputs\\n])\\ncell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\ncell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nNOTE\\nIn\\tgeneral\\tyou\\twould\\thave\\tmore\\tthan\\tjust\\tone\\tinput\\tfeature.\\tFor\\texample,\\tif\\tyou\\twere\\ttrying\\tto\\tpredict\\tstock\\tprices,\\tyou\\twould\\nlikely\\thave\\tmany\\tother\\tinput\\tfeatures\\tat\\teach\\ttime\\tstep,\\tsuch\\tas\\tprices\\tof\\tcompeting\\tstocks,\\tratings\\tfrom\\tanalysts,\\tor\\tany\\tother\\nfeature\\tthat\\tmight\\thelp\\tthe\\tsystem\\tmake\\tits\\tpredictions.\\nAt\\teach\\ttime\\tstep\\twe\\tnow\\thave\\tan\\toutput\\tvector\\tof\\tsize\\t100.\\tBut\\twhat\\twe\\tactually\\twant\\tis\\ta\\tsingle\\toutput\\nvalue\\tat\\teach\\ttime\\tstep.\\tThe\\tsimplest\\tsolution\\tis\\tto\\twrap\\tthe\\tcell\\tin\\t\\nan\\t\\nOutputProjectionWrapper\\n.\\tA\\ncell\\twrapper\\tacts\\tlike\\ta\\tnormal\\tcell,\\tproxying\\tevery\\tmethod\\tcall\\tto\\tan\\tunderlying\\tcell,\\tbut\\tit\\talso\\tadds', 'some\\tfunctionality.\\tThe\\t\\nOutputProjectionWrapper\\n\\tadds\\ta\\tfully\\tconnected\\tlayer\\tof\\tlinear\\tneurons\\t(i.e.,\\nwithout\\tany\\tactivation\\tfunction)\\ton\\ttop\\tof\\teach\\toutput\\t(but\\tit\\tdoes\\tnot\\taffect\\tthe\\tcell\\tstate).\\tAll\\tthese\\tfully\\nconnected\\tlayers\\tshare\\tthe\\tsame\\t(trainable)\\tweights\\tand\\tbias\\tterms.\\tThe\\tresulting\\tRNN\\tis\\trepresented\\tin', 'Figure\\t14-8\\n.\\nFigure\\t14-8.\\t\\nRNN\\tcells\\tusing\\toutput\\tprojections\\nWrapping\\ta\\tcell\\tis\\tquite\\teasy.\\t\\nLet’s\\ttweak\\tthe\\tpreceding\\tcode\\tby\\twrapping\\tthe\\t\\nBasicRNNCell\\n\\tinto\\tan\\nOutputProjectionWrapper\\n:\\ncell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nOutputProjectionWrapper\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n),\\n\\t\\t\\t\\t\\noutput_size\\n=\\nn_outputs\\n)\\nSo\\tfar,\\tso\\tgood.\\tNow\\twe\\tneed\\tto\\tdefine\\tthe\\t\\ncost\\tfunction.\\tWe\\twill\\tuse\\tthe\\tMean\\tSquared\\tError\\t(MSE),\\tas\\nwe\\tdid\\tin\\tprevious\\tregression\\ttasks.\\tNext\\twe\\twill\\tcreate\\tan\\tAdam\\toptimizer,\\tthe\\ttraining\\top,\\tand\\tthe\\nvariable\\tinitialization\\top,\\tas\\tusual:\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.001\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\ny\\n))\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nNow\\ton\\tto\\tthe\\texecution\\tphase:\\nn_iterations\\n\\t\\n=\\n\\t\\n1500\\nbatch_size\\n\\t\\n=\\n\\t\\n50\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:', ':\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\t\\n#\\tfetch\\tthe\\tnext\\ttraining\\tbatch\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\niteration\\n\\t\\n%\\n\\t\\n100\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmse\\n\\t\\n=\\n\\t\\nloss\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ny\\n:\\n\\t\\ny_batch\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nprint\\n(\\niteration\\n,\\n\\t\\n\"\\n\\\\t\\nMSE:\"\\n,\\n\\t\\nmse\\n)', 'The\\tprogram’s\\toutput\\tshould\\tlook\\tlike\\tthis:\\n0\\t\\t\\t\\t\\t\\t\\tMSE:\\t13.6543\\n100\\t\\t\\t\\t\\tMSE:\\t0.538476\\n200\\t\\t\\t\\t\\tMSE:\\t0.168532\\n300\\t\\t\\t\\t\\tMSE:\\t0.0879579\\n400\\t\\t\\t\\t\\tMSE:\\t0.0633425\\n[...]\\nOnce\\tthe\\tmodel\\tis\\ttrained,\\tyou\\tcan\\tmake\\tpredictions:\\nX_new\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\t\\n#\\tNew\\tsequences\\ny_pred\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\noutputs\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_new\\n})\\nFigure\\t14-9\\n\\tshows\\tthe\\tpredicted\\tsequence\\tfor\\tthe\\tinstance\\twe\\tlooked\\tat\\tearlier\\t(in\\t\\nFigure\\t14-7\\n),\\tafter\\tjust\\n1,000\\ttraining\\titerations.\\nFigure\\t14-9.\\t\\nTime\\tseries\\tprediction\\nAlthough\\tusing\\tan\\t\\nOutputProjectionWrapper\\n\\t\\nis\\tthe\\tsimplest\\tsolution\\tto\\treduce\\tthe\\tdimensionality\\tof\\nthe\\tRNN’s\\toutput\\tsequences\\tdown\\tto\\tjust\\tone\\tvalue\\tper\\ttime\\tstep\\t(per\\tinstance),\\tit\\tis\\tnot\\tthe\\tmost\\nefficient.\\tThere\\tis\\ta\\ttrickier\\tbut\\tmore\\tefficient\\tsolution:\\tyou\\tcan\\treshape\\tthe\\tRNN\\toutputs\\tfrom\\n[batch_size,\\tn_steps,\\tn_neurons]\\n\\tto\\t\\n[batch_size\\t*\\tn_steps,\\tn_neurons]\\n,\\tthen\\tapply\\ta\\tsingle\\nfully\\tconnected\\tlayer\\twith\\tthe\\tappropriate\\toutput\\tsize\\t(in\\tour\\tcase\\tjust\\t1),\\twhich\\twill\\tresult\\tin\\tan\\toutput', 'tensor\\tof\\tshape\\t\\n[batch_size\\t*\\tn_steps,\\tn_outputs]\\n,\\tand\\tthen\\treshape\\tthis\\ttensor\\tto\\t\\n[batch_size,\\nn_steps,\\tn_outputs]\\n.\\tThese\\toperations\\tare\\t\\nrepresented\\tin\\t\\nFigure\\t14-10\\n.', 'Figure\\t14-10.\\t\\nStack\\tall\\tthe\\toutputs,\\tapply\\tthe\\tprojection,\\t\\nthen\\tunstack\\tthe\\tresult\\nTo\\timplement\\tthis\\tsolution,\\twe\\tfirst\\trevert\\tto\\ta\\tbasic\\tcell,\\twithout\\t\\nthe\\t\\nOutputProjectionWrapper\\n:\\ncell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\nrnn_outputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\ncell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nThen\\twe\\tstack\\tall\\tthe\\toutputs\\tusing\\tthe\\t\\nreshape()\\n\\t\\noperation,\\tapply\\tthe\\tfully\\tconnected\\tlinear\\tlayer\\n(without\\tusing\\tany\\tactivation\\tfunction;\\tthis\\tis\\tjust\\ta\\tprojection),\\tand\\tfinally\\tunstack\\tall\\tthe\\toutputs,\\tagain\\nusing\\t\\nreshape()\\n:\\nstacked_rnn_outputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nreshape\\n(\\nrnn_outputs\\n,\\n\\t\\n[\\n-\\n1\\n,\\n\\t\\nn_neurons\\n])\\nstacked_outputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nstacked_rnn_outputs\\n,\\n\\t\\nn_outputs\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nreshape\\n(\\nstacked_outputs\\n,\\n\\t\\n[\\n-\\n1\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_outputs\\n])\\nThe\\trest\\tof\\tthe\\tcode\\tis\\tthe\\tsame\\tas\\tearlier.\\tThis\\tcan\\tprovide\\ta\\tsignificant\\tspeed\\tboost\\tsince\\tthere\\tis\\tjust', 'one\\tfully\\tconnected\\tlayer\\tinstead\\tof\\tone\\tper\\t\\ntime\\tstep.', 'Creative\\tRNN\\nNow\\tthat\\twe\\thave\\ta\\t\\nmodel\\tthat\\tcan\\tpredict\\tthe\\tfuture,\\twe\\tcan\\tuse\\tit\\tto\\tgenerate\\tsome\\tcreative\\tsequences,\\nas\\texplained\\tat\\tthe\\tbeginning\\tof\\tthe\\tchapter.\\tAll\\twe\\tneed\\tis\\tto\\tprovide\\tit\\ta\\tseed\\tsequence\\tcontaining\\nn_steps\\n\\tvalues\\t(e.g.,\\tfull\\tof\\tzeros),\\tuse\\tthe\\tmodel\\tto\\tpredict\\tthe\\tnext\\tvalue,\\tappend\\tthis\\tpredicted\\tvalue\\nto\\tthe\\tsequence,\\tfeed\\tthe\\tlast\\t\\nn_steps\\n\\tvalues\\tto\\tthe\\tmodel\\tto\\tpredict\\tthe\\tnext\\tvalue,\\tand\\tso\\ton.\\tThis\\nprocess\\tgenerates\\ta\\tnew\\tsequence\\tthat\\thas\\tsome\\tresemblance\\tto\\tthe\\toriginal\\ttime\\tseries\\t(see\\t\\nFigure\\t14-\\n11\\n).\\nsequence\\n\\t\\n=\\n\\t\\n[\\n0.\\n]\\n\\t\\n*\\n\\t\\nn_steps\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\n300\\n):\\n\\t\\t\\t\\t\\nX_batch\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n(\\nsequence\\n[\\n-\\nn_steps\\n:])\\n.\\nreshape\\n(\\n1\\n,\\n\\t\\nn_steps\\n,\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\ny_pred\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\noutputs\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n})\\n\\t\\t\\t\\t\\nsequence\\n.\\nappend\\n(\\ny_pred\\n[\\n0\\n,\\n\\t\\n-\\n1\\n,\\n\\t\\n0\\n])\\nFigure\\t14-11.\\t\\nCreative\\tsequences,\\tseeded\\twith\\tzeros\\t(left)\\tor\\twith\\tan\\tinstance\\t(right)', 'Now\\tyou\\tcan\\ttry\\tto\\tfeed\\tall\\tyour\\tJohn\\tLennon\\talbums\\tto\\tan\\tRNN\\tand\\tsee\\tif\\tit\\tcan\\tgenerate\\tthe\\tnext\\n“Imagine.”\\tHowever,\\tyou\\twill\\tprobably\\tneed\\ta\\tmuch\\tmore\\tpowerful\\tRNN,\\twith\\tmore\\tneurons,\\tand\\talso\\nmuch\\tdeeper.\\tLet’s\\tlook\\tat\\tdeep\\tRNNs\\t\\nnow.', 'Deep\\tRNNs\\nIt\\t\\nis\\tquite\\tcommon\\tto\\tstack\\tmultiple\\tlayers\\tof\\tcells,\\tas\\tshown\\tin\\t\\nFigure\\t14-12\\n.\\tThis\\tgives\\tyou\\ta\\t\\ndeep\\nRNN\\n.\\nTo\\timplement\\ta\\tdeep\\tRNN\\tin\\tTensorFlow,\\t\\nyou\\tcan\\tcreate\\tseveral\\tcells\\tand\\tstack\\tthem\\tinto\\ta\\nMultiRNNCell\\n.\\tIn\\tthe\\tfollowing\\tcode\\twe\\tstack\\tthree\\tidentical\\tcells\\t(but\\tyou\\tcould\\tvery\\twell\\tuse\\tvarious\\nkinds\\tof\\tcells\\twith\\ta\\tdifferent\\tnumber\\tof\\tneurons):\\nn_neurons\\n\\t\\n=\\n\\t\\n100\\nn_layers\\n\\t\\n=\\n\\t\\n3\\nlayers\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nlayer\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_layers\\n)]\\nmulti_layer_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nMultiRNNCell\\n(\\nlayers\\n)\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nmulti_layer_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nFigure\\t14-12.\\t\\nDeep\\tRNN\\t(left),\\tunrolled\\tthrough\\ttime\\t(right)\\nThat’s\\tall\\tthere\\tis\\tto\\tit!\\tThe\\t\\nstates\\n\\tvariable\\tis\\ta\\ttuple\\tcontaining\\tone\\ttensor\\tper\\tlayer,\\teach\\trepresenting\\nthe\\tfinal\\tstate\\tof\\tthat\\tlayer’s\\tcell\\t(with\\tshape', '[batch_size,\\tn_neurons]\\n).\\tIf\\tyou\\tset\\nstate_is_tuple=False\\n\\twhen\\tcreating\\tthe\\t\\nMultiRNNCell\\n,\\tthen\\t\\nstates\\n\\tbecomes\\ta\\tsingle\\ttensor\\ncontaining\\tthe\\tstates\\tfrom\\tevery\\tlayer,\\tconcatenated\\talong\\tthe\\tcolumn\\taxis\\t(i.e.,\\tits\\tshape\\tis\\n[batch_size,\\tn_layers\\t*\\tn_neurons]\\n).\\tNote\\tthat\\tbefore\\tTensorFlow\\t0.11.0,\\tthis\\tbehavior\\twas\\tthe\\ndefault.', 'Distributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\nChapter\\t12\\n\\tpointed\\t\\nout\\tthat\\twe\\tcan\\tefficiently\\tdistribute\\tdeep\\tRNNs\\tacross\\tmultiple\\tGPUs\\tby\\tpinning\\neach\\tlayer\\tto\\ta\\tdifferent\\tGPU\\t(see\\t\\nFigure\\t12-16\\n).\\tHowever,\\tif\\tyou\\ttry\\tto\\tcreate\\teach\\tcell\\tin\\ta\\tdifferent\\ndevice()\\n\\tblock,\\tit\\twill\\t\\nnot\\twork:\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/gpu:0\"\\n):\\n\\t\\t\\n#\\tBAD!\\tThis\\tis\\tignored.\\n\\t\\t\\t\\t\\nlayer1\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\n\"/gpu:1\"\\n):\\n\\t\\t\\n#\\tBAD!\\tIgnored\\tagain.\\n\\t\\t\\t\\t\\nlayer2\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\nThis\\tfails\\tbecause\\ta\\t\\nBasicRNNCell\\n\\tis\\t\\na\\tcell\\tfactory,\\tnot\\ta\\tcell\\t\\nper\\tse\\n\\t(as\\tmentioned\\tearlier);\\tno\\tcells\\tget\\ncreated\\twhen\\tyou\\tcreate\\tthe\\tfactory,\\tand\\tthus\\tno\\tvariables\\tdo\\teither.\\tThe\\tdevice\\tblock\\tis\\tsimply\\tignored.\\nThe\\tcells\\tactually\\tget\\tcreated\\tlater.\\tWhen\\tyou\\tcall\\t\\ndynamic_rnn()\\n,\\t\\nit\\tcalls\\tthe\\t\\nMultiRNNCell\\n,\\t\\nwhich\\ncalls\\teach\\tindividual\\t\\nBasicRNNCell\\n,\\t\\nwhich\\tcreate\\tthe\\tactual\\tcells\\t(including\\ttheir\\tvariables).', 'Unfortunately,\\t\\nnone\\tof\\tthese\\tclasses\\tprovide\\tany\\tway\\tto\\tcontrol\\tthe\\tdevices\\ton\\twhich\\tthe\\tvariables\\tget\\ncreated.\\tIf\\tyou\\ttry\\tto\\tput\\tthe\\t\\ndynamic_rnn()\\n\\tcall\\twithin\\ta\\tdevice\\tblock,\\tthe\\twhole\\tRNN\\tgets\\tpinned\\tto\\ta\\nsingle\\tdevice.\\tSo\\tare\\tyou\\tstuck?\\tFortunately\\tnot!\\tThe\\ttrick\\tis\\tto\\tcreate\\tyour\\town\\tcell\\twrapper:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nclass\\n\\t\\nDeviceCellWrapper\\n(\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nRNNCell\\n):\\n\\t\\t\\ndef\\n\\t\\n__init__\\n(\\nself\\n,\\n\\t\\ndevice\\n,\\n\\t\\ncell\\n):\\n\\t\\t\\t\\t\\nself\\n.\\n_cell\\n\\t\\n=\\n\\t\\ncell\\n\\t\\t\\t\\t\\nself\\n.\\n_device\\n\\t\\n=\\n\\t\\ndevice\\n\\t\\t\\n@property\\n\\t\\t\\ndef\\n\\t\\nstate_size\\n(\\nself\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nself\\n.\\n_cell\\n.\\nstate_size\\n\\t\\t\\n@property\\n\\t\\t\\ndef\\n\\t\\noutput_size\\n(\\nself\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nself\\n.\\n_cell\\n.\\noutput_size\\n\\t\\t\\ndef\\n\\t\\n__call__\\n(\\nself\\n,\\n\\t\\ninputs\\n,\\n\\t\\nstate\\n,\\n\\t\\nscope\\n=\\nNone\\n):\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\ndevice\\n(\\nself\\n.\\n_device\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nself\\n.\\n_cell\\n(\\ninputs\\n,\\n\\t\\nstate\\n,\\n\\t\\nscope\\n)\\nThis\\twrapper\\tsimply\\tproxies\\tevery\\tmethod\\tcall\\tto\\tanother\\tcell,\\texcept\\tit\\twraps\\tthe\\t\\n__call__()\\n\\tfunction\\nwithin\\ta\\tdevice\\tblock.\\n2', '2\\n\\tNow\\tyou\\tcan\\tdistribute\\teach\\tlayer\\ton\\ta\\tdifferent\\tGPU:\\ndevices\\n\\t\\n=\\n\\t\\n[\\n\"/gpu:0\"\\n,\\n\\t\\n\"/gpu:1\"\\n,\\n\\t\\n\"/gpu:2\"\\n]\\ncells\\n\\t\\n=\\n\\t\\n[\\nDeviceCellWrapper\\n(\\ndev\\n,\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ndev\\n\\t\\nin\\n\\t\\ndevices\\n]\\nmulti_layer_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nMultiRNNCell\\n(\\ncells\\n)\\noutputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nmulti_layer_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nWARNING\\nDo\\tnot\\tset\\t\\nstate_is_tuple=False\\n,\\t\\nor\\tthe\\t\\nMultiRNNCell\\n\\twill\\tconcatenate\\tall\\tthe\\tcell\\tstates\\tinto\\ta\\tsingle\\ttensor,\\ton\\ta\\tsingle\\tGPU.', 'Applying\\tDropout\\nIf\\t\\nyou\\tbuild\\ta\\tvery\\tdeep\\tRNN,\\tit\\tmay\\tend\\tup\\toverfitting\\tthe\\ttraining\\tset.\\t\\nTo\\tprevent\\tthat,\\ta\\tcommon\\ntechnique\\tis\\tto\\tapply\\tdropout\\t(introduced\\tin\\t\\nChapter\\t11\\n).\\tYou\\tcan\\tsimply\\tadd\\ta\\tdropout\\tlayer\\tbefore\\tor\\nafter\\tthe\\tRNN\\tas\\tusual,\\tbut\\tif\\tyou\\talso\\twant\\tto\\tapply\\tdropout\\tbetween\\tthe\\tRNN\\tlayers,\\tyou\\tneed\\tto\\tuse\\ta\\nDropoutWrapper\\n.\\tThe\\tfollowing\\tcode\\tapplies\\tdropout\\tto\\tthe\\tinputs\\tof\\teach\\tlayer\\tin\\tthe\\tRNN,\\tdropping\\neach\\tinput\\twith\\ta\\t50%\\tprobability:\\nkeep_prob\\n\\t\\n=\\n\\t\\n0.5\\ncells\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nlayer\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_layers\\n)]\\ncells_drop\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nDropoutWrapper\\n(\\ncell\\n,\\n\\t\\ninput_keep_prob\\n=\\nkeep_prob\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ncell\\n\\t\\nin\\n\\t\\ncells\\n]\\nmulti_layer_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nMultiRNNCell\\n(\\ncells_drop\\n)\\nrnn_outputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nmulti_layer_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nNote\\tthat\\tit\\tis\\talso\\tpossible\\tto\\tapply\\tdropout\\tto\\tthe\\toutputs\\tby\\t\\nsetting', 'setting\\t\\noutput_keep_prob\\n.\\nThe\\tmain\\tproblem\\twith\\tthis\\tcode\\tis\\tthat\\tit\\twill\\tapply\\tdropout\\tnot\\tonly\\tduring\\ttraining\\tbut\\talso\\tduring\\ntesting,\\twhich\\tis\\tnot\\twhat\\tyou\\twant\\t(recall\\tthat\\tdropout\\tshould\\tbe\\tapplied\\tonly\\tduring\\ttraining).\\nUnfortunately,\\tthe\\t\\nDropoutWrapper\\n\\t\\ndoes\\tnot\\tsupport\\ta\\t\\ntraining\\n\\t\\nplaceholder\\t(yet?),\\tso\\tyou\\tmust\\teither\\nwrite\\tyour\\town\\tdropout\\twrapper\\tclass,\\tor\\thave\\ttwo\\tdifferent\\tgraphs:\\tone\\tfor\\ttraining,\\tand\\tthe\\tother\\tfor\\ntesting.\\tThe\\tsecond\\toption\\t\\nlooks\\tlike\\tthis:\\nimport\\n\\t\\nsys\\ntraining\\n\\t\\n=\\n\\t\\n(\\nsys\\n.\\nargv\\n[\\n-\\n1\\n]\\n\\t\\n==\\n\\t\\n\"train\"\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_inputs\\n])\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_steps\\n,\\n\\t\\nn_outputs\\n])\\ncells\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicRNNCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nlayer\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_layers\\n)]\\nif\\n\\t\\ntraining\\n:\\n\\t\\t\\t\\t\\ncells\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nDropoutWrapper\\n(\\ncell\\n,\\n\\t\\ninput_keep_prob\\n=\\nkeep_prob\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ncell\\n\\t\\nin\\n\\t\\ncells\\n]', 'cells\\n]\\nmulti_layer_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nMultiRNNCell\\n(\\ncells\\n)\\nrnn_outputs\\n,\\n\\t\\nstates\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\ndynamic_rnn\\n(\\nmulti_layer_cell\\n,\\n\\t\\nX\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\n[\\n...\\n]\\n\\t\\n#\\tbuild\\tthe\\trest\\tof\\tthe\\tgraph\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nif\\n\\t\\ntraining\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\ttrain\\tthe\\tmodel\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsave_path\\n\\t\\n=\\n\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model.ckpt\"\\n)\\n\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\n\"/tmp/my_model.ckpt\"\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tuse\\tthe\\tmodel\\nWith\\tthat\\tyou\\tshould\\tbe\\table\\tto\\ttrain\\tall\\tsorts\\tof\\tRNNs!\\tUnfortunately,\\tif\\tyou\\twant\\tto\\ttrain\\tan\\tRNN\\ton\\tlong\\nsequences,\\tthings\\twill\\tget\\ta\\tbit\\tharder.\\tLet’s\\tsee\\twhy\\tand\\twhat\\tyou\\tcan\\tdo\\tabout\\tit.', 'The\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps\\nTo\\ttrain\\tan\\tRNN\\ton\\t\\nlong\\tsequences,\\tyou\\twill\\tneed\\tto\\trun\\tit\\tover\\tmany\\ttime\\tsteps,\\tmaking\\tthe\\tunrolled\\nRNN\\ta\\tvery\\tdeep\\tnetwork.\\tJust\\tlike\\tany\\tdeep\\tneural\\tnetwork\\tit\\tmay\\tsuffer\\tfrom\\tthe\\t\\nvanishing/exploding\\ngradients\\tproblem\\t(discussed\\tin\\t\\nChapter\\t11\\n)\\tand\\ttake\\tforever\\tto\\ttrain.\\tMany\\tof\\tthe\\ttricks\\twe\\tdiscussed\\tto\\nalleviate\\tthis\\tproblem\\tcan\\tbe\\tused\\tfor\\tdeep\\tunrolled\\tRNNs\\tas\\twell:\\tgood\\tparameter\\tinitialization,\\nnonsaturating\\tactivation\\tfunctions\\t(e.g.,\\tReLU),\\tBatch\\tNormalization,\\tGradient\\tClipping,\\tand\\tfaster\\noptimizers.\\tHowever,\\tif\\tthe\\tRNN\\tneeds\\tto\\thandle\\teven\\tmoderately\\tlong\\tsequences\\t(e.g.,\\t100\\tinputs),\\tthen\\ntraining\\twill\\tstill\\tbe\\tvery\\tslow.\\nThe\\tsimplest\\tand\\tmost\\tcommon\\tsolution\\tto\\tthis\\tproblem\\tis\\tto\\tunroll\\tthe\\tRNN\\tonly\\tover\\ta\\tlimited\\tnumber\\nof\\ttime\\tsteps\\tduring\\ttraining.\\tThis\\tis\\tcalled\\t\\ntruncated\\tbackpropagation\\tthrough\\ttime\\n.\\t\\nIn\\t\\nTensorFlow\\tyou', 'can\\timplement\\tit\\tsimply\\tby\\ttruncating\\tthe\\tinput\\tsequences.\\tFor\\texample,\\tin\\tthe\\ttime\\tseries\\tprediction\\nproblem,\\tyou\\twould\\tsimply\\treduce\\t\\nn_steps\\n\\tduring\\ttraining.\\tThe\\tproblem,\\tof\\tcourse,\\tis\\tthat\\tthe\\tmodel\\nwill\\tnot\\tbe\\table\\tto\\tlearn\\tlong-term\\tpatterns.\\tOne\\tworkaround\\tcould\\tbe\\tto\\tmake\\tsure\\tthat\\tthese\\tshortened\\nsequences\\tcontain\\tboth\\told\\tand\\trecent\\tdata,\\tso\\tthat\\tthe\\tmodel\\tcan\\tlearn\\tto\\tuse\\tboth\\t(e.g.,\\tthe\\tsequence\\ncould\\tcontain\\tmonthly\\tdata\\tfor\\tthe\\tlast\\tfive\\tmonths,\\tthen\\tweekly\\tdata\\tfor\\tthe\\tlast\\tfive\\tweeks,\\tthen\\tdaily\\ndata\\tover\\tthe\\tlast\\tfive\\tdays).\\tBut\\tthis\\tworkaround\\thas\\tits\\tlimits:\\twhat\\tif\\tfine-grained\\tdata\\tfrom\\tlast\\tyear\\tis\\nactually\\tuseful?\\tWhat\\tif\\tthere\\twas\\ta\\tbrief\\tbut\\tsignificant\\tevent\\tthat\\tabsolutely\\tmust\\tbe\\ttaken\\tinto\\taccount,\\neven\\tyears\\tlater\\t(e.g.,\\tthe\\tresult\\tof\\tan\\telection)?\\nBesides\\tthe\\tlong\\ttraining\\ttime,\\ta\\tsecond\\tproblem\\tfaced\\tby\\tlong-running\\tRNNs\\tis\\tthe\\tfact\\tthat\\tthe\\tmemory', 'of\\tthe\\tfirst\\tinputs\\tgradually\\tfades\\taway.\\tIndeed,\\tdue\\tto\\tthe\\ttransformations\\tthat\\tthe\\tdata\\tgoes\\tthrough\\twhen\\ntraversing\\tan\\tRNN,\\tsome\\tinformation\\tis\\tlost\\tafter\\teach\\ttime\\tstep.\\tAfter\\ta\\twhile,\\tthe\\tRNN’s\\tstate\\tcontains\\nvirtually\\tno\\ttrace\\tof\\tthe\\tfirst\\tinputs.\\tThis\\tcan\\tbe\\ta\\tshowstopper.\\tFor\\texample,\\tsay\\tyou\\twant\\tto\\tperform\\nsentiment\\tanalysis\\ton\\ta\\tlong\\treview\\tthat\\tstarts\\twith\\tthe\\tfour\\twords\\t“I\\tloved\\tthis\\tmovie,”\\tbut\\tthe\\trest\\tof\\tthe\\nreview\\tlists\\tthe\\tmany\\tthings\\tthat\\tcould\\thave\\tmade\\tthe\\tmovie\\teven\\tbetter.\\tIf\\tthe\\tRNN\\tgradually\\tforgets\\tthe\\nfirst\\tfour\\twords,\\tit\\twill\\tcompletely\\tmisinterpret\\tthe\\treview.\\tTo\\tsolve\\tthis\\tproblem,\\tvarious\\ttypes\\tof\\tcells\\nwith\\tlong-term\\tmemory\\thave\\tbeen\\tintroduced.\\tThey\\thave\\tproved\\tso\\tsuccessful\\tthat\\tthe\\tbasic\\tcells\\tare\\tnot\\nmuch\\tused\\tanymore.\\tLet’s\\t\\nfirst\\tlook\\tat\\tthe\\tmost\\tpopular\\tof\\tthese\\tlong\\tmemory\\tcells:\\tthe\\tLSTM\\tcell.', 'LSTM\\tCell\\nThe\\t\\nLong\\tShort-Term\\tMemory\\n\\t(LSTM)\\t\\ncell\\twas\\t\\nproposed\\tin\\t1997\\n3\\n\\tby\\tSepp\\tHochreiter\\tand\\tJürgen\\nSchmidhuber,\\tand\\tit\\twas\\tgradually\\timproved\\tover\\tthe\\tyears\\tby\\tseveral\\tresearchers,\\tsuch\\tas\\tAlex\\tGraves,\\nHaşim\\tSak\\n,\\n4\\n\\t\\nWojciech\\tZaremba\\n,\\n5\\n\\tand\\tmany\\tmore.\\tIf\\tyou\\tconsider\\tthe\\tLSTM\\tcell\\tas\\ta\\tblack\\tbox,\\tit\\tcan\\tbe\\nused\\tvery\\tmuch\\tlike\\ta\\tbasic\\tcell,\\texcept\\tit\\twill\\tperform\\tmuch\\tbetter;\\ttraining\\twill\\tconverge\\tfaster\\tand\\tit\\nwill\\tdetect\\tlong-term\\tdependencies\\tin\\tthe\\tdata.\\tIn\\tTensorFlow,\\tyou\\tcan\\tsimply\\tuse\\ta\\t\\nBasicLSTMCell\\ninstead\\tof\\ta\\t\\nBasicRNNCell\\n:\\nlstm_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nBasicLSTMCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\nLSTM\\tcells\\tmanage\\ttwo\\tstate\\tvectors,\\tand\\tfor\\tperformance\\treasons\\tthey\\tare\\tkept\\t\\nseparate\\n\\tby\\tdefault.\\tYou\\ncan\\tchange\\tthis\\tdefault\\tbehavior\\tby\\tsetting\\t\\nstate_is_tuple=False\\n\\t\\nwhen\\tcreating\\tthe\\t\\nBasicLSTMCell\\n.\\nSo\\thow\\tdoes\\tan\\tLSTM\\tcell\\twork?\\tThe\\tarchitecture\\tof\\ta\\tbasic\\tLSTM\\tcell\\tis\\tshown\\tin\\t\\nFigure\\t14-13\\n.\\nFigure\\t14-13.\\t\\nLSTM\\tcell', 'LSTM\\tcell\\nIf\\tyou\\tdon’t\\tlook\\tat\\twhat’s\\tinside\\tthe\\tbox,\\tthe\\tLSTM\\tcell\\tlooks\\texactly\\tlike\\ta\\tregular\\tcell,\\texcept\\tthat\\tits\\nstate\\tis\\tsplit\\tin\\ttwo\\tvectors:\\t\\nh\\n(\\nt\\n)\\n\\tand\\t\\nc\\n(\\nt\\n)\\n\\t(“c”\\tstands\\tfor\\t“cell”).\\tYou\\tcan\\tthink\\tof\\t\\nh\\n(\\nt\\n)\\n\\tas\\tthe\\tshort-term\\tstate\\nand\\t\\nc\\n(\\nt\\n)\\n\\tas\\tthe\\tlong-term\\tstate.\\nNow\\tlet’s\\topen\\tthe\\tbox!\\tThe\\tkey\\tidea\\tis\\tthat\\tthe\\tnetwork\\tcan\\tlearn\\twhat\\tto\\tstore\\tin\\tthe\\tlong-term\\tstate,\\nwhat\\tto\\tthrow\\taway,\\tand\\twhat\\tto\\tread\\tfrom\\tit.\\tAs\\tthe\\tlong-term\\tstate\\t\\nc\\n(\\nt\\n–1)\\n\\ttraverses\\tthe\\tnetwork\\tfrom\\tleft\\nto\\tright,\\tyou\\tcan\\tsee\\tthat\\tit\\tfirst\\tgoes\\tthrough\\t\\na\\t\\nforget\\tgate\\n,\\tdropping\\tsome\\tmemories,\\tand\\tthen\\tit\\tadds\\nsome\\tnew\\tmemories\\tvia\\tthe\\taddition\\toperation\\t(which\\tadds\\tthe\\tmemories\\tthat\\twere\\tselected\\tby\\tan\\t\\ninput\\ngate\\n).\\tThe\\tresult\\t\\nc\\n(\\nt\\n)\\n\\tis\\tsent\\tstraight\\tout,\\twithout\\tany\\tfurther\\ttransformation.\\tSo,\\tat\\teach\\ttime\\tstep,\\tsome\\nmemories\\tare\\tdropped\\tand\\tsome\\tmemories\\tare\\tadded.\\tMoreover,\\tafter\\tthe\\taddition\\toperation,\\tthe\\tlong-', 'term\\tstate\\tis\\tcopied\\tand\\tpassed\\tthrough\\tthe\\ttanh\\tfunction,\\tand\\tthen\\tthe\\tresult\\tis\\tfiltered\\tby\\t\\nthe\\t\\noutput\\tgate\\n.', 'This\\tproduces\\tthe\\tshort-term\\tstate\\t\\nh\\n(\\nt\\n)\\n\\t(which\\tis\\tequal\\tto\\tthe\\tcell’s\\toutput\\tfor\\tthis\\ttime\\tstep\\t\\ny\\n(\\nt\\n)\\n).\\tNow\\tlet’s\\nlook\\tat\\twhere\\tnew\\tmemories\\tcome\\tfrom\\tand\\thow\\tthe\\tgates\\twork.\\nFirst,\\tthe\\tcurrent\\tinput\\tvector\\t\\nx\\n(\\nt\\n)\\n\\tand\\tthe\\tprevious\\tshort-term\\tstate\\t\\nh\\n(\\nt\\n–1)\\n\\tare\\tfed\\tto\\tfour\\tdifferent\\tfully\\nconnected\\tlayers.\\tThey\\tall\\tserve\\ta\\tdifferent\\tpurpose:\\nThe\\tmain\\tlayer\\tis\\tthe\\tone\\tthat\\toutputs\\t\\ng\\n(\\nt\\n)\\n.\\tIt\\thas\\tthe\\tusual\\trole\\tof\\tanalyzing\\tthe\\tcurrent\\tinputs\\t\\nx\\n(\\nt\\n)\\n\\tand\\nthe\\tprevious\\t(short-term)\\tstate\\t\\nh\\n(\\nt\\n–1)\\n.\\tIn\\ta\\tbasic\\tcell,\\tthere\\tis\\tnothing\\telse\\tthan\\tthis\\tlayer,\\tand\\tits\\noutput\\tgoes\\tstraight\\tout\\tto\\t\\ny\\n(\\nt\\n)\\n\\tand\\t\\nh\\n(\\nt\\n)\\n.\\tIn\\tcontrast,\\tin\\tan\\tLSTM\\tcell\\tthis\\tlayer’s\\toutput\\tdoes\\tnot\\tgo\\nstraight\\tout,\\tbut\\tinstead\\tit\\tis\\tpartially\\tstored\\tin\\tthe\\tlong-term\\tstate.\\nThe\\tthree\\tother\\tlayers\\tare\\t\\ngate\\tcontrollers\\n.\\tSince\\tthey\\tuse\\tthe\\tlogistic\\tactivation\\tfunction,\\ttheir\\noutputs\\trange\\tfrom\\t0\\tto\\t1.\\tAs\\tyou\\tcan\\tsee,\\ttheir\\toutputs\\tare\\tfed\\tto\\telement-wise\\tmultiplication', 'operations,\\tso\\tif\\tthey\\toutput\\t0s,\\tthey\\tclose\\tthe\\tgate,\\tand\\tif\\tthey\\toutput\\t1s,\\tthey\\topen\\tit.\\tSpecifically:\\nThe\\t\\nforget\\tgate\\n\\t(controlled\\tby\\t\\nf\\n(\\nt\\n)\\n)\\tcontrols\\twhich\\tparts\\tof\\tthe\\tlong-term\\tstate\\tshould\\tbe\\terased.\\nThe\\t\\ninput\\tgate\\n\\t(controlled\\tby\\t\\ni\\n(\\nt\\n)\\n)\\tcontrols\\twhich\\tparts\\tof\\t\\ng\\n(\\nt\\n)\\n\\tshould\\tbe\\tadded\\tto\\tthe\\tlong-term\\nstate\\t(this\\tis\\twhy\\twe\\tsaid\\tit\\twas\\tonly\\t“partially\\tstored”).\\nFinally,\\tthe\\t\\noutput\\tgate\\n\\t(controlled\\tby\\t\\no\\n(\\nt\\n)\\n)\\tcontrols\\twhich\\tparts\\tof\\tthe\\tlong-term\\tstate\\tshould\\tbe\\nread\\tand\\toutput\\tat\\tthis\\ttime\\tstep\\t(both\\tto\\t\\nh\\n(\\nt\\n)\\n)\\tand\\t\\ny\\n(\\nt\\n)\\n.\\nIn\\tshort,\\tan\\tLSTM\\tcell\\tcan\\tlearn\\tto\\trecognize\\tan\\timportant\\tinput\\t(that’s\\tthe\\trole\\tof\\tthe\\tinput\\tgate),\\tstore\\tit\\nin\\tthe\\tlong-term\\tstate,\\tlearn\\tto\\tpreserve\\tit\\tfor\\tas\\tlong\\tas\\tit\\tis\\tneeded\\t(that’s\\tthe\\trole\\tof\\tthe\\tforget\\tgate),\\tand\\nlearn\\tto\\textract\\tit\\twhenever\\tit\\tis\\tneeded.\\tThis\\texplains\\twhy\\tthey\\thave\\tbeen\\tamazingly\\tsuccessful\\tat\\ncapturing\\tlong-term\\tpatterns\\tin\\ttime\\tseries,\\tlong\\ttexts,\\taudio\\trecordings,\\tand\\tmore.\\nEquation\\t14-3', 'summarizes\\thow\\tto\\tcompute\\tthe\\tcell’s\\tlong-term\\tstate,\\tits\\tshort-term\\tstate,\\tand\\tits\\toutput\\tat\\neach\\ttime\\tstep\\tfor\\ta\\tsingle\\tinstance\\t(the\\tequations\\tfor\\ta\\twhole\\tmini-batch\\tare\\tvery\\tsimilar).\\nEquation\\t14-3.\\t\\nLSTM\\tcomputations\\nW\\nxi\\n,\\t\\nW\\nxf\\n,\\t\\nW\\nxo\\n,\\t\\nW\\nxg\\n\\tare\\tthe\\tweight\\tmatrices\\tof\\teach\\tof\\tthe\\tfour\\tlayers\\tfor\\ttheir\\tconnection\\tto\\tthe', 'input\\tvector\\t\\nx\\n(\\nt\\n)\\n.\\nW\\nhi\\n,\\t\\nW\\nhf\\n,\\t\\nW\\nho\\n,\\tand\\t\\nW\\nhg\\n\\tare\\tthe\\tweight\\tmatrices\\tof\\teach\\tof\\tthe\\tfour\\tlayers\\tfor\\ttheir\\tconnection\\tto\\tthe\\nprevious\\tshort-term\\tstate\\t\\nh\\n(\\nt\\n–1)\\n.\\nb\\ni\\n,\\t\\nb\\nf\\n,\\t\\nb\\no\\n,\\tand\\t\\nb\\ng\\n\\tare\\tthe\\tbias\\tterms\\tfor\\teach\\tof\\tthe\\tfour\\tlayers.\\tNote\\tthat\\tTensorFlow\\tinitializes\\t\\nb\\nf\\n\\tto\\na\\tvector\\tfull\\tof\\t1s\\tinstead\\tof\\t0s.\\tThis\\tprevents\\tforgetting\\teverything\\tat\\tthe\\tbeginning\\tof\\ttraining.', 'Peephole\\tConnections\\nIn\\ta\\tbasic\\tLSTM\\tcell,\\t\\nthe\\tgate\\tcontrollers\\tcan\\tlook\\tonly\\tat\\tthe\\tinput\\t\\nx\\n(\\nt\\n)\\n\\tand\\tthe\\tprevious\\tshort-term\\tstate\\nh\\n(\\nt\\n–1)\\n.\\tIt\\tmay\\tbe\\ta\\tgood\\tidea\\tto\\tgive\\tthem\\ta\\tbit\\tmore\\tcontext\\tby\\tletting\\tthem\\tpeek\\tat\\tthe\\tlong-term\\tstate\\tas\\nwell.\\tThis\\tidea\\twas\\t\\nproposed\\tby\\tFelix\\tGers\\tand\\tJürgen\\tSchmidhuber\\tin\\t2000\\n.\\n6\\n\\tThey\\tproposed\\tan\\tLSTM\\nvariant\\twith\\textra\\tconnections\\tcalled\\t\\npeephole\\tconnections\\n:\\tthe\\tprevious\\tlong-term\\tstate\\t\\nc\\n(\\nt\\n–1)\\n\\tis\\tadded\\nas\\tan\\tinput\\tto\\tthe\\tcontrollers\\tof\\tthe\\tforget\\tgate\\tand\\tthe\\tinput\\tgate,\\tand\\tthe\\tcurrent\\tlong-term\\tstate\\t\\nc\\n(\\nt\\n)\\n\\tis\\nadded\\tas\\tinput\\tto\\tthe\\tcontroller\\tof\\tthe\\toutput\\tgate.\\nTo\\timplement\\tpeephole\\tconnections\\tin\\tTensorFlow,\\tyou\\tmust\\tuse\\tthe\\t\\nLSTMCell\\n\\tinstead\\tof\\tthe\\nBasicLSTMCell\\n\\tand\\t\\nset\\t\\nuse_peepholes=True\\n:\\nlstm_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nLSTMCell\\n(\\nnum_units\\n=\\nn_neurons\\n,\\n\\t\\nuse_peepholes\\n=\\nTrue\\n)\\nThere\\tare\\tmany\\tother\\tvariants\\tof\\tthe\\tLSTM\\tcell.\\tOne\\tparticularly\\tpopular\\tvariant\\tis\\tthe\\tGRU\\tcell,\\twhich\\nwe\\twill\\tlook\\tat\\tnow.', 'GRU\\tCell\\nThe\\t\\nGated\\tRecurrent\\tUnit\\n\\t(GRU)\\t\\ncell\\t(see\\t\\nFigure\\t14-14\\n)\\twas\\tproposed\\tby\\tKyunghyun\\tCho\\tet\\tal.\\tin\\ta\\n2014\\tpaper\\n7\\n\\tthat\\talso\\tintroduced\\tthe\\tEncoder–Decoder\\tnetwork\\twe\\tmentioned\\tearlier.\\nFigure\\t14-14.\\t\\nGRU\\tcell\\nThe\\tGRU\\tcell\\tis\\ta\\tsimplified\\tversion\\tof\\tthe\\tLSTM\\tcell,\\tand\\tit\\tseems\\tto\\tperform\\tjust\\tas\\twell\\n8\\n\\t(which\\nexplains\\tits\\tgrowing\\tpopularity).\\tThe\\tmain\\tsimplifications\\tare:\\nBoth\\tstate\\tvectors\\tare\\tmerged\\tinto\\ta\\tsingle\\tvector\\t\\nh\\n(\\nt\\n)\\n.\\nA\\tsingle\\tgate\\tcontroller\\tcontrols\\tboth\\tthe\\tforget\\tgate\\tand\\tthe\\tinput\\tgate.\\tIf\\tthe\\tgate\\tcontroller\\toutputs\\ta\\n1,\\tthe\\tinput\\tgate\\tis\\topen\\tand\\tthe\\tforget\\tgate\\tis\\tclosed.\\tIf\\tit\\toutputs\\ta\\t0,\\tthe\\topposite\\thappens.\\tIn\\tother\\nwords,\\twhenever\\ta\\tmemory\\tmust\\tbe\\tstored,\\tthe\\tlocation\\twhere\\tit\\twill\\tbe\\tstored\\tis\\terased\\tfirst.\\tThis\\nis\\tactually\\ta\\tfrequent\\tvariant\\tto\\tthe\\tLSTM\\tcell\\tin\\tand\\tof\\titself.\\nThere\\tis\\tno\\toutput\\tgate;\\tthe\\tfull\\tstate\\tvector\\tis\\toutput\\tat\\tevery\\ttime\\tstep.\\tHowever,\\tthere\\tis\\ta\\tnew\\tgate', 'controller\\tthat\\tcontrols\\twhich\\tpart\\tof\\tthe\\tprevious\\tstate\\twill\\tbe\\tshown\\tto\\tthe\\tmain\\tlayer.\\nEquation\\t14-4\\n\\tsummarizes\\thow\\tto\\tcompute\\tthe\\tcell’s\\tstate\\tat\\teach\\ttime\\tstep\\tfor\\ta\\tsingle\\tinstance.\\nEquation\\t14-4.\\t\\nGRU\\tcomputations', 'Creating\\ta\\tGRU\\tcell\\tin\\tTensorFlow\\tis\\ttrivial:\\ngru_cell\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nrnn\\n.\\nGRUCell\\n(\\nnum_units\\n=\\nn_neurons\\n)\\nLSTM\\tor\\tGRU\\tcells\\t\\nare\\tone\\tof\\tthe\\tmain\\treasons\\tbehind\\tthe\\tsuccess\\tof\\tRNNs\\tin\\trecent\\tyears,\\tin\\tparticular\\nfor\\tapplications\\tin\\t\\nnatural\\tlanguage\\tprocessing\\n\\t(NLP).', 'Natural\\tLanguage\\tProcessing\\nMost\\t\\nof\\tthe\\tstate-of-the-art\\tNLP\\tapplications,\\tsuch\\tas\\tmachine\\ttranslation,\\tautomatic\\tsummarization,\\nparsing,\\tsentiment\\tanalysis,\\tand\\tmore,\\tare\\tnow\\tbased\\t(at\\tleast\\tin\\tpart)\\ton\\tRNNs.\\tIn\\tthis\\tlast\\tsection,\\twe\\nwill\\ttake\\ta\\tquick\\tlook\\tat\\twhat\\ta\\tmachine\\ttranslation\\tmodel\\tlooks\\tlike.\\tThis\\ttopic\\tis\\tvery\\twell\\tcovered\\tby\\nTensorFlow’s\\tawesome\\t\\nWord2Vec\\n\\tand\\t\\nSeq2Seq\\n\\ttutorials,\\tso\\tyou\\tshould\\tdefinitely\\tcheck\\tthem\\tout.', 'Word\\tEmbeddings\\nBefore\\t\\nwe\\tstart,\\twe\\tneed\\tto\\tchoose\\ta\\tword\\trepresentation.\\tOne\\toption\\tcould\\tbe\\tto\\trepresent\\teach\\tword\\nusing\\ta\\tone-hot\\tvector.\\tSuppose\\tyour\\tvocabulary\\tcontains\\t50,000\\twords,\\tthen\\tthe\\tn\\nth\\n\\tword\\twould\\tbe\\nrepresented\\tas\\ta\\t50,000-dimensional\\tvector,\\tfull\\tof\\t0s\\texcept\\tfor\\ta\\t1\\tat\\tthe\\tn\\nth\\n\\tposition.\\tHowever,\\twith\\nsuch\\ta\\tlarge\\tvocabulary,\\tthis\\tsparse\\trepresentation\\twould\\tnot\\tbe\\tefficient\\tat\\tall.\\tIdeally,\\tyou\\twant\\tsimilar\\nwords\\tto\\thave\\tsimilar\\trepresentations,\\tmaking\\tit\\teasy\\tfor\\tthe\\tmodel\\tto\\tgeneralize\\twhat\\tit\\tlearns\\tabout\\ta\\nword\\tto\\tall\\tsimilar\\twords.\\tFor\\texample,\\tif\\tthe\\tmodel\\tis\\ttold\\tthat\\t“I\\tdrink\\tmilk”\\tis\\ta\\tvalid\\tsentence,\\tand\\tif\\nit\\tknows\\tthat\\t“milk”\\tis\\tclose\\tto\\t“water”\\tbut\\tfar\\tfrom\\t“shoes,”\\tthen\\tit\\twill\\tknow\\tthat\\t“I\\tdrink\\twater”\\tis\\nprobably\\ta\\tvalid\\tsentence\\tas\\twell,\\twhile\\t“I\\tdrink\\tshoes”\\tis\\tprobably\\tnot.\\tBut\\thow\\tcan\\tyou\\tcome\\tup\\twith\\nsuch\\ta\\tmeaningful\\trepresentation?\\nThe\\tmost\\tcommon\\tsolution\\tis\\tto\\trepresent\\teach\\tword\\tin\\tthe\\tvocabulary\\tusing\\ta\\tfairly\\tsmall\\tand\\tdense', 'vector\\t(e.g.,\\t150\\tdimensions),\\tcalled\\tan\\t\\nembedding\\n,\\tand\\tjust\\tlet\\tthe\\tneural\\tnetwork\\tlearn\\ta\\tgood\\nembedding\\tfor\\teach\\tword\\tduring\\ttraining.\\tAt\\tthe\\tbeginning\\tof\\ttraining,\\tembeddings\\tare\\tsimply\\tchosen\\nrandomly,\\tbut\\tduring\\ttraining,\\tbackpropagation\\tautomatically\\tmoves\\tthe\\tembeddings\\taround\\tin\\ta\\tway\\tthat\\nhelps\\tthe\\tneural\\tnetwork\\tperform\\tits\\ttask.\\tTypically\\tthis\\tmeans\\tthat\\tsimilar\\twords\\twill\\tgradually\\tcluster\\nclose\\tto\\tone\\tanother,\\tand\\teven\\tend\\tup\\torganized\\tin\\ta\\trather\\tmeaningful\\tway.\\tFor\\texample,\\tembeddings\\nmay\\tend\\tup\\tplaced\\talong\\tvarious\\taxes\\tthat\\trepresent\\tgender,\\tsingular/plural,\\tadjective/noun,\\tand\\tso\\ton.\\nThe\\tresult\\tcan\\tbe\\ttruly\\tamazing.\\n9\\nIn\\tTensorFlow,\\tyou\\tfirst\\tneed\\tto\\tcreate\\tthe\\tvariable\\trepresenting\\tthe\\tembeddings\\tfor\\tevery\\tword\\tin\\tyour\\nvocabulary\\t(initialized\\trandomly):\\nvocabulary_size\\n\\t\\n=\\n\\t\\n50000\\nembedding_size\\n\\t\\n=\\n\\t\\n150\\ninit_embeds\\n\\t\\n=\\n\\t\\ntf\\n.\\nrandom_uniform\\n([\\nvocabulary_size\\n,\\n\\t\\nembedding_size\\n],\\n\\t\\n-\\n1.0\\n,\\n\\t\\n1.0\\n)\\nembeddings\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ninit_embeds\\n)', ')\\nNow\\tsuppose\\tyou\\twant\\tto\\tfeed\\tthe\\tsentence\\t“I\\tdrink\\tmilk”\\tto\\tyour\\tneural\\tnetwork.\\tYou\\tshould\\tfirst\\npreprocess\\tthe\\tsentence\\tand\\tbreak\\tit\\tinto\\ta\\tlist\\tof\\tknown\\twords.\\tFor\\texample\\tyou\\tmay\\tremove\\nunnecessary\\tcharacters,\\treplace\\tunknown\\twords\\tby\\ta\\tpredefined\\ttoken\\tword\\tsuch\\tas\\t“[UNK]”,\\treplace\\nnumerical\\tvalues\\tby\\t“[NUM]”,\\treplace\\tURLs\\tby\\t“[URL]”,\\tand\\tso\\ton.\\tOnce\\tyou\\thave\\ta\\tlist\\tof\\tknown\\nwords,\\tyou\\tcan\\tlook\\tup\\teach\\tword’s\\tinteger\\tidentifier\\t(from\\t0\\tto\\t49999)\\tin\\ta\\tdictionary,\\tfor\\texample\\t[72,\\n3335,\\t288].\\tAt\\tthat\\tpoint,\\tyou\\tare\\tready\\tto\\tfeed\\tthese\\tword\\tidentifiers\\tto\\tTensorFlow\\tusing\\ta\\tplaceholder,\\nand\\tapply\\tthe\\t\\nembedding_lookup()\\n\\t\\nfunction\\tto\\tget\\tthe\\tcorresponding\\tembeddings:\\ntrain_inputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n])\\n\\t\\t\\n#\\tfrom\\tids...\\nembed\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nembedding_lookup\\n(\\nembeddings\\n,\\n\\t\\ntrain_inputs\\n)\\n\\t\\t\\n#\\t...to\\tembeddings\\nOnce\\tyour\\tmodel\\thas\\tlearned\\tgood\\tword\\tembeddings,\\tthey\\tcan\\tactually\\tbe\\treused\\tfairly\\tefficiently\\tin\\tany', 'NLP\\tapplication:\\tafter\\tall,\\t“milk”\\tis\\tstill\\tclose\\tto\\t“water”\\tand\\tfar\\tfrom\\t“shoes”\\tno\\tmatter\\twhat\\tyour\\napplication\\tis.\\tIn\\tfact,\\tinstead\\tof\\ttraining\\tyour\\town\\tword\\tembeddings,\\tyou\\tmay\\twant\\tto\\tdownload\\npretrained\\tword\\tembeddings.\\tJust\\tlike\\twhen\\treusing\\tpretrained\\tlayers\\t(see\\t\\nChapter\\t11\\n),\\tyou\\tcan\\tchoose\\tto\\nfreeze\\tthe\\tpretrained\\tembeddings\\t(e.g.,\\tcreating\\tthe\\t\\nembeddings\\n\\tvariable\\tusing\\t\\ntrainable=False\\n)\\tor\\tlet', 'backpropagation\\ttweak\\tthem\\tfor\\tyour\\tapplication.\\tThe\\tfirst\\toption\\twill\\tspeed\\tup\\ttraining,\\tbut\\tthe\\tsecond\\nmay\\tlead\\tto\\tslightly\\thigher\\tperformance.\\nTIP\\nEmbeddings\\tare\\talso\\tuseful\\tfor\\trepresenting\\tcategorical\\tattributes\\tthat\\tcan\\ttake\\ton\\ta\\tlarge\\tnumber\\tof\\tdifferent\\tvalues,\\tespecially\\nwhen\\tthere\\tare\\tcomplex\\tsimilarities\\tbetween\\tvalues.\\tFor\\texample,\\tconsider\\tprofessions,\\thobbies,\\tdishes,\\tspecies,\\tbrands,\\tand\\tso\\non.\\nYou\\tnow\\thave\\talmost\\tall\\tthe\\ttools\\tyou\\tneed\\tto\\timplement\\ta\\tmachine\\ttranslation\\tsystem.\\t\\nLet’s\\tlook\\tat\\tthis\\nnow.', 'An\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nLet’s\\t\\ntake\\ta\\tlook\\tat\\ta\\t\\nsimple\\tmachine\\ttranslation\\tmodel\\n10\\n\\tthat\\twill\\ttranslate\\tEnglish\\tsentences\\tto\\tFrench\\n(see\\t\\nFigure\\t14-15\\n).\\nFigure\\t14-15.\\t\\nA\\tsimple\\tmachine\\ttranslation\\tmodel\\nThe\\tEnglish\\tsentences\\tare\\tfed\\tto\\tthe\\tencoder,\\tand\\tthe\\tdecoder\\toutputs\\tthe\\tFrench\\ttranslations.\\tNote\\tthat\\nthe\\tFrench\\ttranslations\\tare\\talso\\tused\\tas\\tinputs\\tto\\tthe\\tdecoder,\\tbut\\tpushed\\tback\\tby\\tone\\tstep.\\tIn\\tother\\nwords,\\tthe\\tdecoder\\tis\\tgiven\\tas\\tinput\\tthe\\tword\\tthat\\tit\\t\\nshould\\n\\thave\\toutput\\tat\\tthe\\tprevious\\tstep\\t(regardless\\tof\\nwhat\\tit\\tactually\\toutput).\\tFor\\tthe\\tvery\\tfirst\\tword,\\tit\\tis\\tgiven\\ta\\ttoken\\tthat\\trepresents\\tthe\\tbeginning\\tof\\tthe\\nsentence\\t(e.g.,\\t“<go>”).\\tThe\\tdecoder\\tis\\texpected\\tto\\tend\\tthe\\tsentence\\twith\\tan\\tend-of-sequence\\t(EOS)\\ntoken\\t(e.g.,\\t“<eos>”).\\nNote\\tthat\\tthe\\tEnglish\\tsentences\\tare\\treversed\\tbefore\\tthey\\tare\\tfed\\tto\\tthe\\tencoder.\\tFor\\texample\\t“I\\tdrink\\nmilk”\\tis\\treversed\\tto\\t“milk\\tdrink\\tI.”\\tThis\\tensures\\tthat\\tthe\\tbeginning\\tof\\tthe\\tEnglish\\tsentence\\twill\\tbe\\tfed\\tlast', 'to\\tthe\\tencoder,\\twhich\\tis\\tuseful\\tbecause\\tthat’s\\tgenerally\\tthe\\tfirst\\tthing\\tthat\\tthe\\tdecoder\\tneeds\\tto\\ttranslate.\\nEach\\tword\\tis\\tinitially\\trepresented\\tby\\ta\\tsimple\\tinteger\\tidentifier\\t(e.g.,\\t288\\tfor\\tthe\\tword\\t“milk”).\\tNext,\\tan\\nembedding\\tlookup\\treturns\\tthe\\tword\\tembedding\\t(as\\texplained\\tearlier,\\tthis\\tis\\ta\\tdense,\\tfairly\\tlow-\\ndimensional\\tvector).\\tThese\\tword\\tembeddings\\tare\\twhat\\tis\\tactually\\tfed\\tto\\tthe\\tencoder\\tand\\tthe\\tdecoder.\\nAt\\teach\\tstep,\\tthe\\tdecoder\\toutputs\\ta\\tscore\\tfor\\teach\\tword\\tin\\tthe\\toutput\\tvocabulary\\t(i.e.,\\tFrench),\\tand\\tthen\\nthe\\tSoftmax\\tlayer\\tturns\\tthese\\tscores\\tinto\\tprobabilities.\\tFor\\texample,\\tat\\tthe\\tfirst\\tstep\\tthe\\tword\\t“Je”\\tmay\\nhave\\ta\\tprobability\\tof\\t20%,\\t“Tu”\\tmay\\thave\\ta\\tprobability\\tof\\t1%,\\tand\\tso\\ton.\\tThe\\tword\\twith\\tthe\\thighest\\nprobability\\tis\\toutput.\\tThis\\tis\\tvery\\tmuch\\tlike\\ta\\tregular\\tclassification\\ttask,\\tso\\tyou\\tcan\\ttrain\\tthe\\tmodel\\tusing\\nthe\\t\\nsoftmax_cross_entropy_with_logits()\\n\\tfunction.\\nNote\\tthat\\tat\\t\\ninference\\ttime\\t(after\\ttraining),\\tyou\\twill\\tnot\\thave\\tthe\\ttarget\\tsentence\\tto\\tfeed\\tto\\tthe\\tdecoder.', 'Instead,\\tsimply\\tfeed\\tthe\\tdecoder\\tthe\\tword\\tthat\\tit\\toutput\\tat\\tthe\\tprevious\\tstep,\\tas\\tshown\\tin\\t\\nFigure\\t14-16\\n(this\\twill\\trequire\\tan\\tembedding\\tlookup\\tthat\\tis\\tnot\\tshown\\ton\\tthe\\tdiagram).\\nFigure\\t14-16.\\t\\nFeeding\\tthe\\tprevious\\toutput\\tword\\tas\\tinput\\tat\\tinference\\ttime\\nOkay,\\tnow\\tyou\\thave\\tthe\\tbig\\tpicture.\\tHowever,\\tif\\tyou\\tgo\\tthrough\\tTensorFlow’s\\tsequence-to-sequence\\ntutorial\\tand\\tyou\\tlook\\tat\\tthe\\tcode\\tin\\t\\nrnn/translate/seq2seq_model.py\\n\\t(in\\tthe\\t\\nTensorFlow\\tmodels\\n),\\tyou\\nwill\\tnotice\\ta\\tfew\\timportant\\tdifferences:\\nFirst,\\tso\\tfar\\twe\\thave\\tassumed\\tthat\\tall\\tinput\\tsequences\\t(to\\tthe\\tencoder\\tand\\tto\\tthe\\tdecoder)\\t\\nhave\\ta\\nconstant\\tlength.\\tBut\\tobviously\\tsentence\\tlengths\\tmay\\tvary.\\tThere\\tare\\tseveral\\tways\\tthat\\tthis\\tcan\\tbe\\nhandled\\t—\\tfor\\texample,\\tusing\\tthe\\t\\nsequence_length\\n\\t\\nargument\\tto\\tthe\\t\\nstatic_rnn()\\n\\tor\\ndynamic_rnn()\\n\\t\\nfunctions\\tto\\tspecify\\teach\\tsentence’s\\tlength\\t(as\\tdiscussed\\tearlier).\\tHowever,\\tanother\\napproach\\tis\\tused\\tin\\tthe\\ttutorial\\t(presumably\\tfor\\tperformance\\treasons):\\tsentences\\tare\\tgrouped\\tinto', 'buckets\\tof\\tsimilar\\tlengths\\t(e.g.,\\ta\\tbucket\\tfor\\tthe\\t1-\\tto\\t6-word\\tsentences,\\tanother\\tfor\\tthe\\t7-\\tto\\t12-\\nword\\tsentences,\\tand\\tso\\ton\\n11\\n),\\tand\\tthe\\tshorter\\tsentences\\tare\\tpadded\\tusing\\ta\\tspecial\\tpadding\\ttoken\\n(e.g.,\\t“<pad>”).\\tFor\\texample\\t“I\\tdrink\\tmilk”\\tbecomes\\t“<pad>\\t<pad>\\t<pad>\\tmilk\\tdrink\\tI”,\\tand\\tits\\ntranslation\\tbecomes\\t“Je\\tbois\\tdu\\tlait\\t<eos>\\t<pad>”.\\tOf\\tcourse,\\twe\\twant\\tto\\tignore\\tany\\toutput\\tpast\\tthe\\nEOS\\ttoken.\\tFor\\tthis,\\tthe\\ttutorial’s\\timplementation\\tuses\\ta\\t\\ntarget_weights\\n\\t\\nvector.\\tFor\\texample,\\tfor\\nthe\\ttarget\\tsentence\\t“Je\\tbois\\tdu\\tlait\\t<eos>\\t<pad>”,\\tthe\\tweights\\twould\\tbe\\tset\\tto\\t\\n[1.0,\\t1.0,\\t1.0,\\n1.0,\\t1.0,\\t0.0]\\n\\t(notice\\tthe\\tweight\\t0.0\\tthat\\tcorresponds\\tto\\tthe\\tpadding\\ttoken\\tin\\tthe\\ttarget\\tsentence).\\nSimply\\tmultiplying\\tthe\\tlosses\\tby\\tthe\\ttarget\\tweights\\twill\\tzero\\tout\\tthe\\tlosses\\tthat\\tcorrespond\\tto\\twords\\npast\\tEOS\\ttokens.\\nSecond,\\twhen\\tthe\\toutput\\tvocabulary\\tis\\tlarge\\t(which\\tis\\tthe\\tcase\\there),\\toutputting\\ta\\tprobability\\tfor', 'each\\tand\\tevery\\tpossible\\tword\\twould\\tbe\\tterribly\\tslow.\\tIf\\tthe\\ttarget\\tvocabulary\\tcontains,\\tsay,\\t50,000\\nFrench\\twords,\\tthen\\tthe\\tdecoder\\twould\\toutput\\t50,000-dimensional\\tvectors,\\tand\\tthen\\tcomputing\\tthe\\nsoftmax\\tfunction\\tover\\tsuch\\ta\\tlarge\\tvector\\twould\\tbe\\tvery\\tcomputationally\\tintensive.\\tTo\\tavoid\\tthis,\\none\\tsolution\\tis\\tto\\tlet\\tthe\\tdecoder\\toutput\\tmuch\\tsmaller\\tvectors,\\tsuch\\tas\\t1,000-dimensional\\tvectors,\\nthen\\tuse\\ta\\tsampling\\ttechnique\\tto\\testimate\\tthe\\tloss\\twithout\\thaving\\tto\\tcompute\\tit\\tover\\tevery\\tsingle\\nword\\tin\\tthe\\ttarget\\tvocabulary.\\tThis\\t\\nSampled\\tSoftmax\\n\\t\\ntechnique\\twas\\t\\nintroduced\\tin\\t2015\\tby\\tSébastien\\nJean\\tet\\tal\\n.\\n12\\n\\tIn\\tTensorFlow\\tyou\\tcan\\tuse\\tthe\\t\\nsampled_softmax_loss()\\n\\tfunction.\\nThird,\\tthe\\ttutorial’s\\timplementation\\tuses\\tan\\t\\nattention\\tmechanism\\n\\t\\nthat\\tlets\\tthe\\tdecoder\\tpeek\\tinto\\tthe\\ninput\\tsequence.\\tAttention\\taugmented\\tRNNs\\tare\\tbeyond\\tthe\\tscope\\tof\\tthis\\tbook,\\tbut\\tif\\tyou\\tare', 'interested\\tthere\\tare\\thelpful\\tpapers\\tabout\\t\\nmachine\\ttranslation\\n,\\n13\\n\\t\\nmachine\\treading\\n,\\n14\\n\\tand\\t\\nimage\\ncaptions\\n15\\n\\tusing\\tattention.\\nFinally,\\tthe\\ttutorial’s\\timplementation\\tmakes\\tuse\\tof\\tthe\\t\\ntf.nn.legacy_seq2seq\\n\\tmodule,\\twhich\\nprovides\\ttools\\tto\\tbuild\\tvarious\\tEncoder–Decoder\\tmodels\\teasily.\\tFor\\texample,\\tthe\\nembedding_rnn_seq2seq()\\n\\tfunction\\tcreates\\ta\\tsimple\\tEncoder–Decoder\\tmodel\\tthat\\tautomatically\\ntakes\\tcare\\tof\\tword\\tembeddings\\tfor\\tyou,\\tjust\\tlike\\tthe\\tone\\trepresented\\tin\\t\\nFigure\\t14-15\\n.\\tThis\\tcode\\twill\\nlikely\\tbe\\tupdated\\tquickly\\tto\\tuse\\tthe\\tnew\\t\\ntf.nn.seq2seq\\n\\tmodule.\\nYou\\tnow\\thave\\tall\\tthe\\ttools\\tyou\\tneed\\tto\\tunderstand\\tthe\\tsequence-to-sequence\\ttutorial’s\\timplementation.\\nCheck\\tit\\tout\\tand\\ttrain\\tyour\\town\\tEnglish-to-French\\ttranslator!', 'Exercises\\n1\\n.\\t\\nCan\\tyou\\tthink\\tof\\ta\\tfew\\tapplications\\tfor\\ta\\tsequence-to-sequence\\tRNN?\\tWhat\\tabout\\ta\\tsequence-to-\\nvector\\tRNN?\\tAnd\\ta\\tvector-to-sequence\\tRNN?\\n2\\n.\\t\\nWhy\\tdo\\tpeople\\tuse\\tencoder–decoder\\tRNNs\\trather\\tthan\\tplain\\tsequence-to-sequence\\tRNNs\\tfor\\nautomatic\\ttranslation?\\n3\\n.\\t\\nHow\\tcould\\tyou\\tcombine\\ta\\tconvolutional\\tneural\\tnetwork\\twith\\tan\\tRNN\\tto\\tclassify\\tvideos?\\n4\\n.\\t\\nWhat\\tare\\t\\nthe\\tadvantages\\tof\\tbuilding\\tan\\tRNN\\tusing\\t\\ndynamic_rnn()\\n\\trather\\tthan\\t\\nstatic_rnn()\\n?\\n5\\n.\\t\\nHow\\tcan\\tyou\\tdeal\\twith\\tvariable-length\\tinput\\tsequences?\\tWhat\\tabout\\tvariable-length\\toutput\\nsequences?\\n6\\n.\\t\\nWhat\\tis\\ta\\tcommon\\tway\\tto\\tdistribute\\ttraining\\tand\\texecution\\tof\\ta\\tdeep\\tRNN\\tacross\\tmultiple\\tGPUs?\\n7\\n.\\t\\nEmbedded\\tReber\\tgrammars\\n\\t\\nwere\\tused\\tby\\tHochreiter\\tand\\tSchmidhuber\\tin\\ttheir\\tpaper\\tabout\\tLSTMs.\\nThey\\tare\\tartificial\\tgrammars\\tthat\\tproduce\\tstrings\\tsuch\\tas\\t“BPBTSXXVPSEPE.”\\tCheck\\tout\\tJenny\\nOrr’s\\t\\nnice\\tintroduction\\n\\tto\\tthis\\ttopic.\\tChoose\\ta\\tparticular\\tembedded\\tReber\\tgrammar\\t(such\\tas\\tthe\\tone', 'represented\\ton\\tJenny\\tOrr’s\\tpage),\\tthen\\ttrain\\tan\\tRNN\\tto\\tidentify\\twhether\\ta\\tstring\\trespects\\tthat\\ngrammar\\tor\\tnot.\\tYou\\twill\\tfirst\\tneed\\tto\\twrite\\ta\\tfunction\\tcapable\\tof\\tgenerating\\ta\\ttraining\\tbatch\\ncontaining\\tabout\\t50%\\tstrings\\tthat\\trespect\\tthe\\tgrammar,\\tand\\t50%\\tthat\\tdon’t.\\n8\\n.\\t\\nTackle\\tthe\\t“How\\tmuch\\tdid\\tit\\train?\\tII”\\t\\nKaggle\\tcompetition\\n.\\tThis\\tis\\ta\\ttime\\tseries\\tprediction\\ttask:\\tyou\\nare\\tgiven\\tsnapshots\\tof\\tpolarimetric\\tradar\\tvalues\\tand\\tasked\\tto\\tpredict\\tthe\\thourly\\train\\tgauge\\ttotal.\\nLuis\\tAndre\\tDutra\\te\\tSilva’s\\t\\ninterview\\n\\tgives\\tsome\\tinteresting\\tinsights\\tinto\\tthe\\ttechniques\\the\\tused\\tto\\nreach\\tsecond\\tplace\\tin\\tthe\\tcompetition.\\tIn\\tparticular,\\the\\tused\\tan\\tRNN\\tcomposed\\tof\\ttwo\\tLSTM\\tlayers.\\n9\\n.\\t\\nGo\\tthrough\\tTensorFlow’s\\t\\nWord2Vec\\n\\ttutorial\\tto\\tcreate\\tword\\tembeddings,\\tand\\tthen\\tgo\\tthrough\\tthe\\nSeq2Seq\\n\\ttutorial\\tto\\ttrain\\tan\\tEnglish-to-French\\ttranslation\\tsystem.\\nSolutions\\tto\\tthese\\texercises\\tare\\t\\navailable\\tin\\t\\nAppendix\\tA\\n.', '.\\nNote\\tthat\\tmany\\tresearchers\\tprefer\\tto\\tuse\\tthe\\thyperbolic\\ttangent\\t(tanh)\\tactivation\\tfunction\\tin\\tRNNs\\trather\\tthan\\tthe\\tReLU\\tactivation\\nfunction.\\tFor\\texample,\\ttake\\ta\\tlook\\tat\\tby\\tVu\\tPham\\tet\\tal.’s\\tpaper\\t\\n“Dropout\\tImproves\\tRecurrent\\tNeural\\tNetworks\\tfor\\tHandwriting\\nRecognition”\\n.\\t\\nHowever,\\tReLU-based\\tRNNs\\tare\\talso\\tpossible,\\tas\\tshown\\tin\\tQuoc\\tV.\\tLe\\tet\\tal.’s\\tpaper\\t\\n“A\\tSimple\\tWay\\tto\\tInitialize\\nRecurrent\\tNetworks\\tof\\tRectified\\tLinear\\tUnits”\\n.\\nThis\\tuses\\tthe\\t\\ndecorator\\n\\tdesign\\tpattern.\\n“Long\\tShort-Term\\tMemory,”\\tS.\\tHochreiter\\tand\\tJ.\\tSchmidhuber\\t(1997).\\n“Long\\tShort-Term\\tMemory\\tRecurrent\\tNeural\\tNetwork\\tArchitectures\\tfor\\tLarge\\tScale\\tAcoustic\\tModeling,”\\tH.\\tSak\\tet\\tal.\\t(2014).\\n“Recurrent\\tNeural\\tNetwork\\tRegularization,”\\tW.\\tZaremba\\tet\\tal.\\t(2015).\\n“Recurrent\\tNets\\tthat\\tTime\\tand\\tCount,”\\tF.\\tGers\\tand\\tJ.\\tSchmidhuber\\t(2000).\\n“Learning\\tPhrase\\tRepresentations\\tusing\\tRNN\\tEncoder–Decoder\\tfor\\tStatistical\\tMachine\\tTranslation,”\\tK.\\tCho\\tet\\tal.\\t(2014).\\nA\\t2015\\tpaper\\tby\\tKlaus\\tGreff\\tet\\tal.,\\t\\n“LSTM:\\tA\\tSearch\\tSpace\\tOdyssey,”', 'seems\\tto\\tshow\\tthat\\tall\\tLSTM\\tvariants\\tperform\\troughly\\tthe\\tsame.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9', 'For\\tmore\\tdetails,\\tcheck\\tout\\tChristopher\\tOlah’s\\t\\ngreat\\tpost\\n,\\tor\\tSebastian\\tRuder’s\\t\\nseries\\tof\\tposts\\n.\\n“Sequence\\tto\\tSequence\\tlearning\\twith\\tNeural\\tNetworks,”\\tI.\\tSutskever\\tet\\tal.\\t(2014).\\nThe\\tbucket\\tsizes\\tused\\tin\\tthe\\ttutorial\\tare\\tdifferent.\\n“On\\tUsing\\tVery\\tLarge\\tTarget\\tVocabulary\\tfor\\tNeural\\tMachine\\tTranslation,”\\tS.\\tJean\\tet\\tal.\\t(2015).\\n“Neural\\tMachine\\tTranslation\\tby\\tJointly\\tLearning\\tto\\tAlign\\tand\\tTranslate,”\\tD.\\tBahdanau\\tet\\tal.\\t(2014).\\n“Long\\tShort-Term\\tMemory-Networks\\tfor\\tMachine\\tReading,”\\tJ.\\tCheng\\t(2016).\\n“Show,\\tAttend\\tand\\tTell:\\tNeural\\tImage\\tCaption\\tGeneration\\twith\\tVisual\\tAttention,”\\tK.\\tXu\\tet\\tal.\\t(2015).\\n9\\n10\\n11\\n12\\n13\\n14\\n15', 'Chapter\\t15.\\t\\nAutoencoders\\nAutoencoders\\t\\nare\\tartificial\\tneural\\tnetworks\\tcapable\\tof\\tlearning\\tefficient\\trepresentations\\tof\\tthe\\tinput\\tdata,\\ncalled\\t\\ncodings\\n,\\t\\nwithout\\tany\\tsupervision\\t(i.e.,\\tthe\\ttraining\\tset\\tis\\tunlabeled).\\tThese\\tcodings\\ttypically\\thave\\na\\tmuch\\tlower\\tdimensionality\\tthan\\tthe\\tinput\\tdata,\\tmaking\\tautoencoders\\tuseful\\tfor\\t\\ndimensionality\\treduction\\n(see\\t\\nChapter\\t8\\n).\\tMore\\timportantly,\\tautoencoders\\tact\\tas\\tpowerful\\t\\nfeature\\tdetectors,\\tand\\tthey\\tcan\\tbe\\tused\\nfor\\tunsupervised\\tpretraining\\tof\\tdeep\\tneural\\tnetworks\\t(as\\twe\\tdiscussed\\tin\\t\\nChapter\\t11\\n).\\tLastly,\\tthey\\tare\\ncapable\\tof\\trandomly\\tgenerating\\tnew\\tdata\\tthat\\tlooks\\tvery\\tsimilar\\tto\\tthe\\ttraining\\tdata;\\tthis\\tis\\tcalled\\t\\na\\ngenerative\\tmodel\\n.\\tFor\\texample,\\tyou\\tcould\\ttrain\\tan\\tautoencoder\\ton\\tpictures\\tof\\tfaces,\\tand\\tit\\twould\\tthen\\tbe\\nable\\tto\\tgenerate\\tnew\\tfaces.\\nSurprisingly,\\tautoencoders\\twork\\tby\\tsimply\\tlearning\\tto\\tcopy\\ttheir\\tinputs\\tto\\ttheir\\toutputs.\\tThis\\tmay\\tsound', 'like\\ta\\ttrivial\\ttask,\\tbut\\twe\\twill\\tsee\\tthat\\tconstraining\\tthe\\tnetwork\\tin\\tvarious\\tways\\tcan\\tmake\\tit\\trather\\ndifficult.\\tFor\\texample,\\tyou\\tcan\\tlimit\\tthe\\tsize\\tof\\tthe\\tinternal\\trepresentation,\\tor\\tyou\\tcan\\tadd\\tnoise\\tto\\tthe\\ninputs\\tand\\ttrain\\tthe\\tnetwork\\tto\\trecover\\tthe\\toriginal\\tinputs.\\tThese\\tconstraints\\tprevent\\tthe\\tautoencoder\\tfrom\\ntrivially\\tcopying\\tthe\\tinputs\\tdirectly\\tto\\tthe\\toutputs,\\twhich\\tforces\\tit\\tto\\tlearn\\tefficient\\tways\\tof\\trepresenting\\nthe\\tdata.\\tIn\\tshort,\\tthe\\tcodings\\tare\\tbyproducts\\tof\\tthe\\tautoencoder’s\\tattempt\\tto\\tlearn\\tthe\\tidentity\\tfunction\\nunder\\tsome\\tconstraints.\\nIn\\tthis\\tchapter\\twe\\twill\\texplain\\tin\\tmore\\tdepth\\thow\\tautoencoders\\twork,\\twhat\\ttypes\\tof\\tconstraints\\tcan\\tbe\\nimposed,\\tand\\thow\\tto\\timplement\\tthem\\tusing\\tTensorFlow,\\twhether\\tit\\tis\\tfor\\tdimensionality\\treduction,\\nfeature\\textraction,\\tunsupervised\\tpretraining,\\tor\\tas\\tgenerative\\tmodels.', 'Efficient\\tData\\tRepresentations\\nWhich\\t\\nof\\tthe\\tfollowing\\tnumber\\tsequences\\tdo\\tyou\\tfind\\tthe\\teasiest\\tto\\tmemorize?\\n40,\\t27,\\t25,\\t36,\\t81,\\t57,\\t10,\\t73,\\t19,\\t68\\n50,\\t25,\\t76,\\t38,\\t19,\\t58,\\t29,\\t88,\\t44,\\t22,\\t11,\\t34,\\t17,\\t52,\\t26,\\t13,\\t40,\\t20\\nAt\\tfirst\\tglance,\\tit\\twould\\tseem\\tthat\\tthe\\tfirst\\tsequence\\tshould\\tbe\\teasier,\\tsince\\tit\\tis\\tmuch\\tshorter.\\tHowever,\\nif\\tyou\\tlook\\tcarefully\\tat\\tthe\\tsecond\\tsequence,\\tyou\\tmay\\tnotice\\tthat\\tit\\tfollows\\ttwo\\tsimple\\trules:\\teven\\nnumbers\\tare\\tfollowed\\tby\\ttheir\\thalf,\\tand\\todd\\tnumbers\\tare\\tfollowed\\tby\\ttheir\\ttriple\\tplus\\tone\\t(this\\tis\\ta\\nfamous\\tsequence\\tknown\\tas\\t\\nthe\\t\\nhailstone\\tsequence\\n).\\tOnce\\tyou\\tnotice\\tthis\\tpattern,\\tthe\\tsecond\\tsequence\\nbecomes\\tmuch\\teasier\\tto\\tmemorize\\tthan\\tthe\\tfirst\\tbecause\\tyou\\tonly\\tneed\\tto\\tmemorize\\tthe\\ttwo\\trules,\\tthe\\tfirst\\nnumber,\\tand\\tthe\\tlength\\tof\\tthe\\tsequence.\\tNote\\tthat\\tif\\tyou\\tcould\\tquickly\\tand\\teasily\\tmemorize\\tvery\\tlong\\nsequences,\\tyou\\twould\\tnot\\tcare\\tmuch\\tabout\\tthe\\texistence\\tof\\ta\\tpattern\\tin\\tthe\\tsecond\\tsequence.\\tYou\\twould', 'just\\tlearn\\tevery\\tnumber\\tby\\theart,\\tand\\tthat\\twould\\tbe\\tthat.\\tIt\\tis\\tthe\\tfact\\tthat\\tit\\tis\\thard\\tto\\tmemorize\\tlong\\nsequences\\tthat\\tmakes\\tit\\tuseful\\tto\\trecognize\\tpatterns,\\tand\\thopefully\\tthis\\tclarifies\\twhy\\tconstraining\\tan\\nautoencoder\\tduring\\ttraining\\tpushes\\tit\\tto\\tdiscover\\tand\\texploit\\tpatterns\\tin\\tthe\\tdata.\\nThe\\trelationship\\tbetween\\tmemory,\\tperception,\\tand\\tpattern\\tmatching\\twas\\t\\nfamously\\tstudied\\tby\\tWilliam\\nChase\\tand\\tHerbert\\tSimon\\tin\\tthe\\tearly\\t1970s\\n.\\n1\\n\\tThey\\tobserved\\tthat\\texpert\\tchess\\tplayers\\twere\\table\\tto\\nmemorize\\tthe\\tpositions\\tof\\tall\\tthe\\tpieces\\tin\\ta\\tgame\\tby\\tlooking\\tat\\tthe\\tboard\\tfor\\tjust\\t5\\tseconds,\\ta\\ttask\\tthat\\nmost\\tpeople\\twould\\tfind\\timpossible.\\tHowever,\\tthis\\twas\\tonly\\tthe\\tcase\\twhen\\tthe\\tpieces\\twere\\tplaced\\tin\\nrealistic\\tpositions\\t(from\\tactual\\tgames),\\tnot\\twhen\\tthe\\tpieces\\twere\\tplaced\\trandomly.\\tChess\\texperts\\tdon’t\\nhave\\ta\\tmuch\\tbetter\\tmemory\\tthan\\tyou\\tand\\tI,\\tthey\\tjust\\tsee\\tchess\\tpatterns\\tmore\\teasily\\tthanks\\tto\\ttheir\\nexperience\\twith\\tthe\\tgame.\\tNoticing\\tpatterns\\thelps\\tthem\\tstore\\tinformation\\tefficiently.', 'Just\\tlike\\tthe\\tchess\\tplayers\\tin\\tthis\\tmemory\\texperiment,\\tan\\tautoencoder\\tlooks\\tat\\tthe\\tinputs,\\tconverts\\tthem\\tto\\nan\\tefficient\\tinternal\\trepresentation,\\tand\\tthen\\tspits\\tout\\tsomething\\tthat\\t(hopefully)\\tlooks\\tvery\\tclose\\tto\\tthe\\ninputs.\\tAn\\tautoencoder\\tis\\talways\\tcomposed\\tof\\ttwo\\tparts:\\tan\\t\\nencoder\\n\\t\\n(or\\t\\nrecognition\\tnetwork\\n)\\tthat\\nconverts\\tthe\\tinputs\\tto\\tan\\tinternal\\trepresentation,\\tfollowed\\tby\\ta\\t\\ndecoder\\n\\t\\n(or\\t\\ngenerative\\tnetwork\\n)\\tthat\\nconverts\\tthe\\tinternal\\trepresentation\\tto\\tthe\\toutputs\\t(see\\t\\nFigure\\t15-1\\n).\\nAs\\tyou\\tcan\\tsee,\\tan\\tautoencoder\\ttypically\\thas\\tthe\\tsame\\tarchitecture\\tas\\ta\\tMulti-Layer\\tPerceptron\\t(MLP;\\nsee\\t\\nChapter\\t10\\n),\\texcept\\tthat\\tthe\\tnumber\\tof\\tneurons\\tin\\tthe\\toutput\\tlayer\\tmust\\tbe\\tequal\\tto\\tthe\\tnumber\\tof\\ninputs.\\tIn\\tthis\\texample,\\tthere\\tis\\tjust\\tone\\thidden\\tlayer\\tcomposed\\tof\\ttwo\\tneurons\\t(the\\tencoder),\\tand\\tone\\noutput\\tlayer\\tcomposed\\tof\\tthree\\tneurons\\t(the\\tdecoder).\\tThe\\toutputs\\tare\\toften\\tcalled\\tthe\\t\\nreconstructions\\nsince\\tthe\\tautoencoder\\ttries\\tto\\treconstruct\\tthe\\tinputs,\\tand\\tthe\\tcost\\tfunction\\tcontains\\ta', 'reconstruction\\tloss\\nthat\\tpenalizes\\tthe\\tmodel\\twhen\\tthe\\treconstructions\\tare\\tdifferent\\tfrom\\tthe\\tinputs.', 'Figure\\t15-1.\\t\\nThe\\tchess\\tmemory\\texperiment\\t(left)\\tand\\ta\\tsimple\\tautoencoder\\t(right)\\nBecause\\tthe\\tinternal\\trepresentation\\thas\\ta\\tlower\\tdimensionality\\tthan\\tthe\\tinput\\tdata\\t(it\\tis\\t2D\\tinstead\\tof\\t3D),\\nthe\\tautoencoder\\tis\\tsaid\\tto\\tbe\\t\\nundercomplete\\n.\\t\\nAn\\tundercomplete\\tautoencoder\\tcannot\\ttrivially\\tcopy\\tits\\ninputs\\tto\\tthe\\tcodings,\\tyet\\tit\\tmust\\tfind\\ta\\tway\\tto\\toutput\\ta\\tcopy\\tof\\tits\\tinputs.\\tIt\\tis\\tforced\\tto\\tlearn\\tthe\\tmost\\nimportant\\tfeatures\\tin\\tthe\\tinput\\tdata\\t(and\\tdrop\\tthe\\tunimportant\\tones).\\nLet’s\\tsee\\thow\\tto\\timplement\\ta\\tvery\\tsimple\\tundercomplete\\tautoencoder\\tfor\\tdimensionality\\treduction.', 'Performing\\tPCA\\twith\\tan\\tUndercomplete\\tLinear\\tAutoencoder\\nIf\\t\\nthe\\tautoencoder\\tuses\\tonly\\tlinear\\tactivations\\tand\\tthe\\tcost\\tfunction\\tis\\tthe\\tMean\\tSquared\\tError\\t(MSE),\\nthen\\tit\\tcan\\tbe\\tshown\\tthat\\tit\\tends\\tup\\tperforming\\tPrincipal\\tComponent\\tAnalysis\\t(see\\t\\nChapter\\t8\\n).\\nThe\\tfollowing\\tcode\\tbuilds\\ta\\tsimple\\tlinear\\tautoencoder\\tto\\tperform\\tPCA\\ton\\ta\\t3D\\tdataset,\\tprojecting\\tit\\tto\\n2D:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nn_inputs\\n\\t\\n=\\n\\t\\n3\\n\\t\\t\\n#\\t3D\\tinputs\\nn_hidden\\n\\t\\n=\\n\\t\\n2\\n\\t\\t\\n#\\t2D\\tcodings\\nn_outputs\\n\\t\\n=\\n\\t\\nn_inputs\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nhidden\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden\\n,\\n\\t\\nn_outputs\\n)\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\t\\n#\\tMSE\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nreconstruction_loss\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nThis\\tcode\\tis', 'really\\tnot\\tvery\\tdifferent\\tfrom\\tall\\tthe\\tMLPs\\twe\\tbuilt\\tin\\tpast\\tchapters.\\tThe\\ttwo\\tthings\\tto\\tnote\\nare:\\nThe\\tnumber\\tof\\toutputs\\tis\\tequal\\tto\\tthe\\tnumber\\tof\\tinputs.\\nTo\\tperform\\tsimple\\tPCA,\\twe\\tdo\\tnot\\tuse\\tany\\tactivation\\tfunction\\t(i.e.,\\tall\\tneurons\\tare\\tlinear)\\tand\\tthe\\ncost\\tfunction\\tis\\tthe\\tMSE.\\tWe\\twill\\tsee\\tmore\\tcomplex\\tautoencoders\\tshortly.\\nNow\\tlet’s\\tload\\tthe\\tdataset,\\ttrain\\tthe\\tmodel\\ton\\tthe\\ttraining\\tset,\\tand\\tuse\\tit\\tto\\tencode\\tthe\\ttest\\tset\\t(i.e.,\\tproject\\nit\\tto\\t2D):\\nX_train\\n,\\n\\t\\nX_test\\n\\t\\n=\\n\\t\\n[\\n...\\n]\\n\\t\\n#\\tload\\tthe\\tdataset\\nn_iterations\\n\\t\\n=\\n\\t\\n1000\\ncodings\\n\\t\\n=\\n\\t\\nhidden\\n\\t\\t\\n#\\tthe\\toutput\\tof\\tthe\\thidden\\tlayer\\tprovides\\tthe\\tcodings\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntraining_op\\n.\\nrun\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_train\\n})\\n\\t\\t\\n#\\tno\\tlabels\\t(unsupervised)\\n\\t\\t\\t\\t\\ncodings_val\\n\\t\\n=\\n\\t\\ncodings\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_test\\n})\\nFigure\\t15-2\\n\\tshows\\tthe\\toriginal\\t3D\\tdataset\\t(at\\tthe\\tleft)\\tand\\tthe\\toutput\\tof\\tthe\\tautoencoder’s\\thidden\\tlayer', '(i.e.,\\tthe\\tcoding\\tlayer,\\tat\\tthe\\tright).\\tAs\\tyou\\tcan\\tsee,\\tthe\\tautoencoder\\tfound\\tthe\\tbest\\t2D\\tplane\\tto\\tproject\\tthe\\ndata\\tonto,\\tpreserving\\tas\\tmuch\\tvariance\\tin\\tthe\\tdata\\tas\\tit\\tcould\\t(just\\tlike\\tPCA).', 'Figure\\t15-2.\\t\\nPCA\\tperformed\\tby\\tan\\tundercomplete\\tlinear\\tautoencoder', 'Stacked\\tAutoencoders\\nJust\\t\\nlike\\tother\\tneural\\tnetworks\\twe\\thave\\tdiscussed,\\tautoencoders\\tcan\\thave\\tmultiple\\thidden\\tlayers.\\tIn\\tthis\\ncase\\tthey\\tare\\tcalled\\t\\nstacked\\tautoencoders\\n\\t(or\\t\\ndeep\\tautoencoders\\n).\\t\\nAdding\\tmore\\tlayers\\thelps\\tthe\\nautoencoder\\tlearn\\tmore\\tcomplex\\tcodings.\\tHowever,\\tone\\tmust\\tbe\\tcareful\\tnot\\tto\\tmake\\tthe\\tautoencoder\\ttoo\\npowerful.\\tImagine\\tan\\tencoder\\tso\\tpowerful\\tthat\\tit\\tjust\\tlearns\\tto\\tmap\\teach\\tinput\\tto\\ta\\tsingle\\tarbitrary\\tnumber\\n(and\\tthe\\tdecoder\\tlearns\\tthe\\treverse\\tmapping).\\tObviously\\tsuch\\tan\\tautoencoder\\twill\\treconstruct\\tthe\\ttraining\\ndata\\tperfectly,\\tbut\\tit\\twill\\tnot\\thave\\tlearned\\tany\\tuseful\\tdata\\trepresentation\\tin\\tthe\\tprocess\\t(and\\tit\\tis\\tunlikely\\nto\\tgeneralize\\twell\\tto\\tnew\\tinstances).\\nThe\\tarchitecture\\tof\\ta\\tstacked\\tautoencoder\\tis\\ttypically\\tsymmetrical\\twith\\tregards\\tto\\tthe\\tcentral\\thidden\\tlayer\\n(the\\tcoding\\tlayer).\\tTo\\tput\\tit\\tsimply,\\tit\\tlooks\\tlike\\ta\\tsandwich.\\tFor\\texample,\\tan\\tautoencoder\\tfor\\tMNIST\\n(introduced\\tin\\t\\nChapter\\t3', 'Chapter\\t3\\n)\\tmay\\thave\\t784\\tinputs,\\tfollowed\\tby\\ta\\thidden\\tlayer\\twith\\t300\\tneurons,\\tthen\\ta\\ncentral\\thidden\\tlayer\\tof\\t150\\tneurons,\\tthen\\tanother\\thidden\\tlayer\\twith\\t300\\tneurons,\\tand\\tan\\toutput\\tlayer\\twith\\n784\\tneurons.\\tThis\\tstacked\\tautoencoder\\tis\\trepresented\\tin\\t\\nFigure\\t15-3\\n.\\nFigure\\t15-3.\\t\\nStacked\\tautoencoder', 'TensorFlow\\tImplementation\\nYou\\t\\ncan\\timplement\\ta\\tstacked\\tautoencoder\\tvery\\tmuch\\tlike\\ta\\tregular\\tdeep\\tMLP.\\tIn\\tparticular,\\tthe\\tsame\\ntechniques\\twe\\tused\\tin\\t\\nChapter\\t11\\n\\tfor\\ttraining\\tdeep\\tnets\\tcan\\tbe\\tapplied.\\tFor\\texample,\\tthe\\tfollowing\\tcode\\nbuilds\\ta\\tstacked\\tautoencoder\\tfor\\tMNIST,\\tusing\\tHe\\tinitialization,\\tthe\\tELU\\tactivation\\tfunction,\\tand\\tℓ\\n2\\nregularization.\\tThe\\tcode\\tshould\\tlook\\tvery\\tfamiliar,\\texcept\\tthat\\tthere\\tare\\tno\\tlabels\\t(no\\t\\ny\\n):\\nfrom\\n\\t\\nfunctools\\n\\t\\nimport\\n\\t\\npartial\\nn_inputs\\n\\t\\n=\\n\\t\\n28\\n\\t\\n*\\n\\t\\n28\\n\\t\\t\\n#\\tfor\\tMNIST\\nn_hidden1\\n\\t\\n=\\n\\t\\n300\\nn_hidden2\\n\\t\\n=\\n\\t\\n150\\n\\t\\t\\n#\\tcodings\\nn_hidden3\\n\\t\\n=\\n\\t\\nn_hidden1\\nn_outputs\\n\\t\\n=\\n\\t\\nn_inputs\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nl2_reg\\n\\t\\n=\\n\\t\\n0.0001\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nhe_init\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\nl2_regularizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nl2_regularizer\\n(\\nl2_reg\\n)\\nmy_dense_layer\\n\\t\\n=\\n\\t\\npartial\\n(\\ntf\\n.\\nlayers\\n.\\ndense\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nelu\\n,', '.\\nelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\nhe_init\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_regularizer\\n=\\nl2_regularizer\\n)\\nhidden1\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n)\\nhidden2\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n)\\n\\t\\t\\n#\\tcodings\\nhidden3\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden2\\n,\\n\\t\\nn_hidden3\\n)\\noutputs\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden3\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nactivation\\n=\\nNone\\n)\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\t\\n#\\tMSE\\nreg_losses\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nREGULARIZATION_LOSSES\\n)\\nloss\\n\\t\\n=\\n\\t\\ntf\\n.\\nadd_n\\n([\\nreconstruction_loss\\n]\\n\\t\\n+\\n\\t\\nreg_losses\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nYou\\tcan\\tthen\\ttrain\\t\\nthe\\tmodel\\tnormally.\\tNote\\tthat\\tthe\\tdigit\\tlabels\\t(\\ny_batch\\n)\\tare\\tunused:\\nn_epochs\\n\\t\\n=\\n\\t\\n5\\nbatch_size\\n\\t\\n=\\n\\t\\n150\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):', '):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nn_batches\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_batches\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n})', 'Tying\\tWeights\\nWhen\\t\\nan\\tautoencoder\\tis\\tneatly\\tsymmetrical,\\tlike\\tthe\\tone\\twe\\tjust\\tbuilt,\\ta\\tcommon\\ttechnique\\tis\\tto\\t\\ntie\\tthe\\nweights\\n\\tof\\t\\nthe\\tdecoder\\tlayers\\tto\\tthe\\tweights\\tof\\tthe\\tencoder\\tlayers.\\tThis\\thalves\\tthe\\tnumber\\tof\\tweights\\tin\\nthe\\tmodel,\\tspeeding\\tup\\ttraining\\tand\\tlimiting\\tthe\\trisk\\tof\\toverfitting.\\tSpecifically,\\tif\\tthe\\tautoencoder\\thas\\ta\\ntotal\\tof\\t\\nN\\n\\tlayers\\t(not\\tcounting\\tthe\\tinput\\tlayer),\\tand\\t\\nW\\nL\\n\\trepresents\\tthe\\tconnection\\tweights\\tof\\tthe\\t\\nL\\nth\\n\\tlayer\\n(e.g.,\\tlayer\\t1\\tis\\tthe\\tfirst\\thidden\\tlayer,\\tlayer\\t\\n\\tis\\tthe\\tcoding\\tlayer,\\tand\\tlayer\\t\\nN\\n\\tis\\tthe\\toutput\\tlayer),\\tthen\\tthe\\ndecoder\\tlayer\\tweights\\tcan\\tbe\\tdefined\\tsimply\\tas:\\t\\nW\\nN–L\\n+1\\n\\t=\\t\\nW\\nL\\nT\\n\\t(with\\t\\nL\\n\\t=\\t1,\\t2,\\t\\n).\\nUnfortunately,\\timplementing\\ttied\\tweights\\tin\\tTensorFlow\\tusing\\tthe\\t\\ndense()\\n\\t\\nfunction\\tis\\ta\\tbit\\tcumbersome;\\nit’s\\tactually\\teasier\\tto\\tjust\\tdefine\\tthe\\tlayers\\tmanually.\\tThe\\tcode\\tends\\tup\\tsignificantly\\tmore\\tverbose:\\nactivation\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nelu\\nregularizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nl2_regularizer\\n(\\nl2_reg\\n)\\ninitializer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.', 'contrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nweights1_init\\n\\t\\n=\\n\\t\\ninitializer\\n([\\nn_inputs\\n,\\n\\t\\nn_hidden1\\n])\\nweights2_init\\n\\t\\n=\\n\\t\\ninitializer\\n([\\nn_hidden1\\n,\\n\\t\\nn_hidden2\\n])\\nweights1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\nweights1_init\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"weights1\"\\n)\\nweights2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\nweights2_init\\n,\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n,\\n\\t\\nname\\n=\\n\"weights2\"\\n)\\nweights3\\n\\t\\n=\\n\\t\\ntf\\n.\\ntranspose\\n(\\nweights2\\n,\\n\\t\\nname\\n=\\n\"weights3\"\\n)\\n\\t\\t\\n#\\ttied\\tweights\\nweights4\\n\\t\\n=\\n\\t\\ntf\\n.\\ntranspose\\n(\\nweights1\\n,\\n\\t\\nname\\n=\\n\"weights4\"\\n)\\n\\t\\t\\n#\\ttied\\tweights\\nbiases1\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n(\\nn_hidden1\\n),\\n\\t\\nname\\n=\\n\"biases1\"\\n)\\nbiases2\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n(\\nn_hidden2\\n),\\n\\t\\nname\\n=\\n\"biases2\"\\n)\\nbiases3\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n(\\nn_hidden3\\n),\\n\\t\\nname\\n=\\n\"biases3\"\\n)\\nbiases4\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\ntf\\n.\\nzeros\\n(\\nn_outputs\\n),\\n\\t\\nname\\n=\\n\"biases4\"\\n)\\nhidden1\\n\\t\\n=\\n\\t\\nactivation\\n(\\ntf\\n.\\nmatmul\\n(\\nX\\n,\\n\\t\\nweights1\\n)\\n\\t\\n+\\n\\t\\nbiases1\\n)\\nhidden2\\n\\t\\n=', '=\\n\\t\\nactivation\\n(\\ntf\\n.\\nmatmul\\n(\\nhidden1\\n,\\n\\t\\nweights2\\n)\\n\\t\\n+\\n\\t\\nbiases2\\n)\\nhidden3\\n\\t\\n=\\n\\t\\nactivation\\n(\\ntf\\n.\\nmatmul\\n(\\nhidden2\\n,\\n\\t\\nweights3\\n)\\n\\t\\n+\\n\\t\\nbiases3\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nmatmul\\n(\\nhidden3\\n,\\n\\t\\nweights4\\n)\\n\\t\\n+\\n\\t\\nbiases4\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\nreg_loss\\n\\t\\n=\\n\\t\\nregularizer\\n(\\nweights1\\n)\\n\\t\\n+\\n\\t\\nregularizer\\n(\\nweights2\\n)\\nloss\\n\\t\\n=\\n\\t\\nreconstruction_loss\\n\\t\\n+\\n\\t\\nreg_loss\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nThis\\tcode\\tis\\tfairly\\tstraightforward,\\t\\nbut\\tthere\\tare\\ta\\tfew\\timportant\\tthings\\tto\\tnote:\\nFirst,\\t\\nweight3\\n\\tand\\t\\nweights4\\n\\tare\\tnot\\tvariables,\\tthey\\tare\\trespectively\\tthe\\ttranspose\\tof\\t\\nweights2\\n\\tand\\nweights1\\n\\t(they\\tare\\t“tied”\\tto\\tthem).\\nSecond,\\tsince\\tthey\\tare\\tnot\\tvariables,\\tit’s\\tno\\tuse\\tregularizing\\tthem:\\twe\\tonly\\tregularize\\t\\nweights1\\n\\tand\\nweights2\\n.\\nThird,\\tbiases\\tare\\tnever\\ttied,\\tand\\tnever\\t\\nregularized.', 'Training\\tOne\\tAutoencoder\\tat\\ta\\tTime\\nRather\\t\\nthan\\ttraining\\tthe\\twhole\\tstacked\\tautoencoder\\tin\\tone\\tgo\\tlike\\twe\\tjust\\tdid,\\tit\\tis\\toften\\tmuch\\tfaster\\tto\\ntrain\\tone\\tshallow\\tautoencoder\\tat\\ta\\ttime,\\tthen\\tstack\\tall\\tof\\tthem\\tinto\\ta\\tsingle\\tstacked\\tautoencoder\\t(hence\\nthe\\tname),\\tas\\tshown\\ton\\t\\nFigure\\t15-4\\n.\\tThis\\tis\\tespecially\\tuseful\\tfor\\tvery\\tdeep\\tautoencoders.\\nFigure\\t15-4.\\t\\nTraining\\tone\\tautoencoder\\tat\\ta\\ttime\\nDuring\\tthe\\tfirst\\tphase\\tof\\ttraining,\\tthe\\tfirst\\tautoencoder\\tlearns\\tto\\treconstruct\\tthe\\tinputs.\\tDuring\\tthe\\tsecond\\nphase,\\tthe\\tsecond\\tautoencoder\\tlearns\\tto\\treconstruct\\tthe\\toutput\\tof\\tthe\\tfirst\\tautoencoder’s\\thidden\\tlayer.\\nFinally,\\tyou\\tjust\\tbuild\\ta\\tbig\\tsandwich\\tusing\\tall\\tthese\\tautoencoders,\\tas\\tshown\\tin\\t\\nFigure\\t15-4\\n\\t(i.e.,\\tyou\\tfirst\\nstack\\tthe\\thidden\\tlayers\\tof\\teach\\tautoencoder,\\tthen\\tthe\\toutput\\tlayers\\tin\\treverse\\torder).\\tThis\\tgives\\tyou\\tthe\\nfinal\\tstacked\\tautoencoder.\\tYou\\tcould\\teasily\\ttrain\\tmore\\tautoencoders\\tthis\\tway,\\tbuilding\\ta\\tvery\\tdeep\\nstacked\\tautoencoder.', 'To\\timplement\\tthis\\tmultiphase\\ttraining\\talgorithm,\\tthe\\tsimplest\\tapproach\\tis\\tto\\tuse\\ta\\tdifferent\\tTensorFlow\\ngraph\\tfor\\teach\\tphase.\\tAfter\\ttraining\\tan\\tautoencoder,\\tyou\\tjust\\trun\\tthe\\ttraining\\tset\\tthrough\\tit\\tand\\tcapture\\tthe\\noutput\\tof\\tthe\\thidden\\tlayer.\\tThis\\toutput\\tthen\\tserves\\tas\\tthe\\ttraining\\tset\\tfor\\tthe\\tnext\\tautoencoder.\\tOnce\\tall\\nautoencoders\\thave\\tbeen\\ttrained\\tthis\\tway,\\tyou\\tsimply\\tcopy\\tthe\\tweights\\tand\\tbiases\\tfrom\\teach\\tautoencoder\\nand\\tuse\\tthem\\tto\\tbuild\\tthe\\tstacked\\tautoencoder.\\tImplementing\\tthis\\tapproach\\tis\\tquite\\tstraightforward,\\tso\\twe\\nwon’t\\tdetail\\tit\\there,\\tbut\\tplease\\tcheck\\tout\\tthe\\tcode\\tin\\tthe\\t\\nJupyter\\tnotebooks\\n\\tfor\\tan\\texample.\\nAnother\\tapproach\\tis\\tto\\tuse\\ta\\tsingle\\tgraph\\tcontaining\\tthe\\twhole\\tstacked\\tautoencoder,\\tplus\\tsome\\textra\\noperations\\tto\\tperform\\teach\\ttraining\\tphase,\\tas\\tshown\\tin\\t\\nFigure\\t15-5\\n.', 'Figure\\t15-5.\\t\\nA\\tsingle\\tgraph\\tto\\ttrain\\ta\\tstacked\\tautoencoder\\nThis\\tdeserves\\ta\\tbit\\tof\\texplanation:\\nThe\\tcentral\\tcolumn\\tin\\tthe\\tgraph\\tis\\tthe\\tfull\\tstacked\\tautoencoder.\\tThis\\tpart\\tcan\\tbe\\tused\\tafter\\ttraining.\\nThe\\tleft\\tcolumn\\tis\\tthe\\tset\\tof\\toperations\\tneeded\\tto\\trun\\tthe\\tfirst\\tphase\\tof\\ttraining.\\tIt\\tcreates\\tan\\toutput\\nlayer\\tthat\\tbypasses\\thidden\\tlayers\\t2\\tand\\t3.\\tThis\\toutput\\tlayer\\tshares\\tthe\\tsame\\tweights\\tand\\tbiases\\tas\\nthe\\tstacked\\tautoencoder’s\\toutput\\tlayer.\\tOn\\ttop\\tof\\tthat\\tare\\tthe\\ttraining\\toperations\\tthat\\twill\\taim\\tat\\nmaking\\tthe\\toutput\\tas\\tclose\\tas\\tpossible\\tto\\tthe\\tinputs.\\tThus,\\tthis\\tphase\\twill\\ttrain\\tthe\\tweights\\tand\\nbiases\\tfor\\tthe\\thidden\\tlayer\\t1\\tand\\tthe\\toutput\\tlayer\\t(i.e.,\\tthe\\tfirst\\tautoencoder).\\nThe\\tright\\tcolumn\\tin\\tthe\\tgraph\\tis\\tthe\\tset\\tof\\toperations\\tneeded\\tto\\trun\\tthe\\tsecond\\tphase\\tof\\ttraining.\\tIt\\nadds\\tthe\\ttraining\\toperation\\tthat\\twill\\taim\\tat\\tmaking\\tthe\\toutput\\tof\\thidden\\tlayer\\t3\\tas\\tclose\\tas\\tpossible\\nto\\tthe\\toutput\\tof\\thidden\\tlayer\\t1.\\tNote\\tthat\\twe\\tmust\\tfreeze\\thidden\\tlayer\\t1\\twhile\\trunning\\tphase\\t2.\\tThis', 'phase\\twill\\ttrain\\tthe\\tweights\\tand\\tbiases\\tfor\\thidden\\tlayers\\t2\\tand\\t3\\t(i.e.,\\tthe\\tsecond\\tautoencoder).\\nThe\\tTensorFlow\\tcode\\tlooks\\t\\nlike\\tthis:\\n[\\n...\\n]\\n\\t\\n#\\tBuild\\tthe\\twhole\\tstacked\\tautoencoder\\tnormally.\\n\\t\\t\\t\\t\\t\\t\\n#\\tIn\\tthis\\texample,\\tthe\\tweights\\tare\\tnot\\ttied.\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"phase1\"\\n):\\n\\t\\t\\t\\t\\nphase1_outputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nmatmul\\n(\\nhidden1\\n,\\n\\t\\nweights4\\n)\\n\\t\\n+\\n\\t\\nbiases4\\n\\t\\t\\t\\t\\nphase1_reconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\nphase1_outputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\t\\t\\t\\nphase1_reg_loss\\n\\t\\n=\\n\\t\\nregularizer\\n(\\nweights1\\n)\\n\\t\\n+\\n\\t\\nregularizer\\n(\\nweights4\\n)\\n\\t\\t\\t\\t\\nphase1_loss\\n\\t\\n=\\n\\t\\nphase1_reconstruction_loss\\n\\t\\n+\\n\\t\\nphase1_reg_loss\\n\\t\\t\\t\\t\\nphase1_training_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nphase1_loss\\n)\\nwith\\n\\t\\ntf\\n.\\nname_scope\\n(\\n\"phase2\"\\n):\\n\\t\\t\\t\\t\\nphase2_reconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\nhidden3\\n\\t\\n-\\n\\t\\nhidden1\\n))\\n\\t\\t\\t\\t\\nphase2_reg_loss\\n\\t\\n=\\n\\t\\nregularizer\\n(\\nweights2\\n)\\n\\t\\n+\\n\\t\\nregularizer\\n(\\nweights3\\n)\\n\\t\\t\\t\\t\\nphase2_loss\\n\\t\\n=', '=\\n\\t\\nphase2_reconstruction_loss\\n\\t\\n+\\n\\t\\nphase2_reg_loss\\n\\t\\t\\t\\t\\ntrain_vars\\n\\t\\n=\\n\\t\\n[\\nweights2\\n,\\n\\t\\nbiases2\\n,\\n\\t\\nweights3\\n,\\n\\t\\nbiases3\\n]\\n\\t\\t\\t\\t\\nphase2_training_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nphase2_loss\\n,\\n\\t\\nvar_list\\n=\\ntrain_vars\\n)\\nThe\\tfirst\\tphase\\tis\\trather\\tstraightforward:\\twe\\t\\njust\\tcreate\\tan\\toutput\\tlayer\\tthat\\tskips\\thidden\\tlayers\\t2\\tand\\t3,\\nthen\\tbuild\\tthe\\ttraining\\toperations\\tto\\tminimize\\tthe\\tdistance\\tbetween\\tthe\\toutputs\\tand\\tthe\\tinputs\\t(plus\\tsome', 'regularization).\\nThe\\tsecond\\tphase\\tjust\\tadds\\tthe\\toperations\\tneeded\\tto\\tminimize\\tthe\\tdistance\\tbetween\\tthe\\toutput\\tof\\thidden\\nlayer\\t3\\tand\\thidden\\tlayer\\t1\\t(also\\twith\\tsome\\tregularization).\\tMost\\timportantly,\\twe\\tprovide\\tthe\\tlist\\tof\\ntrainable\\tvariables\\tto\\tthe\\t\\nminimize()\\n\\tmethod,\\tmaking\\tsure\\tto\\tleave\\tout\\t\\nweights1\\n\\tand\\t\\nbiases1\\n;\\tthis\\neffectively\\tfreezes\\thidden\\tlayer\\t1\\tduring\\tphase\\t2.\\nDuring\\tthe\\texecution\\tphase,\\tall\\tyou\\tneed\\tto\\tdo\\tis\\trun\\tthe\\tphase\\t1\\ttraining\\top\\tfor\\ta\\tnumber\\tof\\tepochs,\\tthen\\nthe\\tphase\\t2\\ttraining\\top\\tfor\\tsome\\tmore\\tepochs.\\nTIP\\nSince\\thidden\\tlayer\\t1\\tis\\tfrozen\\tduring\\tphase\\t2,\\tits\\toutput\\twill\\talways\\tbe\\tthe\\tsame\\tfor\\tany\\tgiven\\ttraining\\tinstance.\\tTo\\tavoid\\thaving\\nto\\trecompute\\tthe\\toutput\\tof\\thidden\\tlayer\\t1\\tat\\tevery\\tsingle\\tepoch,\\tyou\\tcan\\tcompute\\tit\\tfor\\tthe\\twhole\\ttraining\\tset\\tat\\tthe\\tend\\tof\\tphase\\n1,\\tthen\\tdirectly\\tfeed\\tthe\\tcached\\toutput\\tof\\thidden\\tlayer\\t1\\tduring\\tphase\\t2.\\tThis\\tcan\\tgive\\tyou\\ta\\tnice\\tperformance\\t\\nboost.', 'Visualizing\\tthe\\tReconstructions\\nOne\\t\\nway\\tto\\tensure\\tthat\\tan\\tautoencoder\\tis\\tproperly\\ttrained\\tis\\tto\\tcompare\\tthe\\tinputs\\tand\\tthe\\toutputs.\\tThey\\nmust\\tbe\\tfairly\\tsimilar,\\tand\\tthe\\tdifferences\\tshould\\tbe\\tunimportant\\tdetails.\\tLet’s\\tplot\\ttwo\\trandom\\tdigits\\tand\\ntheir\\treconstructions:\\nn_test_digits\\n\\t\\n=\\n\\t\\n2\\nX_test\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntest\\n.\\nimages\\n[:\\nn_test_digits\\n]\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\tTrain\\tthe\\tAutoencoder\\n\\t\\t\\t\\t\\noutputs_val\\n\\t\\n=\\n\\t\\noutputs\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_test\\n})\\ndef\\n\\t\\nplot_image\\n(\\nimage\\n,\\n\\t\\nshape\\n=\\n[\\n28\\n,\\n\\t\\n28\\n]):\\n\\t\\t\\t\\t\\nplt\\n.\\nimshow\\n(\\nimage\\n.\\nreshape\\n(\\nshape\\n),\\n\\t\\ncmap\\n=\\n\"Greys\"\\n,\\n\\t\\ninterpolation\\n=\\n\"nearest\"\\n)\\n\\t\\t\\t\\t\\nplt\\n.\\naxis\\n(\\n\"off\"\\n)\\nfor\\n\\t\\ndigit_index\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_test_digits\\n):\\n\\t\\t\\t\\t\\nplt\\n.\\nsubplot\\n(\\nn_test_digits\\n,\\n\\t\\n2\\n,\\n\\t\\ndigit_index\\n\\t\\n*\\n\\t\\n2\\n\\t\\n+\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\nplot_image\\n(\\nX_test\\n[\\ndigit_index\\n])\\n\\t\\t\\t\\t\\nplt\\n.\\nsubplot\\n(\\nn_test_digits\\n,\\n\\t\\n2\\n,\\n\\t\\ndigit_index\\n\\t\\n*\\n\\t\\n2\\n\\t\\n+\\n\\t\\n2\\n)\\n\\t\\t\\t\\t\\nplot_image\\n(\\noutputs_val\\n[\\ndigit_index\\n])\\nFigure\\t15-6\\n\\tshows\\tthe\\tresulting\\timages.', 'Figure\\t15-6.\\t\\nOriginal\\tdigits\\t(left)\\tand\\ttheir\\treconstructions\\t(right)', 'Looks\\tclose\\tenough.\\tSo\\tthe\\tautoencoder\\thas\\tproperly\\tlearned\\tto\\treproduce\\tits\\tinputs,\\tbut\\thas\\tit\\tlearned\\nuseful\\tfeatures?\\t\\nLet’s\\ttake\\ta\\tlook.', 'Visualizing\\tFeatures\\nOnce\\t\\nyour\\tautoencoder\\thas\\tlearned\\tsome\\tfeatures,\\tyou\\tmay\\twant\\tto\\ttake\\ta\\tlook\\tat\\tthem.\\tThere\\tare\\tvarious\\ntechniques\\tfor\\tthis.\\tArguably\\tthe\\tsimplest\\ttechnique\\tis\\tto\\tconsider\\teach\\tneuron\\tin\\tevery\\thidden\\tlayer,\\tand\\nfind\\tthe\\ttraining\\tinstances\\tthat\\tactivate\\tit\\tthe\\tmost.\\tThis\\tis\\tespecially\\tuseful\\tfor\\tthe\\ttop\\thidden\\tlayers\\tsince\\nthey\\toften\\tcapture\\trelatively\\tlarge\\tfeatures\\tthat\\tyou\\tcan\\teasily\\tspot\\tin\\ta\\tgroup\\tof\\ttraining\\tinstances\\tthat\\ncontain\\tthem.\\tFor\\texample,\\tif\\ta\\tneuron\\tstrongly\\tactivates\\twhen\\tit\\tsees\\ta\\tcat\\tin\\ta\\tpicture,\\tit\\twill\\tbe\\tpretty\\nobvious\\tthat\\tthe\\tpictures\\tthat\\tactivate\\tit\\tthe\\tmost\\tall\\tcontain\\tcats.\\tHowever,\\tfor\\tlower\\tlayers,\\tthis\\ntechnique\\tdoes\\tnot\\twork\\tso\\twell,\\tas\\tthe\\tfeatures\\tare\\tsmaller\\tand\\tmore\\tabstract,\\tso\\tit’s\\toften\\thard\\tto\\nunderstand\\texactly\\twhat\\tthe\\tneuron\\tis\\tgetting\\tall\\texcited\\tabout.\\nLet’s\\tlook\\tat\\tanother\\ttechnique.\\tFor\\teach\\tneuron\\tin\\tthe\\tfirst\\thidden\\tlayer,\\tyou\\tcan\\tcreate\\tan\\timage\\twhere\\ta', 'pixel’s\\tintensity\\tcorresponds\\tto\\tthe\\tweight\\tof\\tthe\\tconnection\\tto\\tthe\\tgiven\\tneuron.\\tFor\\texample,\\tthe\\nfollowing\\tcode\\tplots\\tthe\\tfeatures\\tlearned\\tby\\tfive\\tneurons\\tin\\tthe\\tfirst\\thidden\\tlayer:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\n[\\n...\\n]\\n\\t\\n#\\ttrain\\tautoencoder\\n\\t\\t\\t\\t\\nweights1_val\\n\\t\\n=\\n\\t\\nweights1\\n.\\neval\\n()\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n5\\n):\\n\\t\\t\\t\\t\\nplt\\n.\\nsubplot\\n(\\n1\\n,\\n\\t\\n5\\n,\\n\\t\\ni\\n\\t\\n+\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\nplot_image\\n(\\nweights1_val\\n.\\nT\\n[\\ni\\n])\\nYou\\tmay\\tget\\tlow-level\\tfeatures\\tsuch\\tas\\tthe\\tones\\tshown\\tin\\t\\nFigure\\t15-7\\n.\\nFigure\\t15-7.\\t\\nFeatures\\tlearned\\tby\\tfive\\tneurons\\tfrom\\tthe\\tfirst\\thidden\\tlayer\\nThe\\tfirst\\tfour\\tfeatures\\tseem\\tto\\tcorrespond\\tto\\tsmall\\tpatches,\\twhile\\tthe\\tfifth\\tfeature\\tseems\\tto\\tlook\\tfor\\nvertical\\tstrokes\\t(note\\tthat\\tthese\\tfeatures\\tcome\\tfrom\\tthe\\t\\nstacked\\tdenoising\\tautoencoder\\tthat\\twe\\twill\\ndiscuss\\tlater).\\nAnother\\ttechnique\\tis\\tto\\tfeed\\tthe\\tautoencoder\\ta\\trandom\\tinput\\timage,\\tmeasure\\tthe\\tactivation\\tof\\tthe\\tneuron\\nyou\\tare\\tinterested\\tin,\\tand\\tthen\\tperform\\tbackpropagation\\t\\nto\\ttweak\\tthe\\timage\\tin\\tsuch\\ta\\tway\\tthat\\tthe\\tneuron', 'will\\tactivate\\teven\\tmore.\\tIf\\tyou\\titerate\\tseveral\\ttimes\\t(performing\\tgradient\\tascent),\\tthe\\timage\\twill\\ngradually\\tturn\\tinto\\tthe\\tmost\\texciting\\timage\\t(for\\tthe\\tneuron).\\tThis\\tis\\ta\\tuseful\\ttechnique\\tto\\tvisualize\\tthe\\nkinds\\tof\\tinputs\\tthat\\ta\\tneuron\\tis\\tlooking\\tfor.\\nFinally,\\tif\\tyou\\tare\\tusing\\tan\\tautoencoder\\tto\\tperform\\tunsupervised\\tpretraining\\t—\\tfor\\texample,\\tfor\\ta\\nclassification\\ttask\\t—\\ta\\tsimple\\tway\\tto\\tverify\\tthat\\tthe\\tfeatures\\tlearned\\tby\\tthe\\tautoencoder\\tare\\tuseful\\tis\\tto\\nmeasure\\tthe\\tperformance\\tof\\tthe\\t\\nclassifier.', 'Unsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\nAs\\t\\nwe\\tdiscussed\\tin\\t\\nChapter\\t11\\n,\\tif\\tyou\\tare\\ttackling\\ta\\tcomplex\\tsupervised\\ttask\\tbut\\tyou\\tdo\\tnot\\thave\\ta\\tlot\\tof\\nlabeled\\ttraining\\tdata,\\tone\\tsolution\\tis\\tto\\tfind\\ta\\tneural\\tnetwork\\tthat\\tperforms\\ta\\tsimilar\\ttask,\\tand\\tthen\\treuse\\nits\\tlower\\tlayers.\\tThis\\tmakes\\tit\\tpossible\\tto\\ttrain\\ta\\thigh-performance\\tmodel\\tusing\\tonly\\tlittle\\ttraining\\tdata\\nbecause\\tyour\\tneural\\tnetwork\\twon’t\\thave\\tto\\tlearn\\tall\\tthe\\tlow-level\\tfeatures;\\tit\\twill\\tjust\\treuse\\tthe\\tfeature\\ndetectors\\tlearned\\tby\\tthe\\texisting\\tnet.\\nSimilarly,\\tif\\tyou\\thave\\ta\\tlarge\\tdataset\\tbut\\tmost\\tof\\tit\\tis\\tunlabeled,\\tyou\\tcan\\tfirst\\ttrain\\ta\\tstacked\\tautoencoder\\nusing\\tall\\tthe\\tdata,\\tthen\\treuse\\tthe\\tlower\\tlayers\\tto\\tcreate\\ta\\tneural\\tnetwork\\tfor\\tyour\\tactual\\ttask,\\tand\\ttrain\\tit\\nusing\\tthe\\tlabeled\\tdata.\\tFor\\texample,\\t\\nFigure\\t15-8\\n\\tshows\\thow\\tto\\tuse\\ta\\tstacked\\tautoencoder\\tto\\tperform\\nunsupervised\\tpretraining\\tfor\\ta\\tclassification\\tneural\\tnetwork.\\tThe\\tstacked\\tautoencoder\\titself\\tis\\ttypically', 'trained\\tone\\tautoencoder\\tat\\ta\\ttime,\\tas\\tdiscussed\\tearlier.\\tWhen\\ttraining\\tthe\\tclassifier,\\tif\\tyou\\treally\\tdon’t\\nhave\\tmuch\\tlabeled\\ttraining\\tdata,\\tyou\\tmay\\twant\\tto\\tfreeze\\tthe\\tpretrained\\tlayers\\t(at\\tleast\\tthe\\tlower\\tones).\\nFigure\\t15-8.\\t\\nUnsupervised\\tpretraining\\tusing\\tautoencoders\\nNOTE\\nThis\\tsituation\\tis\\tactually\\tquite\\tcommon,\\tbecause\\tbuilding\\ta\\tlarge\\tunlabeled\\tdataset\\tis\\toften\\tcheap\\t(e.g.,\\ta\\tsimple\\tscript\\tcan\\ndownload\\tmillions\\tof\\timages\\toff\\tthe\\tinternet),\\tbut\\tlabeling\\tthem\\tcan\\tonly\\tbe\\tdone\\treliably\\tby\\thumans\\t(e.g.,\\tclassifying\\timages\\tas\\ncute\\tor\\tnot).\\tLabeling\\tinstances\\tis\\ttime-consuming\\tand\\tcostly,\\tso\\tit\\tis\\tquite\\tcommon\\tto\\thave\\tonly\\ta\\tfew\\tthousand\\tlabeled\\ninstances.\\nAs\\twe\\tdiscussed\\tearlier,\\tone\\tof\\tthe\\ttriggers\\tof\\tthe\\tcurrent\\tDeep\\tLearning\\ttsunami\\tis\\tthe\\tdiscovery\\tin\\t2006\\nby\\tGeoffrey\\tHinton\\tet\\tal.\\tthat\\tdeep\\tneural\\tnetworks\\tcan\\tbe\\tpretrained\\tin\\tan\\tunsupervised\\tfashion.\\tThey\\nused\\trestricted\\tBoltzmann\\tmachines\\tfor\\tthat\\t(see\\t\\nAppendix\\tE\\n),\\tbut\\tin\\t\\n2007\\tYoshua\\tBengio\\tet\\tal.\\tshowed\\n2', '2\\nthat\\tautoencoders\\tworked\\tjust\\tas\\twell.', 'There\\tis\\tnothing\\tspecial\\tabout\\tthe\\tTensorFlow\\timplementation:\\tjust\\ttrain\\tan\\tautoencoder\\tusing\\tall\\tthe\\ntraining\\tdata,\\tthen\\treuse\\tits\\tencoder\\tlayers\\tto\\tcreate\\ta\\tnew\\tneural\\tnetwork\\t(see\\t\\nChapter\\t11\\n\\tfor\\tmore\\ndetails\\ton\\thow\\tto\\treuse\\tpretrained\\tlayers,\\tor\\tcheck\\tout\\tthe\\tcode\\texamples\\tin\\tthe\\tJupyter\\tnotebooks).\\nUp\\tto\\tnow,\\tin\\torder\\tto\\tforce\\tthe\\tautoencoder\\tto\\tlearn\\tinteresting\\tfeatures,\\twe\\thave\\tlimited\\tthe\\tsize\\tof\\tthe\\ncoding\\tlayer,\\tmaking\\tit\\tundercomplete.\\tThere\\tare\\tactually\\tmany\\tother\\tkinds\\tof\\tconstraints\\tthat\\tcan\\tbe\\nused,\\tincluding\\tones\\tthat\\tallow\\tthe\\tcoding\\tlayer\\tto\\tbe\\tjust\\tas\\tlarge\\tas\\tthe\\tinputs,\\tor\\teven\\tlarger,\\tresulting\\nin\\t\\nan\\t\\novercomplete\\tautoencoder\\n.\\tLet’s\\tlook\\tat\\tsome\\tof\\tthose\\t\\napproaches\\tnow.', 'Denoising\\tAutoencoders\\nAnother\\t\\nway\\tto\\tforce\\tthe\\tautoencoder\\tto\\tlearn\\tuseful\\tfeatures\\tis\\tto\\tadd\\tnoise\\tto\\tits\\tinputs,\\ttraining\\tit\\tto\\nrecover\\tthe\\toriginal,\\tnoise-free\\tinputs.\\tThis\\tprevents\\tthe\\tautoencoder\\tfrom\\ttrivially\\tcopying\\tits\\tinputs\\tto\\nits\\toutputs,\\tso\\tit\\tends\\tup\\thaving\\tto\\tfind\\tpatterns\\tin\\tthe\\tdata.\\nThe\\tidea\\tof\\tusing\\tautoencoders\\tto\\tremove\\tnoise\\thas\\tbeen\\taround\\tsince\\tthe\\t1980s\\t(e.g.,\\tit\\tis\\tmentioned\\tin\\nYann\\tLeCun’s\\t1987\\tmaster’s\\tthesis).\\tIn\\ta\\t\\n2008\\tpaper\\n,\\n3\\n\\tPascal\\tVincent\\tet\\tal.\\tshowed\\tthat\\tautoencoders\\ncould\\talso\\tbe\\tused\\tfor\\tfeature\\textraction.\\tIn\\ta\\t\\n2010\\tpaper\\n,\\n4\\n\\tVincent\\tet\\tal.\\t\\nintroduced\\t\\nstacked\\tdenoising\\nautoencoders\\n.\\nThe\\t\\nnoise\\tcan\\tbe\\tpure\\tGaussian\\tnoise\\tadded\\tto\\tthe\\tinputs,\\tor\\tit\\tcan\\tbe\\trandomly\\tswitched\\toff\\tinputs,\\tjust\\nlike\\tin\\tdropout\\t(introduced\\tin\\t\\nChapter\\t11\\n).\\t\\nFigure\\t15-9\\n\\tshows\\tboth\\toptions.\\nFigure\\t15-9.\\t\\nDenoising\\tautoencoders,\\twith\\tGaussian\\tnoise\\t(left)\\tor\\tdropout\\t(right)', 'TensorFlow\\tImplementation\\nImplementing\\t\\ndenoising\\tautoencoders\\tin\\tTensorFlow\\tis\\tnot\\ttoo\\thard.\\tLet’s\\tstart\\twith\\tGaussian\\tnoise.\\tIt’s\\nreally\\tjust\\tlike\\ttraining\\ta\\tregular\\tautoencoder,\\texcept\\tyou\\tadd\\tnoise\\tto\\tthe\\tinputs,\\tand\\tthe\\treconstruction\\nloss\\tis\\tcalculated\\tbased\\ton\\tthe\\toriginal\\tinputs:\\nnoise_level\\n\\t\\n=\\n\\t\\n1.0\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nX_noisy\\n\\t\\n=\\n\\t\\nX\\n\\t\\n+\\n\\t\\nnoise_level\\n\\t\\n*\\n\\t\\ntf\\n.\\nrandom_normal\\n(\\ntf\\n.\\nshape\\n(\\nX\\n))\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX_noisy\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden1\"\\n)\\n[\\n...\\n]\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\n#\\tMSE\\n[\\n...\\n]\\nWARNING\\nSince\\tthe\\tshape\\tof\\t\\nX\\n\\tis\\tonly\\tpartially\\tdefined\\tduring\\tthe\\tconstruction\\tphase,\\twe\\tcannot\\tknow\\tin\\tadvance\\tthe\\tshape\\tof\\tthe\\tnoise\\nthat\\twe\\tmust\\tadd\\tto\\t\\nX\\n.\\tWe\\tcannot\\tcall\\t\\nX.get_shape()\\n\\tbecause\\tthis\\twould\\tjust\\treturn\\tthe\\tpartially\\tdefined\\tshape\\tof\\t\\nX\\n\\t(\\n[None,\\nn_inputs]\\n),\\tand', '),\\tand\\t\\nrandom_normal()\\n\\texpects\\ta\\tfully\\tdefined\\tshape\\tso\\tit\\twould\\traise\\tan\\texception.\\tInstead,\\twe\\tcall\\t\\ntf.shape(X)\\n,\\nwhich\\tcreates\\tan\\toperation\\tthat\\twill\\treturn\\tthe\\tshape\\tof\\t\\nX\\n\\tat\\truntime,\\twhich\\twill\\tbe\\tfully\\tdefined\\tat\\tthat\\tpoint.\\nImplementing\\tthe\\tdropout\\tversion,\\t\\nwhich\\tis\\tmore\\tcommon,\\tis\\tnot\\tmuch\\tharder:\\ndropout_rate\\n\\t\\n=\\n\\t\\n0.3\\ntraining\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder_with_default\\n(\\nFalse\\n,\\n\\t\\nshape\\n=\\n(),\\n\\t\\nname\\n=\\n\\'training\\'\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nX_drop\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndropout\\n(\\nX\\n,\\n\\t\\ndropout_rate\\n,\\n\\t\\ntraining\\n=\\ntraining\\n)\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX_drop\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nrelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nname\\n=\\n\"hidden1\"\\n)\\n[\\n...\\n]\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\n#\\tMSE\\n[\\n...\\n]\\nDuring\\ttraining\\twe\\tmust\\tset\\t\\ntraining\\n\\tto\\t\\nTrue\\n\\t(as\\texplained\\tin\\t\\nChapter\\t11\\n)\\tusing\\tthe\\t\\nfeed_dict\\n:\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n,\\n\\t\\ntraining\\n:', ':\\n\\t\\nTrue\\n})\\nDuring\\ttesting\\tit\\tis\\tnot\\tnecessary\\tto\\tset\\t\\ntraining\\n\\tto\\t\\nFalse\\n,\\tsince\\twe\\tset\\tthat\\tas\\tthe\\tdefault\\tin\\tthe\\tcall\\tto\\nthe\\t\\nplaceholder_with_default()\\n\\t\\nfunction.', 'Sparse\\tAutoencoders\\nAnother\\t\\nkind\\tof\\tconstraint\\tthat\\toften\\tleads\\tto\\tgood\\tfeature\\textraction\\tis\\t\\nsparsity\\n:\\tby\\tadding\\tan\\tappropriate\\nterm\\tto\\tthe\\tcost\\tfunction,\\tthe\\tautoencoder\\tis\\tpushed\\tto\\treduce\\tthe\\tnumber\\tof\\tactive\\tneurons\\tin\\tthe\\tcoding\\nlayer.\\tFor\\texample,\\tit\\tmay\\tbe\\tpushed\\tto\\thave\\ton\\taverage\\tonly\\t5%\\tsignificantly\\tactive\\tneurons\\tin\\tthe\\ncoding\\tlayer.\\tThis\\tforces\\tthe\\tautoencoder\\tto\\trepresent\\teach\\tinput\\tas\\ta\\tcombination\\tof\\ta\\tsmall\\tnumber\\tof\\nactivations.\\tAs\\ta\\tresult,\\teach\\tneuron\\tin\\tthe\\tcoding\\tlayer\\ttypically\\tends\\tup\\trepresenting\\ta\\tuseful\\tfeature\\t(if\\nyou\\tcould\\tspeak\\tonly\\ta\\tfew\\twords\\tper\\tmonth,\\tyou\\twould\\tprobably\\ttry\\tto\\tmake\\tthem\\tworth\\tlistening\\tto).\\nIn\\torder\\tto\\tfavor\\tsparse\\tmodels,\\twe\\tmust\\tfirst\\tmeasure\\tthe\\tactual\\tsparsity\\tof\\tthe\\tcoding\\tlayer\\tat\\teach\\ntraining\\titeration.\\tWe\\tdo\\tso\\tby\\tcomputing\\tthe\\taverage\\tactivation\\tof\\teach\\tneuron\\tin\\tthe\\tcoding\\tlayer,\\tover\\nthe\\twhole\\ttraining\\tbatch.\\tThe\\tbatch\\tsize\\tmust\\tnot\\tbe\\ttoo\\tsmall,\\tor\\telse\\tthe\\tmean\\twill\\tnot\\tbe\\taccurate.', 'Once\\twe\\thave\\tthe\\tmean\\tactivation\\tper\\tneuron,\\twe\\twant\\tto\\tpenalize\\tthe\\tneurons\\tthat\\tare\\ttoo\\tactive\\tby\\nadding\\t\\na\\t\\nsparsity\\tloss\\n\\tto\\tthe\\tcost\\tfunction.\\tFor\\texample,\\tif\\twe\\tmeasure\\tthat\\ta\\tneuron\\thas\\tan\\taverage\\nactivation\\tof\\t0.3,\\tbut\\tthe\\ttarget\\tsparsity\\tis\\t0.1,\\tit\\tmust\\tbe\\tpenalized\\tto\\tactivate\\tless.\\tOne\\tapproach\\tcould\\nbe\\tsimply\\tadding\\tthe\\tsquared\\terror\\t(0.3\\t–\\t0.1)\\n2\\n\\tto\\tthe\\tcost\\tfunction,\\tbut\\tin\\tpractice\\ta\\tbetter\\tapproach\\tis\\tto\\nuse\\tthe\\t\\nKullback–Leibler\\tdivergence\\t(briefly\\tdiscussed\\tin\\t\\nChapter\\t4\\n),\\twhich\\thas\\tmuch\\tstronger\\tgradients\\nthan\\tthe\\tMean\\tSquared\\tError,\\t\\nas\\tyou\\tcan\\tsee\\tin\\t\\nFigure\\t15-10\\n.\\nFigure\\t15-10.\\t\\nSparsity\\tloss\\nGiven\\ttwo\\tdiscrete\\tprobability\\tdistributions\\t\\nP\\n\\tand\\t\\nQ\\n,\\tthe\\tKL\\tdivergence\\tbetween\\tthese\\tdistributions,\\nnoted\\t\\nD\\nKL\\n(\\nP\\n\\t\\t\\nQ\\n),\\tcan\\tbe\\tcomputed\\tusing\\t\\nEquation\\t15-1\\n.\\nEquation\\t15-1.\\t\\nKullback–Leibler\\tdivergence', 'In\\tour\\tcase,\\twe\\twant\\tto\\tmeasure\\tthe\\tdivergence\\tbetween\\tthe\\ttarget\\tprobability\\t\\np\\n\\tthat\\ta\\tneuron\\tin\\tthe\\ncoding\\tlayer\\twill\\tactivate,\\tand\\tthe\\tactual\\tprobability\\t\\nq\\n\\t(i.e.,\\tthe\\tmean\\tactivation\\tover\\tthe\\ttraining\\tbatch).\\nSo\\tthe\\tKL\\tdivergence\\tsimplifies\\tto\\t\\nEquation\\t15-2\\n.\\nEquation\\t15-2.\\t\\nKL\\tdivergence\\tbetween\\tthe\\ttarget\\tsparsity\\t\\np\\n\\tand\\tthe\\tactual\\tsparsity\\t\\nq\\nOnce\\twe\\thave\\tcomputed\\tthe\\tsparsity\\tloss\\tfor\\teach\\tneuron\\tin\\tthe\\tcoding\\tlayer,\\twe\\tjust\\tsum\\tup\\tthese\\tlosses,\\nand\\tadd\\tthe\\tresult\\tto\\tthe\\tcost\\tfunction.\\tIn\\torder\\tto\\tcontrol\\tthe\\trelative\\timportance\\tof\\tthe\\tsparsity\\tloss\\tand\\nthe\\treconstruction\\tloss,\\twe\\tcan\\tmultiply\\tthe\\tsparsity\\tloss\\tby\\ta\\tsparsity\\tweight\\thyperparameter.\\tIf\\tthis\\nweight\\tis\\ttoo\\thigh,\\tthe\\tmodel\\twill\\tstick\\tclosely\\tto\\tthe\\ttarget\\tsparsity,\\tbut\\tit\\tmay\\tnot\\treconstruct\\tthe\\tinputs\\nproperly,\\tmaking\\tthe\\tmodel\\tuseless.\\tConversely,\\tif\\tit\\tis\\ttoo\\tlow,\\tthe\\tmodel\\twill\\tmostly\\tignore\\tthe\\tsparsity\\nobjective\\tand\\tit\\twill\\tnot\\tlearn\\tany\\tinteresting\\tfeatures.', 'TensorFlow\\tImplementation\\nWe\\t\\nnow\\thave\\tall\\twe\\tneed\\tto\\timplement\\ta\\tsparse\\tautoencoder\\t\\nusing\\tTensorFlow:\\ndef\\n\\t\\nkl_divergence\\n(\\np\\n,\\n\\t\\nq\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\np\\n\\t\\n*\\n\\t\\ntf\\n.\\nlog\\n(\\np\\n\\t\\n/\\n\\t\\nq\\n)\\n\\t\\n+\\n\\t\\n(\\n1\\n\\t\\n-\\n\\t\\np\\n)\\n\\t\\n*\\n\\t\\ntf\\n.\\nlog\\n((\\n1\\n\\t\\n-\\n\\t\\np\\n)\\n\\t\\n/\\n\\t\\n(\\n1\\n\\t\\n-\\n\\t\\nq\\n))\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nsparsity_target\\n\\t\\n=\\n\\t\\n0.1\\nsparsity_weight\\n\\t\\n=\\n\\t\\n0.2\\n[\\n...\\n]\\n\\t\\n#\\tBuild\\ta\\tnormal\\tautoencoder\\t(in\\tthis\\texample\\tthe\\tcoding\\tlayer\\tis\\thidden1)\\nhidden1_mean\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\nhidden1\\n,\\n\\t\\naxis\\n=\\n0\\n)\\n\\t\\n#\\tbatch\\tmean\\nsparsity_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\nkl_divergence\\n(\\nsparsity_target\\n,\\n\\t\\nhidden1_mean\\n))\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\noutputs\\n\\t\\n-\\n\\t\\nX\\n))\\n\\t\\n#\\tMSE\\nloss\\n\\t\\n=\\n\\t\\nreconstruction_loss\\n\\t\\n+\\n\\t\\nsparsity_weight\\n\\t\\n*\\n\\t\\nsparsity_loss\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\nAn\\timportant\\tdetail\\tis\\tthe\\tfact\\tthat\\t\\nthe\\tactivations\\tof\\tthe\\tcoding\\tlayer\\tmust\\tbe\\tbetween\\t0\\tand\\t1\\t(but\\tnot', 'equal\\tto\\t0\\tor\\t1),\\tor\\telse\\tthe\\tKL\\tdivergence\\twill\\treturn\\tNaN\\t(Not\\ta\\tNumber).\\tA\\tsimple\\tsolution\\tis\\tto\\tuse\\nthe\\tlogistic\\tactivation\\tfunction\\tfor\\tthe\\tcoding\\tlayer:\\nhidden1\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nsigmoid\\n)\\nOne\\tsimple\\ttrick\\tcan\\tspeed\\tup\\tconvergence:\\tinstead\\tof\\tusing\\tthe\\tMSE,\\twe\\tcan\\tchoose\\ta\\t\\nreconstruction\\nloss\\tthat\\twill\\thave\\tlarger\\tgradients.\\t\\nCross\\tentropy\\tis\\toften\\ta\\tgood\\tchoice.\\tTo\\tuse\\tit,\\twe\\tmust\\tnormalize\\tthe\\ninputs\\tto\\tmake\\tthem\\ttake\\ton\\tvalues\\tfrom\\t0\\tto\\t1,\\tand\\tuse\\tthe\\tlogistic\\tactivation\\tfunction\\tin\\tthe\\toutput\\tlayer\\nso\\tthe\\toutputs\\talso\\ttake\\ton\\tvalues\\tfrom\\t0\\tto\\t1.\\tTensorFlow’s\\t\\nsigmoid_cross_entropy_with_logits()\\nfunction\\t\\ntakes\\tcare\\tof\\tefficiently\\tapplying\\tthe\\tlogistic\\t(sigmoid)\\tactivation\\tfunction\\tto\\tthe\\toutputs\\tand\\ncomputing\\tthe\\tcross\\t\\nentropy:\\n[\\n...\\n]\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden1\\n,\\n\\t\\nn_outputs\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid\\n(\\nlogits\\n)\\nxentropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid_cross_entropy_with_logits\\n(\\nlabels\\n=\\nX\\n,\\n\\t\\nlogits\\n=', 'logits\\n=\\nlogits\\n)\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\nxentropy\\n)\\nNote\\tthat\\tthe\\t\\noutputs\\n\\toperation\\tis\\tnot\\tneeded\\tduring\\ttraining\\t(we\\tuse\\tit\\tonly\\twhen\\twe\\twant\\tto\\tlook\\tat\\tthe\\nreconstructions).', 'Variational\\tAutoencoders\\nAnother\\t\\nimportant\\tcategory\\tof\\tautoencoders\\twas\\t\\nintroduced\\tin\\t2014\\n\\tby\\tDiederik\\tKingma\\tand\\tMax\\nWelling,\\n5\\n\\tand\\thas\\tquickly\\tbecome\\tone\\tof\\tthe\\tmost\\tpopular\\ttypes\\tof\\tautoencoders:\\t\\nvariational\\nautoencoders\\n.\\nThey\\tare\\tquite\\tdifferent\\tfrom\\tall\\tthe\\tautoencoders\\twe\\thave\\tdiscussed\\tso\\tfar,\\tin\\tparticular:\\nThey\\tare\\t\\nprobabilistic\\tautoencoders\\n,\\t\\nmeaning\\tthat\\ttheir\\toutputs\\tare\\tpartly\\tdetermined\\tby\\tchance,\\neven\\tafter\\ttraining\\t(as\\topposed\\tto\\tdenoising\\tautoencoders,\\twhich\\tuse\\trandomness\\tonly\\tduring\\ntraining).\\nMost\\timportantly,\\tthey\\tare\\t\\ngenerative\\tautoencoders\\n,\\t\\nmeaning\\tthat\\tthey\\tcan\\tgenerate\\tnew\\tinstances\\nthat\\tlook\\tlike\\tthey\\twere\\tsampled\\tfrom\\tthe\\ttraining\\tset.\\nBoth\\tthese\\tproperties\\tmake\\tthem\\trather\\tsimilar\\tto\\tRBMs\\t(see\\t\\nAppendix\\tE\\n),\\tbut\\tthey\\tare\\teasier\\tto\\ttrain\\tand\\nthe\\tsampling\\tprocess\\tis\\tmuch\\tfaster\\t(with\\tRBMs\\tyou\\tneed\\tto\\twait\\tfor\\tthe\\tnetwork\\tto\\tstabilize\\tinto\\ta\\n“thermal\\tequilibrium”\\tbefore\\tyou\\tcan\\tsample\\ta\\tnew\\tinstance).\\nLet’s\\ttake\\ta\\tlook\\tat\\thow\\tthey\\twork.\\t\\nFigure\\t15-11', '(left)\\tshows\\ta\\tvariational\\tautoencoder.\\tYou\\tcan\\nrecognize,\\tof\\tcourse,\\tthe\\tbasic\\tstructure\\tof\\tall\\tautoencoders,\\twith\\tan\\tencoder\\tfollowed\\tby\\ta\\tdecoder\\t(in\\nthis\\texample,\\tthey\\tboth\\thave\\ttwo\\thidden\\tlayers),\\tbut\\tthere\\tis\\ta\\ttwist:\\tinstead\\tof\\tdirectly\\tproducing\\ta\\ncoding\\tfor\\ta\\tgiven\\tinput,\\tthe\\tencoder\\tproduces\\t\\na\\t\\nmean\\tcoding\\n\\t\\nμ\\n\\tand\\ta\\tstandard\\tdeviation\\t\\nσ\\n.\\tThe\\tactual\\ncoding\\tis\\tthen\\tsampled\\trandomly\\tfrom\\ta\\t\\nGaussian\\tdistribution\\twith\\tmean\\t\\nμ\\n\\tand\\tstandard\\tdeviation\\t\\nσ\\n.\\nAfter\\tthat\\tthe\\tdecoder\\tjust\\tdecodes\\tthe\\tsampled\\tcoding\\tnormally.\\tThe\\tright\\tpart\\tof\\tthe\\tdiagram\\tshows\\ta\\ntraining\\tinstance\\tgoing\\tthrough\\tthis\\tautoencoder.\\tFirst,\\tthe\\tencoder\\tproduces\\t\\nμ\\n\\tand\\t\\nσ\\n,\\tthen\\ta\\tcoding\\tis\\nsampled\\trandomly\\t(notice\\tthat\\tit\\tis\\tnot\\texactly\\tlocated\\tat\\t\\nμ\\n),\\tand\\tfinally\\tthis\\tcoding\\tis\\tdecoded,\\tand\\tthe\\nfinal\\toutput\\tresembles\\tthe\\ttraining\\tinstance.', 'Figure\\t15-11.\\t\\nVariational\\tautoencoder\\t(left),\\tand\\tan\\tinstance\\tgoing\\tthrough\\tit\\t(right)\\nAs\\tyou\\tcan\\tsee\\ton\\tthe\\tdiagram,\\talthough\\tthe\\tinputs\\tmay\\thave\\ta\\tvery\\tconvoluted\\tdistribution,\\ta\\tvariational\\nautoencoder\\ttends\\tto\\tproduce\\tcodings\\tthat\\tlook\\tas\\tthough\\tthey\\twere\\tsampled\\tfrom\\ta\\tsimple\\tGaussian\\ndistribution:\\n6\\n\\tduring\\ttraining,\\tthe\\tcost\\tfunction\\t(discussed\\tnext)\\tpushes\\tthe\\tcodings\\tto\\tgradually\\tmigrate\\nwithin\\tthe\\t\\ncoding\\tspace\\t(also\\tcalled\\t\\nthe\\t\\nlatent\\tspace\\n)\\tto\\toccupy\\ta\\troughly\\t(hyper)spherical\\tregion\\tthat\\nlooks\\tlike\\ta\\tcloud\\tof\\tGaussian\\tpoints.\\tOne\\tgreat\\tconsequence\\tis\\tthat\\tafter\\ttraining\\ta\\tvariational\\nautoencoder,\\tyou\\tcan\\tvery\\teasily\\tgenerate\\ta\\tnew\\tinstance:\\tjust\\tsample\\ta\\trandom\\tcoding\\tfrom\\tthe\\tGaussian\\ndistribution,\\tdecode\\tit,\\t\\nand\\tvoilà!\\nSo\\tlet’s\\tlook\\tat\\tthe\\t\\ncost\\tfunction.\\tIt\\tis\\tcomposed\\tof\\ttwo\\tparts.\\tThe\\tfirst\\tis\\tthe\\tusual\\t\\nreconstruction\\tloss\\tthat\\npushes\\tthe\\tautoencoder\\tto\\treproduce\\tits\\tinputs\\t(we\\tcan\\tuse\\tcross\\tentropy\\tfor\\tthis,\\tas\\tdiscussed\\tearlier).\\nThe\\tsecond\\tis\\tthe\\t\\nlatent\\tloss', 'that\\tpushes\\tthe\\tautoencoder\\tto\\thave\\tcodings\\tthat\\tlook\\tas\\tthough\\tthey\\twere\\nsampled\\tfrom\\ta\\tsimple\\tGaussian\\tdistribution,\\tfor\\twhich\\twe\\tuse\\tthe\\tKL\\tdivergence\\tbetween\\tthe\\ttarget\\ndistribution\\t(the\\tGaussian\\tdistribution)\\tand\\tthe\\tactual\\tdistribution\\tof\\tthe\\tcodings.\\tThe\\tmath\\tis\\ta\\tbit\\tmore\\ncomplex\\tthan\\tearlier,\\tin\\tparticular\\tbecause\\tof\\tthe\\tGaussian\\tnoise,\\twhich\\tlimits\\tthe\\tamount\\tof\\tinformation\\nthat\\tcan\\tbe\\ttransmitted\\tto\\tthe\\tcoding\\tlayer\\t(thus\\tpushing\\tthe\\tautoencoder\\tto\\tlearn\\tuseful\\tfeatures).\\tLuckily,\\nthe\\tequations\\t\\nsimplify\\tto\\tthe\\tfollowing\\tcode\\tfor\\tthe\\t\\nlatent\\tloss:\\n7\\neps\\n\\t\\n=\\n\\t\\n1e-10\\n\\t\\t\\n#\\tsmoothing\\tterm\\tto\\tavoid\\tcomputing\\tlog(0)\\twhich\\tis\\tNaN\\nlatent_loss\\n\\t\\n=\\n\\t\\n0.5\\n\\t\\n*\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\nsquare\\n(\\nhidden3_sigma\\n)\\n\\t\\n+\\n\\t\\ntf\\n.\\nsquare\\n(\\nhidden3_mean\\n)\\n\\t\\t\\t\\t\\n-\\n\\t\\n1\\n\\t\\n-\\n\\t\\ntf\\n.\\nlog\\n(\\neps\\n\\t\\n+\\n\\t\\ntf\\n.\\nsquare\\n(\\nhidden3_sigma\\n)))\\nOne\\tcommon\\tvariant\\tis\\tto\\ttrain\\tthe\\tencoder\\tto\\toutput\\t\\nγ\\n\\t=\\tlog(\\nσ\\n2\\n)\\trather\\tthan\\t\\nσ\\n.\\tWherever\\twe\\tneed\\t\\nσ\\n\\twe\\ncan\\tjust\\tcompute', '.\\tThis\\tmakes\\tit\\ta\\tbit\\teasier\\tfor\\tthe\\tencoder\\tto\\tcapture\\tsigmas\\tof\\tdifferent', 'scales,\\tand\\tthus\\tit\\thelps\\tspeed\\tup\\tconvergence.\\tThe\\tlatent\\t\\nloss\\tends\\tup\\ta\\tbit\\tsimpler:\\nlatent_loss\\n\\t\\n=\\n\\t\\n0.5\\n\\t\\n*\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\nexp\\n(\\nhidden3_gamma\\n)\\n\\t\\n+\\n\\t\\ntf\\n.\\nsquare\\n(\\nhidden3_mean\\n)\\n\\t\\n-\\n\\t\\n1\\n\\t\\n-\\n\\t\\nhidden3_gamma\\n)\\nThe\\tfollowing\\tcode\\tbuilds\\tthe\\tvariational\\tautoencoder\\tshown\\tin\\t\\nFigure\\t15-11\\n\\t(left),\\t\\nusing\\tthe\\tlog(\\nσ\\n2\\n)\\nvariant:\\nfrom\\n\\t\\nfunctools\\n\\t\\nimport\\n\\t\\npartial\\nn_inputs\\n\\t\\n=\\n\\t\\n28\\n\\t\\n*\\n\\t\\n28\\nn_hidden1\\n\\t\\n=\\n\\t\\n500\\nn_hidden2\\n\\t\\n=\\n\\t\\n500\\nn_hidden3\\n\\t\\n=\\n\\t\\n20\\n\\t\\t\\n#\\tcodings\\nn_hidden4\\n\\t\\n=\\n\\t\\nn_hidden2\\nn_hidden5\\n\\t\\n=\\n\\t\\nn_hidden1\\nn_outputs\\n\\t\\n=\\n\\t\\nn_inputs\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.001\\ninitializer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\nmy_dense_layer\\n\\t\\n=\\n\\t\\npartial\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n,\\n\\t\\t\\t\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nelu\\n,\\n\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nhidden1\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nX\\n,\\n\\t\\nn_hidden1\\n)\\nhidden2\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden1\\n,\\n\\t\\nn_hidden2\\n)\\nhidden3_mean\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(', '(\\nhidden2\\n,\\n\\t\\nn_hidden3\\n,\\n\\t\\nactivation\\n=\\nNone\\n)\\nhidden3_gamma\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden2\\n,\\n\\t\\nn_hidden3\\n,\\n\\t\\nactivation\\n=\\nNone\\n)\\nnoise\\n\\t\\n=\\n\\t\\ntf\\n.\\nrandom_normal\\n(\\ntf\\n.\\nshape\\n(\\nhidden3_gamma\\n),\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nhidden3\\n\\t\\n=\\n\\t\\nhidden3_mean\\n\\t\\n+\\n\\t\\ntf\\n.\\nexp\\n(\\n0.5\\n\\t\\n*\\n\\t\\nhidden3_gamma\\n)\\n\\t\\n*\\n\\t\\nnoise\\nhidden4\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden3\\n,\\n\\t\\nn_hidden4\\n)\\nhidden5\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden4\\n,\\n\\t\\nn_hidden5\\n)\\nlogits\\n\\t\\n=\\n\\t\\nmy_dense_layer\\n(\\nhidden5\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\nactivation\\n=\\nNone\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nsigmoid\\n(\\nlogits\\n)\\nxentropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid_cross_entropy_with_logits\\n(\\nlabels\\n=\\nX\\n,\\n\\t\\nlogits\\n=\\nlogits\\n)\\nreconstruction_loss\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\nxentropy\\n)\\nlatent_loss\\n\\t\\n=\\n\\t\\n0.5\\n\\t\\n*\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\n\\t\\t\\t\\t\\ntf\\n.\\nexp\\n(\\nhidden3_gamma\\n)\\n\\t\\n+\\n\\t\\ntf\\n.\\nsquare\\n(\\nhidden3_mean\\n)\\n\\t\\n-\\n\\t\\n1\\n\\t\\n-\\n\\t\\nhidden3_gamma\\n)\\nloss\\n\\t\\n=\\n\\t\\nreconstruction_loss\\n\\t\\n+\\n\\t\\nlatent_loss\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n=\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\nloss\\n)\\ninit\\n\\t\\n=', 'init\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()', 'Generating\\tDigits\\nNow\\tlet’s\\tuse\\tthis\\tvariational\\t\\nautoencoder\\tto\\tgenerate\\timages\\tthat\\tlook\\tlike\\thandwritten\\tdigits.\\tAll\\twe\\nneed\\tto\\tdo\\tis\\ttrain\\tthe\\tmodel,\\tthen\\tsample\\trandom\\tcodings\\tfrom\\ta\\t\\nGaussian\\tdistribution\\tand\\tdecode\\tthem.\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\nn_digits\\n\\t\\n=\\n\\t\\n60\\nn_epochs\\n\\t\\n=\\n\\t\\n50\\nbatch_size\\n\\t\\n=\\n\\t\\n150\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nepoch\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_epochs\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nn_batches\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnum_examples\\n\\t\\n//\\n\\t\\nbatch_size\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_batches\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_batch\\n,\\n\\t\\ny_batch\\n\\t\\n=\\n\\t\\nmnist\\n.\\ntrain\\n.\\nnext_batch\\n(\\nbatch_size\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nX_batch\\n})\\n\\t\\t\\t\\t\\ncodings_rnd\\n\\t\\n=\\n\\t\\nnp\\n.\\nrandom\\n.\\nnormal\\n(\\nsize\\n=\\n[\\nn_digits\\n,\\n\\t\\nn_hidden3\\n])\\n\\t\\t\\t\\t\\noutputs_val\\n\\t\\n=\\n\\t\\noutputs\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nhidden3\\n:\\n\\t\\ncodings_rnd\\n})\\nThat’s\\tit.\\tNow\\twe\\tcan\\tsee\\twhat\\tthe\\t“handwritten”\\tdigits\\tproduced\\tby\\tthe\\tautoencoder\\tlook\\tlike\\t(see\\nFigure\\t15-12\\n):\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(', 'range\\n(\\nn_digits\\n):\\n\\t\\t\\t\\t\\nplt\\n.\\nsubplot\\n(\\nn_digits\\n,\\n\\t\\n10\\n,\\n\\t\\niteration\\n\\t\\n+\\n\\t\\n1\\n)\\n\\t\\t\\t\\t\\nplot_image\\n(\\noutputs_val\\n[\\niteration\\n])\\nFigure\\t15-12.\\t\\nImages\\tof\\thandwritten\\tdigits\\tgenerated\\tby\\tthe\\tvariational\\tautoencoder\\nA\\tmajority\\tof\\tthese\\tdigits\\tlook\\tpretty\\tconvincing,\\twhile\\ta\\tfew\\tare\\trather\\t“creative.”\\tBut\\tdon’t\\tbe\\ttoo\\tharsh\\non\\tthe\\tautoencoder\\t—\\tit\\tonly\\tstarted\\tlearning\\tless\\tthan\\tan\\thour\\tago.\\tGive\\tit\\ta\\tbit\\tmore\\ttraining\\ttime,\\tand\\nthose\\tdigits\\twill\\tlook\\tbetter\\tand\\t\\nbetter.', 'Other\\tAutoencoders\\nThe\\tamazing\\tsuccesses\\tof\\tsupervised\\tlearning\\tin\\timage\\trecognition,\\tspeech\\trecognition,\\ttext\\ttranslation,\\nand\\tmore\\thave\\tsomewhat\\tovershadowed\\tunsupervised\\tlearning,\\tbut\\tit\\tis\\tactually\\tbooming.\\tNew\\narchitectures\\tfor\\tautoencoders\\tand\\tother\\tunsupervised\\tlearning\\talgorithms\\tare\\tinvented\\tregularly,\\tso\\tmuch\\nso\\tthat\\twe\\tcannot\\tcover\\tthem\\tall\\tin\\tthis\\tbook.\\tHere\\tis\\ta\\tbrief\\t(by\\tno\\tmeans\\texhaustive)\\toverview\\tof\\ta\\tfew\\nmore\\ttypes\\tof\\tautoencoders\\tthat\\tyou\\tmay\\twant\\tto\\tcheck\\tout:\\nContractive\\tautoencoder\\n\\t(CAE)\\n8\\nThe\\tautoencoder\\tis\\tconstrained\\tduring\\ttraining\\tso\\tthat\\tthe\\tderivatives\\tof\\tthe\\tcodings\\twith\\tregards\\tto\\nthe\\tinputs\\tare\\tsmall.\\tIn\\tother\\twords,\\ttwo\\tsimilar\\tinputs\\tmust\\thave\\tsimilar\\tcodings.\\nStacked\\tconvolutional\\tautoencoders\\n9\\nAutoencoders\\tthat\\tlearn\\tto\\textract\\tvisual\\tfeatures\\tby\\treconstructing\\timages\\tprocessed\\tthrough\\nconvolutional\\tlayers.\\nGenerative\\tstochastic\\tnetwork\\n\\t(GSN)\\n10\\nA\\tgeneralization\\tof\\tdenoising\\tautoencoders,\\twith\\tthe\\tadded\\tcapability\\tto\\tgenerate\\tdata.', 'Winner-take-all\\t(WTA)\\tautoencoder\\n11\\nDuring\\ttraining,\\tafter\\tcomputing\\tthe\\tactivations\\tof\\tall\\tthe\\tneurons\\tin\\tthe\\tcoding\\tlayer,\\tonly\\tthe\\ttop\\t\\nk\\n%\\nactivations\\tfor\\teach\\tneuron\\tover\\tthe\\ttraining\\tbatch\\tare\\tpreserved,\\tand\\tthe\\trest\\tare\\tset\\tto\\tzero.\\nNaturally\\tthis\\tleads\\tto\\tsparse\\tcodings.\\tMoreover,\\ta\\tsimilar\\tWTA\\tapproach\\tcan\\tbe\\tused\\tto\\tproduce\\nsparse\\tconvolutional\\tautoencoders.\\nAdversarial\\tautoencoders\\n12\\nOne\\tnetwork\\tis\\ttrained\\tto\\treproduce\\tits\\tinputs,\\tand\\tat\\tthe\\tsame\\ttime\\tanother\\tis\\ttrained\\tto\\tfind\\tinputs\\nthat\\tthe\\tfirst\\tnetwork\\tis\\tunable\\tto\\tproperly\\treconstruct.\\tThis\\tpushes\\tthe\\tfirst\\tautoencoder\\tto\\tlearn\\nrobust\\tcodings.', 'Exercises\\n1\\n.\\t\\nWhat\\tare\\tthe\\tmain\\ttasks\\tthat\\tautoencoders\\tare\\tused\\tfor?\\n2\\n.\\t\\nSuppose\\tyou\\twant\\tto\\ttrain\\ta\\tclassifier\\tand\\tyou\\thave\\tplenty\\tof\\tunlabeled\\ttraining\\tdata,\\tbut\\tonly\\ta\\tfew\\nthousand\\tlabeled\\tinstances.\\tHow\\tcan\\tautoencoders\\thelp?\\tHow\\twould\\tyou\\tproceed?\\n3\\n.\\t\\nIf\\tan\\tautoencoder\\tperfectly\\treconstructs\\tthe\\tinputs,\\tis\\tit\\tnecessarily\\ta\\tgood\\tautoencoder?\\tHow\\tcan\\nyou\\tevaluate\\tthe\\tperformance\\tof\\tan\\tautoencoder?\\n4\\n.\\t\\nWhat\\tare\\tundercomplete\\tand\\tovercomplete\\tautoencoders?\\tWhat\\tis\\tthe\\tmain\\trisk\\tof\\tan\\texcessively\\nundercomplete\\tautoencoder?\\tWhat\\tabout\\tthe\\tmain\\trisk\\tof\\tan\\tovercomplete\\tautoencoder?\\n5\\n.\\t\\nHow\\tdo\\tyou\\ttie\\tweights\\tin\\ta\\tstacked\\tautoencoder?\\tWhat\\tis\\tthe\\tpoint\\tof\\tdoing\\tso?\\n6\\n.\\t\\nWhat\\tis\\ta\\tcommon\\ttechnique\\tto\\tvisualize\\tfeatures\\tlearned\\tby\\tthe\\tlower\\tlayer\\tof\\ta\\tstacked\\nautoencoder?\\tWhat\\tabout\\thigher\\tlayers?\\n7\\n.\\t\\nWhat\\tis\\ta\\tgenerative\\tmodel?\\tCan\\tyou\\tname\\ta\\ttype\\tof\\tgenerative\\tautoencoder?\\n8\\n.\\t\\nLet’s\\tuse\\ta\\tdenoising\\tautoencoder\\tto\\tpretrain\\tan\\timage\\tclassifier:', 'You\\tcan\\tuse\\tMNIST\\t(simplest),\\tor\\tanother\\tlarge\\tset\\tof\\timages\\tsuch\\tas\\t\\nCIFAR10\\n\\tif\\tyou\\twant\\ta\\nbigger\\tchallenge.\\tIf\\tyou\\tchoose\\tCIFAR10,\\tyou\\tneed\\tto\\twrite\\tcode\\tto\\tload\\tbatches\\tof\\timages\\tfor\\ntraining.\\tIf\\tyou\\twant\\tto\\tskip\\tthis\\tpart,\\tTensorFlow’s\\tmodel\\tzoo\\tcontains\\t\\ntools\\tto\\tdo\\tjust\\tthat\\n.\\nSplit\\tthe\\tdataset\\tinto\\ta\\ttraining\\tset\\tand\\ta\\ttest\\tset.\\tTrain\\ta\\tdeep\\tdenoising\\tautoencoder\\ton\\tthe\\tfull\\ntraining\\tset.\\nCheck\\tthat\\tthe\\timages\\tare\\tfairly\\twell\\treconstructed,\\tand\\tvisualize\\tthe\\tlow-level\\tfeatures.\\nVisualize\\tthe\\timages\\tthat\\tmost\\tactivate\\teach\\tneuron\\tin\\tthe\\tcoding\\tlayer.\\nBuild\\ta\\tclassification\\tdeep\\tneural\\tnetwork,\\treusing\\tthe\\tlower\\tlayers\\tof\\tthe\\tautoencoder.\\tTrain\\tit\\nusing\\tonly\\t10%\\tof\\tthe\\ttraining\\tset.\\tCan\\tyou\\tget\\tit\\tto\\tperform\\tas\\twell\\tas\\tthe\\tsame\\tclassifier\\ntrained\\ton\\tthe\\tfull\\ttraining\\tset?\\n9\\n.\\t\\nSemantic\\thashing\\n,\\t\\nintroduced\\tin\\t2008\\tby\\tRuslan\\tSalakhutdinov\\tand\\tGeoffrey\\tHinton\\n,\\n13\\n\\tis\\t\\na\\ntechnique\\tused\\tfor\\tefficient\\t\\ninformation\\tretrieval\\n:\\ta\\tdocument\\t(e.g.,\\tan\\timage)\\tis\\tpassed\\tthrough\\ta', 'system,\\ttypically\\ta\\tneural\\tnetwork,\\twhich\\toutputs\\ta\\tfairly\\tlow-dimensional\\tbinary\\tvector\\t(e.g.,\\t30\\nbits).\\tTwo\\tsimilar\\tdocuments\\tare\\tlikely\\tto\\thave\\tidentical\\tor\\tvery\\tsimilar\\thashes.\\tBy\\tindexing\\teach\\ndocument\\tusing\\tits\\thash,\\tit\\tis\\tpossible\\tto\\tretrieve\\tmany\\tdocuments\\tsimilar\\tto\\ta\\tparticular\\tdocument\\nalmost\\tinstantly,\\teven\\tif\\tthere\\tare\\tbillions\\tof\\tdocuments:\\tjust\\tcompute\\tthe\\thash\\tof\\tthe\\tdocument\\tand\\nlook\\tup\\tall\\tdocuments\\twith\\tthat\\tsame\\thash\\t(or\\thashes\\tdiffering\\tby\\tjust\\tone\\tor\\ttwo\\tbits).\\tLet’s\\nimplement\\tsemantic\\thashing\\tusing\\ta\\tslightly\\ttweaked\\tstacked\\tautoencoder:\\nCreate\\ta\\tstacked\\tautoencoder\\tcontaining\\ttwo\\thidden\\tlayers\\tbelow\\tthe\\tcoding\\tlayer,\\tand\\ttrain\\tit\\non\\tthe\\timage\\tdataset\\tyou\\tused\\tin\\tthe\\tprevious\\texercise.\\tThe\\tcoding\\tlayer\\tshould\\tcontain\\t30\\nneurons\\tand\\tuse\\tthe\\tlogistic\\tactivation\\tfunction\\tto\\toutput\\tvalues\\tbetween\\t0\\tand\\t1.\\tAfter\\ttraining,', 'to\\tproduce\\tthe\\thash\\tof\\tan\\timage,\\tyou\\tcan\\tsimply\\trun\\tit\\tthrough\\tthe\\tautoencoder,\\ttake\\tthe\\toutput\\nof\\tthe\\tcoding\\tlayer,\\tand\\tround\\tevery\\tvalue\\tto\\tthe\\tclosest\\tinteger\\t(0\\tor\\t1).\\nOne\\tneat\\ttrick\\tproposed\\tby\\tSalakhutdinov\\tand\\tHinton\\tis\\tto\\tadd\\tGaussian\\tnoise\\t(with\\tzero\\nmean)\\tto\\tthe\\tinputs\\tof\\tthe\\tcoding\\tlayer,\\tduring\\ttraining\\tonly.\\tIn\\torder\\tto\\tpreserve\\ta\\thigh\\tsignal-\\nto-noise\\tratio,\\tthe\\tautoencoder\\twill\\tlearn\\tto\\tfeed\\tlarge\\tvalues\\tto\\tthe\\tcoding\\tlayer\\t(so\\tthat\\tthe\\nnoise\\tbecomes\\tnegligible).\\tIn\\tturn,\\tthis\\tmeans\\tthat\\tthe\\tlogistic\\tfunction\\tof\\tthe\\tcoding\\tlayer\\twill\\nlikely\\tsaturate\\tat\\t0\\tor\\t1.\\tAs\\ta\\tresult,\\trounding\\tthe\\tcodings\\tto\\t0\\tor\\t1\\twon’t\\tdistort\\tthem\\ttoo\\tmuch,\\nand\\tthis\\twill\\timprove\\tthe\\treliability\\tof\\tthe\\thashes.\\nCompute\\tthe\\thash\\tof\\tevery\\timage,\\tand\\tsee\\tif\\timages\\twith\\tidentical\\thashes\\tlook\\talike.\\tSince\\nMNIST\\tand\\tCIFAR10\\tare\\tlabeled,\\ta\\tmore\\tobjective\\tway\\tto\\tmeasure\\tthe\\tperformance\\tof\\tthe\\nautoencoder\\tfor\\tsemantic\\thashing\\tis\\tto\\tensure\\tthat\\timages\\twith\\tthe\\tsame\\thash\\tgenerally\\thave\\tthe', 'same\\tclass.\\tOne\\tway\\tto\\tdo\\tthis\\tis\\tto\\tmeasure\\tthe\\taverage\\tGini\\tpurity\\t(introduced\\tin\\t\\nChapter\\t6\\n)\\nof\\tthe\\tsets\\tof\\timages\\twith\\tidentical\\t(or\\tvery\\tsimilar)\\thashes.\\nTry\\tfine-tuning\\tthe\\thyperparameters\\tusing\\tcross-validation.\\nNote\\tthat\\twith\\ta\\tlabeled\\tdataset,\\tanother\\tapproach\\tis\\tto\\ttrain\\ta\\tconvolutional\\tneural\\tnetwork\\n(see\\t\\nChapter\\t13\\n)\\tfor\\tclassification,\\tthen\\tuse\\tthe\\tlayer\\tbelow\\tthe\\toutput\\tlayer\\tto\\tproduce\\tthe\\nhashes.\\tSee\\tJinma\\tGua\\tand\\tJianmin\\tLi’s\\t\\n2015\\tpaper\\n.\\n14\\n\\tSee\\tif\\tthat\\tperforms\\tbetter.\\n10\\n.\\t\\nTrain\\ta\\tvariational\\tautoencoder\\ton\\tthe\\timage\\tdataset\\tused\\tin\\tthe\\tprevious\\texercises\\t(MNIST\\tor\\nCIFAR10),\\tand\\tmake\\tit\\tgenerate\\timages.\\tAlternatively,\\tyou\\tcan\\ttry\\tto\\tfind\\tan\\tunlabeled\\tdataset\\tthat\\nyou\\tare\\tinterested\\tin\\tand\\tsee\\tif\\tyou\\tcan\\tgenerate\\tnew\\tsamples.\\nSolutions\\tto\\tthese\\texercises\\tare\\t\\navailable\\tin\\t\\nAppendix\\tA\\n.\\n“Perception\\tin\\tchess,”\\tW.\\tChase\\tand\\tH.\\tSimon\\t(1973).\\n“Greedy\\tLayer-Wise\\tTraining\\tof\\tDeep\\tNetworks,”\\tY.\\tBengio\\tet\\tal.\\t(2007).', '“Extracting\\tand\\tComposing\\tRobust\\tFeatures\\twith\\tDenoising\\tAutoencoders,”\\tP.\\tVincent\\tet\\tal.\\t(2008).\\n“Stacked\\tDenoising\\tAutoencoders:\\tLearning\\tUseful\\tRepresentations\\tin\\ta\\tDeep\\tNetwork\\twith\\ta\\tLocal\\tDenoising\\tCriterion,”\\tP.\\tVincent\\tet\\nal.\\t(2010).\\n“Auto-Encoding\\tVariational\\tBayes,”\\tD.\\tKingma\\tand\\tM.\\tWelling\\t(2014).\\nVariational\\tautoencoders\\tare\\tactually\\tmore\\tgeneral;\\tthe\\tcodings\\tare\\tnot\\tlimited\\tto\\tGaussian\\tdistributions.\\nFor\\tmore\\tmathematical\\tdetails,\\tcheck\\tout\\tthe\\toriginal\\tpaper\\ton\\tvariational\\tautoencoders,\\tor\\tCarl\\tDoersch’s\\t\\ngreat\\ttutorial\\n\\t(2016).\\n“Contractive\\tAuto-Encoders:\\tExplicit\\tInvariance\\tDuring\\tFeature\\tExtraction,”\\tS.\\tRifai\\tet\\tal.\\t(2011).\\n“Stacked\\tConvolutional\\tAuto-Encoders\\tfor\\tHierarchical\\tFeature\\tExtraction,”\\tJ.\\tMasci\\tet\\tal.\\t(2011).\\n“GSNs:\\tGenerative\\tStochastic\\tNetworks,”\\tG.\\tAlain\\tet\\tal.\\t(2015).\\n“Winner-Take-All\\tAutoencoders,”\\tA.\\tMakhzani\\tand\\tB.\\tFrey\\t(2015).\\n“Adversarial\\tAutoencoders,”\\tA.\\tMakhzani\\tet\\tal.\\t(2016).\\n“Semantic\\tHashing,”\\tR.\\tSalakhutdinov\\tand\\tG.\\tHinton\\t(2008).', '“CNN\\tBased\\tHashing\\tfor\\tImage\\tRetrieval,”\\tJ.\\tGua\\tand\\tJ.\\tLi\\t(2015).\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14', 'Chapter\\t16.\\t\\nReinforcement\\tLearning\\nReinforcement\\tLearning\\n\\t(RL)\\t\\nis\\tone\\tof\\tthe\\tmost\\texciting\\tfields\\tof\\tMachine\\tLearning\\ttoday,\\tand\\talso\\tone\\nof\\tthe\\toldest.\\tIt\\thas\\tbeen\\taround\\tsince\\tthe\\t1950s,\\tproducing\\tmany\\tinteresting\\tapplications\\tover\\tthe\\tyears,\\n1\\nin\\tparticular\\tin\\tgames\\t(e.g.,\\t\\nTD-Gammon\\n,\\ta\\t\\nBackgammon\\n\\tplaying\\tprogram)\\tand\\tin\\t\\nmachine\\tcontrol,\\tbut\\nseldom\\tmaking\\tthe\\theadline\\tnews.\\tBut\\ta\\trevolution\\ttook\\tplace\\tin\\t2013\\twhen\\tresearchers\\tfrom\\tan\\tEnglish\\nstartup\\tcalled\\t\\nDeepMind\\t\\ndemonstrated\\ta\\tsystem\\tthat\\tcould\\tlearn\\tto\\tplay\\tjust\\tabout\\tany\\tAtari\\tgame\\tfrom\\nscratch\\n,\\n2\\n\\teventually\\t\\noutperforming\\thumans\\n3\\n\\tin\\tmost\\tof\\tthem,\\tusing\\tonly\\traw\\tpixels\\tas\\tinputs\\tand\\twithout\\nany\\tprior\\tknowledge\\tof\\tthe\\trules\\tof\\tthe\\tgames.\\n4\\n\\tThis\\twas\\tthe\\tfirst\\tof\\ta\\tseries\\tof\\tamazing\\tfeats,\\tculminating\\nin\\tMarch\\t2016\\twith\\tthe\\tvictory\\tof\\ttheir\\tsystem\\t\\nAlphaGo\\tagainst\\tLee\\tSedol,\\tthe\\tworld\\tchampion\\tof\\tthe\\ngame\\tof\\t\\nGo\\n.\\tNo\\tprogram\\thad\\tever\\tcome\\tclose\\tto\\tbeating\\ta\\tmaster\\tof\\tthis\\tgame,\\tlet\\talone\\tthe\\tworld', 'champion.\\tToday\\tthe\\twhole\\tfield\\tof\\tRL\\tis\\tboiling\\twith\\tnew\\tideas,\\twith\\ta\\twide\\trange\\tof\\tapplications.\\nDeepMind\\twas\\tbought\\tby\\tGoogle\\tfor\\tover\\t500\\tmillion\\tdollars\\tin\\t2014.\\nSo\\thow\\tdid\\tthey\\tdo\\tit?\\tWith\\thindsight\\tit\\tseems\\trather\\tsimple:\\tthey\\tapplied\\tthe\\tpower\\tof\\t\\nDeep\\tLearning\\tto\\nthe\\tfield\\tof\\tReinforcement\\tLearning,\\tand\\tit\\tworked\\tbeyond\\ttheir\\twildest\\tdreams.\\tIn\\tthis\\tchapter\\twe\\twill\\nfirst\\texplain\\twhat\\tReinforcement\\tLearning\\tis\\tand\\twhat\\tit\\tis\\tgood\\tat,\\tand\\tthen\\twe\\twill\\tpresent\\ttwo\\tof\\tthe\\nmost\\timportant\\ttechniques\\tin\\tdeep\\tReinforcement\\tLearning:\\t\\npolicy\\tgradients\\n\\tand\\t\\ndeep\\tQ-networks\\n(DQN),\\tincluding\\ta\\tdiscussion\\tof\\t\\nMarkov\\tdecision\\tprocesses\\n\\t(MDP).\\tWe\\twill\\tuse\\tthese\\ttechniques\\tto\\ntrain\\ta\\tmodel\\tto\\tbalance\\ta\\tpole\\ton\\ta\\tmoving\\tcart,\\tand\\tanother\\tto\\tplay\\tAtari\\tgames.\\tThe\\tsame\\ttechniques\\ncan\\tbe\\tused\\tfor\\ta\\twide\\tvariety\\tof\\ttasks,\\tfrom\\twalking\\trobots\\tto\\tself-driving\\tcars.', 'Learning\\tto\\tOptimize\\tRewards\\nIn\\t\\nReinforcement\\tLearning,\\ta\\tsoftware\\t\\nagent\\n\\t\\nmakes\\t\\nobservations\\n\\tand\\ttakes\\t\\nactions\\n\\twithin\\tan\\nenvironment\\n,\\t\\nand\\tin\\treturn\\tit\\treceives\\t\\nrewards\\n.\\tIts\\tobjective\\tis\\tto\\tlearn\\tto\\tact\\tin\\ta\\tway\\tthat\\twill\\tmaximize\\nits\\texpected\\tlong-term\\trewards.\\tIf\\tyou\\tdon’t\\tmind\\ta\\tbit\\tof\\tanthropomorphism,\\tyou\\tcan\\tthink\\tof\\tpositive\\nrewards\\tas\\tpleasure,\\tand\\tnegative\\trewards\\tas\\tpain\\t(the\\tterm\\t“reward”\\tis\\ta\\tbit\\tmisleading\\tin\\tthis\\tcase).\\tIn\\nshort,\\tthe\\tagent\\tacts\\tin\\tthe\\tenvironment\\tand\\tlearns\\tby\\ttrial\\tand\\terror\\tto\\tmaximize\\tits\\tpleasure\\tand\\tminimize\\nits\\tpain.\\nThis\\tis\\t\\nquite\\ta\\tbroad\\tsetting,\\twhich\\tcan\\tapply\\tto\\ta\\twide\\tvariety\\tof\\ttasks.\\tHere\\tare\\ta\\tfew\\texamples\\t(see\\nFigure\\t16-1\\n):\\n1\\n.\\t\\nThe\\tagent\\tcan\\tbe\\tthe\\tprogram\\tcontrolling\\ta\\twalking\\trobot.\\tIn\\tthis\\tcase,\\tthe\\tenvironment\\tis\\tthe\\treal\\nworld,\\tthe\\tagent\\tobserves\\tthe\\tenvironment\\tthrough\\ta\\tset\\tof\\t\\nsensors\\n\\tsuch\\tas\\tcameras\\tand\\ttouch\\nsensors,\\tand\\tits\\tactions\\tconsist\\tof\\tsending\\tsignals\\tto\\tactivate\\tmotors.\\tIt\\tmay\\tbe\\tprogrammed\\tto\\tget', 'positive\\trewards\\twhenever\\tit\\tapproaches\\tthe\\ttarget\\tdestination,\\tand\\tnegative\\trewards\\twhenever\\tit\\nwastes\\ttime,\\tgoes\\tin\\tthe\\twrong\\tdirection,\\tor\\tfalls\\tdown.\\n2\\n.\\t\\nThe\\tagent\\tcan\\tbe\\tthe\\tprogram\\tcontrolling\\tMs.\\tPac-Man.\\tIn\\tthis\\tcase,\\tthe\\tenvironment\\tis\\ta\\tsimulation\\nof\\tthe\\tAtari\\tgame,\\tthe\\tactions\\tare\\tthe\\tnine\\tpossible\\tjoystick\\tpositions\\t(upper\\tleft,\\tdown,\\tcenter,\\tand\\nso\\ton),\\tthe\\tobservations\\tare\\tscreenshots,\\tand\\tthe\\trewards\\tare\\tjust\\tthe\\tgame\\tpoints.\\n3\\n.\\t\\nSimilarly,\\tthe\\tagent\\tcan\\tbe\\tthe\\tprogram\\tplaying\\ta\\tboard\\tgame\\tsuch\\tas\\tthe\\tgame\\tof\\t\\nGo\\n.\\n4\\n.\\t\\nThe\\tagent\\tdoes\\tnot\\thave\\tto\\tcontrol\\ta\\tphysically\\t(or\\tvirtually)\\tmoving\\tthing.\\tFor\\texample,\\tit\\tcan\\tbe\\ta\\nsmart\\tthermostat,\\tgetting\\trewards\\twhenever\\tit\\tis\\tclose\\tto\\tthe\\ttarget\\ttemperature\\tand\\tsaves\\tenergy,\\tand\\nnegative\\trewards\\twhen\\thumans\\tneed\\tto\\ttweak\\tthe\\ttemperature,\\tso\\tthe\\tagent\\tmust\\tlearn\\tto\\tanticipate\\nhuman\\tneeds.\\n5\\n.\\t\\nThe\\tagent\\tcan\\tobserve\\tstock\\tmarket\\tprices\\tand\\tdecide\\thow\\tmuch\\tto\\tbuy\\tor\\tsell\\tevery\\tsecond.', 'Rewards\\tare\\tobviously\\tthe\\tmonetary\\tgains\\tand\\tlosses.', 'Figure\\t16-1.\\t\\nReinforcement\\tLearning\\texamples:\\t(a)\\twalking\\trobot,\\t(b)\\tMs.\\tPac-Man,\\t(c)\\tGo\\tplayer,\\t(d)\\tthermostat,\\t(e)\\tautomatic\\ntrader\\n5\\nNote\\tthat\\tthere\\tmay\\tnot\\tbe\\tany\\tpositive\\trewards\\tat\\tall;\\tfor\\texample,\\tthe\\tagent\\tmay\\tmove\\taround\\tin\\ta\\tmaze,\\ngetting\\ta\\tnegative\\treward\\tat\\tevery\\ttime\\tstep,\\tso\\tit\\tbetter\\tfind\\tthe\\texit\\tas\\tquickly\\tas\\tpossible!\\tThere\\tare\\nmany\\tother\\texamples\\tof\\ttasks\\twhere\\tReinforcement\\tLearning\\tis\\twell\\tsuited,\\tsuch\\tas\\tself-driving\\tcars,\\nplacing\\tads\\ton\\ta\\tweb\\tpage,\\tor\\tcontrolling\\twhere\\tan\\timage\\tclassification\\tsystem\\tshould\\tfocus\\tits\\t\\nattention.', 'Policy\\tSearch\\nThe\\t\\nalgorithm\\tused\\tby\\tthe\\tsoftware\\tagent\\tto\\tdetermine\\tits\\tactions\\tis\\tcalled\\tits\\t\\npolicy\\n.\\t\\nFor\\texample,\\tthe\\npolicy\\tcould\\tbe\\ta\\tneural\\tnetwork\\ttaking\\tobservations\\tas\\tinputs\\tand\\toutputting\\tthe\\taction\\tto\\ttake\\t(see\\nFigure\\t16-2\\n).\\nFigure\\t16-2.\\t\\nReinforcement\\tLearning\\tusing\\ta\\tneural\\tnetwork\\tpolicy\\nThe\\tpolicy\\tcan\\tbe\\tany\\talgorithm\\tyou\\tcan\\tthink\\tof,\\tand\\tit\\tdoes\\tnot\\teven\\thave\\tto\\tbe\\tdeterministic.\\tFor\\nexample,\\tconsider\\ta\\trobotic\\tvacuum\\tcleaner\\twhose\\treward\\tis\\tthe\\tamount\\tof\\tdust\\tit\\tpicks\\tup\\tin\\t30\\tminutes.\\nIts\\tpolicy\\tcould\\tbe\\tto\\tmove\\tforward\\twith\\tsome\\tprobability\\t\\np\\n\\tevery\\tsecond,\\tor\\trandomly\\trotate\\tleft\\tor\\nright\\twith\\tprobability\\t1\\t–\\t\\np\\n.\\tThe\\trotation\\tangle\\twould\\tbe\\ta\\trandom\\tangle\\tbetween\\t–r\\tand\\t+r.\\tSince\\tthis\\npolicy\\tinvolves\\tsome\\trandomness,\\tit\\tis\\tcalled\\t\\na\\t\\nstochastic\\tpolicy\\n.\\tThe\\trobot\\twill\\thave\\tan\\terratic\\ntrajectory,\\twhich\\tguarantees\\tthat\\tit\\twill\\teventually\\tget\\tto\\tany\\tplace\\tit\\tcan\\treach\\tand\\tpick\\tup\\tall\\tthe\\tdust.\\nThe\\tquestion\\tis:\\thow\\tmuch\\tdust\\twill\\tit\\tpick\\tup\\tin\\t30\\tminutes?', 'How\\twould\\tyou\\ttrain\\tsuch\\ta\\trobot?\\tThere\\tare\\tjust\\ttwo\\t\\npolicy\\tparameters\\n\\tyou\\tcan\\ttweak:\\tthe\\tprobability\\np\\n\\tand\\tthe\\tangle\\trange\\t\\nr\\n.\\tOne\\tpossible\\tlearning\\talgorithm\\tcould\\tbe\\tto\\ttry\\tout\\tmany\\tdifferent\\tvalues\\tfor\\nthese\\tparameters,\\tand\\tpick\\tthe\\tcombination\\tthat\\tperforms\\tbest\\t(see\\t\\nFigure\\t16-3\\n).\\tThis\\tis\\tan\\texample\\tof\\npolicy\\tsearch\\n,\\tin\\tthis\\tcase\\tusing\\ta\\tbrute\\tforce\\tapproach.\\tHowever,\\twhen\\tthe\\t\\npolicy\\tspace\\n\\t\\nis\\ttoo\\tlarge\\n(which\\tis\\tgenerally\\tthe\\tcase),\\tfinding\\ta\\tgood\\tset\\tof\\tparameters\\tthis\\tway\\tis\\tlike\\tsearching\\tfor\\ta\\tneedle\\tin\\ta\\ngigantic\\thaystack.\\nAnother\\tway\\tto\\texplore\\tthe\\tpolicy\\tspace\\tis\\tto\\t\\nuse\\t\\ngenetic\\talgorithms\\n.\\tFor\\texample,\\tyou\\tcould\\trandomly\\ncreate\\ta\\tfirst\\tgeneration\\tof\\t100\\tpolicies\\tand\\ttry\\tthem\\tout,\\tthen\\t“kill”\\tthe\\t80\\tworst\\tpolicies\\n6\\n\\tand\\tmake\\tthe\\n20\\tsurvivors\\tproduce\\t4\\toffspring\\teach.\\tAn\\toffspring\\tis\\tjust\\ta\\tcopy\\tof\\tits\\tparent\\n7\\n\\tplus\\tsome\\trandom\\nvariation.\\tThe\\tsurviving\\tpolicies\\tplus\\ttheir\\toffspring\\ttogether\\tconstitute\\tthe\\tsecond\\tgeneration.\\tYou\\tcan', 'continue\\tto\\titerate\\tthrough\\tgenerations\\tthis\\tway,\\tuntil\\tyou\\tfind\\ta\\tgood\\tpolicy.', 'Figure\\t16-3.\\t\\nFour\\tpoints\\tin\\tpolicy\\tspace\\tand\\tthe\\tagent’s\\tcorresponding\\tbehavior\\nYet\\tanother\\tapproach\\tis\\tto\\tuse\\toptimization\\ttechniques,\\tby\\tevaluating\\tthe\\tgradients\\tof\\tthe\\trewards\\twith\\nregards\\tto\\tthe\\tpolicy\\tparameters,\\tthen\\ttweaking\\tthese\\tparameters\\tby\\tfollowing\\tthe\\tgradient\\ttoward\\thigher\\nrewards\\t(\\ngradient\\tascent\\n).\\t\\nThis\\tapproach\\tis\\tcalled\\t\\npolicy\\tgradients\\n\\t(PG),\\twhich\\twe\\twill\\tdiscuss\\tin\\nmore\\tdetail\\tlater\\tin\\tthis\\tchapter.\\tFor\\texample,\\tgoing\\tback\\tto\\tthe\\tvacuum\\tcleaner\\trobot,\\tyou\\tcould\\tslightly\\nincrease\\t\\np\\n\\tand\\tevaluate\\twhether\\tthis\\tincreases\\tthe\\tamount\\tof\\tdust\\tpicked\\tup\\tby\\tthe\\trobot\\tin\\t30\\tminutes;\\tif\\nit\\tdoes,\\tthen\\tincrease\\t\\np\\n\\tsome\\tmore,\\tor\\telse\\treduce\\t\\np\\n.\\tWe\\twill\\timplement\\ta\\tpopular\\tPG\\talgorithm\\tusing\\nTensorFlow,\\tbut\\tbefore\\twe\\tdo\\twe\\tneed\\tto\\tcreate\\tan\\tenvironment\\tfor\\tthe\\tagent\\tto\\tlive\\tin,\\tso\\tit’s\\ttime\\tto\\nintroduce\\tOpenAI\\tgym.', 'Introduction\\tto\\tOpenAI\\tGym\\nOne\\tof\\t\\nthe\\tchallenges\\tof\\tReinforcement\\tLearning\\tis\\tthat\\tin\\torder\\tto\\ttrain\\tan\\tagent,\\tyou\\tfirst\\tneed\\tto\\thave\\ta\\nworking\\tenvironment.\\tIf\\tyou\\twant\\tto\\tprogram\\tan\\tagent\\tthat\\twill\\tlearn\\tto\\tplay\\tan\\tAtari\\tgame,\\tyou\\twill\\tneed\\nan\\tAtari\\tgame\\tsimulator.\\tIf\\tyou\\twant\\tto\\tprogram\\ta\\twalking\\trobot,\\tthen\\tthe\\tenvironment\\tis\\tthe\\treal\\tworld\\nand\\tyou\\tcan\\tdirectly\\ttrain\\tyour\\trobot\\tin\\tthat\\tenvironment,\\tbut\\tthis\\thas\\tits\\tlimits:\\tif\\tthe\\trobot\\tfalls\\toff\\ta\\tcliff,\\nyou\\tcan’t\\tjust\\tclick\\t“undo.”\\tYou\\tcan’t\\tspeed\\tup\\ttime\\teither;\\tadding\\tmore\\tcomputing\\tpower\\twon’t\\tmake\\tthe\\nrobot\\tmove\\tany\\tfaster.\\tAnd\\tit’s\\tgenerally\\ttoo\\texpensive\\tto\\ttrain\\t1,000\\trobots\\tin\\tparallel.\\tIn\\tshort,\\ttraining\\nis\\thard\\tand\\tslow\\tin\\tthe\\treal\\tworld,\\tso\\tyou\\tgenerally\\tneed\\ta\\t\\nsimulated\\tenvironment\\n\\t\\nat\\tleast\\tto\\t\\nbootstrap\\ntraining.\\nOpenAI\\tgym\\n8\\n\\tis\\ta\\ttoolkit\\tthat\\tprovides\\ta\\twide\\tvariety\\tof\\tsimulated\\tenvironments\\t(Atari\\tgames,\\tboard\\ngames,\\t2D\\tand\\t3D\\tphysical\\tsimulations,\\tand\\tso\\ton),\\tso\\tyou\\tcan\\ttrain\\tagents,\\tcompare\\tthem,\\tor\\tdevelop', 'new\\tRL\\talgorithms.\\nLet’s\\tinstall\\tOpenAI\\tgym.\\tFor\\ta\\tminimal\\tOpenAI\\tgym\\tinstallation,\\tsimply\\tuse\\tpip:\\n$\\tpip3\\tinstall\\t--upgrade\\tgym\\nNext\\topen\\tup\\ta\\tPython\\tshell\\tor\\ta\\tJupyter\\tnotebook\\tand\\tcreate\\tyour\\tfirst\\tenvironment:\\n>>>\\t\\nimport\\n\\t\\ngym\\n>>>\\t\\nenv\\n\\t\\n=\\n\\t\\ngym\\n.\\nmake\\n(\\n\"CartPole-v0\"\\n)\\n[2016-10-14\\t16:03:23,199]\\tMaking\\tnew\\tenv:\\tMsPacman-v0\\n>>>\\t\\nobs\\n\\t\\n=\\n\\t\\nenv\\n.\\nreset\\n()\\n>>>\\t\\nobs\\narray([-0.03799846,\\t-0.03288115,\\t\\t0.02337094,\\t\\t0.00720711])\\n>>>\\t\\nenv\\n.\\nrender\\n()\\nThe\\t\\nmake()\\n\\t\\nfunction\\tcreates\\tan\\tenvironment,\\tin\\tthis\\tcase\\ta\\tCartPole\\tenvironment.\\tThis\\tis\\ta\\t2D\\tsimulation\\nin\\twhich\\ta\\tcart\\tcan\\tbe\\taccelerated\\tleft\\tor\\tright\\tin\\torder\\tto\\tbalance\\ta\\tpole\\tplaced\\ton\\ttop\\tof\\tit\\t(see\\nFigure\\t16-4\\n).\\tAfter\\tthe\\tenvironment\\tis\\tcreated,\\twe\\tmust\\tinitialize\\tit\\tusing\\tthe\\t\\nreset()\\n\\t\\nmethod.\\tThis\\nreturns\\tthe\\tfirst\\tobservation.\\tObservations\\tdepend\\ton\\tthe\\ttype\\tof\\tenvironment.\\tFor\\tthe\\tCartPole\\nenvironment,\\teach\\tobservation\\tis\\ta\\t1D\\tNumPy\\tarray\\tcontaining\\tfour\\tfloats:\\tthese\\tfloats\\trepresent\\tthe\\ncart’s\\thorizontal\\tposition\\t(\\n0.0', '0.0\\n\\t=\\tcenter),\\tits\\tvelocity,\\tthe\\tangle\\tof\\tthe\\tpole\\t(\\n0.0\\n\\t=\\tvertical),\\tand\\tits\\nangular\\tvelocity.\\tFinally,\\tthe\\t\\nrender()\\n\\t\\nmethod\\tdisplays\\tthe\\tenvironment\\tas\\tshown\\tin\\t\\nFigure\\t16-4\\n.', 'Figure\\t16-4.\\t\\nThe\\tCartPole\\tenvironment\\nIf\\tyou\\twant\\t\\nrender()\\n\\tto\\treturn\\tthe\\trendered\\timage\\tas\\ta\\tNumPy\\tarray,\\tyou\\tcan\\tset\\tthe\\t\\nmode\\n\\tparameter\\tto\\nrgb_array\\n\\t\\n(note\\tthat\\tother\\tenvironments\\tmay\\tsupport\\tdifferent\\tmodes):\\n>>>\\t\\nimg\\n\\t\\n=\\n\\t\\nenv\\n.\\nrender\\n(\\nmode\\n=\\n\"rgb_array\"\\n)\\n>>>\\t\\nimg\\n.\\nshape\\n\\t\\t\\n#\\theight,\\twidth,\\tchannels\\t(3=RGB)\\n(400,\\t600,\\t3)\\nTIP\\nUnfortunately,\\tthe\\tCartPole\\t(and\\ta\\tfew\\tother\\tenvironments)\\trenders\\tthe\\timage\\tto\\tthe\\tscreen\\teven\\tif\\tyou\\tset\\tthe\\tmode\\tto\\n\"rgb_array\"\\n.\\tThe\\tonly\\tway\\tto\\tavoid\\tthis\\tis\\tto\\tuse\\ta\\tfake\\tX\\tserver\\n\\tsuch\\tas\\tXvfb\\tor\\tXdummy.\\tFor\\texample,\\tyou\\tcan\\tinstall\\tXvfb\\nand\\tstart\\tPython\\tusing\\tthe\\tfollowing\\tcommand:\\t\\nxvfb-run\\t-s\\t\"-screen\\t0\\t1400x900x24\"\\tpython\\n.\\tOr\\tuse\\tthe\\t\\nxvfbwrapper\\npackage\\n.\\nLet’s\\task\\tthe\\tenvironment\\twhat\\tactions\\tare\\tpossible:\\n>>>\\t\\nenv\\n.\\naction_space\\nDiscrete(2)\\nDiscrete(2)\\n\\tmeans\\tthat\\tthe\\tpossible\\tactions\\tare\\tintegers\\t0\\tand\\t1,\\twhich\\trepresent\\taccelerating\\tleft\\t(0)', 'or\\tright\\t(1).\\tOther\\tenvironments\\tmay\\thave\\tmore\\tdiscrete\\tactions,\\tor\\tother\\tkinds\\tof\\tactions\\t(e.g.,\\ncontinuous).\\tSince\\tthe\\tpole\\tis\\tleaning\\ttoward\\tthe\\tright,\\tlet’s\\taccelerate\\tthe\\tcart\\ttoward\\tthe\\tright:\\n>>>\\t\\naction\\n\\t\\n=\\n\\t\\n1\\n\\t\\t\\n#\\taccelerate\\tright\\n>>>\\t\\nobs\\n,\\n\\t\\nreward\\n,\\n\\t\\ndone\\n,\\n\\t\\ninfo\\n\\t\\n=\\n\\t\\nenv\\n.\\nstep\\n(\\naction\\n)\\n>>>\\t\\nobs\\narray([-0.03865608,\\t\\t0.16189797,\\t\\t0.02351508,\\t-0.27801135])\\n>>>\\t\\nreward\\n1.0\\n>>>\\t\\ndone\\nFalse\\n>>>\\t\\ninfo', '{}\\nThe\\t\\nstep()\\n\\tmethod\\t\\nexecutes\\tthe\\tgiven\\taction\\tand\\treturns\\tfour\\tvalues:\\nobs\\nThis\\tis\\tthe\\tnew\\tobservation.\\tThe\\tcart\\tis\\tnow\\tmoving\\ttoward\\tthe\\tright\\t(\\nobs[1]>0\\n).\\tThe\\tpole\\tis\\tstill\\ntilted\\ttoward\\tthe\\tright\\t(\\nobs[2]>0\\n),\\tbut\\tits\\tangular\\tvelocity\\tis\\tnow\\tnegative\\t(\\nobs[3]<0\\n),\\tso\\tit\\twill\\nlikely\\tbe\\ttilted\\ttoward\\tthe\\tleft\\tafter\\tthe\\tnext\\tstep.\\nreward\\nIn\\tthis\\tenvironment,\\tyou\\tget\\ta\\treward\\tof\\t1.0\\tat\\tevery\\tstep,\\tno\\tmatter\\twhat\\tyou\\tdo,\\tso\\tthe\\tgoal\\tis\\tto\\nkeep\\trunning\\tas\\tlong\\tas\\tpossible.\\ndone\\nThis\\tvalue\\twill\\tbe\\t\\nTrue\\n\\twhen\\t\\nthe\\t\\nepisode\\n\\tis\\tover.\\tThis\\twill\\thappen\\twhen\\tthe\\tpole\\ttilts\\ttoo\\tmuch.\\nAfter\\tthat,\\tthe\\tenvironment\\tmust\\tbe\\treset\\tbefore\\tit\\tcan\\tbe\\tused\\tagain.\\ninfo\\nThis\\tdictionary\\tmay\\tprovide\\textra\\tdebug\\tinformation\\tin\\tother\\tenvironments.\\tThis\\tdata\\tshould\\tnot\\tbe\\nused\\tfor\\ttraining\\t(it\\twould\\tbe\\tcheating).\\nLet’s\\thardcode\\ta\\tsimple\\tpolicy\\tthat\\taccelerates\\tleft\\twhen\\tthe\\tpole\\tis\\tleaning\\ttoward\\tthe\\tleft\\tand', \"accelerates\\tright\\twhen\\tthe\\tpole\\tis\\tleaning\\ttoward\\tthe\\tright.\\tWe\\twill\\trun\\tthis\\tpolicy\\tto\\tsee\\tthe\\taverage\\nrewards\\tit\\tgets\\tover\\t500\\tepisodes:\\ndef\\n\\t\\nbasic_policy\\n(\\nobs\\n):\\n\\t\\t\\t\\t\\nangle\\n\\t\\n=\\n\\t\\nobs\\n[\\n2\\n]\\n\\t\\t\\t\\t\\nreturn\\n\\t\\n0\\n\\t\\nif\\n\\t\\nangle\\n\\t\\n<\\n\\t\\n0\\n\\t\\nelse\\n\\t\\n1\\ntotals\\n\\t\\n=\\n\\t\\n[]\\nfor\\n\\t\\nepisode\\n\\t\\nin\\n\\t\\nrange\\n(\\n500\\n):\\n\\t\\t\\t\\t\\nepisode_rewards\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\t\\t\\nobs\\n\\t\\n=\\n\\t\\nenv\\n.\\nreset\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\nstep\\n\\t\\nin\\n\\t\\nrange\\n(\\n1000\\n):\\n\\t\\n#\\t1000\\tsteps\\tmax,\\twe\\tdon't\\twant\\tto\\trun\\tforever\\n\\t\\t\\t\\t\\t\\t\\t\\t\\naction\\n\\t\\n=\\n\\t\\nbasic_policy\\n(\\nobs\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n,\\n\\t\\nreward\\n,\\n\\t\\ndone\\n,\\n\\t\\ninfo\\n\\t\\n=\\n\\t\\nenv\\n.\\nstep\\n(\\naction\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nepisode_rewards\\n\\t\\n+=\\n\\t\\nreward\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\ndone\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbreak\\n\\t\\t\\t\\t\\ntotals\\n.\\nappend\\n(\\nepisode_rewards\\n)\\nThis\\tcode\\tis\\thopefully\\tself-explanatory.\\tLet’s\\tlook\\tat\\tthe\\tresult:\\n>>>\\t\\nimport\\n\\t\\nnumpy\\n\\t\\nas\\n\\t\\nnp\\n>>>\\t\\nnp\\n.\\nmean\\n(\\ntotals\\n),\\n\\t\\nnp\\n.\\nstd\\n(\\ntotals\\n),\\n\\t\\nnp\\n.\\nmin\\n(\\ntotals\\n),\\n\\t\\nnp\\n.\\nmax\\n(\\ntotals\\n)\\n(42.125999999999998,\\t9.1237121830974033,\\t24.0,\\t68.0)\", 'Even\\twith\\t500\\ttries,\\tthis\\tpolicy\\tnever\\tmanaged\\tto\\tkeep\\tthe\\tpole\\tupright\\tfor\\tmore\\tthan\\t68\\tconsecutive\\nsteps.\\tNot\\tgreat.\\tIf\\tyou\\tlook\\tat\\tthe\\tsimulation\\tin\\tthe\\t\\nJupyter\\tnotebooks\\n,\\tyou\\twill\\tsee\\tthat\\tthe\\tcart\\toscillates\\nleft\\tand\\tright\\tmore\\tand\\tmore\\tstrongly\\tuntil\\tthe\\tpole\\ttilts\\ttoo\\tmuch.\\tLet’s\\tsee\\tif\\ta\\tneural\\tnetwork\\tcan\\tcome\\nup\\twith\\ta\\tbetter\\t\\npolicy.', 'Neural\\tNetwork\\tPolicies\\nLet’s\\t\\ncreate\\ta\\tneural\\tnetwork\\tpolicy.\\tJust\\tlike\\tthe\\tpolicy\\twe\\thardcoded\\tearlier,\\tthis\\tneural\\tnetwork\\twill\\ntake\\tan\\tobservation\\tas\\tinput,\\tand\\tit\\twill\\toutput\\tthe\\taction\\tto\\tbe\\texecuted.\\tMore\\tprecisely,\\tit\\twill\\testimate\\ta\\nprobability\\tfor\\teach\\taction,\\tand\\tthen\\twe\\twill\\tselect\\tan\\taction\\trandomly\\taccording\\tto\\tthe\\testimated\\nprobabilities\\t(see\\t\\nFigure\\t16-5\\n).\\tIn\\tthe\\tcase\\tof\\tthe\\tCartPole\\tenvironment,\\tthere\\tare\\tjust\\ttwo\\tpossible\\nactions\\t(left\\tor\\tright),\\tso\\twe\\tonly\\tneed\\tone\\toutput\\tneuron.\\tIt\\twill\\toutput\\tthe\\tprobability\\t\\np\\n\\tof\\taction\\t0\\t(left),\\nand\\tof\\tcourse\\tthe\\tprobability\\tof\\taction\\t1\\t(right)\\twill\\tbe\\t1\\t–\\t\\np\\n.\\tFor\\texample,\\tif\\tit\\toutputs\\t0.7,\\tthen\\twe\\twill\\npick\\taction\\t0\\twith\\t70%\\tprobability,\\tand\\taction\\t1\\twith\\t30%\\tprobability.\\nFigure\\t16-5.\\t\\nNeural\\tnetwork\\tpolicy\\nYou\\tmay\\twonder\\twhy\\twe\\tare\\tpicking\\ta\\trandom\\taction\\tbased\\ton\\tthe\\tprobability\\tgiven\\tby\\tthe\\tneural\\nnetwork,\\trather\\tthan\\tjust\\tpicking\\tthe\\taction\\twith\\tthe\\thighest\\tscore.\\tThis\\tapproach\\tlets\\tthe\\tagent\\tfind\\tthe', 'right\\tbalance\\tbetween\\t\\nexploring\\n\\tnew\\tactions\\tand\\t\\nexploiting\\n\\tthe\\tactions\\tthat\\tare\\tknown\\tto\\twork\\twell.\\nHere’s\\tan\\tanalogy:\\tsuppose\\tyou\\tgo\\tto\\ta\\trestaurant\\tfor\\tthe\\tfirst\\ttime,\\tand\\tall\\tthe\\tdishes\\tlook\\tequally\\nappealing\\tso\\tyou\\trandomly\\tpick\\tone.\\tIf\\tit\\tturns\\tout\\tto\\tbe\\tgood,\\tyou\\tcan\\tincrease\\tthe\\tprobability\\tto\\torder\\tit\\nnext\\ttime,\\tbut\\tyou\\tshouldn’t\\tincrease\\tthat\\tprobability\\tup\\tto\\t100%,\\tor\\telse\\tyou\\twill\\tnever\\ttry\\tout\\tthe\\tother\\ndishes,\\tsome\\tof\\twhich\\tmay\\tbe\\teven\\tbetter\\tthan\\tthe\\tone\\tyou\\ttried.\\nAlso\\tnote\\tthat\\tin\\tthis\\tparticular\\tenvironment,\\tthe\\tpast\\tactions\\tand\\tobservations\\tcan\\tsafely\\tbe\\tignored,', \"since\\teach\\tobservation\\tcontains\\tthe\\tenvironment’s\\tfull\\tstate.\\tIf\\tthere\\twere\\tsome\\thidden\\tstate,\\tthen\\tyou\\tmay\\nneed\\tto\\tconsider\\tpast\\tactions\\tand\\tobservations\\tas\\twell.\\tFor\\texample,\\tif\\tthe\\tenvironment\\tonly\\trevealed\\tthe\\nposition\\tof\\tthe\\tcart\\tbut\\tnot\\tits\\tvelocity,\\tyou\\twould\\thave\\tto\\tconsider\\tnot\\tonly\\tthe\\tcurrent\\tobservation\\tbut\\nalso\\tthe\\tprevious\\tobservation\\tin\\torder\\tto\\testimate\\tthe\\tcurrent\\tvelocity.\\tAnother\\texample\\tis\\twhen\\tthe\\nobservations\\tare\\tnoisy;\\tin\\tthat\\tcase,\\tyou\\tgenerally\\twant\\tto\\tuse\\tthe\\tpast\\tfew\\tobservations\\tto\\testimate\\tthe\\nmost\\tlikely\\tcurrent\\tstate.\\tThe\\tCartPole\\tproblem\\tis\\tthus\\tas\\tsimple\\tas\\tcan\\tbe;\\tthe\\tobservations\\tare\\tnoise-\\nfree\\tand\\tthey\\tcontain\\tthe\\tenvironment’s\\tfull\\tstate.\\nHere\\tis\\tthe\\tcode\\tto\\tbuild\\tthis\\tneural\\tnetwork\\tpolicy\\tusing\\t\\nTensorFlow:\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\n#\\t1.\\tSpecify\\tthe\\tneural\\tnetwork\\tarchitecture\\nn_inputs\\n\\t\\n=\\n\\t\\n4\\n\\t\\t\\n#\\t==\\tenv.observation_space.shape[0]\\nn_hidden\\n\\t\\n=\\n\\t\\n4\\n\\t\\t\\n#\\tit's\\ta\\tsimple\\ttask,\\twe\\tdon't\\tneed\\tmore\\thidden\\tneurons\\nn_outputs\\n\\t\\n=\\n\\t\\n1\", '=\\n\\t\\n1\\n\\t\\n#\\tonly\\toutputs\\tthe\\tprobability\\tof\\taccelerating\\tleft\\ninitializer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\n#\\t2.\\tBuild\\tthe\\tneural\\tnetwork\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nhidden\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid\\n(\\nlogits\\n)\\n#\\t3.\\tSelect\\ta\\trandom\\taction\\tbased\\ton\\tthe\\testimated\\tprobabilities\\np_left_and_right\\n\\t\\n=\\n\\t\\ntf\\n.\\nconcat\\n(\\naxis\\n=\\n1\\n,\\n\\t\\nvalues\\n=\\n[\\noutputs\\n,\\n\\t\\n1\\n\\t\\n-\\n\\t\\noutputs\\n])\\naction\\n\\t\\n=\\n\\t\\ntf\\n.\\nmultinomial\\n(\\ntf\\n.\\nlog\\n(\\np_left_and_right\\n),\\n\\t\\nnum_samples\\n=\\n1\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nLet’s\\tgo\\tthrough\\tthis\\tcode:\\n1\\n.\\t\\nAfter\\tthe\\timports,\\twe\\tdefine\\tthe\\tneural\\tnetwork\\tarchitecture.\\tThe\\tnumber\\tof\\tinputs\\tis\\tthe\\tsize\\tof\\tthe', 'observation\\tspace\\t(which\\tin\\tthe\\tcase\\tof\\tthe\\tCartPole\\tis\\tfour),\\twe\\tjust\\thave\\tfour\\thidden\\tunits\\tand\\tno\\nneed\\tfor\\tmore,\\tand\\twe\\thave\\tjust\\tone\\toutput\\tprobability\\t(the\\tprobability\\tof\\tgoing\\tleft).\\n2\\n.\\t\\nNext\\twe\\tbuild\\tthe\\tneural\\tnetwork.\\tIn\\tthis\\texample,\\tit’s\\ta\\tvanilla\\tMulti-Layer\\tPerceptron,\\t\\nwith\\ta\\nsingle\\toutput.\\tNote\\tthat\\tthe\\toutput\\tlayer\\tuses\\tthe\\tlogistic\\t(sigmoid)\\tactivation\\tfunction\\tin\\torder\\tto\\noutput\\ta\\tprobability\\tfrom\\t0.0\\tto\\t1.0.\\tIf\\tthere\\twere\\tmore\\tthan\\ttwo\\tpossible\\tactions,\\tthere\\twould\\tbe\\none\\toutput\\tneuron\\tper\\taction,\\tand\\tyou\\twould\\tuse\\tthe\\tsoftmax\\tactivation\\tfunction\\tinstead.\\n3\\n.\\t\\nLastly,\\twe\\tcall\\tthe\\t\\nmultinomial()\\n\\t\\nfunction\\tto\\tpick\\ta\\trandom\\taction.\\tThis\\tfunction\\tindependently\\nsamples\\tone\\t(or\\tmore)\\tintegers,\\tgiven\\tthe\\tlog\\tprobability\\tof\\teach\\tinteger.\\tFor\\texample,\\tif\\tyou\\tcall\\tit\\nwith\\tthe\\tarray\\t\\n[np.log(0.5),\\tnp.log(0.2),\\tnp.log(0.3)]\\n\\tand\\twith\\t\\nnum_samples=5\\n,\\tthen\\tit\\nwill\\toutput\\tfive\\tintegers,\\teach\\tof\\twhich\\twill\\thave\\ta\\t50%\\tprobability\\tof\\tbeing\\t0,\\t20%\\tof\\tbeing\\t1,\\tand', '30%\\tof\\tbeing\\t2.\\tIn\\tour\\tcase\\twe\\tjust\\tneed\\tone\\tinteger\\trepresenting\\tthe\\taction\\tto\\ttake.\\tSince\\tthe\\noutputs\\n\\ttensor\\tonly\\tcontains\\tthe\\tprobability\\tof\\tgoing\\tleft,\\twe\\tmust\\tfirst\\tconcatenate\\t\\n1-outputs\\n\\tto\\tit\\nto\\thave\\ta\\ttensor\\tcontaining\\tthe\\tprobability\\tof\\tboth\\tleft\\tand\\tright\\tactions.\\tNote\\tthat\\tif\\tthere\\twere\\tmore\\nthan\\ttwo\\tpossible\\tactions,\\tthe\\tneural\\tnetwork\\twould\\thave\\tto\\toutput\\tone\\tprobability\\tper\\taction\\tso\\tyou\\nwould\\tnot\\tneed\\tthe\\tconcatenation\\tstep.\\nOkay,\\twe\\tnow\\thave\\ta\\tneural\\tnetwork\\tpolicy\\tthat\\twill\\ttake\\tobservations\\tand\\toutput\\tactions.\\tBut\\thow\\tdo\\nwe\\t\\ntrain\\tit?', 'Evaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\nIf\\twe\\t\\nknew\\twhat\\tthe\\tbest\\taction\\twas\\tat\\teach\\tstep,\\twe\\tcould\\ttrain\\tthe\\tneural\\tnetwork\\tas\\tusual,\\tby\\nminimizing\\tthe\\tcross\\tentropy\\tbetween\\tthe\\testimated\\tprobability\\tand\\tthe\\ttarget\\tprobability.\\tIt\\twould\\tjust\\tbe\\nregular\\tsupervised\\tlearning.\\tHowever,\\tin\\tReinforcement\\tLearning\\tthe\\tonly\\tguidance\\tthe\\tagent\\tgets\\tis\\nthrough\\trewards,\\tand\\trewards\\tare\\ttypically\\tsparse\\tand\\tdelayed.\\tFor\\texample,\\tif\\tthe\\tagent\\tmanages\\tto\\nbalance\\tthe\\tpole\\tfor\\t100\\tsteps,\\thow\\tcan\\tit\\tknow\\twhich\\tof\\tthe\\t100\\tactions\\tit\\ttook\\twere\\tgood,\\tand\\twhich\\tof\\nthem\\twere\\tbad?\\tAll\\tit\\tknows\\tis\\tthat\\tthe\\tpole\\tfell\\tafter\\tthe\\tlast\\taction,\\tbut\\tsurely\\tthis\\tlast\\taction\\tis\\tnot\\nentirely\\tresponsible.\\tThis\\tis\\tcalled\\tthe\\t\\ncredit\\tassignment\\tproblem\\n:\\twhen\\tthe\\tagent\\tgets\\ta\\treward,\\tit\\tis\\nhard\\tfor\\tit\\tto\\tknow\\twhich\\tactions\\tshould\\tget\\tcredited\\t(or\\tblamed)\\tfor\\tit.\\tThink\\tof\\ta\\tdog\\tthat\\tgets\\trewarded\\nhours\\tafter\\tit\\tbehaved\\twell;\\twill\\tit\\tunderstand\\twhat\\tit\\tis\\trewarded\\tfor?', 'To\\ttackle\\tthis\\tproblem,\\ta\\tcommon\\tstrategy\\tis\\tto\\tevaluate\\tan\\taction\\tbased\\ton\\tthe\\tsum\\tof\\tall\\tthe\\trewards\\tthat\\ncome\\tafter\\tit,\\tusually\\tapplying\\ta\\t\\ndiscount\\trate\\n\\t\\nr\\n\\tat\\teach\\tstep.\\tFor\\texample\\t(see\\t\\nFigure\\t16-6\\n),\\tif\\tan\\tagent\\ndecides\\tto\\tgo\\tright\\tthree\\ttimes\\tin\\ta\\trow\\tand\\tgets\\t+10\\treward\\tafter\\tthe\\tfirst\\tstep,\\t0\\tafter\\tthe\\tsecond\\tstep,\\nand\\tfinally\\t–50\\tafter\\tthe\\tthird\\tstep,\\tthen\\tassuming\\twe\\tuse\\ta\\tdiscount\\trate\\t\\nr\\n\\t=\\t0.8,\\tthe\\tfirst\\taction\\twill\\thave\\na\\ttotal\\tscore\\tof\\t10\\t+\\t\\nr\\n\\t×\\t0\\t+\\t\\nr\\n2\\n\\t×\\t(–50)\\t=\\t–22.\\tIf\\tthe\\tdiscount\\trate\\tis\\tclose\\tto\\t0,\\tthen\\tfuture\\trewards\\twon’t\\ncount\\tfor\\tmuch\\tcompared\\tto\\timmediate\\trewards.\\tConversely,\\tif\\tthe\\t\\ndiscount\\trate\\tis\\tclose\\tto\\t1,\\tthen\\nrewards\\tfar\\tinto\\tthe\\tfuture\\twill\\tcount\\talmost\\tas\\tmuch\\tas\\timmediate\\trewards.\\tTypical\\tdiscount\\trates\\tare\\n0.95\\tor\\t0.99.\\tWith\\ta\\tdiscount\\trate\\tof\\t0.95,\\trewards\\t13\\tsteps\\tinto\\tthe\\tfuture\\tcount\\troughly\\tfor\\thalf\\tas\\tmuch\\nas\\timmediate\\trewards\\t(since\\t0.95\\n13\\n\\t≈\\t0.5),\\twhile\\twith\\ta\\tdiscount\\trate\\tof\\t0.99,\\trewards\\t69\\tsteps\\tinto\\tthe', 'future\\tcount\\tfor\\thalf\\tas\\tmuch\\tas\\timmediate\\trewards.\\tIn\\tthe\\tCartPole\\tenvironment,\\tactions\\thave\\tfairly\\nshort-term\\teffects,\\tso\\tchoosing\\ta\\tdiscount\\trate\\tof\\t0.95\\tseems\\treasonable\\n.\\nFigure\\t16-6.\\t\\nDiscounted\\trewards\\nOf\\tcourse,\\ta\\tgood\\taction\\tmay\\tbe\\tfollowed\\tby\\tseveral\\tbad\\tactions\\tthat\\tcause\\tthe\\tpole\\tto\\tfall\\tquickly,\\nresulting\\tin\\tthe\\tgood\\taction\\tgetting\\ta\\tlow\\tscore\\t(similarly,\\ta\\tgood\\tactor\\tmay\\tsometimes\\tstar\\tin\\ta\\tterrible\\nmovie).\\tHowever,\\tif\\twe\\tplay\\tthe\\tgame\\tenough\\ttimes,\\ton\\taverage\\tgood\\tactions\\twill\\tget\\ta\\tbetter\\tscore\\tthan\\nbad\\tones.\\tSo,\\tto\\tget\\tfairly\\treliable\\taction\\tscores,\\twe\\tmust\\trun\\tmany\\t\\nepisodes\\tand\\tnormalize\\tall\\tthe\\taction', 'scores\\t(by\\tsubtracting\\tthe\\tmean\\tand\\tdividing\\tby\\tthe\\tstandard\\tdeviation).\\tAfter\\tthat,\\twe\\tcan\\treasonably\\nassume\\tthat\\tactions\\twith\\ta\\tnegative\\tscore\\twere\\tbad\\twhile\\tactions\\twith\\ta\\tpositive\\tscore\\twere\\tgood.\\nPerfect\\t—\\tnow\\tthat\\twe\\thave\\ta\\tway\\tto\\tevaluate\\teach\\taction,\\twe\\tare\\tready\\tto\\ttrain\\tour\\tfirst\\tagent\\tusing\\npolicy\\tgradients.\\t\\nLet’s\\tsee\\thow.', 'Policy\\tGradients\\nAs\\t\\ndiscussed\\tearlier,\\tPG\\talgorithms\\toptimize\\tthe\\tparameters\\tof\\ta\\tpolicy\\tby\\tfollowing\\tthe\\tgradients\\ntoward\\thigher\\trewards.\\tOne\\tpopular\\tclass\\tof\\tPG\\talgorithms,\\tcalled\\t\\nREINFORCE\\talgorithms\\n,\\t\\nwas\\nintroduced\\tback\\tin\\t1992\\n9\\n\\tby\\tRonald\\tWilliams.\\tHere\\tis\\tone\\tcommon\\tvariant:\\n1\\n.\\t\\nFirst,\\tlet\\tthe\\tneural\\tnetwork\\tpolicy\\tplay\\tthe\\tgame\\tseveral\\ttimes\\tand\\tat\\teach\\tstep\\tcompute\\tthe\\ngradients\\tthat\\twould\\tmake\\tthe\\tchosen\\taction\\teven\\tmore\\tlikely,\\tbut\\tdon’t\\tapply\\tthese\\tgradients\\tyet.\\n2\\n.\\t\\nOnce\\tyou\\thave\\trun\\tseveral\\tepisodes,\\tcompute\\teach\\taction’s\\tscore\\t(using\\tthe\\tmethod\\tdescribed\\tin\\tthe\\nprevious\\tparagraph).\\n3\\n.\\t\\nIf\\tan\\taction’s\\tscore\\tis\\tpositive,\\tit\\tmeans\\tthat\\tthe\\taction\\twas\\tgood\\tand\\tyou\\twant\\tto\\tapply\\tthe\\tgradients\\ncomputed\\tearlier\\tto\\tmake\\tthe\\taction\\teven\\tmore\\tlikely\\tto\\tbe\\tchosen\\tin\\tthe\\tfuture.\\tHowever,\\tif\\tthe\\nscore\\tis\\tnegative,\\tit\\tmeans\\tthe\\taction\\twas\\tbad\\tand\\tyou\\twant\\tto\\tapply\\tthe\\topposite\\tgradients\\tto\\tmake\\nthis\\taction\\tslightly\\t\\nless', 'less\\n\\tlikely\\tin\\tthe\\tfuture.\\tThe\\tsolution\\tis\\tsimply\\tto\\tmultiply\\teach\\tgradient\\tvector\\tby\\nthe\\tcorresponding\\taction’s\\tscore.\\n4\\n.\\t\\nFinally,\\tcompute\\tthe\\tmean\\tof\\tall\\tthe\\tresulting\\tgradient\\tvectors,\\tand\\tuse\\tit\\tto\\tperform\\ta\\tGradient\\nDescent\\tstep.\\nLet’s\\timplement\\tthis\\talgorithm\\tusing\\tTensorFlow.\\tWe\\twill\\ttrain\\tthe\\tneural\\tnetwork\\tpolicy\\twe\\tbuilt\\tearlier\\nso\\tthat\\tit\\tlearns\\tto\\tbalance\\tthe\\tpole\\ton\\tthe\\tcart.\\tLet’s\\tstart\\tby\\tcompleting\\tthe\\tconstruction\\tphase\\twe\\tcoded\\nearlier\\tto\\tadd\\tthe\\ttarget\\tprobability,\\tthe\\t\\ncost\\tfunction,\\tand\\tthe\\ttraining\\toperation.\\tSince\\twe\\tare\\tacting\\tas\\nthough\\tthe\\tchosen\\taction\\tis\\tthe\\tbest\\tpossible\\taction,\\tthe\\ttarget\\tprobability\\tmust\\tbe\\t1.0\\tif\\tthe\\tchosen\\taction\\nis\\taction\\t0\\t(left)\\tand\\t0.0\\tif\\tit\\tis\\t\\naction\\t1\\t(right):\\ny\\n\\t\\n=\\n\\t\\n1.\\n\\t\\n-\\n\\t\\ntf\\n.\\nto_float\\n(\\naction\\n)\\nNow\\tthat\\twe\\thave\\ta\\ttarget\\tprobability,\\twe\\tcan\\tdefine\\tthe\\tcost\\tfunction\\t(cross\\tentropy)\\t\\nand\\tcompute\\tthe\\ngradients:\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\ncross_entropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid_cross_entropy_with_logits\\n(\\nlabels\\n=\\ny\\n,', '=\\ny\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlogits\\n=\\nlogits\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ngrads_and_vars\\n\\t\\n=\\n\\t\\noptimizer\\n.\\ncompute_gradients\\n(\\ncross_entropy\\n)\\nNote\\tthat\\twe\\tare\\tcalling\\tthe\\toptimizer’s\\t\\ncompute_gradients()\\n\\t\\nmethod\\tinstead\\tof\\tthe\\t\\nminimize()\\nmethod.\\tThis\\tis\\tbecause\\twe\\twant\\tto\\ttweak\\tthe\\tgradients\\tbefore\\twe\\tapply\\tthem.\\n10\\n\\tThe\\ncompute_gradients()\\n\\tmethod\\treturns\\ta\\tlist\\tof\\tgradient\\tvector/variable\\tpairs\\t(one\\tpair\\tper\\ttrainable\\nvariable).\\tLet’s\\tput\\tall\\tthe\\tgradients\\tin\\ta\\tlist,\\tto\\tmake\\tit\\tmore\\tconvenient\\tto\\tobtain\\ttheir\\tvalues:\\ngradients\\n\\t\\n=\\n\\t\\n[\\ngrad\\n\\t\\nfor\\n\\t\\ngrad\\n,\\n\\t\\nvariable\\n\\t\\nin\\n\\t\\ngrads_and_vars\\n]\\nOkay,\\tnow\\tcomes\\tthe\\ttricky\\tpart.\\tDuring\\tthe\\texecution\\tphase,\\tthe\\talgorithm\\twill\\trun\\tthe\\tpolicy\\tand\\tat\\teach', 'step\\tit\\twill\\tevaluate\\tthese\\tgradient\\ttensors\\tand\\tstore\\ttheir\\tvalues.\\t\\nAfter\\ta\\tnumber\\tof\\tepisodes\\tit\\twill\\ttweak\\nthese\\tgradients\\tas\\texplained\\tearlier\\t(i.e.,\\tmultiply\\tthem\\tby\\tthe\\taction\\tscores\\tand\\tnormalize\\tthem)\\tand\\ncompute\\tthe\\tmean\\tof\\tthe\\ttweaked\\tgradients.\\tNext,\\tit\\twill\\tneed\\tto\\tfeed\\tthe\\tresulting\\tgradients\\tback\\tto\\tthe\\noptimizer\\tso\\tthat\\tit\\tcan\\tperform\\tan\\toptimization\\tstep.\\tThis\\tmeans\\twe\\tneed\\tone\\tplaceholder\\tper\\tgradient\\nvector.\\tMoreover,\\twe\\tmust\\tcreate\\tthe\\toperation\\tthat\\twill\\tapply\\tthe\\tupdated\\tgradients.\\tFor\\tthis\\twe\\twill\\ncall\\tthe\\toptimizer’s\\t\\napply_gradients()\\n\\t\\nfunction,\\twhich\\ttakes\\ta\\tlist\\tof\\tgradient\\tvector/variable\\tpairs.\\nInstead\\tof\\tgiving\\tit\\tthe\\toriginal\\tgradient\\tvectors,\\twe\\twill\\tgive\\tit\\ta\\tlist\\tcontaining\\tthe\\tupdated\\tgradients\\n(i.e.,\\tthe\\tones\\tfed\\tthrough\\tthe\\tgradient\\tplaceholders):\\ngradient_placeholders\\n\\t\\n=\\n\\t\\n[]\\ngrads_and_vars_feed\\n\\t\\n=\\n\\t\\n[]\\nfor\\n\\t\\ngrad\\n,\\n\\t\\nvariable\\n\\t\\nin\\n\\t\\ngrads_and_vars\\n:\\n\\t\\t\\t\\t\\ngradient_placeholder\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\ngrad\\n.\\nget_shape\\n())', '())\\n\\t\\t\\t\\t\\ngradient_placeholders\\n.\\nappend\\n(\\ngradient_placeholder\\n)\\n\\t\\t\\t\\t\\ngrads_and_vars_feed\\n.\\nappend\\n((\\ngradient_placeholder\\n,\\n\\t\\nvariable\\n))\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\napply_gradients\\n(\\ngrads_and_vars_feed\\n)\\nLet’s\\tstep\\tback\\tand\\ttake\\ta\\tlook\\tat\\tthe\\tfull\\t\\nconstruction\\tphase:\\nn_inputs\\n\\t\\n=\\n\\t\\n4\\nn_hidden\\n\\t\\n=\\n\\t\\n4\\nn_outputs\\n\\t\\n=\\n\\t\\n1\\ninitializer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\nlearning_rate\\n\\t\\n=\\n\\t\\n0.01\\nX\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\nn_inputs\\n])\\nhidden\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nX\\n,\\n\\t\\nn_hidden\\n,\\n\\t\\nactivation\\n=\\ntf\\n.\\nnn\\n.\\nelu\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\nlogits\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid\\n(\\nlogits\\n)\\np_left_and_right\\n\\t\\n=\\n\\t\\ntf\\n.\\nconcat\\n(\\naxis\\n=\\n1\\n,\\n\\t\\nvalues\\n=\\n[\\noutputs\\n,\\n\\t\\n1\\n\\t\\n-\\n\\t\\noutputs\\n])\\naction\\n\\t\\n=\\n\\t\\ntf\\n.\\nmultinomial\\n(\\ntf\\n.\\nlog\\n(\\np_left_and_right\\n),\\n\\t\\nnum_samples\\n=\\n1\\n)\\ny\\n\\t\\n=\\n\\t\\n1.\\n\\t\\n-', '1.\\n\\t\\n-\\n\\t\\ntf\\n.\\nto_float\\n(\\naction\\n)\\ncross_entropy\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nsigmoid_cross_entropy_with_logits\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nlabels\\n=\\ny\\n,\\n\\t\\nlogits\\n=\\nlogits\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ngrads_and_vars\\n\\t\\n=\\n\\t\\noptimizer\\n.\\ncompute_gradients\\n(\\ncross_entropy\\n)\\ngradients\\n\\t\\n=\\n\\t\\n[\\ngrad\\n\\t\\nfor\\n\\t\\ngrad\\n,\\n\\t\\nvariable\\n\\t\\nin\\n\\t\\ngrads_and_vars\\n]\\ngradient_placeholders\\n\\t\\n=\\n\\t\\n[]\\ngrads_and_vars_feed\\n\\t\\n=\\n\\t\\n[]\\nfor\\n\\t\\ngrad\\n,\\n\\t\\nvariable\\n\\t\\nin\\n\\t\\ngrads_and_vars\\n:\\n\\t\\t\\t\\t\\ngradient_placeholder\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\ngrad\\n.\\nget_shape\\n())\\n\\t\\t\\t\\t\\ngradient_placeholders\\n.\\nappend\\n(\\ngradient_placeholder\\n)\\n\\t\\t\\t\\t\\ngrads_and_vars_feed\\n.\\nappend\\n((\\ngradient_placeholder\\n,\\n\\t\\nvariable\\n))\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\napply_gradients\\n(\\ngrads_and_vars_feed\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\nOn\\tto\\tthe\\texecution\\tphase!\\tWe\\twill\\tneed\\ta\\tcouple\\tof\\tfunctions\\tto\\tcompute\\tthe\\ttotal\\tdiscounted\\trewards,', 'given\\tthe\\traw\\trewards,\\tand\\tto\\tnormalize\\tthe\\tresults\\tacross\\tmultiple\\t\\nepisodes:\\ndef\\n\\t\\ndiscount_rewards\\n(\\nrewards\\n,\\n\\t\\ndiscount_rate\\n):\\n\\t\\t\\t\\t\\ndiscounted_rewards\\n\\t\\n=\\n\\t\\nnp\\n.\\nempty\\n(\\nlen\\n(\\nrewards\\n))\\n\\t\\t\\t\\t\\ncumulative_rewards\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\t\\t\\nfor\\n\\t\\nstep\\n\\t\\nin\\n\\t\\nreversed\\n(\\nrange\\n(\\nlen\\n(\\nrewards\\n))):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ncumulative_rewards\\n\\t\\n=\\n\\t\\nrewards\\n[\\nstep\\n]\\n\\t\\n+\\n\\t\\ncumulative_rewards\\n\\t\\n*\\n\\t\\ndiscount_rate\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ndiscounted_rewards\\n[\\nstep\\n]\\n\\t\\n=\\n\\t\\ncumulative_rewards\\n\\t\\t\\t\\t\\nreturn\\n\\t\\ndiscounted_rewards', 'def\\n\\t\\ndiscount_and_normalize_rewards\\n(\\nall_rewards\\n,\\n\\t\\ndiscount_rate\\n):\\n\\t\\t\\t\\t\\nall_discounted_rewards\\n\\t\\n=\\n\\t\\n[\\ndiscount_rewards\\n(\\nrewards\\n,\\n\\t\\ndiscount_rate\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nrewards\\n\\t\\nin\\n\\t\\nall_rewards\\n]\\n\\t\\t\\t\\t\\nflat_rewards\\n\\t\\n=\\n\\t\\nnp\\n.\\nconcatenate\\n(\\nall_discounted_rewards\\n)\\n\\t\\t\\t\\t\\nreward_mean\\n\\t\\n=\\n\\t\\nflat_rewards\\n.\\nmean\\n()\\n\\t\\t\\t\\t\\nreward_std\\n\\t\\n=\\n\\t\\nflat_rewards\\n.\\nstd\\n()\\n\\t\\t\\t\\t\\nreturn\\n\\t\\n[(\\ndiscounted_rewards\\n\\t\\n-\\n\\t\\nreward_mean\\n)\\n/\\nreward_std\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ndiscounted_rewards\\n\\t\\nin\\n\\t\\nall_discounted_rewards\\n]\\nLet’s\\tcheck\\tthat\\tthis\\tworks:\\n>>>\\t\\ndiscount_rewards\\n([\\n10\\n,\\n\\t\\n0\\n,\\n\\t\\n-\\n50\\n],\\n\\t\\ndiscount_rate\\n=\\n0.8\\n)\\narray([-22.,\\t-40.,\\t-50.])\\n>>>\\t\\ndiscount_and_normalize_rewards\\n([[\\n10\\n,\\n\\t\\n0\\n,\\n\\t\\n-\\n50\\n],\\n\\t\\n[\\n10\\n,\\n\\t\\n20\\n]],\\n\\t\\ndiscount_rate\\n=\\n0.8\\n)\\n[array([-0.28435071,\\t-0.86597718,\\t-1.18910299]),\\n\\tarray([\\t1.26665318,\\t\\t1.0727777\\t])]\\nThe\\tcall\\tto\\t\\ndiscount_rewards()\\n\\treturns\\texactly\\twhat\\twe\\texpect\\t(see\\t\\nFigure\\t16-6\\n).\\tYou\\tcan\\tverify\\tthat\\nthe\\tfunction\\t\\ndiscount_and_normalize_rewards()', 'does\\tindeed\\treturn\\tthe\\tnormalized\\tscores\\tfor\\teach\\naction\\tin\\tboth\\tepisodes.\\tNotice\\tthat\\tthe\\tfirst\\tepisode\\twas\\tmuch\\tworse\\tthan\\tthe\\tsecond,\\tso\\tits\\tnormalized\\nscores\\tare\\tall\\tnegative;\\tall\\tactions\\tfrom\\tthe\\tfirst\\tepisode\\twould\\tbe\\tconsidered\\tbad,\\tand\\tconversely\\tall\\nactions\\tfrom\\tthe\\tsecond\\tepisode\\twould\\tbe\\tconsidered\\tgood.\\nWe\\tnow\\thave\\tall\\twe\\tneed\\tto\\ttrain\\tthe\\tpolicy:\\nn_iterations\\n\\t\\n=\\n\\t\\n250\\n\\t\\t\\t\\t\\t\\t\\n#\\tnumber\\tof\\ttraining\\titerations\\nn_max_steps\\n\\t\\n=\\n\\t\\n1000\\n\\t\\t\\t\\t\\t\\t\\n#\\tmax\\tsteps\\tper\\tepisode\\nn_games_per_update\\n\\t\\n=\\n\\t\\n10\\n\\t\\n#\\ttrain\\tthe\\tpolicy\\tevery\\t10\\tepisodes\\nsave_iterations\\n\\t\\n=\\n\\t\\n10\\n\\t\\t\\t\\t\\n#\\tsave\\tthe\\tmodel\\tevery\\t10\\ttraining\\titerations\\ndiscount_rate\\n\\t\\n=\\n\\t\\n0.95\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nall_rewards\\n\\t\\n=\\n\\t\\n[]\\n\\t\\t\\t\\t\\n#\\tall\\tsequences\\tof\\traw\\trewards\\tfor\\teach\\tepisode\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nall_gradients\\n\\t\\n=\\n\\t\\n[]\\n\\t\\t\\n#\\tgradients\\tsaved\\tat\\teach\\tstep\\tof\\teach\\tepisode\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ngame\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_games_per_update\\n):', 'current_rewards\\n\\t\\n=\\n\\t\\n[]\\n\\t\\t\\t\\n#\\tall\\traw\\trewards\\tfrom\\tthe\\tcurrent\\tepisode\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncurrent_gradients\\n\\t\\n=\\n\\t\\n[]\\n\\t\\n#\\tall\\tgradients\\tfrom\\tthe\\tcurrent\\tepisode\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n\\t\\n=\\n\\t\\nenv\\n.\\nreset\\n()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nstep\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_max_steps\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\naction_val\\n,\\n\\t\\ngradients_val\\n\\t\\n=\\n\\t\\nsess\\n.\\nrun\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\naction\\n,\\n\\t\\ngradients\\n],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n=\\n{\\nX\\n:\\n\\t\\nobs\\n.\\nreshape\\n(\\n1\\n,\\n\\t\\nn_inputs\\n)})\\n\\t\\n#\\tone\\tobs\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n,\\n\\t\\nreward\\n,\\n\\t\\ndone\\n,\\n\\t\\ninfo\\n\\t\\n=\\n\\t\\nenv\\n.\\nstep\\n(\\naction_val\\n[\\n0\\n][\\n0\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncurrent_rewards\\n.\\nappend\\n(\\nreward\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncurrent_gradients\\n.\\nappend\\n(\\ngradients_val\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\ndone\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbreak\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nall_rewards\\n.\\nappend\\n(\\ncurrent_rewards\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nall_gradients\\n.\\nappend\\n(\\ncurrent_gradients\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tAt\\tthis\\tpoint\\twe\\thave\\trun\\tthe\\tpolicy\\tfor\\t10\\tepisodes,\\tand\\twe\\tare\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tready\\tfor\\ta\\tpolicy\\tupdate\\tusing\\tthe\\talgorithm\\tdescribed\\tearlier.', 'all_rewards\\n\\t\\n=\\n\\t\\ndiscount_and_normalize_rewards\\n(\\nall_rewards\\n,\\ndiscount_rate\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n\\t\\n=\\n\\t\\n{}\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nvar_index\\n,\\n\\t\\ngrad_placeholder\\n\\t\\nin\\n\\t\\nenumerate\\n(\\ngradient_placeholders\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tmultiply\\tthe\\tgradients\\tby\\tthe\\taction\\tscores,\\tand\\tcompute\\tthe\\tmean\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nmean_gradients\\n\\t\\n=\\n\\t\\nnp\\n.\\nmean\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n[\\nreward\\n\\t\\n*\\n\\t\\nall_gradients\\n[\\ngame_index\\n][\\nstep\\n][\\nvar_index\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ngame_index\\n,\\n\\t\\nrewards\\n\\t\\nin\\n\\t\\nenumerate\\n(\\nall_rewards\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nstep\\n,\\n\\t\\nreward\\n\\t\\nin\\n\\t\\nenumerate\\n(\\nrewards\\n)],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\naxis\\n=\\n0\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfeed_dict\\n[\\ngrad_placeholder\\n]\\n\\t\\n=\\n\\t\\nmean_gradients\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsess\\n.\\nrun\\n(\\ntraining_op\\n,\\n\\t\\nfeed_dict\\n=\\nfeed_dict\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\niteration\\n\\t\\n%\\n\\t\\nsave_iterations\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\n\"./my_policy_net_pg.ckpt\"\\n)', 'Each\\ttraining\\titeration\\tstarts\\tby\\trunning\\tthe\\tpolicy\\tfor\\t10\\tepisodes\\t(with\\tmaximum\\t1,000\\tsteps\\tper\\nepisode,\\tto\\tavoid\\trunning\\tforever).\\tAt\\teach\\tstep,\\twe\\talso\\tcompute\\tthe\\tgradients,\\tpretending\\tthat\\tthe\\nchosen\\taction\\twas\\tthe\\tbest.\\tAfter\\tthese\\t10\\tepisodes\\thave\\tbeen\\trun,\\twe\\tcompute\\tthe\\taction\\tscores\\tusing\\tthe\\ndiscount_and_normalize_rewards()\\n\\tfunction;\\twe\\tgo\\tthrough\\teach\\ttrainable\\tvariable,\\tacross\\tall\\nepisodes\\tand\\tall\\tsteps,\\tto\\tmultiply\\teach\\tgradient\\tvector\\tby\\tits\\tcorresponding\\taction\\tscore;\\tand\\twe\\ncompute\\tthe\\tmean\\tof\\tthe\\tresulting\\tgradients.\\tFinally,\\twe\\trun\\tthe\\ttraining\\toperation,\\tfeeding\\tit\\tthese\\tmean\\ngradients\\t(one\\tper\\ttrainable\\tvariable).\\tWe\\talso\\tsave\\tthe\\tmodel\\tevery\\t10\\t\\ntraining\\toperations.\\nAnd\\twe’re\\tdone!\\tThis\\tcode\\twill\\ttrain\\tthe\\tneural\\tnetwork\\tpolicy,\\tand\\tit\\twill\\tsuccessfully\\tlearn\\tto\\tbalance\\nthe\\tpole\\ton\\tthe\\tcart\\t(you\\tcan\\ttry\\tit\\tout\\tin\\tthe\\tJupyter\\tnotebooks).\\tNote\\tthat\\tthere\\tare\\tactually\\ttwo\\tways\\tthe', 'agent\\tcan\\tlose\\tthe\\tgame:\\teither\\tthe\\tpole\\tcan\\ttilt\\ttoo\\tmuch,\\tor\\tthe\\tcart\\tcan\\tgo\\tcompletely\\toff\\tthe\\tscreen.\\nWith\\t250\\ttraining\\titerations,\\tthe\\tpolicy\\tlearns\\tto\\tbalance\\tthe\\tpole\\tquite\\twell,\\tbut\\tit\\tis\\tnot\\tyet\\tgood\\tenough\\nat\\tavoiding\\tgoing\\toff\\tthe\\tscreen.\\tA\\tfew\\thundred\\tmore\\ttraining\\titerations\\twill\\tfix\\tthat.\\nTIP\\nResearchers\\ttry\\tto\\tfind\\talgorithms\\tthat\\twork\\twell\\teven\\twhen\\tthe\\tagent\\tinitially\\tknows\\tnothing\\tabout\\tthe\\tenvironment.\\tHowever,\\nunless\\tyou\\tare\\twriting\\ta\\tpaper,\\tyou\\tshould\\tinject\\tas\\tmuch\\tprior\\tknowledge\\tas\\tpossible\\tinto\\tthe\\tagent,\\tas\\tit\\twill\\tspeed\\tup\\ttraining\\ndramatically.\\tFor\\texample,\\tyou\\tcould\\tadd\\tnegative\\trewards\\tproportional\\tto\\tthe\\tdistance\\tfrom\\tthe\\tcenter\\tof\\tthe\\tscreen,\\tand\\tto\\tthe\\npole’s\\tangle.\\tAlso,\\tif\\tyou\\talready\\thave\\ta\\treasonably\\tgood\\tpolicy\\t(e.g.,\\thardcoded),\\tyou\\tmay\\twant\\tto\\ttrain\\tthe\\tneural\\tnetwork\\tto\\nimitate\\tit\\tbefore\\tusing\\tpolicy\\tgradients\\tto\\timprove\\tit.\\nDespite\\tits\\trelative\\tsimplicity,\\tthis\\talgorithm\\tis\\tquite\\tpowerful.\\tYou\\tcan\\tuse\\tit\\tto\\ttackle\\tmuch\\tharder', 'problems\\tthan\\tbalancing\\ta\\tpole\\ton\\ta\\tcart.\\tIn\\tfact,\\t\\nAlphaGo\\twas\\tbased\\ton\\ta\\tsimilar\\tPG\\talgorithm\\t(plus\\nMonte\\tCarlo\\tTree\\tSearch\\n,\\t\\nwhich\\tis\\tbeyond\\tthe\\tscope\\tof\\tthis\\tbook).\\nWe\\twill\\tnow\\tlook\\tat\\tanother\\tpopular\\tfamily\\tof\\talgorithms.\\tWhereas\\tPG\\talgorithms\\tdirectly\\ttry\\tto\\toptimize\\nthe\\tpolicy\\tto\\tincrease\\trewards,\\tthe\\talgorithms\\twe\\twill\\tlook\\tat\\tnow\\tare\\tless\\tdirect:\\tthe\\tagent\\tlearns\\tto\\nestimate\\tthe\\texpected\\tsum\\tof\\tdiscounted\\tfuture\\trewards\\tfor\\teach\\tstate,\\tor\\tthe\\texpected\\tsum\\tof\\tdiscounted\\nfuture\\trewards\\tfor\\teach\\taction\\tin\\teach\\tstate,\\tthen\\tuses\\tthis\\tknowledge\\tto\\tdecide\\thow\\tto\\tact.\\tTo\\tunderstand\\nthese\\talgorithms,\\t\\nwe\\tmust\\tfirst\\tintroduce\\t\\nMarkov\\tdecision\\tprocesses\\n\\t(MDP).', 'Markov\\tDecision\\tProcesses\\nIn\\tthe\\t\\nearly\\t20\\nth\\n\\tcentury,\\tthe\\tmathematician\\tAndrey\\tMarkov\\tstudied\\tstochastic\\tprocesses\\twith\\tno\\tmemory,\\ncalled\\t\\nMarkov\\tchains\\n.\\t\\nSuch\\ta\\tprocess\\thas\\ta\\tfixed\\tnumber\\tof\\tstates,\\tand\\tit\\trandomly\\tevolves\\tfrom\\tone\\nstate\\tto\\tanother\\tat\\teach\\tstep.\\tThe\\tprobability\\tfor\\tit\\tto\\tevolve\\tfrom\\ta\\tstate\\t\\ns\\n\\tto\\ta\\tstate\\t\\ns\\n′\\tis\\tfixed,\\tand\\tit\\ndepends\\tonly\\ton\\tthe\\tpair\\t(\\ns\\n,\\ns\\n′),\\tnot\\ton\\tpast\\tstates\\t(the\\tsystem\\thas\\tno\\tmemory).\\nFigure\\t16-7\\n\\tshows\\tan\\texample\\tof\\ta\\tMarkov\\tchain\\twith\\tfour\\tstates.\\tSuppose\\tthat\\tthe\\tprocess\\tstarts\\tin\\tstate\\ns\\n0\\n,\\tand\\tthere\\tis\\ta\\t70%\\tchance\\tthat\\tit\\twill\\tremain\\tin\\tthat\\tstate\\tat\\tthe\\tnext\\tstep.\\tEventually\\tit\\tis\\tbound\\tto\\nleave\\tthat\\tstate\\tand\\tnever\\tcome\\tback\\tsince\\tno\\tother\\tstate\\tpoints\\tback\\tto\\t\\ns\\n0\\n.\\tIf\\tit\\tgoes\\tto\\tstate\\t\\ns\\n1\\n,\\tit\\twill\\tthen\\nmost\\tlikely\\tgo\\tto\\tstate\\t\\ns\\n2\\n\\t(90%\\tprobability),\\tthen\\timmediately\\tback\\tto\\tstate\\t\\ns\\n1\\n\\t(with\\t100%\\tprobability).\\tIt\\nmay\\talternate\\ta\\tnumber\\tof\\ttimes\\tbetween\\tthese\\ttwo\\tstates,\\tbut\\teventually\\tit\\twill\\tfall\\tinto\\tstate\\t\\ns\\n3\\n\\tand', 's\\n3\\n\\tand\\nremain\\tthere\\tforever\\t(this\\tis\\ta\\t\\nterminal\\tstate\\n).\\tMarkov\\tchains\\tcan\\thave\\tvery\\tdifferent\\tdynamics,\\tand\\tthey\\nare\\theavily\\tused\\tin\\tthermodynamics,\\tchemistry,\\tstatistics,\\tand\\tmuch\\tmore.\\nFigure\\t16-7.\\t\\nExample\\tof\\ta\\tMarkov\\tchain\\nMarkov\\tdecision\\tprocesses\\twere\\t\\nfirst\\tdescribed\\tin\\tthe\\t1950s\\tby\\tRichard\\tBellman\\n.\\n11\\n\\tThey\\tresemble\\nMarkov\\tchains\\tbut\\twith\\ta\\ttwist:\\tat\\teach\\tstep,\\tan\\tagent\\tcan\\tchoose\\tone\\tof\\tseveral\\tpossible\\tactions,\\tand\\tthe\\ntransition\\tprobabilities\\tdepend\\ton\\tthe\\tchosen\\taction.\\tMoreover,\\tsome\\tstate\\ttransitions\\treturn\\tsome\\treward\\n(positive\\tor\\tnegative),\\tand\\tthe\\tagent’s\\tgoal\\tis\\tto\\tfind\\ta\\tpolicy\\tthat\\twill\\tmaximize\\trewards\\tover\\ttime.\\nFor\\texample,\\tthe\\tMDP\\trepresented\\tin\\t\\nFigure\\t16-8\\n\\thas\\tthree\\tstates\\tand\\tup\\tto\\tthree\\tpossible\\tdiscrete\\nactions\\tat\\teach\\tstep.\\tIf\\tit\\tstarts\\tin\\tstate\\t\\ns\\n0\\n,\\tthe\\tagent\\tcan\\tchoose\\tbetween\\tactions\\t\\na\\n0\\n,\\t\\na\\n1\\n,\\tor\\t\\na\\n2\\n.\\tIf\\tit\\tchooses\\naction\\t\\na\\n1\\n,\\tit\\tjust\\tremains\\tin\\tstate\\t\\ns\\n0\\n\\twith\\tcertainty,\\tand\\twithout\\tany\\treward.\\tIt\\tcan\\tthus\\tdecide\\tto\\tstay\\tthere', 'forever\\tif\\tit\\twants.\\tBut\\tif\\tit\\tchooses\\taction\\t\\na\\n0\\n,\\tit\\thas\\ta\\t70%\\tprobability\\tof\\tgaining\\ta\\treward\\tof\\t+10,\\tand\\nremaining\\tin\\tstate\\t\\ns\\n0\\n.\\tIt\\tcan\\tthen\\ttry\\tagain\\tand\\tagain\\tto\\tgain\\tas\\tmuch\\treward\\tas\\tpossible.\\tBut\\tat\\tone\\tpoint\\tit', 'is\\tgoing\\tto\\tend\\tup\\tinstead\\tin\\tstate\\t\\ns\\n1\\n.\\tIn\\tstate\\t\\ns\\n1\\n\\tit\\thas\\tonly\\ttwo\\tpossible\\tactions:\\t\\na\\n0\\n\\tor\\t\\na\\n2\\n.\\tIt\\tcan\\tchoose\\tto\\nstay\\tput\\tby\\trepeatedly\\tchoosing\\taction\\t\\na\\n0\\n,\\tor\\tit\\tcan\\tchoose\\tto\\tmove\\ton\\tto\\tstate\\t\\ns\\n2\\n\\tand\\tget\\ta\\tnegative\\nreward\\tof\\t–50\\t(ouch).\\tIn\\tstate\\t\\ns\\n2\\n\\tit\\thas\\tno\\tother\\tchoice\\tthan\\tto\\ttake\\taction\\t\\na\\n1\\n,\\twhich\\twill\\tmost\\tlikely\\tlead\\nit\\tback\\tto\\tstate\\t\\ns\\n0\\n,\\tgaining\\ta\\treward\\tof\\t+40\\ton\\tthe\\tway.\\tYou\\tget\\tthe\\tpicture.\\tBy\\tlooking\\tat\\tthis\\tMDP,\\tcan\\nyou\\tguess\\twhich\\tstrategy\\twill\\tgain\\tthe\\tmost\\treward\\tover\\ttime?\\tIn\\tstate\\t\\ns\\n0\\n\\tit\\tis\\tclear\\tthat\\taction\\t\\na\\n0\\n\\tis\\tthe\\nbest\\toption,\\tand\\tin\\tstate\\t\\ns\\n2\\n\\tthe\\tagent\\thas\\tno\\tchoice\\tbut\\tto\\ttake\\taction\\t\\na\\n1\\n,\\tbut\\tin\\tstate\\t\\ns\\n1\\n\\tit\\tis\\tnot\\tobvious\\nwhether\\tthe\\tagent\\tshould\\tstay\\tput\\t(\\na\\n0\\n)\\tor\\tgo\\tthrough\\tthe\\tfire\\t(\\na\\n2\\n).\\nFigure\\t16-8.\\t\\nExample\\tof\\ta\\tMarkov\\tdecision\\tprocess\\nBellman\\tfound\\ta\\tway\\tto\\testimate\\t\\nthe\\t\\noptimal\\tstate\\tvalue\\n\\tof\\tany\\tstate\\t\\ns\\n,\\tnoted\\t\\nV\\n*(\\ns\\n),\\twhich\\tis\\tthe\\tsum\\tof', 'all\\tdiscounted\\tfuture\\trewards\\tthe\\tagent\\tcan\\texpect\\ton\\taverage\\tafter\\tit\\treaches\\ta\\tstate\\t\\ns\\n,\\tassuming\\tit\\tacts\\noptimally.\\tHe\\tshowed\\tthat\\tif\\tthe\\tagent\\tacts\\toptimally,\\tthen\\t\\nthe\\t\\nBellman\\tOptimality\\tEquation\\n\\tapplies\\t(see\\nEquation\\t16-1\\n).\\tThis\\trecursive\\tequation\\tsays\\tthat\\tif\\tthe\\tagent\\tacts\\toptimally,\\tthen\\tthe\\toptimal\\tvalue\\tof\\tthe\\ncurrent\\tstate\\tis\\tequal\\tto\\tthe\\treward\\tit\\twill\\tget\\ton\\taverage\\tafter\\ttaking\\tone\\toptimal\\taction,\\tplus\\tthe\\texpected\\noptimal\\tvalue\\tof\\tall\\tpossible\\tnext\\tstates\\tthat\\tthis\\taction\\tcan\\tlead\\tto.\\nEquation\\t16-1.\\t\\nBellman\\tOptimality\\tEquation\\nT\\n(\\ns\\n,\\t\\na\\n,\\t\\ns\\n′)\\tis\\tthe\\ttransition\\tprobability\\tfrom\\tstate\\t\\ns\\n\\tto\\tstate\\t\\ns\\n′,\\tgiven\\tthat\\tthe\\tagent\\tchose\\taction\\t\\na\\n.\\nR\\n(\\ns\\n,\\t\\na\\n,\\t\\ns\\n′)\\tis\\tthe\\treward\\tthat\\tthe\\tagent\\tgets\\twhen\\tit\\tgoes\\tfrom\\tstate\\t\\ns\\n\\tto\\tstate\\t\\ns\\n′,\\tgiven\\tthat\\tthe\\tagent\\nchose\\taction\\t\\na\\n.\\nγ\\n\\tis\\tthe\\tdiscount\\trate.\\nThis\\tequation\\tleads\\tdirectly\\tto\\tan\\talgorithm\\tthat\\tcan\\tprecisely\\testimate\\tthe\\toptimal\\tstate\\tvalue\\tof\\tevery', 'possible\\tstate:\\tyou\\tfirst\\tinitialize\\tall\\tthe\\tstate\\tvalue\\testimates\\tto\\tzero,\\tand\\tthen\\tyou\\titeratively\\tupdate\\tthem\\nusing\\tthe\\t\\nValue\\tIteration\\n\\t\\nalgorithm\\t(see\\t\\nEquation\\t16-2\\n).\\tA\\tremarkable\\tresult\\tis\\tthat,\\tgiven\\tenough\\ttime,\\nthese\\testimates\\tare\\tguaranteed\\tto\\tconverge\\tto\\tthe\\toptimal\\tstate\\tvalues,\\tcorresponding\\tto\\tthe\\toptimal', 'policy.\\nEquation\\t16-2.\\t\\nValue\\tIteration\\talgorithm\\nV\\nk\\n(\\ns\\n)\\tis\\tthe\\testimated\\tvalue\\tof\\tstate\\t\\ns\\n\\tat\\tthe\\t\\nk\\nth\\n\\titeration\\tof\\tthe\\talgorithm.\\nNOTE\\nThis\\talgorithm\\tis\\tan\\texample\\tof\\t\\nDynamic\\tProgramming\\n,\\t\\nwhich\\tbreaks\\tdown\\ta\\tcomplex\\tproblem\\t(in\\tthis\\tcase\\testimating\\ta\\npotentially\\tinfinite\\tsum\\tof\\tdiscounted\\tfuture\\trewards)\\tinto\\ttractable\\tsub-problems\\tthat\\tcan\\tbe\\ttackled\\titeratively\\t(in\\tthis\\tcase\\nfinding\\tthe\\taction\\tthat\\tmaximizes\\tthe\\taverage\\treward\\tplus\\tthe\\tdiscounted\\tnext\\tstate\\tvalue).\\nKnowing\\tthe\\toptimal\\tstate\\tvalues\\tcan\\tbe\\tuseful,\\tin\\tparticular\\tto\\tevaluate\\ta\\tpolicy,\\tbut\\tit\\tdoes\\tnot\\ttell\\tthe\\nagent\\texplicitly\\twhat\\tto\\tdo.\\tLuckily,\\tBellman\\tfound\\ta\\tvery\\tsimilar\\talgorithm\\tto\\testimate\\tthe\\toptimal\\t\\nstate-\\naction\\tvalues\\n,\\t\\ngenerally\\tcalled\\t\\nQ-Values\\n.\\t\\nThe\\toptimal\\tQ-Value\\tof\\tthe\\tstate-action\\tpair\\t(\\ns\\n,\\na\\n),\\tnoted\\t\\nQ\\n*\\n(\\ns\\n,\\na\\n),\\tis\\tthe\\tsum\\tof\\tdiscounted\\tfuture\\trewards\\tthe\\tagent\\tcan\\texpect\\ton\\taverage\\tafter\\tit\\treaches\\tthe\\tstate\\t\\ns\\nand\\tchooses\\taction\\t\\na', \"a\\n,\\tbut\\tbefore\\tit\\tsees\\tthe\\toutcome\\tof\\tthis\\taction,\\tassuming\\tit\\tacts\\toptimally\\tafter\\tthat\\naction.\\nHere\\tis\\thow\\tit\\tworks:\\tonce\\tagain,\\tyou\\tstart\\tby\\tinitializing\\tall\\tthe\\tQ-Value\\testimates\\tto\\tzero,\\tthen\\tyou\\nupdate\\tthem\\tusing\\t\\nthe\\t\\nQ-Value\\tIteration\\n\\talgorithm\\t(see\\t\\nEquation\\t16-3\\n).\\nEquation\\t16-3.\\t\\nQ-Value\\tIteration\\talgorithm\\nOnce\\tyou\\thave\\tthe\\toptimal\\tQ-Values,\\tdefining\\tthe\\toptimal\\tpolicy,\\tnoted\\t\\nπ\\n*(\\ns\\n),\\tis\\ttrivial:\\twhen\\tthe\\tagent\\tis\\nin\\tstate\\t\\ns\\n,\\tit\\tshould\\tchoose\\tthe\\taction\\twith\\tthe\\thighest\\tQ-Value\\tfor\\tthat\\tstate:\\t\\n.\\nLet’s\\tapply\\tthis\\talgorithm\\tto\\tthe\\tMDP\\trepresented\\tin\\t\\nFigure\\t16-8\\n.\\tFirst,\\twe\\tneed\\tto\\tdefine\\tthe\\tMDP:\\nnan\\n=\\nnp\\n.\\nnan\\n\\t\\t\\n#\\trepresents\\timpossible\\tactions\\nT\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n\\t\\t\\n#\\tshape=[s,\\ta,\\ts']\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n0.7\\n,\\n\\t\\n0.3\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\n1.0\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\n0.8\\n,\\n\\t\\n0.2\\n,\\n\\t\\n0.0\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n0.0\\n,\\n\\t\\n1.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n],\\n\\t\\n[\\n0.0\\n,\\n\\t\\n0.0\\n,\\n\\t\\n1.0\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n],\\n\\t\\n[\\n0.8\\n,\\n\\t\\n0.1\\n,\\n\\t\\n0.1\\n],\\n\\t\\n[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n]],\\n\\t\\t\\t\\t\\n])\\nR\\n\\t\\n=\", \"])\\nR\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n\\t\\t\\n#\\tshape=[s,\\ta,\\ts']\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n10.\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\n0.0\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\n0.0\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\n10.\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n],\\n\\t\\n[\\n0.0\\n,\\n\\t\\n0.0\\n,\\n\\t\\n-\\n50.\\n]],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n[[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n],\\n\\t\\n[\\n40.\\n,\\n\\t\\n0.0\\n,\\n\\t\\n0.0\\n],\\n\\t\\n[\\nnan\\n,\\n\\t\\nnan\\n,\\n\\t\\nnan\\n]],\\n\\t\\t\\t\\t\\n])\\npossible_actions\\n\\t\\n=\\n\\t\\n[[\\n0\\n,\\n\\t\\n1\\n,\\n\\t\\n2\\n],\\n\\t\\n[\\n0\\n,\\n\\t\\n2\\n],\\n\\t\\n[\\n1\\n]]\\nNow\\tlet’s\\trun\\tthe\\tQ-Value\\tIteration\\talgorithm:\\nQ\\n\\t\\n=\\n\\t\\nnp\\n.\\nfull\\n((\\n3\\n,\\n\\t\\n3\\n),\\n\\t\\n-\\nnp\\n.\\ninf\\n)\\n\\t\\t\\n#\\t-inf\\tfor\\timpossible\\tactions\\nfor\\n\\t\\nstate\\n,\\n\\t\\nactions\\n\\t\\nin\\n\\t\\nenumerate\\n(\\npossible_actions\\n):\\n\\t\\t\\t\\t\\nQ\\n[\\nstate\\n,\\n\\t\\nactions\\n]\\n\\t\\n=\\n\\t\\n0.0\\n\\t\\t\\n#\\tInitial\\tvalue\\t=\\t0.0,\\tfor\\tall\\tpossible\\tactions\", 'learning_rate\\n\\t\\n=\\n\\t\\n0.01\\ndiscount_rate\\n\\t\\n=\\n\\t\\n0.95\\nn_iterations\\n\\t\\n=\\n\\t\\n100\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\nQ_prev\\n\\t\\n=\\n\\t\\nQ\\n.\\ncopy\\n()\\n\\t\\t\\t\\t\\nfor\\n\\t\\ns\\n\\t\\nin\\n\\t\\nrange\\n(\\n3\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\na\\n\\t\\nin\\n\\t\\npossible_actions\\n[\\ns\\n]:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nQ\\n[\\ns\\n,\\n\\t\\na\\n]\\n\\t\\n=\\n\\t\\nnp\\n.\\nsum\\n([\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nT\\n[\\ns\\n,\\n\\t\\na\\n,\\n\\t\\nsp\\n]\\n\\t\\n*\\n\\t\\n(\\nR\\n[\\ns\\n,\\n\\t\\na\\n,\\n\\t\\nsp\\n]\\n\\t\\n+\\n\\t\\ndiscount_rate\\n\\t\\n*\\n\\t\\nnp\\n.\\nmax\\n(\\nQ_prev\\n[\\nsp\\n]))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nsp\\n\\t\\nin\\n\\t\\nrange\\n(\\n3\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n])\\nThe\\tresulting\\tQ-Values\\tlook\\tlike\\tthis:\\n>>>\\t\\nQ\\narray([[\\t21.89498982,\\t\\t20.80024033,\\t\\t16.86353093],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t1.11669335,\\t\\t\\t\\t\\t\\t\\t\\t\\t-inf,\\t\\t\\t1.17573546],\\n\\t\\t\\t\\t\\t\\t\\t[\\t\\t\\t\\t\\t\\t\\t\\t-inf,\\t\\t53.86946068,\\t\\t\\t\\t\\t\\t\\t\\t\\t-inf]])\\n>>>\\t\\nnp\\n.\\nargmax\\n(\\nQ\\n,\\n\\t\\naxis\\n=\\n1\\n)\\n\\t\\t\\n#\\toptimal\\taction\\tfor\\teach\\tstate\\narray([0,\\t2,\\t1])\\nThis\\tgives\\tus\\tthe\\toptimal\\tpolicy\\tfor\\tthis\\tMDP,\\twhen\\tusing\\ta\\tdiscount\\trate\\tof\\t0.95:\\tin\\tstate\\t\\ns\\n0\\n\\tchoose\\naction\\t\\na\\n0\\n,\\tin\\tstate\\t\\ns\\n1\\n\\tchoose\\taction\\t\\na\\n2\\n\\t(go\\tthrough\\tthe\\tfire!),\\tand\\tin\\tstate\\t\\ns\\n2\\n\\tchoose\\taction\\t\\na\\n1\\n\\t(the\\tonly', 'possible\\taction).\\tInterestingly,\\tif\\tyou\\treduce\\tthe\\tdiscount\\trate\\tto\\t0.9,\\tthe\\toptimal\\tpolicy\\tchanges:\\tin\\tstate\\ns\\n1\\n\\tthe\\tbest\\taction\\tbecomes\\t\\na\\n0\\n\\t(stay\\tput;\\tdon’t\\tgo\\tthrough\\tthe\\tfire).\\tIt\\tmakes\\tsense\\tbecause\\tif\\tyou\\tvalue\\tthe\\npresent\\tmuch\\tmore\\tthan\\tthe\\tfuture,\\tthen\\tthe\\tprospect\\tof\\tfuture\\trewards\\tis\\tnot\\tworth\\t\\nimmediate\\tpain.', 'Temporal\\tDifference\\tLearning\\tand\\tQ-Learning\\nReinforcement\\t\\nLearning\\tproblems\\twith\\tdiscrete\\tactions\\tcan\\toften\\tbe\\tmodeled\\tas\\tMarkov\\tdecision\\nprocesses,\\tbut\\tthe\\tagent\\tinitially\\thas\\tno\\tidea\\twhat\\tthe\\ttransition\\tprobabilities\\tare\\t(it\\tdoes\\tnot\\tknow\\t\\nT\\n(\\ns\\n,\\t\\na\\n,\\ns\\n′)),\\tand\\tit\\tdoes\\tnot\\tknow\\twhat\\tthe\\trewards\\tare\\tgoing\\tto\\tbe\\teither\\t(it\\tdoes\\tnot\\tknow\\t\\nR\\n(\\ns\\n,\\t\\na\\n,\\t\\ns\\n′)).\\tIt\\tmust\\nexperience\\teach\\tstate\\tand\\teach\\ttransition\\tat\\tleast\\tonce\\tto\\tknow\\tthe\\trewards,\\tand\\tit\\tmust\\texperience\\tthem\\nmultiple\\ttimes\\tif\\tit\\tis\\tto\\thave\\ta\\treasonable\\testimate\\tof\\tthe\\ttransition\\tprobabilities.\\nThe\\t\\nTemporal\\tDifference\\tLearning\\n\\t(TD\\tLearning)\\talgorithm\\tis\\tvery\\tsimilar\\tto\\tthe\\tValue\\tIteration\\nalgorithm,\\tbut\\ttweaked\\tto\\ttake\\tinto\\taccount\\tthe\\tfact\\tthat\\tthe\\tagent\\thas\\tonly\\tpartial\\tknowledge\\tof\\tthe\\tMDP.\\nIn\\tgeneral\\twe\\tassume\\tthat\\tthe\\tagent\\tinitially\\tknows\\tonly\\tthe\\tpossible\\tstates\\tand\\tactions,\\tand\\tnothing\\tmore.\\nThe\\tagent\\tuses\\tan\\t\\nexploration\\tpolicy\\n\\t—\\tfor\\texample,\\ta\\tpurely\\trandom\\tpolicy\\t—\\tto\\texplore\\tthe\\tMDP,\\tand', 'as\\tit\\tprogresses\\tthe\\tTD\\tLearning\\talgorithm\\tupdates\\tthe\\testimates\\tof\\tthe\\tstate\\tvalues\\tbased\\ton\\tthe\\ntransitions\\tand\\trewards\\tthat\\tare\\tactually\\tobserved\\t(see\\t\\nEquation\\t16-4\\n).\\nEquation\\t16-4.\\t\\nTD\\tLearning\\talgorithm\\nα\\n\\tis\\tthe\\tlearning\\trate\\t(e.g.,\\t0.01).\\nTIP\\nTD\\tLearning\\thas\\tmany\\tsimilarities\\twith\\tStochastic\\tGradient\\tDescent,\\tin\\tparticular\\tthe\\tfact\\tthat\\tit\\thandles\\tone\\tsample\\tat\\ta\\ttime.\\nJust\\tlike\\tSGD,\\tit\\tcan\\tonly\\ttruly\\tconverge\\tif\\tyou\\tgradually\\treduce\\tthe\\tlearning\\trate\\t(otherwise\\tit\\twill\\tkeep\\tbouncing\\taround\\tthe\\noptimum).\\nFor\\teach\\tstate\\t\\ns\\n,\\tthis\\talgorithm\\tsimply\\tkeeps\\ttrack\\tof\\ta\\trunning\\taverage\\tof\\tthe\\timmediate\\trewards\\tthe\\nagent\\tgets\\tupon\\tleaving\\tthat\\tstate,\\tplus\\tthe\\trewards\\tit\\texpects\\tto\\tget\\tlater\\t\\n(assuming\\tit\\tacts\\toptimally).\\nSimilarly,\\tthe\\t\\nQ-Learning\\talgorithm\\tis\\tan\\tadaptation\\tof\\tthe\\tQ-Value\\tIteration\\talgorithm\\tto\\tthe\\tsituation\\nwhere\\tthe\\ttransition\\tprobabilities\\tand\\tthe\\trewards\\tare\\tinitially\\tunknown\\t(see\\t\\nEquation\\t16-5\\n).\\nEquation\\t16-5.\\t\\nQ-Learning\\talgorithm\\nFor\\teach\\tstate-action\\tpair\\t(', 's\\n,\\t\\na\\n),\\tthis\\talgorithm\\tkeeps\\ttrack\\tof\\ta\\trunning\\taverage\\tof\\tthe\\trewards\\t\\nr\\n\\tthe\\tagent\\ngets\\tupon\\tleaving\\tthe\\tstate\\t\\ns\\n\\twith\\taction\\t\\na\\n,\\tplus\\tthe\\trewards\\tit\\texpects\\tto\\tget\\tlater.\\tSince\\tthe\\ttarget\\tpolicy\\nwould\\tact\\toptimally,\\twe\\ttake\\tthe\\tmaximum\\tof\\tthe\\tQ-Value\\testimates\\tfor\\tthe\\tnext\\tstate.\\nHere\\tis\\thow\\tQ-Learning\\tcan\\tbe\\timplemented:\\nimport\\n\\t\\nnumpy.random\\n\\t\\nas\\n\\t\\nrnd\\nlearning_rate0\\n\\t\\n=\\n\\t\\n0.05\\nlearning_rate_decay\\n\\t\\n=\\n\\t\\n0.1', 'n_iterations\\n\\t\\n=\\n\\t\\n20000\\ns\\n\\t\\n=\\n\\t\\n0\\n\\t\\n#\\tstart\\tin\\tstate\\t0\\nQ\\n\\t\\n=\\n\\t\\nnp\\n.\\nfull\\n((\\n3\\n,\\n\\t\\n3\\n),\\n\\t\\n-\\nnp\\n.\\ninf\\n)\\n\\t\\t\\n#\\t-inf\\tfor\\timpossible\\tactions\\nfor\\n\\t\\nstate\\n,\\n\\t\\nactions\\n\\t\\nin\\n\\t\\nenumerate\\n(\\npossible_actions\\n):\\n\\t\\t\\t\\t\\nQ\\n[\\nstate\\n,\\n\\t\\nactions\\n]\\n\\t\\n=\\n\\t\\n0.0\\n\\t\\t\\n#\\tInitial\\tvalue\\t=\\t0.0,\\tfor\\tall\\tpossible\\tactions\\nfor\\n\\t\\niteration\\n\\t\\nin\\n\\t\\nrange\\n(\\nn_iterations\\n):\\n\\t\\t\\t\\t\\na\\n\\t\\n=\\n\\t\\nrnd\\n.\\nchoice\\n(\\npossible_actions\\n[\\ns\\n])\\n\\t\\t\\n#\\tchoose\\tan\\taction\\t(randomly)\\n\\t\\t\\t\\t\\nsp\\n\\t\\n=\\n\\t\\nrnd\\n.\\nchoice\\n(\\nrange\\n(\\n3\\n),\\n\\t\\np\\n=\\nT\\n[\\ns\\n,\\n\\t\\na\\n])\\n\\t\\n#\\tpick\\tnext\\tstate\\tusing\\tT[s,\\ta]\\n\\t\\t\\t\\t\\nreward\\n\\t\\n=\\n\\t\\nR\\n[\\ns\\n,\\n\\t\\na\\n,\\n\\t\\nsp\\n]\\n\\t\\t\\t\\t\\nlearning_rate\\n\\t\\n=\\n\\t\\nlearning_rate0\\n\\t\\n/\\n\\t\\n(\\n1\\n\\t\\n+\\n\\t\\niteration\\n\\t\\n*\\n\\t\\nlearning_rate_decay\\n)\\n\\t\\t\\t\\t\\nQ\\n[\\ns\\n,\\n\\t\\na\\n]\\n\\t\\n=\\n\\t\\nlearning_rate\\n\\t\\n*\\n\\t\\nQ\\n[\\ns\\n,\\n\\t\\na\\n]\\n\\t\\n+\\n\\t\\n(\\n1\\n\\t\\n-\\n\\t\\nlearning_rate\\n)\\n\\t\\n*\\n\\t\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nreward\\n\\t\\n+\\n\\t\\ndiscount_rate\\n\\t\\n*\\n\\t\\nnp\\n.\\nmax\\n(\\nQ\\n[\\nsp\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n)\\n\\t\\t\\t\\t\\ns\\n\\t\\n=\\n\\t\\nsp\\n\\t\\n#\\tmove\\tto\\tnext\\tstate\\nGiven\\tenough\\titerations,\\tthis\\talgorithm\\twill\\tconverge\\tto\\tthe\\toptimal\\tQ-Values.\\tThis\\tis\\tcalled\\tan\\t\\noff-policy', 'algorithm\\tbecause\\tthe\\tpolicy\\tbeing\\ttrained\\tis\\tnot\\tthe\\tone\\tbeing\\texecuted.\\tIt\\tis\\tsomewhat\\tsurprising\\tthat\\nthis\\talgorithm\\tis\\tcapable\\tof\\tlearning\\tthe\\toptimal\\tpolicy\\tby\\tjust\\twatching\\tan\\tagent\\tact\\trandomly\\t\\n(imagine\\nlearning\\tto\\tplay\\tgolf\\twhen\\tyour\\tteacher\\tis\\ta\\tdrunken\\tmonkey).\\tCan\\twe\\tdo\\tbetter?', 'Exploration\\tPolicies\\nOf\\t\\ncourse\\tQ-Learning\\tcan\\twork\\tonly\\tif\\tthe\\texploration\\tpolicy\\texplores\\tthe\\tMDP\\tthoroughly\\tenough.\\nAlthough\\ta\\tpurely\\trandom\\tpolicy\\tis\\tguaranteed\\tto\\teventually\\tvisit\\tevery\\tstate\\tand\\tevery\\ttransition\\tmany\\ntimes,\\tit\\tmay\\ttake\\tan\\textremely\\tlong\\ttime\\tto\\tdo\\tso.\\tTherefore,\\ta\\tbetter\\toption\\tis\\tto\\tuse\\tthe\\t\\nε-greedy\\tpolicy\\n:\\nat\\teach\\tstep\\tit\\tacts\\trandomly\\twith\\tprobability\\tε,\\tor\\tgreedily\\t(choosing\\tthe\\taction\\twith\\tthe\\thighest\\tQ-Value)\\nwith\\tprobability\\t1-ε.\\tThe\\tadvantage\\tof\\tthe\\tε-greedy\\tpolicy\\t(compared\\tto\\ta\\tcompletely\\trandom\\tpolicy)\\tis\\nthat\\tit\\twill\\tspend\\tmore\\tand\\tmore\\ttime\\texploring\\tthe\\tinteresting\\tparts\\tof\\tthe\\t\\nenvironment,\\tas\\tthe\\tQ-Value\\nestimates\\tget\\tbetter\\tand\\tbetter,\\twhile\\tstill\\tspending\\tsome\\ttime\\tvisiting\\tunknown\\tregions\\tof\\tthe\\tMDP.\\tIt\\tis\\nquite\\tcommon\\tto\\tstart\\twith\\ta\\thigh\\tvalue\\tfor\\tε\\t(e.g.,\\t1.0)\\tand\\tthen\\tgradually\\treduce\\tit\\t(e.g.,\\tdown\\tto\\t0.05).\\nAlternatively,\\trather\\tthan\\trelying\\ton\\tchance\\tfor\\texploration,\\tanother\\tapproach\\tis\\tto\\tencourage\\tthe', 'exploration\\tpolicy\\tto\\ttry\\tactions\\tthat\\tit\\thas\\tnot\\ttried\\tmuch\\tbefore.\\tThis\\tcan\\tbe\\timplemented\\tas\\ta\\tbonus\\nadded\\tto\\tthe\\tQ-Value\\testimates,\\tas\\tshown\\tin\\t\\nEquation\\t16-6\\n.\\nEquation\\t16-6.\\t\\nQ-Learning\\tusing\\tan\\texploration\\tfunction\\nN\\n(\\ns\\n′,\\t\\na\\n′)\\tcounts\\tthe\\tnumber\\tof\\ttimes\\tthe\\taction\\t\\na\\n′\\twas\\tchosen\\tin\\tstate\\t\\ns\\n′.\\nf\\n(\\nq\\n,\\t\\nn\\n)\\tis\\tan\\t\\nexploration\\tfunction\\n,\\tsuch\\tas\\t\\nf\\n(\\nq\\n,\\t\\nn\\n)\\t=\\t\\nq\\n\\t+\\t\\nK\\n/(1\\t+\\t\\nn\\n),\\twhere\\t\\nK\\n\\tis\\ta\\tcuriosity\\nhyperparameter\\tthat\\tmeasures\\thow\\tmuch\\tthe\\tagent\\tis\\tattracted\\tto\\tto\\tthe\\tunknown.', 'Approximate\\tQ-Learning\\nThe\\t\\nmain\\tproblem\\twith\\tQ-Learning\\tis\\tthat\\tit\\tdoes\\tnot\\tscale\\twell\\tto\\tlarge\\t(or\\teven\\tmedium)\\tMDPs\\twith\\nmany\\tstates\\tand\\tactions.\\tConsider\\ttrying\\tto\\tuse\\tQ-Learning\\tto\\ttrain\\tan\\tagent\\tto\\tplay\\tMs.\\tPac-Man.\\tThere\\nare\\tover\\t250\\tpellets\\tthat\\tMs.\\tPac-Man\\tcan\\teat,\\teach\\tof\\twhich\\tcan\\tbe\\tpresent\\tor\\tabsent\\t(i.e.,\\talready\\neaten).\\tSo\\tthe\\tnumber\\tof\\tpossible\\tstates\\tis\\tgreater\\tthan\\t2\\n250\\n\\t≈\\t10\\n75\\n\\t(and\\tthat’s\\tconsidering\\tthe\\tpossible\\nstates\\tonly\\tof\\tthe\\tpellets).\\tThis\\tis\\tway\\tmore\\tthan\\tatoms\\tin\\tthe\\tobservable\\tuniverse,\\tso\\tthere’s\\tabsolutely\\nno\\tway\\tyou\\tcan\\tkeep\\ttrack\\tof\\tan\\testimate\\tfor\\tevery\\tsingle\\tQ-Value.\\nThe\\tsolution\\tis\\tto\\tfind\\ta\\tfunction\\tthat\\tapproximates\\tthe\\tQ-Values\\tusing\\ta\\tmanageable\\tnumber\\tof\\nparameters.\\tThis\\tis\\tcalled\\t\\nApproximate\\tQ-Learning\\n.\\tFor\\tyears\\tit\\twas\\trecommended\\tto\\tuse\\tlinear\\ncombinations\\tof\\thand-crafted\\tfeatures\\textracted\\tfrom\\tthe\\tstate\\t(e.g.,\\tdistance\\tof\\tthe\\tclosest\\tghosts,\\ttheir\\ndirections,\\tand\\tso\\ton)\\tto\\testimate\\tQ-Values,\\tbut', 'DeepMind\\tshowed\\tthat\\tusing\\tdeep\\tneural\\tnetworks\\tcan\\nwork\\tmuch\\tbetter,\\tespecially\\tfor\\tcomplex\\tproblems,\\tand\\tit\\tdoes\\tnot\\trequire\\tany\\tfeature\\tengineering.\\tA\\nDNN\\tused\\tto\\testimate\\tQ-Values\\tis\\t\\ncalled\\ta\\t\\ndeep\\tQ-network\\n\\t(DQN),\\tand\\tusing\\ta\\tDQN\\tfor\\tApproximate\\nQ-Learning\\tis\\t\\ncalled\\t\\nDeep\\tQ-Learning\\n.\\nIn\\tthe\\trest\\tof\\tthis\\tchapter,\\twe\\twill\\tuse\\tDeep\\tQ-Learning\\tto\\ttrain\\tan\\tagent\\tto\\tplay\\tMs.\\tPac-Man,\\tmuch\\tlike\\nDeepMind\\tdid\\tin\\t2013.\\tThe\\tcode\\tcan\\teasily\\tbe\\ttweaked\\tto\\tlearn\\tto\\tplay\\tthe\\tmajority\\tof\\tAtari\\tgames\\tquite\\nwell.\\tIt\\tcan\\tachieve\\tsuperhuman\\tskill\\tat\\tmost\\taction\\tgames,\\tbut\\tit\\tis\\tnot\\tso\\tgood\\tat\\tgames\\twith\\tlong-\\nrunning\\tstorylines.', 'Learning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nSince\\t\\nwe\\twill\\tbe\\tusing\\tan\\tAtari\\tenvironment,\\twe\\tmust\\tfirst\\tinstall\\tOpenAI\\tgym’s\\tAtari\\tdependencies.\\nWhile\\twe’re\\tat\\tit,\\twe\\twill\\talso\\tinstall\\tdependencies\\tfor\\tother\\tOpenAI\\tgym\\tenvironments\\tthat\\tyou\\tmay\\nwant\\tto\\tplay\\twith.\\tOn\\tmacOS,\\tassuming\\tyou\\thave\\tinstalled\\t\\nHomebrew\\n,\\tyou\\tneed\\tto\\trun:\\n$\\tbrew\\tinstall\\tcmake\\tboost\\tboost-python\\tsdl2\\tswig\\twget\\nOn\\tUbuntu,\\ttype\\tthe\\tfollowing\\tcommand\\t(replacing\\t\\npython3\\n\\twith\\t\\npython\\n\\tif\\tyou\\tare\\tusing\\tPython\\t2):\\n$\\tapt-get\\tinstall\\t-y\\tpython3-numpy\\tpython3-dev\\tcmake\\tzlib1g-dev\\tlibjpeg-dev\\\\\\n\\t\\t\\t\\txvfb\\tlibav-tools\\txorg-dev\\tpython3-opengl\\tlibboost-all-dev\\tlibsdl2-dev\\tswig\\nThen\\tinstall\\tthe\\textra\\tPython\\tmodules:\\n$\\tpip3\\tinstall\\t--upgrade\\t\\'gym[all]\\'\\nIf\\teverything\\twent\\twell,\\tyou\\tshould\\tbe\\table\\tto\\tcreate\\ta\\tMs.\\tPac-Man\\tenvironment:\\n>>>\\t\\nenv\\n\\t\\n=\\n\\t\\ngym\\n.\\nmake\\n(\\n\"MsPacman-v0\"\\n)\\n>>>\\t\\nobs\\n\\t\\n=\\n\\t\\nenv\\n.\\nreset\\n()\\n>>>\\t\\nobs\\n.\\nshape\\n\\t\\t\\n#\\t[height,\\twidth,\\tchannels]\\n(210,\\t160,\\t3)\\n>>>\\t\\nenv\\n.\\naction_space\\nDiscrete(9)', 'As\\tyou\\tcan\\tsee,\\tthere\\tare\\tnine\\tdiscrete\\tactions\\tavailable,\\twhich\\tcorrespond\\tto\\tthe\\tnine\\tpossible\\tpositions\\nof\\tthe\\tjoystick\\t(left,\\tright,\\tup,\\tdown,\\tcenter,\\tupper\\tleft,\\tand\\tso\\ton),\\tand\\tthe\\tobservations\\tare\\tsimply\\nscreenshots\\tof\\tthe\\tAtari\\tscreen\\t(see\\t\\nFigure\\t16-9\\n,\\tleft),\\trepresented\\tas\\t3D\\tNumPy\\tarrays.\\tThese\\timages\\nare\\ta\\tbit\\tlarge,\\tso\\twe\\twill\\tcreate\\ta\\tsmall\\tpreprocessing\\tfunction\\tthat\\twill\\tcrop\\tthe\\timage\\tand\\tshrink\\tit\\ndown\\tto\\t88\\t×\\t80\\tpixels,\\tconvert\\tit\\tto\\tgrayscale,\\tand\\timprove\\tthe\\tcontrast\\tof\\tMs.\\tPac-Man.\\tThis\\twill\\nreduce\\tthe\\tamount\\tof\\tcomputations\\trequired\\tby\\tthe\\tDQN,\\tand\\tspeed\\tup\\ttraining.\\nmspacman_color\\n\\t\\n=\\n\\t\\nnp\\n.\\narray\\n([\\n210\\n,\\n\\t\\n164\\n,\\n\\t\\n74\\n])\\n.\\nmean\\n()\\ndef\\n\\t\\npreprocess_observation\\n(\\nobs\\n):\\n\\t\\t\\t\\t\\nimg\\n\\t\\n=\\n\\t\\nobs\\n[\\n1\\n:\\n176\\n:\\n2\\n,\\n\\t\\n::\\n2\\n]\\n\\t\\n#\\tcrop\\tand\\tdownsize\\n\\t\\t\\t\\t\\nimg\\n\\t\\n=\\n\\t\\nimg\\n.\\nmean\\n(\\naxis\\n=\\n2\\n)\\n\\t\\n#\\tto\\tgreyscale\\n\\t\\t\\t\\t\\nimg\\n[\\nimg\\n==\\nmspacman_color\\n]\\n\\t\\n=\\n\\t\\n0\\n\\t\\n#\\timprove\\tcontrast\\n\\t\\t\\t\\t\\nimg\\n\\t\\n=\\n\\t\\n(\\nimg\\n\\t\\n-\\n\\t\\n128\\n)\\n\\t\\n/\\n\\t\\n128\\n\\t\\n-\\n\\t\\n1\\n\\t\\n#\\tnormalize\\tfrom\\t-1.\\tto\\t1.\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nimg\\n.\\nreshape\\n(', 'reshape\\n(\\n88\\n,\\n\\t\\n80\\n,\\n\\t\\n1\\n)\\nThe\\tresult\\tof\\tpreprocessing\\tis\\tshown\\tin\\t\\nFigure\\t16-9\\n\\t(right).', 'Figure\\t16-9.\\t\\nMs.\\tPac-Man\\tobservation,\\toriginal\\t(left)\\tand\\tafter\\tpreprocessing\\t(right)\\nNext,\\tlet’s\\tcreate\\tthe\\tDQN.\\tIt\\tcould\\tjust\\ttake\\ta\\tstate-action\\tpair\\t(\\ns\\n,\\na\\n)\\tas\\tinput,\\tand\\toutput\\tan\\testimate\\tof\\nthe\\tcorresponding\\tQ-Value\\t\\nQ\\n(\\ns\\n,\\na\\n),\\tbut\\tsince\\tthe\\tactions\\tare\\tdiscrete\\tit\\tis\\tmore\\tconvenient\\tto\\tuse\\ta\\tneural\\nnetwork\\tthat\\ttakes\\tonly\\ta\\tstate\\t\\ns\\n\\tas\\tinput\\tand\\toutputs\\tone\\tQ-Value\\testimate\\tper\\taction.\\tThe\\tDQN\\twill\\tbe\\ncomposed\\tof\\tthree\\tconvolutional\\tlayers,\\tfollowed\\tby\\ttwo\\tfully\\tconnected\\tlayers,\\tincluding\\tthe\\toutput\\nlayer\\t(see\\t\\nFigure\\t16-10\\n).', 'Figure\\t16-10.\\t\\nDeep\\tQ-network\\tto\\tplay\\tMs.\\tPac-Man\\nAs\\twe\\twill\\tsee,\\tthe\\ttraining\\talgorithm\\twe\\twill\\tuse\\trequires\\ttwo\\tDQNs\\twith\\tthe\\tsame\\tarchitecture\\t(but\\ndifferent\\tparameters):\\tone\\twill\\tbe\\tused\\tto\\tdrive\\tMs.\\tPac-Man\\tduring\\ttraining\\t(the\\t\\nactor\\n),\\t\\nand\\tthe\\tother\\nwill\\twatch\\tthe\\tactor\\tand\\tlearn\\tfrom\\tits\\ttrials\\tand\\terrors\\t(the\\t\\ncritic\\n).\\t\\nAt\\tregular\\tintervals\\twe\\twill\\tcopy\\tthe\\ncritic\\tto\\tthe\\tactor.\\tSince\\twe\\tneed\\ttwo\\tidentical\\tDQNs,\\twe\\twill\\tcreate\\ta\\t\\nq_network()\\n\\t\\nfunction\\tto\\tbuild\\nthem:\\ninput_height\\n\\t\\n=\\n\\t\\n88\\ninput_width\\n\\t\\n=\\n\\t\\n80\\ninput_channels\\n\\t\\n=\\n\\t\\n1\\nconv_n_maps\\n\\t\\n=\\n\\t\\n[\\n32\\n,\\n\\t\\n64\\n,\\n\\t\\n64\\n]\\nconv_kernel_sizes\\n\\t\\n=\\n\\t\\n[(\\n8\\n,\\n8\\n),\\n\\t\\n(\\n4\\n,\\n4\\n),\\n\\t\\n(\\n3\\n,\\n3\\n)]\\nconv_strides\\n\\t\\n=\\n\\t\\n[\\n4\\n,\\n\\t\\n2\\n,\\n\\t\\n1\\n]\\nconv_paddings\\n\\t\\n=\\n\\t\\n[\\n\"SAME\"\\n]\\n\\t\\n*\\n\\t\\n3\\nconv_activation\\n\\t\\n=\\n\\t\\n[\\ntf\\n.\\nnn\\n.\\nrelu\\n]\\n\\t\\n*\\n\\t\\n3', 'n_hidden_in\\n\\t\\n=\\n\\t\\n64\\n\\t\\n*\\n\\t\\n11\\n\\t\\n*\\n\\t\\n10\\n\\t\\t\\n#\\tconv3\\thas\\t64\\tmaps\\tof\\t11x10\\teach\\nn_hidden\\n\\t\\n=\\n\\t\\n512\\nhidden_activation\\n\\t\\n=\\n\\t\\ntf\\n.\\nnn\\n.\\nrelu\\nn_outputs\\n\\t\\n=\\n\\t\\nenv\\n.\\naction_space\\n.\\nn\\n\\t\\t\\n#\\t9\\tdiscrete\\tactions\\tare\\tavailable\\ninitializer\\n\\t\\n=\\n\\t\\ntf\\n.\\ncontrib\\n.\\nlayers\\n.\\nvariance_scaling_initializer\\n()\\ndef\\n\\t\\nq_network\\n(\\nX_state\\n,\\n\\t\\nname\\n):\\n\\t\\t\\t\\t\\nprev_layer\\n\\t\\n=\\n\\t\\nX_state\\n\\t\\t\\t\\t\\nconv_layers\\n\\t\\n=\\n\\t\\n[]\\n\\t\\t\\t\\t\\nwith\\n\\t\\ntf\\n.\\nvariable_scope\\n(\\nname\\n)\\n\\t\\nas\\n\\t\\nscope\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nn_maps\\n,\\n\\t\\nkernel_size\\n,\\n\\t\\nstride\\n,\\n\\t\\npadding\\n,\\n\\t\\nactivation\\n\\t\\nin\\n\\t\\nzip\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nconv_n_maps\\n,\\n\\t\\nconv_kernel_sizes\\n,\\n\\t\\nconv_strides\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nconv_paddings\\n,\\n\\t\\nconv_activation\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nprev_layer\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\nconv2d\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nprev_layer\\n,\\n\\t\\nfilters\\n=\\nn_maps\\n,\\n\\t\\nkernel_size\\n=\\nkernel_size\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nstride\\n=\\nstride\\n,\\n\\t\\npadding\\n=\\npadding\\n,\\n\\t\\nactivation\\n=\\nactivation\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nconv_layers\\n.\\nappend\\n(\\nprev_layer\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nlast_conv_layer_flat', '=\\n\\t\\ntf\\n.\\nreshape\\n(\\nprev_layer\\n,\\n\\t\\nshape\\n=\\n[\\n-\\n1\\n,\\n\\t\\nn_hidden_in\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nhidden\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nlast_conv_layer_flat\\n,\\n\\t\\nn_hidden\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nactivation\\n=\\nhidden_activation\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\noutputs\\n\\t\\n=\\n\\t\\ntf\\n.\\nlayers\\n.\\ndense\\n(\\nhidden\\n,\\n\\t\\nn_outputs\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nkernel_initializer\\n=\\ninitializer\\n)\\n\\t\\t\\t\\t\\ntrainable_vars\\n\\t\\n=\\n\\t\\ntf\\n.\\nget_collection\\n(\\ntf\\n.\\nGraphKeys\\n.\\nTRAINABLE_VARIABLES\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nscope\\n=\\nscope\\n.\\nname\\n)\\n\\t\\t\\t\\t\\ntrainable_vars_by_name\\n\\t\\n=\\n\\t\\n{\\nvar\\n.\\nname\\n[\\nlen\\n(\\nscope\\n.\\nname\\n):]:\\n\\t\\nvar\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nvar\\n\\t\\nin\\n\\t\\ntrainable_vars\\n}\\n\\t\\t\\t\\t\\nreturn\\n\\t\\noutputs\\n,\\n\\t\\ntrainable_vars_by_name\\nThe\\tfirst\\tpart\\tof\\tthis\\tcode\\tdefines\\tthe\\thyperparameters\\tof\\tthe\\tDQN\\tarchitecture.\\tThen\\tthe\\t\\nq_network()\\nfunction\\tcreates\\tthe\\tDQN,\\ttaking\\tthe\\tenvironment’s\\tstate\\t\\nX_state\\n\\tas\\tinput,\\tand\\tthe\\tname\\tof\\tthe\\tvariable', \"scope.\\tNote\\tthat\\twe\\twill\\tjust\\tuse\\tone\\tobservation\\tto\\trepresent\\tthe\\tenvironment’s\\t\\nstate\\tsince\\tthere’s\\talmost\\nno\\thidden\\tstate\\t(except\\tfor\\tblinking\\tobjects\\tand\\tthe\\tghosts’\\tdirections).\\nThe\\t\\ntrainable_vars_by_name\\n\\tdictionary\\tgathers\\tall\\tthe\\ttrainable\\tvariables\\tof\\tthis\\tDQN.\\tIt\\twill\\tbe\\nuseful\\tin\\ta\\tminute\\twhen\\twe\\tcreate\\toperations\\tto\\tcopy\\tthe\\tcritic\\tDQN\\tto\\tthe\\tactor\\tDQN.\\tThe\\tkeys\\tof\\tthe\\ndictionary\\tare\\tthe\\tnames\\tof\\tthe\\tvariables,\\tstripping\\tthe\\tpart\\tof\\tthe\\tprefix\\tthat\\tjust\\tcorresponds\\tto\\tthe\\nscope’s\\tname.\\tIt\\tlooks\\tlike\\tthis:\\n>>>\\t\\ntrainable_vars_by_name\\n{'/conv2d/bias:0':\\t<tensorflow.python.ops.variables.Variable\\tat\\t0x121cf7b50>,\\n\\t'/conv2d/kernel:0':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t'/conv2d_1/bias:0':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t'/conv2d_1/kernel:0':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t'/conv2d_2/bias:0':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t'/conv2d_2/kernel:0':\\t<tensorflow.python.ops.variables.Variable...>,\", '\\'/dense/bias:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/dense/kernel:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/dense_1/bias:0\\':\\t<tensorflow.python.ops.variables.Variable...>,\\n\\t\\'/dense_1/kernel:0\\':\\t<tensorflow.python.ops.variables.Variable...>}\\nNow\\tlet’s\\tcreate\\tthe\\tinput\\tplaceholder,\\tthe\\ttwo\\tDQNs,\\tand\\tthe\\toperation\\tto\\tcopy\\tthe\\tcritic\\tDQN\\tto\\t\\nthe\\nactor\\tDQN:\\nX_state\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\ninput_height\\n,\\n\\t\\ninput_width\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ninput_channels\\n])\\nactor_q_values\\n,\\n\\t\\nactor_vars\\n\\t\\n=\\n\\t\\nq_network\\n(\\nX_state\\n,\\n\\t\\nname\\n=\\n\"q_networks/actor\"\\n)\\ncritic_q_values\\n,\\n\\t\\ncritic_vars\\n\\t\\n=\\n\\t\\nq_network\\n(\\nX_state\\n,\\n\\t\\nname\\n=\\n\"q_networks/critic\"\\n)\\ncopy_ops\\n\\t\\n=\\n\\t\\n[\\nactor_var\\n.\\nassign\\n(\\ncritic_vars\\n[\\nvar_name\\n])\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\nvar_name\\n,\\n\\t\\nactor_var\\n\\t\\nin\\n\\t\\nactor_vars\\n.\\nitems\\n()]\\ncopy_critic_to_actor\\n\\t\\n=\\n\\t\\ntf\\n.\\ngroup\\n(\\n*\\ncopy_ops\\n)', 'Let’s\\tstep\\tback\\tfor\\ta\\tsecond:\\twe\\tnow\\thave\\ttwo\\tDQNs\\tthat\\tare\\tboth\\tcapable\\tof\\ttaking\\tan\\tenvironment\\tstate\\n(i.e.,\\ta\\tpreprocessed\\tobservation)\\tas\\tinput\\tand\\toutputting\\tan\\testimated\\tQ-Value\\tfor\\teach\\tpossible\\taction\\tin\\nthat\\tstate.\\tPlus\\twe\\thave\\tan\\toperation\\tcalled\\t\\ncopy_critic_to_actor\\n\\tto\\tcopy\\tall\\tthe\\ttrainable\\tvariables\\nof\\tthe\\tcritic\\tDQN\\tto\\tthe\\tactor\\tDQN.\\tWe\\tuse\\tTensorFlow’s\\t\\ntf.group()\\n\\t\\nfunction\\tto\\tgroup\\tall\\tthe\\nassignment\\toperations\\tinto\\ta\\tsingle\\tconvenient\\toperation.\\nThe\\tactor\\tDQN\\tcan\\tbe\\tused\\tto\\tplay\\tMs.\\tPac-Man\\t(initially\\tvery\\tbadly).\\tAs\\tdiscussed\\tearlier,\\tyou\\twant\\tit\\nto\\texplore\\tthe\\tgame\\tthoroughly\\tenough,\\tso\\tyou\\tgenerally\\twant\\tto\\tcombine\\tit\\twith\\t\\nan\\t\\nε-greedy\\tpolicy\\n\\tor\\nanother\\texploration\\tstrategy.\\nBut\\twhat\\tabout\\tthe\\tcritic\\tDQN?\\tHow\\twill\\tit\\tlearn\\tto\\tplay\\tthe\\tgame?\\tThe\\tshort\\tanswer\\tis\\tthat\\tit\\twill\\ttry\\tto\\nmake\\tits\\tQ-Value\\tpredictions\\tmatch\\tthe\\tQ-Values\\testimated\\tby\\tthe\\tactor\\tthrough\\tits\\texperience\\tof\\tthe', 'game.\\tSpecifically,\\twe\\twill\\tlet\\tthe\\tactor\\tplay\\tfor\\ta\\twhile,\\tstoring\\tall\\tits\\texperiences\\tin\\ta\\t\\nreplay\\tmemory\\n.\\nEach\\tmemory\\twill\\tbe\\ta\\t5-tuple\\t(state,\\taction,\\tnext\\tstate,\\treward,\\tcontinue),\\twhere\\tthe\\t“continue”\\titem\\twill\\nbe\\tequal\\tto\\t0.0\\twhen\\tthe\\tgame\\tis\\tover,\\tor\\t1.0\\totherwise.\\tNext,\\tat\\tregular\\tintervals\\twe\\twill\\tsample\\ta\\tbatch\\nof\\tmemories\\tfrom\\tthe\\treplay\\tmemory,\\tand\\twe\\twill\\testimate\\tthe\\tQ-Values\\tfrom\\tthese\\tmemories.\\tFinally,\\nwe\\twill\\ttrain\\tthe\\tcritic\\tDQN\\tto\\tpredict\\tthese\\tQ-Values\\tusing\\tregular\\tsupervised\\tlearning\\ttechniques.\\tOnce\\nevery\\tfew\\ttraining\\titerations,\\twe\\twill\\tcopy\\tthe\\tcritic\\tDQN\\tto\\tthe\\tactor\\tDQN.\\tAnd\\tthat’s\\tit!\\t\\nEquation\\t16-7\\nshows\\tthe\\t\\ncost\\tfunction\\tused\\tto\\ttrain\\tthe\\tcritic\\tDQN:\\nEquation\\t16-7.\\t\\nDeep\\tQ-Learning\\tcost\\tfunction\\ns\\n(\\ni\\n)\\n,\\t\\na\\n(\\ni\\n)\\n,\\t\\nr\\n(\\ni\\n)\\n\\tand\\t\\ns\\n′\\n(\\ni\\n)\\n\\tare\\trespectively\\tthe\\tstate,\\taction,\\treward,\\tand\\tnext\\tstate\\tof\\tthe\\ti\\nth\\n\\tmemory\\nsampled\\tfrom\\tthe\\treplay\\tmemory.\\nm\\n\\tis\\tthe\\tsize\\tof\\tthe\\tmemory\\tbatch.\\nθ\\ncritic\\n\\tand\\t\\nθ\\nactor\\n\\tare\\tthe\\tcritic\\tand\\tthe\\tactor’s\\tparameters.', 'Q\\n(\\ns\\n(\\ni\\n)\\n,\\na\\n(\\ni\\n)\\n,\\nθ\\ncritic\\n)\\tis\\tthe\\tcritic\\tDQN’s\\tprediction\\tof\\tthe\\ti\\nth\\n\\tmemorized\\tstate-action’s\\tQ-Value.\\nQ\\n(\\ns\\n′\\n(\\ni\\n)\\n,\\t\\na\\n′,\\t\\nθ\\nactor\\n)\\tis\\tthe\\tactor\\tDQN’s\\tprediction\\tof\\tthe\\tQ-Value\\tit\\tcan\\texpect\\tfrom\\tthe\\tnext\\tstate\\t\\ns\\n′\\n(\\ni\\n)\\n\\tif\\nit\\tchooses\\taction\\t\\na\\n′.\\ny\\n(\\ni\\n)\\n\\tis\\tthe\\ttarget\\tQ-Value\\tfor\\tthe\\ti\\nth\\n\\tmemory.\\tNote\\tthat\\tit\\tis\\tequal\\tto\\tthe\\treward\\tactually\\tobserved\\tby\\nthe\\tactor,\\tplus\\tthe\\tactor’s\\t\\nprediction\\n\\tof\\twhat\\tfuture\\trewards\\tit\\tshould\\texpect\\tif\\tit\\twere\\tto\\tplay\\noptimally\\t(as\\tfar\\tas\\tit\\tknows).\\nJ\\n(\\nθ\\ncritic\\n)\\tis\\tthe\\tcost\\tfunction\\tused\\tto\\ttrain\\tthe\\tcritic\\tDQN.\\tAs\\tyou\\tcan\\tsee,\\tit\\tis\\tjust\\tthe\\tMean\\tSquared', 'Error\\tbetween\\tthe\\ttarget\\tQ-Values\\t\\ny\\n(\\ni\\n)\\n\\tas\\testimated\\tby\\tthe\\tactor\\tDQN,\\tand\\tthe\\tcritic\\tDQN’s\\npredictions\\tof\\tthese\\tQ-Values.\\nNOTE\\nThe\\treplay\\tmemory\\tis\\toptional,\\tbut\\thighly\\trecommended.\\tWithout\\tit,\\tyou\\twould\\ttrain\\tthe\\tcritic\\tDQN\\tusing\\tconsecutive\\nexperiences\\tthat\\tmay\\tbe\\tvery\\tcorrelated.\\tThis\\twould\\tintroduce\\ta\\tlot\\tof\\tbias\\tand\\tslow\\tdown\\tthe\\ttraining\\talgorithm’s\\tconvergence.\\nBy\\tusing\\ta\\treplay\\tmemory,\\twe\\tensure\\tthat\\tthe\\tmemories\\tfed\\tto\\tthe\\ttraining\\talgorithm\\tcan\\tbe\\tfairly\\tuncorrelated.\\nLet’s\\tadd\\tthe\\tcritic\\tDQN’s\\ttraining\\toperations.\\tFirst,\\t\\nwe\\tneed\\tto\\tbe\\table\\tto\\tcompute\\tits\\tpredicted\\tQ-\\nValues\\tfor\\teach\\tstate-action\\tin\\tthe\\tmemory\\tbatch.\\tSince\\tthe\\tDQN\\toutputs\\tone\\tQ-Value\\tfor\\tevery\\tpossible\\naction,\\twe\\tneed\\tto\\tkeep\\tonly\\tthe\\tQ-Value\\tthat\\tcorresponds\\tto\\tthe\\taction\\tthat\\twas\\tactually\\tchosen\\tin\\tthis\\nmemory.\\tFor\\tthis,\\twe\\twill\\tconvert\\tthe\\taction\\tto\\ta\\tone-hot\\tvector\\t(recall\\tthat\\tthis\\tis\\ta\\tvector\\tfull\\tof\\t0s\\nexcept\\tfor\\ta\\t1\\tat\\tthe\\ti\\nth', \"th\\n\\tindex),\\tand\\tmultiply\\tit\\tby\\tthe\\tQ-Values:\\tthis\\twill\\tzero\\tout\\tall\\tQ-Values\\texcept\\tfor\\nthe\\tone\\tcorresponding\\tto\\tthe\\tmemorized\\taction.\\tThen\\tjust\\tsum\\tover\\tthe\\tfirst\\taxis\\tto\\tobtain\\tonly\\tthe\\tdesired\\nQ-Value\\tprediction\\tfor\\teach\\t\\nmemory.\\nX_action\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nint32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n])\\nq_value\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_sum\\n(\\ncritic_q_values\\n\\t\\n*\\n\\t\\ntf\\n.\\none_hot\\n(\\nX_action\\n,\\n\\t\\nn_outputs\\n),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\naxis\\n=\\n1\\n,\\n\\t\\nkeep_dims\\n=\\nTrue\\n)\\nNext\\tlet’s\\tadd\\tthe\\ttraining\\toperations,\\tassuming\\tthe\\ttarget\\tQ-Values\\twill\\tbe\\tfed\\tthrough\\ta\\tplaceholder.\\tWe\\nalso\\tcreate\\ta\\tnontrainable\\tvariable\\tcalled\\t\\nglobal_step\\n.\\t\\nThe\\toptimizer’s\\t\\nminimize()\\n\\toperation\\t\\nwill\\ntake\\tcare\\tof\\tincrementing\\tit.\\tPlus\\twe\\tcreate\\tthe\\tusual\\t\\ninit\\n\\toperation\\t\\nand\\ta\\t\\nSaver\\n.\\ny\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\ntf\\n.\\nfloat32\\n,\\n\\t\\nshape\\n=\\n[\\nNone\\n,\\n\\t\\n1\\n])\\ncost\\n\\t\\n=\\n\\t\\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\ny\\n\\t\\n-\\n\\t\\nq_value\\n))\\nglobal_step\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(\\n0\\n,\\n\\t\\ntrainable\\n=\\nFalse\\n,\\n\\t\\nname\\n=\\n'global_step'\\n)\\noptimizer\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\", '.\\ntrain\\n.\\nAdamOptimizer\\n(\\nlearning_rate\\n)\\ntraining_op\\n\\t\\n=\\n\\t\\noptimizer\\n.\\nminimize\\n(\\ncost\\n,\\n\\t\\nglobal_step\\n=\\nglobal_step\\n)\\ninit\\n\\t\\n=\\n\\t\\ntf\\n.\\nglobal_variables_initializer\\n()\\nsaver\\n\\t\\n=\\n\\t\\ntf\\n.\\ntrain\\n.\\nSaver\\n()\\nThat’s\\tit\\tfor\\tthe\\tconstruction\\tphase.\\tBefore\\twe\\tlook\\tat\\tthe\\texecution\\tphase,\\twe\\twill\\tneed\\ta\\tcouple\\tof\\ntools.\\tFirst,\\tlet’s\\tstart\\tby\\timplementing\\tthe\\treplay\\tmemory.\\tWe\\twill\\tuse\\ta\\t\\ndeque\\n\\tlist\\tsince\\tit\\tis\\tvery\\nefficient\\tat\\tpushing\\titems\\tto\\tthe\\tqueue\\tand\\tpopping\\tthem\\tout\\tfrom\\tthe\\tend\\tof\\tthe\\tlist\\twhen\\tthe\\tmaximum\\nmemory\\tsize\\tis\\treached.\\tWe\\twill\\talso\\twrite\\ta\\tsmall\\tfunction\\tto\\trandomly\\tsample\\ta\\tbatch\\tof\\texperiences\\nfrom\\tthe\\treplay\\tmemory:\\nfrom\\n\\t\\ncollections\\n\\t\\nimport\\n\\t\\ndeque\\nreplay_memory_size\\n\\t\\n=\\n\\t\\n10000\\nreplay_memory\\n\\t\\n=\\n\\t\\ndeque\\n([],\\n\\t\\nmaxlen\\n=\\nreplay_memory_size\\n)\\ndef\\n\\t\\nsample_memories\\n(\\nbatch_size\\n):\\n\\t\\t\\t\\t\\nindices\\n\\t\\n=\\n\\t\\nrnd\\n.\\npermutation\\n(\\nlen\\n(\\nreplay_memory\\n))[:\\nbatch_size\\n]\\n\\t\\t\\t\\t\\ncols\\n\\t\\n=\\n\\t\\n[[],\\n\\t\\n[],\\n\\t\\n[],\\n\\t\\n[],\\n\\t\\n[]]\\n\\t\\n#\\tstate,\\taction,\\treward,\\tnext_state,\\tcontinue\\n\\t\\t\\t\\t\\nfor\\n\\t\\nidx\\n\\t\\nin', 'in\\n\\t\\nindices\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nmemory\\n\\t\\n=\\n\\t\\nreplay_memory\\n[\\nidx\\n]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nfor\\n\\t\\ncol\\n,\\n\\t\\nvalue\\n\\t\\nin\\n\\t\\nzip\\n(\\ncols\\n,\\n\\t\\nmemory\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncol\\n.\\nappend\\n(\\nvalue\\n)\\n\\t\\t\\t\\t\\ncols\\n\\t\\n=\\n\\t\\n[\\nnp\\n.\\narray\\n(\\ncol\\n)\\n\\t\\nfor\\n\\t\\ncol\\n\\t\\nin\\n\\t\\ncols\\n]', 'return\\n\\t\\n(\\ncols\\n[\\n0\\n],\\n\\t\\ncols\\n[\\n1\\n],\\n\\t\\ncols\\n[\\n2\\n]\\n.\\nreshape\\n(\\n-\\n1\\n,\\n\\t\\n1\\n),\\n\\t\\ncols\\n[\\n3\\n],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncols\\n[\\n4\\n]\\n.\\nreshape\\n(\\n-\\n1\\n,\\n\\t\\n1\\n))\\nNext,\\twe\\twill\\tneed\\tthe\\tactor\\tto\\texplore\\tthe\\tgame.\\tWe\\twill\\tuse\\tthe\\tε-greedy\\tpolicy,\\tand\\tgradually\\tdecrease\\nε\\tfrom\\t1.0\\tto\\t0.05,\\tin\\t50,000\\ttraining\\tsteps:\\neps_min\\n\\t\\n=\\n\\t\\n0.05\\neps_max\\n\\t\\n=\\n\\t\\n1.0\\neps_decay_steps\\n\\t\\n=\\n\\t\\n50000\\ndef\\n\\t\\nepsilon_greedy\\n(\\nq_values\\n,\\n\\t\\nstep\\n):\\n\\t\\t\\t\\t\\nepsilon\\n\\t\\n=\\n\\t\\nmax\\n(\\neps_min\\n,\\n\\t\\neps_max\\n\\t\\n-\\n\\t\\n(\\neps_max\\n-\\neps_min\\n)\\n\\t\\n*\\n\\t\\nstep\\n/\\neps_decay_steps\\n)\\n\\t\\t\\t\\t\\nif\\n\\t\\nrnd\\n.\\nrand\\n()\\n\\t\\n<\\n\\t\\nepsilon\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nrnd\\n.\\nrandint\\n(\\nn_outputs\\n)\\n\\t\\n#\\trandom\\taction\\n\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreturn\\n\\t\\nnp\\n.\\nargmax\\n(\\nq_values\\n)\\n\\t\\n#\\toptimal\\taction\\nThat’s\\tit!\\tWe\\thave\\tall\\twe\\tneed\\tto\\tstart\\ttraining.\\tThe\\texecution\\tphase\\tdoes\\tnot\\tcontain\\tanything\\ttoo\\ncomplex,\\tbut\\tit\\tis\\ta\\tbit\\tlong,\\tso\\ttake\\ta\\tdeep\\tbreath.\\tReady?\\tLet’s\\tgo!\\tFirst,\\tlet’s\\tinitialize\\ta\\tfew\\tvariables:\\nn_steps\\n\\t\\n=\\n\\t\\n100000\\n\\t\\t\\n#\\ttotal\\tnumber\\tof\\ttraining\\tsteps\\ntraining_start\\n\\t\\n=\\n\\t\\n1000', '1000\\n\\t\\t\\n#\\tstart\\ttraining\\tafter\\t1,000\\tgame\\titerations\\ntraining_interval\\n\\t\\n=\\n\\t\\n3\\n\\t\\t\\n#\\trun\\ta\\ttraining\\tstep\\tevery\\t3\\tgame\\titerations\\nsave_steps\\n\\t\\n=\\n\\t\\n50\\n\\t\\t\\n#\\tsave\\tthe\\tmodel\\tevery\\t50\\ttraining\\tsteps\\ncopy_steps\\n\\t\\n=\\n\\t\\n25\\n\\t\\t\\n#\\tcopy\\tthe\\tcritic\\tto\\tthe\\tactor\\tevery\\t25\\ttraining\\tsteps\\ndiscount_rate\\n\\t\\n=\\n\\t\\n0.95\\nskip_start\\n\\t\\n=\\n\\t\\n90\\n\\t\\t\\n#\\tskip\\tthe\\tstart\\tof\\tevery\\tgame\\t(it\\'s\\tjust\\twaiting\\ttime)\\nbatch_size\\n\\t\\n=\\n\\t\\n50\\niteration\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\n#\\tgame\\titerations\\ncheckpoint_path\\n\\t\\n=\\n\\t\\n\"./my_dqn.ckpt\"\\ndone\\n\\t\\n=\\n\\t\\nTrue\\n\\t\\n#\\tenv\\tneeds\\tto\\tbe\\treset\\nNext,\\tlet’s\\topen\\tthe\\tsession\\tand\\trun\\tthe\\tmain\\ttraining\\tloop:\\nwith\\n\\t\\ntf\\n.\\nSession\\n()\\n\\t\\nas\\n\\t\\nsess\\n:\\n\\t\\t\\t\\t\\nif\\n\\t\\nos\\n.\\npath\\n.\\nisfile\\n(\\ncheckpoint_path\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nsaver\\n.\\nrestore\\n(\\nsess\\n,\\n\\t\\ncheckpoint_path\\n)\\n\\t\\t\\t\\t\\nelse\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ninit\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\nwhile\\n\\t\\nTrue\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nstep\\n\\t\\n=\\n\\t\\nglobal_step\\n.\\neval\\n()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nstep\\n\\t\\n>=\\n\\t\\nn_steps\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nbreak\\n\\t\\t\\t\\t\\t\\t\\t\\t\\niteration\\n\\t\\n+=\\n\\t\\n1\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\ndone\\n:\\n\\t\\n#\\tgame\\tover,\\tstart\\tagain\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n\\t\\n=\\n\\t\\nenv\\n.\\nreset\\n()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nfor', \"for\\n\\t\\nskip\\n\\t\\nin\\n\\t\\nrange\\n(\\nskip_start\\n):\\n\\t\\n#\\tskip\\tthe\\tstart\\tof\\teach\\tgame\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n,\\n\\t\\nreward\\n,\\n\\t\\ndone\\n,\\n\\t\\ninfo\\n\\t\\n=\\n\\t\\nenv\\n.\\nstep\\n(\\n0\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nstate\\n\\t\\n=\\n\\t\\npreprocess_observation\\n(\\nobs\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tActor\\tevaluates\\twhat\\tto\\tdo\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nq_values\\n\\t\\n=\\n\\t\\nactor_q_values\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nX_state\\n:\\n\\t\\n[\\nstate\\n]})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\naction\\n\\t\\n=\\n\\t\\nepsilon_greedy\\n(\\nq_values\\n,\\n\\t\\nstep\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tActor\\tplays\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nobs\\n,\\n\\t\\nreward\\n,\\n\\t\\ndone\\n,\\n\\t\\ninfo\\n\\t\\n=\\n\\t\\nenv\\n.\\nstep\\n(\\naction\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nnext_state\\n\\t\\n=\\n\\t\\npreprocess_observation\\n(\\nobs\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tLet's\\tmemorize\\twhat\\tjust\\thappened\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nreplay_memory\\n.\\nappend\\n((\\nstate\\n,\\n\\t\\naction\\n,\\n\\t\\nreward\\n,\\n\\t\\nnext_state\\n,\\n\\t\\n1.0\\n\\t\\n-\\n\\t\\ndone\\n))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nstate\\n\\t\\n=\\n\\t\\nnext_state\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\niteration\\n\\t\\n<\\n\\t\\ntraining_start\\n\\t\\nor\\n\\t\\niteration\\n\\t\\n%\\n\\t\\ntraining_interval\\n\\t\\n!=\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncontinue\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tCritic\\tlearns\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nX_state_val\\n,\\n\\t\\nX_action_val\\n,\\n\\t\\nrewards\\n,\\n\\t\\nX_next_state_val\\n,\\n\\t\\ncontinues\\n\\t\\n=\\n\\t\\n(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsample_memories\\n(\\nbatch_size\\n))\", '))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nnext_q_values\\n\\t\\n=\\n\\t\\nactor_q_values\\n.\\neval\\n(', 'feed_dict\\n=\\n{\\nX_state\\n:\\n\\t\\nX_next_state_val\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nmax_next_q_values\\n\\t\\n=\\n\\t\\nnp\\n.\\nmax\\n(\\nnext_q_values\\n,\\n\\t\\naxis\\n=\\n1\\n,\\n\\t\\nkeepdims\\n=\\nTrue\\n)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ny_val\\n\\t\\n=\\n\\t\\nrewards\\n\\t\\n+\\n\\t\\ncontinues\\n\\t\\n*\\n\\t\\ndiscount_rate\\n\\t\\n*\\n\\t\\nmax_next_q_values\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ntraining_op\\n.\\nrun\\n(\\nfeed_dict\\n=\\n{\\nX_state\\n:\\n\\t\\nX_state_val\\n,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nX_action\\n:\\n\\t\\nX_action_val\\n,\\n\\t\\ny\\n:\\n\\t\\ny_val\\n})\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tRegularly\\tcopy\\tcritic\\tto\\tactor\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nstep\\n\\t\\n%\\n\\t\\ncopy_steps\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ncopy_critic_to_actor\\n.\\nrun\\n()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n#\\tAnd\\tsave\\tregularly\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nif\\n\\t\\nstep\\n\\t\\n%\\n\\t\\nsave_steps\\n\\t\\n==\\n\\t\\n0\\n:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nsaver\\n.\\nsave\\n(\\nsess\\n,\\n\\t\\ncheckpoint_path\\n)\\nWe\\tstart\\tby\\trestoring\\tthe\\tmodels\\tif\\ta\\tcheckpoint\\tfile\\texists,\\tor\\telse\\twe\\tjust\\tinitialize\\tthe\\tvariables\\nnormally.\\tThen\\tthe\\tmain\\tloop\\tstarts,\\twhere\\t\\niteration\\n\\tcounts\\tthe\\ttotal\\tnumber\\tof\\tgame\\tsteps\\twe\\thave\\ngone\\tthrough\\tsince\\tthe\\tprogram\\tstarted,\\tand\\t\\nstep\\n\\tcounts\\tthe\\ttotal\\tnumber\\tof\\ttraining\\tsteps\\tsince\\ttraining', 'started\\t(if\\ta\\tcheckpoint\\tis\\trestored,\\tthe\\tglobal\\tstep\\tis\\trestored\\tas\\twell).\\tThen\\tthe\\tcode\\tresets\\tthe\\tgame\\n(and\\tskips\\tthe\\tfirst\\tboring\\tgame\\tsteps,\\twhere\\tnothing\\thappens).\\tNext,\\tthe\\tactor\\tevaluates\\twhat\\tto\\tdo,\\tand\\nplays\\tthe\\tgame,\\tand\\tits\\texperience\\tis\\tmemorized\\tin\\treplay\\tmemory.\\tThen,\\tat\\tregular\\tintervals\\t(after\\ta\\nwarmup\\tperiod),\\tthe\\tcritic\\tgoes\\tthrough\\ta\\ttraining\\tstep.\\tIt\\tsamples\\ta\\tbatch\\tof\\tmemories\\tand\\tasks\\tthe\\tactor\\nto\\testimate\\tthe\\tQ-Values\\tof\\tall\\tactions\\tfor\\tthe\\tnext\\tstate,\\tand\\tit\\tapplies\\t\\nEquation\\t16-7\\n\\tto\\tcompute\\tthe\\ttarget\\nQ-Value\\t\\ny_val\\n.\\tThe\\tonly\\ttricky\\tpart\\there\\tis\\tthat\\twe\\tmust\\tmultiply\\tthe\\tnext\\tstate’s\\tQ-Values\\tby\\tthe\\ncontinues\\n\\tvector\\tto\\tzero\\tout\\tthe\\tQ-Values\\tcorresponding\\tto\\tmemories\\twhere\\tthe\\tgame\\twas\\tover.\\tNext\\nwe\\trun\\ta\\ttraining\\toperation\\tto\\timprove\\tthe\\tcritic’s\\tability\\tto\\tpredict\\tQ-Values.\\tFinally,\\tat\\tregular\\tintervals\\nwe\\tcopy\\tthe\\tcritic\\tto\\tthe\\tactor,\\tand\\twe\\tsave\\tthe\\tmodel.\\nTIP', 'TIP\\nUnfortunately,\\ttraining\\tis\\tvery\\tslow:\\tif\\tyou\\tuse\\tyour\\tlaptop\\tfor\\ttraining,\\tit\\twill\\ttake\\tdays\\tbefore\\tMs.\\tPac-Man\\tgets\\tany\\tgood,\\tand\\tif\\nyou\\tlook\\tat\\tthe\\tlearning\\tcurve,\\tmeasuring\\tthe\\t\\naverage\\trewards\\tper\\tepisode,\\tyou\\twill\\tnotice\\tthat\\tit\\tis\\textremely\\tnoisy.\\tAt\\tsome\\npoints\\tthere\\tmay\\tbe\\tno\\tapparent\\tprogress\\tfor\\ta\\tvery\\tlong\\ttime\\tuntil\\tsuddenly\\tthe\\tagent\\tlearns\\tto\\tsurvive\\ta\\treasonable\\tamount\\tof\\ntime.\\tAs\\tmentioned\\tearlier,\\tone\\tsolution\\tis\\tto\\tinject\\tas\\tmuch\\tprior\\tknowledge\\tas\\tpossible\\tinto\\tthe\\tmodel\\t(e.g.,\\tthrough\\npreprocessing,\\trewards,\\tand\\tso\\ton),\\tand\\tyou\\tcan\\talso\\ttry\\tto\\tbootstrap\\t\\nthe\\tmodel\\tby\\tfirst\\ttraining\\tit\\tto\\timitate\\ta\\tbasic\\tstrategy.\\tIn\\nany\\tcase,\\tRL\\tstill\\trequires\\tquite\\ta\\tlot\\tof\\tpatience\\tand\\ttweaking,\\tbut\\tthe\\tend\\tresult\\tis\\tvery\\t\\nexciting.', 'Exercises\\n1\\n.\\t\\nHow\\twould\\tyou\\tdefine\\tReinforcement\\tLearning?\\tHow\\tis\\tit\\tdifferent\\tfrom\\tregular\\tsupervised\\tor\\nunsupervised\\tlearning?\\n2\\n.\\t\\nCan\\tyou\\tthink\\tof\\tthree\\tpossible\\tapplications\\tof\\tRL\\tthat\\twere\\tnot\\tmentioned\\tin\\tthis\\tchapter?\\tFor\\teach\\nof\\tthem,\\twhat\\tis\\tthe\\tenvironment?\\tWhat\\tis\\tthe\\tagent?\\tWhat\\tare\\tpossible\\tactions?\\tWhat\\tare\\tthe\\nrewards?\\n3\\n.\\t\\nWhat\\tis\\tthe\\tdiscount\\trate?\\tCan\\tthe\\toptimal\\tpolicy\\tchange\\tif\\tyou\\tmodify\\tthe\\tdiscount\\trate?\\n4\\n.\\t\\nHow\\tdo\\tyou\\tmeasure\\tthe\\tperformance\\tof\\ta\\tReinforcement\\tLearning\\tagent?\\n5\\n.\\t\\nWhat\\tis\\tthe\\tcredit\\tassignment\\tproblem?\\tWhen\\tdoes\\tit\\toccur?\\tHow\\tcan\\tyou\\talleviate\\tit?\\n6\\n.\\t\\nWhat\\tis\\tthe\\tpoint\\tof\\tusing\\ta\\treplay\\tmemory?\\n7\\n.\\t\\nWhat\\tis\\tan\\toff-policy\\tRL\\talgorithm?\\n8\\n.\\t\\nUse\\tDeep\\tQ-Learning\\tto\\ttackle\\tOpenAI\\tgym’s\\t“BypedalWalker-v2.”\\tThe\\tQ-networks\\tdo\\tnot\\tneed\\tto\\nbe\\tvery\\tdeep\\tfor\\tthis\\ttask.\\n9\\n.\\t\\nUse\\tpolicy\\tgradients\\tto\\ttrain\\tan\\tagent\\tto\\tplay\\t\\nPong\\n,\\tthe\\tfamous\\tAtari\\tgame\\t(\\nPong-v0\\n\\tin\\tthe\\tOpenAI', 'gym).\\tBeware:\\tan\\tindividual\\tobservation\\tis\\tinsufficient\\tto\\ttell\\tthe\\tdirection\\tand\\tspeed\\tof\\tthe\\tball.\\nOne\\tsolution\\tis\\tto\\tpass\\ttwo\\tobservations\\tat\\ta\\ttime\\tto\\tthe\\tneural\\tnetwork\\tpolicy.\\tTo\\treduce\\ndimensionality\\tand\\tspeed\\tup\\ttraining,\\tyou\\tshould\\tdefinitely\\tpreprocess\\tthese\\timages\\t(crop,\\tresize,\\nand\\tconvert\\tthem\\tto\\tblack\\tand\\twhite),\\tand\\tpossibly\\tmerge\\tthem\\tinto\\ta\\tsingle\\timage\\t(e.g.,\\tby\\noverlaying\\tthem).\\n10\\n.\\t\\nIf\\tyou\\thave\\tabout\\t$100\\tto\\tspare,\\tyou\\tcan\\tpurchase\\ta\\tRaspberry\\tPi\\t3\\tplus\\tsome\\tcheap\\trobotics\\ncomponents,\\tinstall\\tTensorFlow\\ton\\tthe\\tPi,\\tand\\tgo\\twild!\\tFor\\tan\\texample,\\tcheck\\tout\\tthis\\t\\nfun\\tpost\\n\\tby\\nLukas\\tBiewald,\\tor\\ttake\\ta\\tlook\\tat\\tGoPiGo\\tor\\tBrickPi.\\tWhy\\tnot\\ttry\\tto\\tbuild\\ta\\treal-life\\tcartpole\\tby\\ntraining\\tthe\\trobot\\tusing\\tpolicy\\tgradients?\\tOr\\tbuild\\ta\\trobotic\\tspider\\tthat\\tlearns\\tto\\twalk;\\tgive\\tit\\nrewards\\tany\\ttime\\tit\\tgets\\tcloser\\tto\\tsome\\tobjective\\t(you\\twill\\tneed\\tsensors\\tto\\tmeasure\\tthe\\tdistance\\tto\\nthe\\tobjective).\\tThe\\tonly\\tlimit\\tis\\tyour\\timagination.\\nSolutions\\tto\\tthese\\texercises\\tare\\tavailable\\tin', 'Appendix\\tA\\n.', 'Thank\\tYou!\\nBefore\\twe\\tclose\\tthe\\tlast\\tchapter\\tof\\tthis\\tbook,\\tI\\twould\\tlike\\tto\\tthank\\tyou\\tfor\\treading\\tit\\tup\\tto\\tthe\\tlast\\nparagraph.\\tI\\ttruly\\thope\\tthat\\tyou\\thad\\tas\\tmuch\\tpleasure\\treading\\tthis\\tbook\\tas\\tI\\thad\\twriting\\tit,\\tand\\tthat\\tit\\twill\\nbe\\tuseful\\tfor\\tyour\\tprojects,\\tbig\\tor\\tsmall.\\nIf\\tyou\\tfind\\terrors,\\tplease\\tsend\\tfeedback.\\tMore\\tgenerally,\\tI\\twould\\tlove\\tto\\tknow\\twhat\\tyou\\tthink,\\tso\\tplease\\ndon’t\\thesitate\\tto\\tcontact\\tme\\tvia\\tO’Reilly,\\tor\\tthrough\\tthe\\t\\nageron/handson-ml\\n\\tGitHub\\tproject.\\nGoing\\tforward,\\tmy\\tbest\\tadvice\\tto\\tyou\\tis\\tto\\tpractice\\tand\\tpractice:\\ttry\\tgoing\\tthrough\\tall\\tthe\\texercises\\tif\\tyou\\nhave\\tnot\\tdone\\tso\\talready,\\tplay\\twith\\tthe\\tJupyter\\tnotebooks,\\tjoin\\tKaggle.com\\tor\\tsome\\tother\\tML\\tcommunity,\\nwatch\\tML\\tcourses,\\tread\\tpapers,\\tattend\\tconferences,\\tmeet\\texperts.\\tYou\\tmay\\talso\\twant\\tto\\tstudy\\tsome\\ttopics\\nthat\\twe\\tdid\\tnot\\tcover\\tin\\tthis\\tbook,\\tincluding\\trecommender\\tsystems,\\tclustering\\talgorithms,\\tanomaly\\ndetection\\talgorithms,\\tand\\tgenetic\\talgorithms.', 'My\\tgreatest\\thope\\tis\\tthat\\tthis\\tbook\\twill\\tinspire\\tyou\\tto\\tbuild\\ta\\twonderful\\tML\\tapplication\\tthat\\twill\\tbenefit\\nall\\tof\\tus!\\t\\nWhat\\twill\\tit\\tbe?\\nAurélien\\tGéron,\\tNovember\\t26th,\\t2016\\nFor\\tmore\\tdetails,\\tbe\\tsure\\tto\\tcheck\\tout\\tRichard\\tSutton\\tand\\tAndrew\\tBarto’s\\t\\nbook\\ton\\tRL\\n,\\t\\nReinforcement\\tLearning:\\tAn\\tIntroduction\\n\\t(MIT\\nPress),\\tor\\tDavid\\tSilver’s\\tfree\\t\\nonline\\tRL\\tcourse\\n\\tat\\tUniversity\\tCollege\\tLondon.\\n“Playing\\tAtari\\twith\\tDeep\\tReinforcement\\tLearning,”\\tV.\\tMnih\\tet\\tal.\\t(2013).\\n“Human-level\\tcontrol\\tthrough\\tdeep\\treinforcement\\tlearning,”\\tV.\\tMnih\\tet\\tal.\\t(2015).\\nCheck\\tout\\tthe\\tvideos\\tof\\tDeepMind’s\\tsystem\\tlearning\\tto\\tplay\\t\\nSpace\\tInvaders\\n,\\t\\nBreakout\\n,\\tand\\tmore\\tat\\t\\nhttps://goo.gl/yTsH6X\\n.\\nImages\\t(a),\\t(c),\\tand\\t(d)\\tare\\treproduced\\tfrom\\tWikipedia.\\t(a)\\tand\\t(d)\\tare\\tin\\tthe\\tpublic\\tdomain.\\t(c)\\twas\\tcreated\\tby\\tuser\\tStevertigo\\tand\\nreleased\\tunder\\t\\nCreative\\tCommons\\tBY-SA\\t2.0\\n.\\t(b)\\tis\\ta\\tscreenshot\\tfrom\\tthe\\tMs.\\tPac-Man\\tgame,\\tcopyright\\tAtari\\t(the\\tauthor\\tbelieves\\tit\\tto', 'be\\tfair\\tuse\\tin\\tthis\\tchapter).\\t(e)\\twas\\treproduced\\tfrom\\tPixabay,\\treleased\\tunder\\t\\nCreative\\tCommons\\tCC0\\n.\\nIt\\tis\\toften\\tbetter\\tto\\tgive\\tthe\\tpoor\\tperformers\\ta\\tslight\\tchance\\tof\\tsurvival,\\tto\\tpreserve\\tsome\\tdiversity\\tin\\tthe\\t“gene\\tpool.”\\nIf\\tthere\\tis\\ta\\tsingle\\tparent,\\tthis\\tis\\tcalled\\t\\nasexual\\treproduction\\n.\\tWith\\ttwo\\t(or\\tmore)\\tparents,\\tit\\tis\\tcalled\\t\\nsexual\\treproduction\\n.\\tAn\\toffspring’s\\ngenome\\t(in\\tthis\\tcase\\ta\\tset\\tof\\tpolicy\\tparameters)\\tis\\trandomly\\tcomposed\\tof\\tparts\\tof\\tits\\tparents’\\tgenomes.\\nOpenAI\\tis\\ta\\tnonprofit\\tartificial\\tintelligence\\tresearch\\tcompany,\\tfunded\\tin\\tpart\\tby\\tElon\\tMusk.\\tIts\\tstated\\tgoal\\tis\\tto\\tpromote\\tand\\tdevelop\\nfriendly\\tAIs\\tthat\\twill\\tbenefit\\thumanity\\t(rather\\tthan\\texterminate\\tit).\\n“Simple\\tStatistical\\tGradient-Following\\tAlgorithms\\tfor\\tConnectionist\\tReinforcement\\tLearning,”\\tR.\\tWilliams\\t(1992).\\nWe\\talready\\tdid\\tsomething\\tsimilar\\tin\\t\\nChapter\\t11\\n\\twhen\\twe\\tdiscussed\\tGradient\\tClipping:\\twe\\tfirst\\tcomputed\\tthe\\tgradients,\\tthen\\twe\\tclipped\\nthem,\\tand\\tfinally\\twe\\tapplied\\tthe\\tclipped\\tgradients.', '“A\\tMarkovian\\tDecision\\tProcess,”\\tR.\\tBellman\\t(1957).\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11', 'Appendix\\tA.\\t\\nExercise\\tSolutions\\nNOTE\\nSolutions\\tto\\tthe\\tcoding\\texercises\\tare\\tavailable\\tin\\tthe\\tonline\\tJupyter\\tnotebooks\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t1\\n:\\tThe\\tMachine\\tLearning\\tLandscape\\n1\\n.\\t\\nMachine\\tLearning\\tis\\tabout\\tbuilding\\tsystems\\tthat\\tcan\\tlearn\\tfrom\\tdata.\\tLearning\\tmeans\\tgetting\\tbetter\\tat\\nsome\\ttask,\\tgiven\\tsome\\tperformance\\tmeasure.\\n2\\n.\\t\\nMachine\\tLearning\\tis\\tgreat\\tfor\\tcomplex\\tproblems\\tfor\\twhich\\twe\\thave\\tno\\talgorithmic\\tsolution,\\tto\\nreplace\\tlong\\tlists\\tof\\thand-tuned\\trules,\\tto\\tbuild\\tsystems\\tthat\\tadapt\\tto\\tfluctuating\\tenvironments,\\tand\\nfinally\\tto\\thelp\\thumans\\tlearn\\t(e.g.,\\tdata\\tmining).\\n3\\n.\\t\\nA\\tlabeled\\ttraining\\tset\\tis\\ta\\ttraining\\tset\\tthat\\tcontains\\tthe\\tdesired\\tsolution\\t(a.k.a.\\ta\\tlabel)\\tfor\\teach\\ninstance.\\n4\\n.\\t\\nThe\\ttwo\\tmost\\tcommon\\tsupervised\\ttasks\\tare\\tregression\\tand\\tclassification.\\n5\\n.\\t\\nCommon\\tunsupervised\\ttasks\\tinclude\\tclustering,\\tvisualization,\\tdimensionality\\treduction,\\tand\\nassociation\\trule\\tlearning.\\n6\\n.\\t\\nReinforcement\\tLearning\\tis\\tlikely\\tto\\tperform\\tbest\\tif\\twe\\twant\\ta\\trobot\\tto\\tlearn\\tto\\twalk\\tin\\tvarious\\nunknown\\tterrains\\tsince\\tthis\\tis\\ttypically\\tthe\\ttype\\tof\\tproblem\\tthat\\tReinforcement\\tLearning\\ttackles.\\tIt', 'might\\tbe\\tpossible\\tto\\texpress\\tthe\\tproblem\\tas\\ta\\tsupervised\\tor\\tsemisupervised\\tlearning\\tproblem,\\tbut\\tit\\nwould\\tbe\\tless\\tnatural.\\n7\\n.\\t\\nIf\\tyou\\tdon’t\\tknow\\thow\\tto\\tdefine\\tthe\\tgroups,\\tthen\\tyou\\tcan\\tuse\\ta\\tclustering\\talgorithm\\t(unsupervised\\nlearning)\\tto\\tsegment\\tyour\\tcustomers\\tinto\\tclusters\\tof\\tsimilar\\tcustomers.\\tHowever,\\tif\\tyou\\tknow\\twhat\\ngroups\\tyou\\twould\\tlike\\tto\\thave,\\tthen\\tyou\\tcan\\tfeed\\tmany\\texamples\\tof\\teach\\tgroup\\tto\\ta\\tclassification\\nalgorithm\\t(supervised\\tlearning),\\tand\\tit\\twill\\tclassify\\tall\\tyour\\tcustomers\\tinto\\tthese\\tgroups.\\n8\\n.\\t\\nSpam\\tdetection\\tis\\ta\\ttypical\\tsupervised\\tlearning\\tproblem:\\tthe\\talgorithm\\tis\\tfed\\tmany\\temails\\talong\\nwith\\ttheir\\tlabel\\t(spam\\tor\\tnot\\tspam).\\n9\\n.\\t\\nAn\\tonline\\tlearning\\tsystem\\tcan\\tlearn\\tincrementally,\\tas\\topposed\\tto\\ta\\tbatch\\tlearning\\tsystem.\\tThis\\nmakes\\tit\\tcapable\\tof\\tadapting\\trapidly\\tto\\tboth\\tchanging\\tdata\\tand\\tautonomous\\tsystems,\\tand\\tof\\ttraining\\non\\tvery\\tlarge\\tquantities\\tof\\tdata.\\n10\\n.\\t\\nOut-of-core\\talgorithms\\tcan\\thandle\\tvast\\tquantities\\tof\\tdata\\tthat\\tcannot\\tfit\\tin\\ta\\tcomputer’s\\tmain', 'memory.\\tAn\\tout-of-core\\tlearning\\talgorithm\\tchops\\tthe\\tdata\\tinto\\tmini-batches\\tand\\tuses\\tonline\\tlearning\\ntechniques\\tto\\tlearn\\tfrom\\tthese\\tmini-batches.\\n11\\n.\\t\\nAn\\tinstance-based\\tlearning\\tsystem\\tlearns\\tthe\\ttraining\\tdata\\tby\\theart;\\tthen,\\twhen\\tgiven\\ta\\tnew\\tinstance,\\nit\\tuses\\ta\\tsimilarity\\tmeasure\\tto\\tfind\\tthe\\tmost\\tsimilar\\tlearned\\tinstances\\tand\\tuses\\tthem\\tto\\tmake\\npredictions.\\n12\\n.\\t\\nA\\tmodel\\thas\\tone\\tor\\tmore\\tmodel\\tparameters\\tthat\\tdetermine\\twhat\\tit\\twill\\tpredict\\tgiven\\ta\\tnew\\tinstance\\n(e.g.,\\tthe\\tslope\\tof\\ta\\tlinear\\tmodel).\\tA\\tlearning\\talgorithm\\ttries\\tto\\tfind\\toptimal\\tvalues\\tfor\\tthese\\nparameters\\tsuch\\tthat\\tthe\\tmodel\\tgeneralizes\\twell\\tto\\tnew\\tinstances.\\tA\\thyperparameter\\tis\\ta\\tparameter\\nof\\tthe\\tlearning\\talgorithm\\titself,\\tnot\\tof\\tthe\\tmodel\\t(e.g.,\\tthe\\tamount\\tof\\tregularization\\tto\\tapply).', '13\\n.\\t\\nModel-based\\tlearning\\talgorithms\\tsearch\\tfor\\tan\\toptimal\\tvalue\\tfor\\tthe\\tmodel\\tparameters\\tsuch\\tthat\\tthe\\nmodel\\twill\\tgeneralize\\twell\\tto\\tnew\\tinstances.\\tWe\\tusually\\ttrain\\tsuch\\tsystems\\tby\\tminimizing\\ta\\tcost\\nfunction\\tthat\\tmeasures\\thow\\tbad\\tthe\\tsystem\\tis\\tat\\tmaking\\tpredictions\\ton\\tthe\\ttraining\\tdata,\\tplus\\ta\\npenalty\\tfor\\tmodel\\tcomplexity\\tif\\tthe\\tmodel\\tis\\tregularized.\\tTo\\tmake\\tpredictions,\\twe\\tfeed\\tthe\\tnew\\ninstance’s\\tfeatures\\tinto\\tthe\\tmodel’s\\tprediction\\tfunction,\\tusing\\tthe\\tparameter\\tvalues\\tfound\\tby\\tthe\\nlearning\\talgorithm.\\n14\\n.\\t\\nSome\\tof\\tthe\\tmain\\tchallenges\\tin\\tMachine\\tLearning\\tare\\tthe\\tlack\\tof\\tdata,\\tpoor\\tdata\\tquality,\\nnonrepresentative\\tdata,\\tuninformative\\tfeatures,\\texcessively\\tsimple\\tmodels\\tthat\\tunderfit\\tthe\\ttraining\\ndata,\\tand\\texcessively\\tcomplex\\tmodels\\tthat\\toverfit\\tthe\\tdata.\\n15\\n.\\t\\nIf\\ta\\tmodel\\tperforms\\tgreat\\ton\\tthe\\ttraining\\tdata\\tbut\\tgeneralizes\\tpoorly\\tto\\tnew\\tinstances,\\tthe\\tmodel\\tis\\nlikely\\toverfitting\\tthe\\ttraining\\tdata\\t(or\\twe\\tgot\\textremely\\tlucky\\ton\\tthe\\ttraining\\tdata).\\tPossible\\tsolutions', 'to\\toverfitting\\tare\\tgetting\\tmore\\tdata,\\tsimplifying\\tthe\\tmodel\\t(selecting\\ta\\tsimpler\\talgorithm,\\treducing\\nthe\\tnumber\\tof\\tparameters\\tor\\tfeatures\\tused,\\tor\\tregularizing\\tthe\\tmodel),\\tor\\treducing\\tthe\\tnoise\\tin\\tthe\\ntraining\\tdata.\\n16\\n.\\t\\nA\\ttest\\tset\\tis\\tused\\tto\\testimate\\tthe\\tgeneralization\\terror\\tthat\\ta\\tmodel\\twill\\tmake\\ton\\tnew\\tinstances,\\tbefore\\nthe\\tmodel\\tis\\tlaunched\\tin\\tproduction.\\n17\\n.\\t\\nA\\tvalidation\\tset\\tis\\tused\\tto\\tcompare\\tmodels.\\tIt\\tmakes\\tit\\tpossible\\tto\\tselect\\tthe\\tbest\\tmodel\\tand\\ttune\\tthe\\nhyperparameters.\\n18\\n.\\t\\nIf\\tyou\\ttune\\thyperparameters\\tusing\\tthe\\ttest\\tset,\\tyou\\trisk\\toverfitting\\tthe\\ttest\\tset,\\tand\\tthe\\tgeneralization\\nerror\\tyou\\tmeasure\\twill\\tbe\\toptimistic\\t(you\\tmay\\tlaunch\\ta\\tmodel\\tthat\\tperforms\\tworse\\tthan\\tyou\\texpect).\\n19\\n.\\t\\nCross-validation\\tis\\ta\\ttechnique\\tthat\\tmakes\\tit\\tpossible\\tto\\tcompare\\tmodels\\t(for\\tmodel\\tselection\\tand\\nhyperparameter\\ttuning)\\twithout\\tthe\\tneed\\tfor\\ta\\tseparate\\tvalidation\\tset.\\tThis\\tsaves\\tprecious\\ttraining\\ndata.', 'Chapter\\t2\\n:\\tEnd-to-End\\tMachine\\tLearning\\tProject\\nSee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t3\\n:\\tClassification\\nSee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t4\\n:\\tTraining\\tModels\\n1\\n.\\t\\nIf\\tyou\\thave\\ta\\ttraining\\tset\\twith\\tmillions\\tof\\tfeatures\\tyou\\tcan\\tuse\\tStochastic\\tGradient\\tDescent\\tor\\tMini-\\nbatch\\tGradient\\tDescent,\\tand\\tperhaps\\tBatch\\tGradient\\tDescent\\tif\\tthe\\ttraining\\tset\\tfits\\tin\\tmemory.\\tBut\\nyou\\tcannot\\tuse\\tthe\\tNormal\\tEquation\\tbecause\\tthe\\tcomputational\\tcomplexity\\tgrows\\tquickly\\t(more\\tthan\\nquadratically)\\twith\\tthe\\tnumber\\tof\\tfeatures.\\n2\\n.\\t\\nIf\\tthe\\tfeatures\\tin\\tyour\\ttraining\\tset\\thave\\tvery\\tdifferent\\tscales,\\tthe\\tcost\\tfunction\\twill\\thave\\tthe\\tshape\\tof\\nan\\telongated\\tbowl,\\tso\\tthe\\tGradient\\tDescent\\talgorithms\\twill\\ttake\\ta\\tlong\\ttime\\tto\\tconverge.\\tTo\\tsolve\\nthis\\tyou\\tshould\\tscale\\tthe\\tdata\\tbefore\\ttraining\\tthe\\tmodel.\\tNote\\tthat\\tthe\\tNormal\\tEquation\\twill\\twork\\njust\\tfine\\twithout\\tscaling.\\tMoreover,\\tregularized\\tmodels\\tmay\\tconverge\\tto\\ta\\tsuboptimal\\tsolution\\tif\\tthe\\nfeatures\\tare\\tnot\\tscaled:\\tindeed,\\tsince\\tregularization\\tpenalizes\\tlarge\\tweights,\\tfeatures\\twith\\tsmaller\\nvalues\\twill\\ttend\\tto\\tbe\\tignored\\tcompared\\tto\\tfeatures\\twith\\tlarger\\tvalues.\\n3\\n.', '3\\n.\\t\\nGradient\\tDescent\\tcannot\\tget\\tstuck\\tin\\ta\\tlocal\\tminimum\\twhen\\ttraining\\ta\\tLogistic\\tRegression\\tmodel\\nbecause\\tthe\\tcost\\tfunction\\tis\\tconvex.\\n1\\n4\\n.\\t\\nIf\\tthe\\toptimization\\tproblem\\tis\\tconvex\\t(such\\tas\\tLinear\\tRegression\\tor\\tLogistic\\tRegression),\\tand\\nassuming\\tthe\\tlearning\\trate\\tis\\tnot\\ttoo\\thigh,\\tthen\\tall\\tGradient\\tDescent\\talgorithms\\twill\\tapproach\\tthe\\nglobal\\toptimum\\tand\\tend\\tup\\tproducing\\tfairly\\tsimilar\\tmodels.\\tHowever,\\tunless\\tyou\\tgradually\\treduce\\nthe\\tlearning\\trate,\\tStochastic\\tGD\\tand\\tMini-batch\\tGD\\twill\\tnever\\ttruly\\tconverge;\\tinstead,\\tthey\\twill\\nkeep\\tjumping\\tback\\tand\\tforth\\taround\\tthe\\tglobal\\toptimum.\\tThis\\tmeans\\tthat\\teven\\tif\\tyou\\tlet\\tthem\\trun\\tfor\\na\\tvery\\tlong\\ttime,\\tthese\\tGradient\\tDescent\\talgorithms\\twill\\tproduce\\tslightly\\tdifferent\\tmodels.\\n5\\n.\\t\\nIf\\tthe\\tvalidation\\terror\\tconsistently\\tgoes\\tup\\tafter\\tevery\\tepoch,\\tthen\\tone\\tpossibility\\tis\\tthat\\tthe\\tlearning\\nrate\\tis\\ttoo\\thigh\\tand\\tthe\\talgorithm\\tis\\tdiverging.\\tIf\\tthe\\ttraining\\terror\\talso\\tgoes\\tup,\\tthen\\tthis\\tis\\tclearly', 'the\\tproblem\\tand\\tyou\\tshould\\treduce\\tthe\\tlearning\\trate.\\tHowever,\\tif\\tthe\\ttraining\\terror\\tis\\tnot\\tgoing\\tup,\\nthen\\tyour\\tmodel\\tis\\toverfitting\\tthe\\ttraining\\tset\\tand\\tyou\\tshould\\tstop\\ttraining.\\n6\\n.\\t\\nDue\\tto\\ttheir\\trandom\\tnature,\\tneither\\tStochastic\\tGradient\\tDescent\\tnor\\tMini-batch\\tGradient\\tDescent\\tis\\nguaranteed\\tto\\tmake\\tprogress\\tat\\tevery\\tsingle\\ttraining\\titeration.\\tSo\\tif\\tyou\\timmediately\\tstop\\ttraining\\nwhen\\tthe\\tvalidation\\terror\\tgoes\\tup,\\tyou\\tmay\\tstop\\tmuch\\ttoo\\tearly,\\tbefore\\tthe\\toptimum\\tis\\treached.\\tA\\nbetter\\toption\\tis\\tto\\tsave\\tthe\\tmodel\\tat\\tregular\\tintervals,\\tand\\twhen\\tit\\thas\\tnot\\timproved\\tfor\\ta\\tlong\\ttime\\n(meaning\\tit\\twill\\tprobably\\tnever\\tbeat\\tthe\\trecord),\\tyou\\tcan\\trevert\\tto\\tthe\\tbest\\tsaved\\tmodel.\\n7\\n.\\t\\nStochastic\\tGradient\\tDescent\\thas\\tthe\\tfastest\\ttraining\\titeration\\tsince\\tit\\tconsiders\\tonly\\tone\\ttraining\\ninstance\\tat\\ta\\ttime,\\tso\\tit\\tis\\tgenerally\\tthe\\tfirst\\tto\\treach\\tthe\\tvicinity\\tof\\tthe\\tglobal\\toptimum\\t(or\\tMini-\\nbatch\\tGD\\twith\\ta\\tvery\\tsmall\\tmini-batch\\tsize).\\tHowever,\\tonly\\tBatch\\tGradient\\tDescent\\twill\\tactually', 'converge,\\tgiven\\tenough\\ttraining\\ttime.\\tAs\\tmentioned,\\tStochastic\\tGD\\tand\\tMini-batch\\tGD\\twill\\tbounce\\naround\\tthe\\toptimum,\\tunless\\tyou\\tgradually\\treduce\\tthe\\tlearning\\trate.\\n8\\n.\\t\\nIf\\tthe\\tvalidation\\terror\\tis\\tmuch\\thigher\\tthan\\tthe\\ttraining\\terror,\\tthis\\tis\\tlikely\\tbecause\\tyour\\tmodel\\tis\\noverfitting\\tthe\\ttraining\\tset.\\tOne\\tway\\tto\\ttry\\tto\\tfix\\tthis\\tis\\tto\\treduce\\tthe\\tpolynomial\\tdegree:\\ta\\tmodel\\nwith\\tfewer\\tdegrees\\tof\\tfreedom\\tis\\tless\\tlikely\\tto\\toverfit.\\tAnother\\tthing\\tyou\\tcan\\ttry\\tis\\tto\\tregularize\\tthe\\nmodel\\t—\\tfor\\texample,\\tby\\tadding\\tan\\tℓ\\n2\\n\\tpenalty\\t(Ridge)\\tor\\tan\\tℓ\\n1\\n\\tpenalty\\t(Lasso)\\tto\\tthe\\tcost\\tfunction.\\nThis\\twill\\talso\\treduce\\tthe\\tdegrees\\tof\\tfreedom\\tof\\tthe\\tmodel.\\tLastly,\\tyou\\tcan\\ttry\\tto\\tincrease\\tthe\\tsize\\tof', 'the\\ttraining\\tset.\\n9\\n.\\t\\nIf\\tboth\\tthe\\ttraining\\terror\\tand\\tthe\\tvalidation\\terror\\tare\\talmost\\tequal\\tand\\tfairly\\thigh,\\tthe\\tmodel\\tis\\tlikely\\nunderfitting\\tthe\\ttraining\\tset,\\twhich\\tmeans\\tit\\thas\\ta\\thigh\\tbias.\\tYou\\tshould\\ttry\\treducing\\tthe\\nregularization\\thyperparameter\\t\\nα\\n.\\n10\\n.\\t\\nLet’s\\tsee:\\nA\\tmodel\\twith\\tsome\\tregularization\\ttypically\\tperforms\\tbetter\\tthan\\ta\\tmodel\\twithout\\tany\\nregularization,\\tso\\tyou\\tshould\\tgenerally\\tprefer\\tRidge\\tRegression\\tover\\tplain\\tLinear\\tRegression.\\n2\\nLasso\\tRegression\\tuses\\tan\\tℓ\\n1\\n\\tpenalty,\\twhich\\ttends\\tto\\tpush\\tthe\\tweights\\tdown\\tto\\texactly\\tzero.\\nThis\\tleads\\tto\\tsparse\\tmodels,\\twhere\\tall\\tweights\\tare\\tzero\\texcept\\tfor\\tthe\\tmost\\timportant\\tweights.\\nThis\\tis\\ta\\tway\\tto\\tperform\\tfeature\\tselection\\tautomatically,\\twhich\\tis\\tgood\\tif\\tyou\\tsuspect\\tthat\\tonly\\na\\tfew\\tfeatures\\tactually\\tmatter.\\tWhen\\tyou\\tare\\tnot\\tsure,\\tyou\\tshould\\tprefer\\tRidge\\tRegression.\\nElastic\\tNet\\tis\\tgenerally\\tpreferred\\tover\\tLasso\\tsince\\tLasso\\tmay\\tbehave\\terratically\\tin\\tsome\\tcases', '(when\\tseveral\\tfeatures\\tare\\tstrongly\\tcorrelated\\tor\\twhen\\tthere\\tare\\tmore\\tfeatures\\tthan\\ttraining\\ninstances).\\tHowever,\\tit\\tdoes\\tadd\\tan\\textra\\thyperparameter\\tto\\ttune.\\tIf\\tyou\\tjust\\twant\\tLasso\\nwithout\\tthe\\terratic\\tbehavior,\\tyou\\tcan\\tjust\\tuse\\tElastic\\tNet\\twith\\tan\\t\\nl1_ratio\\n\\tclose\\tto\\t1.\\n11\\n.\\t\\nIf\\tyou\\twant\\tto\\tclassify\\tpictures\\tas\\toutdoor/indoor\\tand\\tdaytime/nighttime,\\tsince\\tthese\\tare\\tnot\\nexclusive\\tclasses\\t(i.e.,\\tall\\tfour\\tcombinations\\tare\\tpossible)\\tyou\\tshould\\ttrain\\ttwo\\tLogistic\\tRegression\\nclassifiers.\\n12\\n.\\t\\nSee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t5\\n:\\tSupport\\tVector\\tMachines\\n1\\n.\\t\\nThe\\tfundamental\\tidea\\tbehind\\tSupport\\tVector\\tMachines\\tis\\tto\\tfit\\tthe\\twidest\\tpossible\\t“street”\\tbetween\\nthe\\tclasses.\\tIn\\tother\\twords,\\tthe\\tgoal\\tis\\tto\\thave\\tthe\\tlargest\\tpossible\\tmargin\\tbetween\\tthe\\tdecision\\nboundary\\tthat\\tseparates\\tthe\\ttwo\\tclasses\\tand\\tthe\\ttraining\\tinstances.\\tWhen\\tperforming\\tsoft\\tmargin\\nclassification,\\tthe\\tSVM\\tsearches\\tfor\\ta\\tcompromise\\tbetween\\tperfectly\\tseparating\\tthe\\ttwo\\tclasses\\tand\\nhaving\\tthe\\twidest\\tpossible\\tstreet\\t(i.e.,\\ta\\tfew\\tinstances\\tmay\\tend\\tup\\ton\\tthe\\tstreet).\\tAnother\\tkey\\tidea\\tis\\nto\\tuse\\tkernels\\twhen\\ttraining\\ton\\tnonlinear\\tdatasets.\\n2\\n.\\t\\nAfter\\ttraining\\tan\\tSVM,\\ta\\t\\nsupport\\tvector\\n\\tis\\tany\\tinstance\\tlocated\\ton\\tthe\\t“street”\\t(see\\tthe\\tprevious\\nanswer),\\tincluding\\tits\\tborder.\\tThe\\tdecision\\tboundary\\tis\\tentirely\\tdetermined\\tby\\tthe\\tsupport\\tvectors.\\nAny\\tinstance\\tthat\\tis\\t\\nnot\\n\\ta\\tsupport\\tvector\\t(i.e.,\\toff\\tthe\\tstreet)\\thas\\tno\\tinfluence\\twhatsoever;\\tyou\\tcould\\nremove\\tthem,\\tadd\\tmore\\tinstances,\\tor\\tmove\\tthem\\taround,\\tand\\tas\\tlong\\tas\\tthey\\tstay\\toff\\tthe\\tstreet\\tthey', 'won’t\\taffect\\tthe\\tdecision\\tboundary.\\tComputing\\tthe\\tpredictions\\tonly\\tinvolves\\tthe\\tsupport\\tvectors,\\tnot\\nthe\\twhole\\ttraining\\tset.\\n3\\n.\\t\\nSVMs\\ttry\\tto\\tfit\\tthe\\tlargest\\tpossible\\t“street”\\tbetween\\tthe\\tclasses\\t(see\\tthe\\tfirst\\tanswer),\\tso\\tif\\tthe\\ntraining\\tset\\tis\\tnot\\tscaled,\\tthe\\tSVM\\twill\\ttend\\tto\\tneglect\\tsmall\\tfeatures\\t(see\\t\\nFigure\\t5-2\\n).\\n4\\n.\\t\\nAn\\tSVM\\tclassifier\\tcan\\toutput\\tthe\\tdistance\\tbetween\\tthe\\ttest\\tinstance\\tand\\tthe\\tdecision\\tboundary,\\tand\\nyou\\tcan\\tuse\\tthis\\tas\\ta\\tconfidence\\tscore.\\tHowever,\\tthis\\tscore\\tcannot\\tbe\\tdirectly\\tconverted\\tinto\\tan\\nestimation\\tof\\tthe\\tclass\\tprobability.\\tIf\\tyou\\tset\\t\\nprobability=True\\n\\twhen\\tcreating\\tan\\tSVM\\tin\\tScikit-\\nLearn,\\tthen\\tafter\\ttraining\\tit\\twill\\tcalibrate\\tthe\\tprobabilities\\tusing\\tLogistic\\tRegression\\ton\\tthe\\tSVM’s\\nscores\\t(trained\\tby\\tan\\tadditional\\tfive-fold\\tcross-validation\\ton\\tthe\\ttraining\\tdata).\\tThis\\twill\\tadd\\tthe\\npredict_proba()\\n\\tand\\t\\npredict_log_proba()\\n\\tmethods\\tto\\tthe\\tSVM.\\n5\\n.\\t\\nThis\\tquestion\\tapplies\\tonly\\tto\\tlinear\\tSVMs\\tsince\\tkernelized\\tcan\\tonly\\tuse\\tthe\\tdual\\tform.\\tThe', 'computational\\tcomplexity\\tof\\tthe\\tprimal\\tform\\tof\\tthe\\tSVM\\tproblem\\tis\\tproportional\\tto\\tthe\\tnumber\\tof\\ntraining\\tinstances\\t\\nm\\n,\\twhile\\tthe\\tcomputational\\tcomplexity\\tof\\tthe\\tdual\\tform\\tis\\tproportional\\tto\\ta\\nnumber\\tbetween\\t\\nm\\n2\\n\\tand\\t\\nm\\n3\\n.\\tSo\\tif\\tthere\\tare\\tmillions\\tof\\tinstances,\\tyou\\tshould\\tdefinitely\\tuse\\tthe\\tprimal\\nform,\\tbecause\\tthe\\tdual\\tform\\twill\\tbe\\tmuch\\ttoo\\tslow.\\n6\\n.\\t\\nIf\\tan\\tSVM\\tclassifier\\ttrained\\twith\\tan\\tRBF\\tkernel\\tunderfits\\tthe\\ttraining\\tset,\\tthere\\tmight\\tbe\\ttoo\\tmuch\\nregularization.\\tTo\\tdecrease\\tit,\\tyou\\tneed\\tto\\tincrease\\t\\ngamma\\n\\tor\\t\\nC\\n\\t(or\\tboth).\\n7\\n.\\t\\nLet’s\\tcall\\tthe\\tQP\\tparameters\\tfor\\tthe\\thard-margin\\tproblem\\t\\nH\\n′,\\t\\nf\\n′,\\t\\nA\\n′\\tand\\t\\nb\\n′\\t(see\\t\\n“Quadratic\\nProgramming”\\n).\\tThe\\tQP\\tparameters\\tfor\\tthe\\tsoft-margin\\tproblem\\thave\\t\\nm\\n\\tadditional\\tparameters\\t(\\nn\\np\\n\\t=\\nn\\n\\t+\\t1\\t+\\t\\nm\\n)\\tand\\t\\nm\\n\\tadditional\\tconstraints\\t(\\nn\\nc\\n\\t=\\t2\\nm\\n).\\tThey\\tcan\\tbe\\tdefined\\tlike\\tso:\\nH\\n\\tis\\tequal\\tto\\t\\nH\\n′,\\tplus\\t\\nm\\n\\tcolumns\\tof\\t0s\\ton\\tthe\\tright\\tand\\t\\nm\\n\\trows\\tof\\t0s\\tat\\tthe\\tbottom:\\t\\nf\\n\\tis\\tequal\\tto\\t\\nf\\n′\\twith\\t\\nm', '′\\twith\\t\\nm\\n\\tadditional\\telements,\\tall\\tequal\\tto\\tthe\\tvalue\\tof\\tthe\\thyperparameter\\t\\nC\\n.\\nb\\n\\tis\\tequal\\tto\\t\\nb\\n′\\twith\\t\\nm\\n\\tadditional\\telements,\\tall\\tequal\\tto\\t0.\\nA\\n\\tis\\tequal\\tto\\t\\nA\\n′,\\twith\\tan\\textra\\t\\nm\\n\\t×\\t\\nm\\n\\tidentity\\tmatrix\\t\\nI\\nm\\n\\tappended\\tto\\tthe\\tright,\\t–\\t\\nI\\nm\\n\\tjust\\tbelow\\tit,', 'and\\tthe\\trest\\tfilled\\twith\\tzeros:\\t\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t6\\n:\\tDecision\\tTrees\\n1\\n.\\t\\nThe\\tdepth\\tof\\ta\\twell-balanced\\tbinary\\ttree\\tcontaining\\t\\nm\\n\\tleaves\\tis\\tequal\\tto\\tlog\\n2\\n(\\nm\\n)\\n3\\n,\\trounded\\tup.\\tA\\nbinary\\tDecision\\tTree\\t(one\\tthat\\tmakes\\tonly\\tbinary\\tdecisions,\\tas\\tis\\tthe\\tcase\\tof\\tall\\ttrees\\tin\\tScikit-\\nLearn)\\twill\\tend\\tup\\tmore\\tor\\tless\\twell\\tbalanced\\tat\\tthe\\tend\\tof\\ttraining,\\twith\\tone\\tleaf\\tper\\ttraining\\ninstance\\tif\\tit\\tis\\ttrained\\twithout\\trestrictions.\\tThus,\\tif\\tthe\\ttraining\\tset\\tcontains\\tone\\tmillion\\tinstances,\\nthe\\tDecision\\tTree\\twill\\thave\\ta\\tdepth\\tof\\tlog\\n2\\n(10\\n6\\n)\\t≈\\t20\\t(actually\\ta\\tbit\\tmore\\tsince\\tthe\\ttree\\twill\\ngenerally\\tnot\\tbe\\tperfectly\\twell\\tbalanced).\\n2\\n.\\t\\nA\\tnode’s\\tGini\\timpurity\\tis\\tgenerally\\tlower\\tthan\\tits\\tparent’s.\\tThis\\tis\\tdue\\tto\\tthe\\tCART\\ttraining\\nalgorithm’s\\tcost\\tfunction,\\twhich\\tsplits\\teach\\tnode\\tin\\ta\\tway\\tthat\\tminimizes\\tthe\\tweighted\\tsum\\tof\\tits\\nchildren’s\\tGini\\timpurities.\\tHowever,\\tit\\tis\\tpossible\\tfor\\ta\\tnode\\tto\\thave\\ta\\thigher\\tGini\\timpurity\\tthan\\tits\\nparent,\\tas\\tlong\\tas\\tthis\\tincrease\\tis\\tmore\\tthan\\tcompensated\\tfor\\tby\\ta\\tdecrease\\tof\\tthe\\tother\\tchild’s', 'impurity.\\tFor\\texample,\\tconsider\\ta\\tnode\\tcontaining\\tfour\\tinstances\\tof\\tclass\\tA\\tand\\t1\\tof\\tclass\\tB.\\tIts\\tGini\\nimpurity\\tis\\t\\n\\t=\\t0.32.\\tNow\\tsuppose\\tthe\\tdataset\\tis\\tone-dimensional\\tand\\tthe\\tinstances\\tare\\nlined\\tup\\tin\\tthe\\tfollowing\\torder:\\tA,\\tB,\\tA,\\tA,\\tA.\\tYou\\tcan\\tverify\\tthat\\tthe\\talgorithm\\twill\\tsplit\\tthis\\tnode\\nafter\\tthe\\tsecond\\tinstance,\\tproducing\\tone\\tchild\\tnode\\twith\\tinstances\\tA,\\tB,\\tand\\tthe\\tother\\tchild\\tnode\\nwith\\tinstances\\tA,\\tA,\\tA.\\tThe\\tfirst\\tchild\\tnode’s\\tGini\\timpurity\\tis\\t\\n\\t=\\t0.5,\\twhich\\tis\\thigher\\nthan\\tits\\tparent.\\tThis\\tis\\tcompensated\\tfor\\tby\\tthe\\tfact\\tthat\\tthe\\tother\\tnode\\tis\\tpure,\\tso\\tthe\\toverall\\nweighted\\tGini\\timpurity\\tis\\t\\n\\t0.5\\t+\\t\\n\\t=\\t0.2\\t,\\twhich\\tis\\tlower\\tthan\\tthe\\tparent’s\\tGini\\timpurity.\\n3\\n.\\t\\nIf\\ta\\tDecision\\tTree\\tis\\toverfitting\\tthe\\ttraining\\tset,\\tit\\tmay\\tbe\\ta\\tgood\\tidea\\tto\\tdecrease\\t\\nmax_depth\\n,\\tsince\\nthis\\twill\\tconstrain\\tthe\\tmodel,\\tregularizing\\tit.\\n4\\n.\\t\\nDecision\\tTrees\\tdon’t\\tcare\\twhether\\tor\\tnot\\tthe\\ttraining\\tdata\\tis\\tscaled\\tor\\tcentered;\\tthat’s\\tone\\tof\\tthe\\tnice', 'things\\tabout\\tthem.\\tSo\\tif\\ta\\tDecision\\tTree\\tunderfits\\tthe\\ttraining\\tset,\\tscaling\\tthe\\tinput\\tfeatures\\twill\\tjust\\nbe\\ta\\twaste\\tof\\ttime.\\n5\\n.\\t\\nThe\\tcomputational\\tcomplexity\\tof\\ttraining\\ta\\tDecision\\tTree\\tis\\t\\nO\\n(\\nn\\n\\t×\\t\\nm\\n\\tlog(\\nm\\n)).\\tSo\\tif\\tyou\\tmultiply\\tthe\\ntraining\\tset\\tsize\\tby\\t10,\\tthe\\ttraining\\ttime\\twill\\tbe\\tmultiplied\\tby\\t\\nK\\n\\t=\\t(\\nn\\n\\t×\\t10\\nm\\n\\t×\\tlog(10\\nm\\n))\\t/\\t(\\nn\\n\\t×\\t\\nm\\n\\t×\\nlog(\\nm\\n))\\t=\\t10\\t×\\tlog(10\\nm\\n)\\t/\\tlog(\\nm\\n).\\tIf\\t\\nm\\n\\t=\\t10\\n6\\n,\\tthen\\t\\nK\\n\\t≈\\t11.7,\\tso\\tyou\\tcan\\texpect\\tthe\\ttraining\\ttime\\tto\\tbe\\nroughly\\t11.7\\thours.\\n6\\n.\\t\\nPresorting\\tthe\\ttraining\\tset\\tspeeds\\tup\\ttraining\\tonly\\tif\\tthe\\tdataset\\tis\\tsmaller\\tthan\\ta\\tfew\\tthousand\\ninstances.\\tIf\\tit\\tcontains\\t100,000\\tinstances,\\tsetting\\t\\npresort=True\\n\\twill\\tconsiderably\\tslow\\tdown\\ntraining.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t7\\tand\\t8,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t7\\n:\\tEnsemble\\tLearning\\tand\\tRandom\\tForests\\n1\\n.\\t\\nIf\\tyou\\thave\\ttrained\\tfive\\tdifferent\\tmodels\\tand\\tthey\\tall\\tachieve\\t95%\\tprecision,\\tyou\\tcan\\ttry\\tcombining\\nthem\\tinto\\ta\\tvoting\\tensemble,\\twhich\\twill\\toften\\tgive\\tyou\\teven\\tbetter\\tresults.\\tIt\\tworks\\tbetter\\tif\\tthe\\nmodels\\tare\\tvery\\tdifferent\\t(e.g.,\\tan\\tSVM\\tclassifier,\\ta\\tDecision\\tTree\\tclassifier,\\ta\\tLogistic\\tRegression\\nclassifier,\\tand\\tso\\ton).\\tIt\\tis\\teven\\tbetter\\tif\\tthey\\tare\\ttrained\\ton\\tdifferent\\ttraining\\tinstances\\t(that’s\\tthe\\nwhole\\tpoint\\tof\\tbagging\\tand\\tpasting\\tensembles),\\tbut\\tif\\tnot\\tit\\twill\\tstill\\twork\\tas\\tlong\\tas\\tthe\\tmodels\\tare\\nvery\\tdifferent.\\n2\\n.\\t\\nA\\thard\\tvoting\\tclassifier\\tjust\\tcounts\\tthe\\tvotes\\tof\\teach\\tclassifier\\tin\\tthe\\tensemble\\tand\\tpicks\\tthe\\tclass\\nthat\\tgets\\tthe\\tmost\\tvotes.\\tA\\tsoft\\tvoting\\tclassifier\\tcomputes\\tthe\\taverage\\testimated\\tclass\\tprobability\\tfor\\neach\\tclass\\tand\\tpicks\\tthe\\tclass\\twith\\tthe\\thighest\\tprobability.\\tThis\\tgives\\thigh-confidence\\tvotes\\tmore\\nweight\\tand\\toften\\tperforms\\tbetter,\\tbut\\tit\\tworks\\tonly\\tif\\tevery\\tclassifier\\tis\\table\\tto\\testimate\\tclass', 'probabilities\\t(e.g.,\\tfor\\tthe\\tSVM\\tclassifiers\\tin\\tScikit-Learn\\tyou\\tmust\\tset\\t\\nprobability=True\\n).\\n3\\n.\\t\\nIt\\tis\\tquite\\tpossible\\tto\\tspeed\\tup\\ttraining\\tof\\ta\\tbagging\\tensemble\\tby\\tdistributing\\tit\\tacross\\tmultiple\\nservers,\\tsince\\teach\\tpredictor\\tin\\tthe\\tensemble\\tis\\tindependent\\tof\\tthe\\tothers.\\tThe\\tsame\\tgoes\\tfor\\tpasting\\nensembles\\tand\\tRandom\\tForests,\\tfor\\tthe\\tsame\\treason.\\tHowever,\\teach\\tpredictor\\tin\\ta\\tboosting\\nensemble\\tis\\tbuilt\\tbased\\ton\\tthe\\tprevious\\tpredictor,\\tso\\ttraining\\tis\\tnecessarily\\tsequential,\\tand\\tyou\\twill\\nnot\\tgain\\tanything\\tby\\tdistributing\\ttraining\\tacross\\tmultiple\\tservers.\\tRegarding\\tstacking\\tensembles,\\tall\\nthe\\tpredictors\\tin\\ta\\tgiven\\tlayer\\tare\\tindependent\\tof\\teach\\tother,\\tso\\tthey\\tcan\\tbe\\ttrained\\tin\\tparallel\\ton\\nmultiple\\tservers.\\tHowever,\\tthe\\tpredictors\\tin\\tone\\tlayer\\tcan\\tonly\\tbe\\ttrained\\tafter\\tthe\\tpredictors\\tin\\tthe\\nprevious\\tlayer\\thave\\tall\\tbeen\\ttrained.\\n4\\n.\\t\\nWith\\tout-of-bag\\tevaluation,\\teach\\tpredictor\\tin\\ta\\tbagging\\tensemble\\tis\\tevaluated\\tusing\\tinstances\\tthat\\tit', 'was\\tnot\\ttrained\\ton\\t(they\\twere\\theld\\tout).\\tThis\\tmakes\\tit\\tpossible\\tto\\thave\\ta\\tfairly\\tunbiased\\tevaluation\\nof\\tthe\\tensemble\\twithout\\tthe\\tneed\\tfor\\tan\\tadditional\\tvalidation\\tset.\\tThus,\\tyou\\thave\\tmore\\tinstances\\navailable\\tfor\\ttraining,\\tand\\tyour\\tensemble\\tcan\\tperform\\tslightly\\tbetter.\\n5\\n.\\t\\nWhen\\tyou\\tare\\tgrowing\\ta\\ttree\\tin\\ta\\tRandom\\tForest,\\tonly\\ta\\trandom\\tsubset\\tof\\tthe\\tfeatures\\tis\\tconsidered\\nfor\\tsplitting\\tat\\teach\\tnode.\\tThis\\tis\\ttrue\\tas\\twell\\tfor\\tExtra-Trees,\\tbut\\tthey\\tgo\\tone\\tstep\\tfurther:\\trather\\nthan\\tsearching\\tfor\\tthe\\tbest\\tpossible\\tthresholds,\\tlike\\tregular\\tDecision\\tTrees\\tdo,\\tthey\\tuse\\trandom\\nthresholds\\tfor\\teach\\tfeature.\\tThis\\textra\\trandomness\\tacts\\tlike\\ta\\tform\\tof\\tregularization:\\tif\\ta\\tRandom\\nForest\\toverfits\\tthe\\ttraining\\tdata,\\tExtra-Trees\\tmight\\tperform\\tbetter.\\tMoreover,\\tsince\\tExtra-Trees\\ndon’t\\tsearch\\tfor\\tthe\\tbest\\tpossible\\tthresholds,\\tthey\\tare\\tmuch\\tfaster\\tto\\ttrain\\tthan\\tRandom\\tForests.\\nHowever,\\tthey\\tare\\tneither\\tfaster\\tnor\\tslower\\tthan\\tRandom\\tForests\\twhen\\tmaking\\tpredictions.\\n6\\n.', '6\\n.\\t\\nIf\\tyour\\tAdaBoost\\tensemble\\tunderfits\\tthe\\ttraining\\tdata,\\tyou\\tcan\\ttry\\tincreasing\\tthe\\tnumber\\tof\\nestimators\\tor\\treducing\\tthe\\tregularization\\thyperparameters\\tof\\tthe\\tbase\\testimator.\\tYou\\tmay\\talso\\ttry\\nslightly\\tincreasing\\tthe\\tlearning\\trate.\\n7\\n.\\t\\nIf\\tyour\\tGradient\\tBoosting\\tensemble\\toverfits\\tthe\\ttraining\\tset,\\tyou\\tshould\\ttry\\tdecreasing\\tthe\\tlearning\\nrate.\\tYou\\tcould\\talso\\tuse\\tearly\\tstopping\\tto\\tfind\\tthe\\tright\\tnumber\\tof\\tpredictors\\t(you\\tprobably\\thave\\ttoo\\nmany).\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8\\tand\\t9,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat', 'https://github.com/ageron/handson-ml\\n.', 'Chapter\\t8\\n:\\tDimensionality\\tReduction\\n1\\n.\\t\\nMotivations\\tand\\tdrawbacks:\\nThe\\tmain\\tmotivations\\tfor\\tdimensionality\\treduction\\tare:\\nTo\\tspeed\\tup\\ta\\tsubsequent\\ttraining\\talgorithm\\t(in\\tsome\\tcases\\tit\\tmay\\teven\\tremove\\tnoise\\tand\\nredundant\\tfeatures,\\tmaking\\tthe\\ttraining\\talgorithm\\tperform\\tbetter).\\nTo\\tvisualize\\tthe\\tdata\\tand\\tgain\\tinsights\\ton\\tthe\\tmost\\timportant\\tfeatures.\\nSimply\\tto\\tsave\\tspace\\t(compression).\\nThe\\tmain\\tdrawbacks\\tare:\\nSome\\tinformation\\tis\\tlost,\\tpossibly\\tdegrading\\tthe\\tperformance\\tof\\tsubsequent\\ttraining\\nalgorithms.\\nIt\\tcan\\tbe\\tcomputationally\\tintensive.\\nIt\\tadds\\tsome\\tcomplexity\\tto\\tyour\\tMachine\\tLearning\\tpipelines.\\nTransformed\\tfeatures\\tare\\toften\\thard\\tto\\tinterpret.\\n2\\n.\\t\\nThe\\tcurse\\tof\\tdimensionality\\trefers\\tto\\tthe\\tfact\\tthat\\tmany\\tproblems\\tthat\\tdo\\tnot\\texist\\tin\\tlow-\\ndimensional\\tspace\\tarise\\tin\\thigh-dimensional\\tspace.\\tIn\\tMachine\\tLearning,\\tone\\tcommon\\tmanifestation\\nis\\tthe\\tfact\\tthat\\trandomly\\tsampled\\thigh-dimensional\\tvectors\\tare\\tgenerally\\tvery\\tsparse,\\tincreasing\\tthe', 'risk\\tof\\toverfitting\\tand\\tmaking\\tit\\tvery\\tdifficult\\tto\\tidentify\\tpatterns\\tin\\tthe\\tdata\\twithout\\thaving\\tplenty\\tof\\ntraining\\tdata.\\n3\\n.\\t\\nOnce\\ta\\tdataset’s\\tdimensionality\\thas\\tbeen\\treduced\\tusing\\tone\\tof\\tthe\\talgorithms\\twe\\tdiscussed,\\tit\\tis\\nalmost\\talways\\timpossible\\tto\\tperfectly\\treverse\\tthe\\toperation,\\tbecause\\tsome\\tinformation\\tgets\\tlost\\nduring\\tdimensionality\\treduction.\\tMoreover,\\twhile\\tsome\\talgorithms\\t(such\\tas\\tPCA)\\thave\\ta\\tsimple\\nreverse\\ttransformation\\tprocedure\\tthat\\tcan\\treconstruct\\ta\\tdataset\\trelatively\\tsimilar\\tto\\tthe\\toriginal,\\nother\\talgorithms\\t(such\\tas\\tT-SNE)\\tdo\\tnot.\\n4\\n.\\t\\nPCA\\tcan\\tbe\\tused\\tto\\tsignificantly\\treduce\\tthe\\tdimensionality\\tof\\tmost\\tdatasets,\\teven\\tif\\tthey\\tare\\thighly\\nnonlinear,\\tbecause\\tit\\tcan\\tat\\tleast\\tget\\trid\\tof\\tuseless\\tdimensions.\\tHowever,\\tif\\tthere\\tare\\tno\\tuseless\\ndimensions\\t—\\tfor\\texample,\\tthe\\tSwiss\\troll\\t—\\tthen\\treducing\\tdimensionality\\twith\\tPCA\\twill\\tlose\\ttoo\\nmuch\\tinformation.\\tYou\\twant\\tto\\tunroll\\tthe\\tSwiss\\troll,\\tnot\\tsquash\\tit.\\n5\\n.', '5\\n.\\t\\nThat’s\\ta\\ttrick\\tquestion:\\tit\\tdepends\\ton\\tthe\\tdataset.\\tLet’s\\tlook\\tat\\ttwo\\textreme\\texamples.\\tFirst,\\tsuppose\\nthe\\tdataset\\tis\\tcomposed\\tof\\tpoints\\tthat\\tare\\talmost\\tperfectly\\taligned.\\tIn\\tthis\\tcase,\\tPCA\\tcan\\treduce\\tthe\\ndataset\\tdown\\tto\\tjust\\tone\\tdimension\\twhile\\tstill\\tpreserving\\t95%\\tof\\tthe\\tvariance.\\tNow\\timagine\\tthat\\tthe\\ndataset\\tis\\tcomposed\\tof\\tperfectly\\trandom\\tpoints,\\tscattered\\tall\\taround\\tthe\\t1,000\\tdimensions.\\tIn\\tthis\\ncase\\troughly\\t950\\tdimensions\\tare\\trequired\\tto\\tpreserve\\t95%\\tof\\tthe\\tvariance.\\tSo\\tthe\\tanswer\\tis,\\tit\\ndepends\\ton\\tthe\\tdataset,\\tand\\tit\\tcould\\tbe\\tany\\tnumber\\tbetween\\t1\\tand\\t950.\\tPlotting\\tthe\\texplained\\nvariance\\tas\\ta\\tfunction\\tof\\tthe\\tnumber\\tof\\tdimensions\\tis\\tone\\tway\\tto\\tget\\ta\\trough\\tidea\\tof\\tthe\\tdataset’s\\nintrinsic\\tdimensionality.', '6\\n.\\t\\nRegular\\tPCA\\tis\\tthe\\tdefault,\\tbut\\tit\\tworks\\tonly\\tif\\tthe\\tdataset\\tfits\\tin\\tmemory.\\tIncremental\\tPCA\\tis\\tuseful\\nfor\\tlarge\\tdatasets\\tthat\\tdon’t\\tfit\\tin\\tmemory,\\tbut\\tit\\tis\\tslower\\tthan\\tregular\\tPCA,\\tso\\tif\\tthe\\tdataset\\tfits\\tin\\nmemory\\tyou\\tshould\\tprefer\\tregular\\tPCA.\\tIncremental\\tPCA\\tis\\talso\\tuseful\\tfor\\tonline\\ttasks,\\twhen\\tyou\\nneed\\tto\\tapply\\tPCA\\ton\\tthe\\tfly,\\tevery\\ttime\\ta\\tnew\\tinstance\\tarrives.\\tRandomized\\tPCA\\tis\\tuseful\\twhen\\nyou\\twant\\tto\\tconsiderably\\treduce\\tdimensionality\\tand\\tthe\\tdataset\\tfits\\tin\\tmemory;\\tin\\tthis\\tcase,\\tit\\tis\\nmuch\\tfaster\\tthan\\tregular\\tPCA.\\tFinally,\\tKernel\\tPCA\\tis\\tuseful\\tfor\\tnonlinear\\tdatasets.\\n7\\n.\\t\\nIntuitively,\\ta\\tdimensionality\\treduction\\talgorithm\\tperforms\\twell\\tif\\tit\\teliminates\\ta\\tlot\\tof\\tdimensions\\nfrom\\tthe\\tdataset\\twithout\\tlosing\\ttoo\\tmuch\\tinformation.\\tOne\\tway\\tto\\tmeasure\\tthis\\tis\\tto\\tapply\\tthe\\nreverse\\ttransformation\\tand\\tmeasure\\tthe\\treconstruction\\terror.\\tHowever,\\tnot\\tall\\tdimensionality\\nreduction\\talgorithms\\tprovide\\ta\\treverse\\ttransformation.\\tAlternatively,\\tif\\tyou\\tare\\tusing\\tdimensionality', 'reduction\\tas\\ta\\tpreprocessing\\tstep\\tbefore\\tanother\\tMachine\\tLearning\\talgorithm\\t(e.g.,\\ta\\tRandom\\tForest\\nclassifier),\\tthen\\tyou\\tcan\\tsimply\\tmeasure\\tthe\\tperformance\\tof\\tthat\\tsecond\\talgorithm;\\tif\\tdimensionality\\nreduction\\tdid\\tnot\\tlose\\ttoo\\tmuch\\tinformation,\\tthen\\tthe\\talgorithm\\tshould\\tperform\\tjust\\tas\\twell\\tas\\twhen\\nusing\\tthe\\toriginal\\tdataset.\\n8\\n.\\t\\nIt\\tcan\\tabsolutely\\tmake\\tsense\\tto\\tchain\\ttwo\\tdifferent\\tdimensionality\\treduction\\talgorithms.\\tA\\tcommon\\nexample\\tis\\tusing\\tPCA\\tto\\tquickly\\tget\\trid\\tof\\ta\\tlarge\\tnumber\\tof\\tuseless\\tdimensions,\\tthen\\tapplying\\nanother\\tmuch\\tslower\\tdimensionality\\treduction\\talgorithm,\\tsuch\\tas\\tLLE.\\tThis\\ttwo-step\\tapproach\\twill\\nlikely\\tyield\\tthe\\tsame\\tperformance\\tas\\tusing\\tLLE\\tonly,\\tbut\\tin\\ta\\tfraction\\tof\\tthe\\ttime.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t9\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t9\\n:\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\n1\\n.\\t\\nMain\\tbenefits\\tand\\tdrawbacks\\tof\\tcreating\\ta\\tcomputation\\tgraph\\trather\\tthan\\tdirectly\\texecuting\\tthe\\ncomputations:\\nMain\\tbenefits:\\nTensorFlow\\tcan\\tautomatically\\tcompute\\tthe\\tgradients\\tfor\\tyou\\t(using\\treverse-mode\\nautodiff).\\nTensorFlow\\tcan\\ttake\\tcare\\tof\\trunning\\tthe\\toperations\\tin\\tparallel\\tin\\tdifferent\\tthreads.\\nIt\\tmakes\\tit\\teasier\\tto\\trun\\tthe\\tsame\\tmodel\\tacross\\tdifferent\\tdevices.\\nIt\\tsimplifies\\tintrospection\\t—\\tfor\\texample,\\tto\\tview\\tthe\\tmodel\\tin\\tTensorBoard.\\nMain\\tdrawbacks:\\nIt\\tmakes\\tthe\\tlearning\\tcurve\\tsteeper.\\nIt\\tmakes\\tstep-by-step\\tdebugging\\tharder.\\n2\\n.\\t\\nYes,\\tthe\\tstatement\\t\\na_val\\n\\t\\n=\\n\\t\\na.eval(session=sess)\\n\\tis\\tindeed\\tequivalent\\tto\\t\\na_val\\n\\t\\n=\\n\\t\\nsess.run(a)\\n.\\n3\\n.\\t\\nNo,\\tthe\\tstatement\\t\\na_val,\\tb_val\\n\\t\\n=\\n\\t\\na.eval(session=sess),\\tb.eval(session=sess)\\n\\tis\\tnot\\nequivalent\\tto\\t\\na_val,\\tb_val\\n\\t\\n=\\n\\t\\nsess.run([a,\\tb])\\n.\\tIndeed,\\tthe\\tfirst\\tstatement\\truns\\tthe\\tgraph\\ttwice\\n(once\\tto\\tcompute\\t\\na\\n,\\tonce\\tto\\tcompute\\t\\nb\\n),\\twhile\\tthe\\tsecond\\tstatement\\truns\\tthe\\tgraph\\tonly\\tonce.\\tIf\\tany', 'of\\tthese\\toperations\\t(or\\tthe\\tops\\tthey\\tdepend\\ton)\\thave\\tside\\teffects\\t(e.g.,\\ta\\tvariable\\tis\\tmodified,\\tan\\nitem\\tis\\tinserted\\tin\\ta\\tqueue,\\tor\\ta\\treader\\treads\\ta\\tfile),\\tthen\\tthe\\teffects\\twill\\tbe\\tdifferent.\\tIf\\tthey\\tdon’t\\nhave\\tside\\teffects,\\tboth\\tstatements\\twill\\treturn\\tthe\\tsame\\tresult,\\tbut\\tthe\\tsecond\\tstatement\\twill\\tbe\\tfaster\\nthan\\tthe\\tfirst.\\n4\\n.\\t\\nNo,\\tyou\\tcannot\\trun\\ttwo\\tgraphs\\tin\\tthe\\tsame\\tsession.\\tYou\\twould\\thave\\tto\\tmerge\\tthe\\tgraphs\\tinto\\ta\\tsingle\\ngraph\\tfirst.\\n5\\n.\\t\\nIn\\tlocal\\tTensorFlow,\\tsessions\\tmanage\\tvariable\\tvalues,\\tso\\tif\\tyou\\tcreate\\ta\\tgraph\\t\\ng\\n\\tcontaining\\ta\\nvariable\\t\\nw\\n,\\tthen\\tstart\\ttwo\\tthreads\\tand\\topen\\ta\\tlocal\\tsession\\tin\\teach\\tthread,\\tboth\\tusing\\tthe\\tsame\\tgraph\\ng\\n,\\tthen\\teach\\tsession\\twill\\thave\\tits\\town\\tcopy\\tof\\tthe\\tvariable\\t\\nw\\n.\\tHowever,\\tin\\tdistributed\\tTensorFlow,\\nvariable\\tvalues\\tare\\tstored\\tin\\tcontainers\\tmanaged\\tby\\tthe\\tcluster,\\tso\\tif\\tboth\\tsessions\\tconnect\\tto\\tthe\\nsame\\tcluster\\tand\\tuse\\tthe\\tsame\\t\\ncontainer,\\tthen\\tthey\\twill\\tshare\\tthe\\tsame\\tvariable\\tvalue\\tfor\\t\\nw\\n.\\n6\\n.', 'w\\n.\\n6\\n.\\t\\nA\\tvariable\\tis\\tinitialized\\twhen\\tyou\\tcall\\tits\\tinitializer,\\tand\\tit\\tis\\tdestroyed\\twhen\\tthe\\tsession\\tends.\\tIn\\ndistributed\\tTensorFlow,\\tvariables\\tlive\\tin\\tcontainers\\ton\\tthe\\tcluster,\\tso\\tclosing\\ta\\tsession\\twill\\tnot\\ndestroy\\tthe\\tvariable.\\tTo\\tdestroy\\ta\\tvariable,\\tyou\\tneed\\tto\\tclear\\tits\\tcontainer.\\n7\\n.\\t\\nVariables\\tand\\tplaceholders\\tare\\textremely\\tdifferent,\\tbut\\tbeginners\\toften\\tconfuse\\tthem:\\nA\\tvariable\\tis\\tan\\toperation\\tthat\\tholds\\ta\\tvalue.\\tIf\\tyou\\trun\\tthe\\tvariable,\\tit\\treturns\\tthat\\tvalue.\\nBefore\\tyou\\tcan\\trun\\tit,\\tyou\\tneed\\tto\\tinitialize\\tit.\\tYou\\tcan\\tchange\\tthe\\tvariable’s\\tvalue\\t(for', 'example,\\tby\\tusing\\tan\\tassignment\\toperation).\\tIt\\tis\\tstateful:\\tthe\\tvariable\\tkeeps\\tthe\\tsame\\tvalue\\nupon\\tsuccessive\\truns\\tof\\tthe\\tgraph.\\tIt\\tis\\ttypically\\tused\\tto\\thold\\tmodel\\tparameters\\tbut\\talso\\tfor\\nother\\tpurposes\\t(e.g.,\\tto\\tcount\\tthe\\tglobal\\ttraining\\tstep).\\nPlaceholders\\ttechnically\\tdon’t\\tdo\\tmuch:\\tthey\\tjust\\thold\\tinformation\\tabout\\tthe\\ttype\\tand\\tshape\\tof\\nthe\\ttensor\\tthey\\trepresent,\\tbut\\tthey\\thave\\tno\\tvalue.\\tIn\\tfact,\\tif\\tyou\\ttry\\tto\\tevaluate\\tan\\toperation\\tthat\\ndepends\\ton\\ta\\tplaceholder,\\tyou\\tmust\\tfeed\\tTensorFlow\\tthe\\tvalue\\tof\\tthe\\tplaceholder\\t(using\\tthe\\nfeed_dict\\n\\targument)\\tor\\telse\\tyou\\twill\\tget\\tan\\texception.\\tPlaceholders\\tare\\ttypically\\tused\\tto\\tfeed\\ntraining\\tor\\ttest\\tdata\\tto\\tTensorFlow\\tduring\\tthe\\texecution\\tphase.\\tThey\\tare\\talso\\tuseful\\tto\\tpass\\ta\\nvalue\\tto\\tan\\tassignment\\tnode,\\tto\\tchange\\tthe\\tvalue\\tof\\ta\\tvariable\\t(e.g.,\\tmodel\\tweights).\\n8\\n.\\t\\nIf\\tyou\\trun\\tthe\\tgraph\\tto\\tevaluate\\tan\\toperation\\tthat\\tdepends\\ton\\ta\\tplaceholder\\tbut\\tyou\\tdon’t\\tfeed\\tits', 'value,\\tyou\\tget\\tan\\texception.\\tIf\\tthe\\toperation\\tdoes\\tnot\\tdepend\\ton\\tthe\\tplaceholder,\\tthen\\tno\\texception\\tis\\nraised.\\n9\\n.\\t\\nWhen\\tyou\\trun\\ta\\tgraph,\\tyou\\tcan\\tfeed\\tthe\\toutput\\tvalue\\tof\\tany\\toperation,\\tnot\\tjust\\tthe\\tvalue\\tof\\nplaceholders.\\tIn\\tpractice,\\thowever,\\tthis\\tis\\trather\\trare\\t(it\\tcan\\tbe\\tuseful,\\tfor\\texample,\\twhen\\tyou\\tare\\ncaching\\tthe\\toutput\\tof\\tfrozen\\tlayers;\\tsee\\t\\nChapter\\t11\\n).\\n10\\n.\\t\\nYou\\tcan\\tspecify\\ta\\tvariable’s\\tinitial\\tvalue\\twhen\\tconstructing\\tthe\\tgraph,\\tand\\tit\\twill\\tbe\\tinitialized\\tlater\\nwhen\\tyou\\trun\\tthe\\tvariable’s\\tinitializer\\tduring\\tthe\\texecution\\tphase.\\tIf\\tyou\\twant\\tto\\tchange\\tthat\\nvariable’s\\tvalue\\tto\\tanything\\tyou\\twant\\tduring\\tthe\\texecution\\tphase,\\tthen\\tthe\\tsimplest\\toption\\tis\\tto\\tcreate\\nan\\tassignment\\tnode\\t(during\\tthe\\tgraph\\tconstruction\\tphase)\\tusing\\tthe\\t\\ntf.assign()\\n\\tfunction,\\t\\npassing\\nthe\\tvariable\\tand\\ta\\tplaceholder\\tas\\tparameters.\\tDuring\\tthe\\texecution\\tphase,\\tyou\\tcan\\trun\\tthe\\tassignment\\noperation\\tand\\tfeed\\tthe\\tvariable’s\\tnew\\tvalue\\tusing\\tthe\\t\\nplaceholder.\\nimport\\n\\t\\ntensorflow\\n\\t\\nas\\n\\t\\ntf\\nx\\n\\t\\n=\\n\\t\\ntf\\n.\\nVariable\\n(', '(\\ntf\\n.\\nrandom_uniform\\n(\\nshape\\n=\\n(),\\n\\t\\nminval\\n=\\n0.0\\n,\\n\\t\\nmaxval\\n=\\n1.0\\n))\\nx_new_val\\n\\t\\n=\\n\\t\\ntf\\n.\\nplaceholder\\n(\\nshape\\n=\\n(),\\n\\t\\ndtype\\n=\\ntf\\n.\\nfloat32\\n)\\nx_assign\\n\\t\\n=\\n\\t\\ntf\\n.\\nassign\\n(\\nx\\n,\\n\\t\\nx_new_val\\n)\\nwith\\n\\t\\ntf\\n.\\nSession\\n():\\n\\t\\t\\t\\t\\nx\\n.\\ninitializer\\n.\\nrun\\n()\\n\\t\\n#\\trandom\\tnumber\\tis\\tsampled\\t*now*\\n\\t\\t\\t\\t\\nprint\\n(\\nx\\n.\\neval\\n())\\n\\t\\n#\\t0.646157\\t(some\\trandom\\tnumber)\\n\\t\\t\\t\\t\\nx_assign\\n.\\neval\\n(\\nfeed_dict\\n=\\n{\\nx_new_val\\n:\\n\\t\\n5.0\\n})\\n\\t\\t\\t\\t\\nprint\\n(\\nx\\n.\\neval\\n())\\n\\t\\n#\\t5.0\\n11\\n.\\t\\nReverse-mode\\tautodiff\\t(implemented\\tby\\tTensorFlow)\\tneeds\\tto\\ttraverse\\tthe\\tgraph\\tonly\\ttwice\\tin\\norder\\tto\\tcompute\\tthe\\tgradients\\tof\\tthe\\tcost\\tfunction\\twith\\tregards\\tto\\tany\\tnumber\\tof\\tvariables.\\tOn\\tthe\\nother\\thand,\\tforward-mode\\tautodiff\\twould\\tneed\\tto\\trun\\tonce\\tfor\\teach\\tvariable\\t(so\\t10\\ttimes\\tif\\twe\\twant\\nthe\\tgradients\\twith\\tregards\\tto\\t10\\tdifferent\\tvariables).\\tAs\\tfor\\tsymbolic\\tdifferentiation,\\tit\\twould\\tbuild\\na\\tdifferent\\tgraph\\tto\\tcompute\\tthe\\tgradients,\\tso\\tit\\twould\\tnot\\ttraverse\\tthe\\toriginal\\tgraph\\tat\\tall\\t(except', 'when\\tbuilding\\tthe\\tnew\\tgradients\\tgraph).\\tA\\thighly\\toptimized\\tsymbolic\\tdifferentiation\\tsystem\\tcould\\npotentially\\trun\\tthe\\tnew\\tgradients\\tgraph\\tonly\\tonce\\tto\\tcompute\\tthe\\tgradients\\twith\\tregards\\tto\\tall\\nvariables,\\tbut\\tthat\\tnew\\tgraph\\tmay\\tbe\\thorribly\\tcomplex\\tand\\tinefficient\\tcompared\\tto\\tthe\\toriginal\\ngraph.\\n12\\n.\\t\\nSee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t10\\n:\\tIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n1\\n.\\t\\nHere\\tis\\ta\\tneural\\tnetwork\\tbased\\ton\\tthe\\toriginal\\tartificial\\tneurons\\tthat\\tcomputes\\t\\nA\\n\\t\\n\\t\\nB\\n\\t(where\\t\\nrepresents\\tthe\\texclusive\\tOR),\\tusing\\tthe\\tfact\\tthat\\t\\nA\\n\\t\\n\\t\\nB\\n\\t=\\t(\\nA\\n\\t\\n\\t¬\\t\\nB\\n)\\t\\n\\t(¬\\t\\nA\\n\\t\\n\\t\\nB\\n).\\tThere\\tare\\tother\\nsolutions\\t—\\tfor\\texample,\\tusing\\tthe\\tfact\\tthat\\t\\nA\\n\\t\\n\\t\\nB\\n\\t=\\t(\\nA\\n\\t\\n\\t\\nB\\n)\\t\\n\\t¬(\\nA\\n\\t\\n\\t\\nB\\n),\\tor\\tthe\\tfact\\tthat\\t\\nA\\n\\t\\n\\t\\nB\\n\\t=\\t(\\nA\\n\\t\\nB\\n)\\t\\n\\t(¬\\t\\nA\\n\\t\\n\\t\\n\\t\\nB\\n),\\tand\\tso\\ton.\\n2\\n.\\t\\nA\\tclassical\\tPerceptron\\twill\\tconverge\\tonly\\tif\\tthe\\tdataset\\tis\\tlinearly\\tseparable,\\tand\\tit\\twon’t\\tbe\\table\\tto\\nestimate\\tclass\\tprobabilities.\\tIn\\tcontrast,\\ta\\tLogistic\\tRegression\\tclassifier\\twill\\tconverge\\tto\\ta\\tgood\\nsolution\\teven\\tif\\tthe\\tdataset\\tis\\tnot\\tlinearly\\tseparable,\\tand\\tit\\twill\\toutput\\tclass\\tprobabilities.\\tIf\\tyou\\nchange\\tthe\\tPerceptron’s\\tactivation\\tfunction\\tto\\tthe\\tlogistic\\tactivation\\tfunction\\t(or\\tthe\\tsoftmax\\nactivation\\tfunction\\tif\\tthere\\tare\\tmultiple\\tneurons),\\tand\\tif\\tyou\\ttrain\\tit\\tusing\\tGradient\\tDescent\\t(or\\tsome', 'other\\toptimization\\talgorithm\\tminimizing\\tthe\\tcost\\tfunction,\\ttypically\\tcross\\tentropy),\\tthen\\tit\\tbecomes\\nequivalent\\tto\\ta\\tLogistic\\tRegression\\tclassifier.\\n3\\n.\\t\\nThe\\tlogistic\\tactivation\\tfunction\\twas\\ta\\tkey\\tingredient\\tin\\ttraining\\tthe\\tfirst\\tMLPs\\tbecause\\tits\\tderivative\\nis\\talways\\tnonzero,\\tso\\tGradient\\tDescent\\tcan\\talways\\troll\\tdown\\tthe\\tslope.\\tWhen\\tthe\\tactivation\\nfunction\\tis\\ta\\tstep\\tfunction,\\tGradient\\tDescent\\tcannot\\tmove,\\tas\\tthere\\tis\\tno\\tslope\\tat\\tall.\\n4\\n.\\t\\nThe\\tstep\\tfunction,\\tthe\\tlogistic\\tfunction,\\tthe\\thyperbolic\\ttangent,\\tthe\\trectified\\tlinear\\tunit\\t(see\\nFigure\\t10-8\\n).\\tSee\\t\\nChapter\\t11\\n\\tfor\\tother\\texamples,\\tsuch\\tas\\tELU\\tand\\tvariants\\tof\\tthe\\tReLU.\\n5\\n.\\t\\nConsidering\\tthe\\tMLP\\tdescribed\\tin\\tthe\\tquestion:\\tsuppose\\tyou\\thave\\tan\\tMLP\\tcomposed\\tof\\tone\\tinput\\nlayer\\twith\\t10\\tpassthrough\\tneurons,\\tfollowed\\tby\\tone\\thidden\\tlayer\\twith\\t50\\tartificial\\tneurons,\\tand\\nfinally\\tone\\toutput\\tlayer\\twith\\t3\\tartificial\\tneurons.\\tAll\\tartificial\\tneurons\\tuse\\tthe\\tReLU\\tactivation\\nfunction.\\nThe\\tshape\\tof\\tthe\\tinput\\tmatrix\\t\\nX\\n\\tis\\t\\nm\\n\\t×\\t10,\\twhere\\t\\nm', 'm\\n\\trepresents\\tthe\\ttraining\\tbatch\\tsize.\\nThe\\tshape\\tof\\tthe\\thidden\\tlayer’s\\tweight\\tvector\\t\\nW\\nh\\n\\tis\\t10\\t×\\t50\\tand\\tthe\\tlength\\tof\\tits\\tbias\\tvector\\t\\nb\\nh\\nis\\t50.\\nThe\\tshape\\tof\\tthe\\toutput\\tlayer’s\\tweight\\tvector\\t\\nW\\no\\n\\tis\\t50\\t×\\t3,\\tand\\tthe\\tlength\\tof\\tits\\tbias\\tvector\\t\\nb\\no', 'is\\t3.\\nThe\\tshape\\tof\\tthe\\tnetwork’s\\toutput\\tmatrix\\t\\nY\\n\\tis\\t\\nm\\n\\t×\\t3.\\nY\\n\\t=\\tReLU(ReLU(\\nX\\n\\t·\\t\\nW\\nh\\n\\t+\\t\\nb\\nh\\n)\\t·\\t\\nW\\no\\n\\t+\\t\\nb\\no\\n).\\tRecall\\tthat\\tthe\\tReLU\\tfunction\\tjust\\tsets\\tevery\\nnegative\\tnumber\\tin\\tthe\\tmatrix\\tto\\tzero.\\tAlso\\tnote\\tthat\\twhen\\tyou\\tare\\tadding\\ta\\tbias\\tvector\\tto\\ta\\nmatrix,\\tit\\tis\\tadded\\tto\\tevery\\tsingle\\trow\\tin\\tthe\\tmatrix,\\twhich\\tis\\tcalled\\t\\nbroadcasting\\n.\\n6\\n.\\t\\nTo\\tclassify\\temail\\tinto\\tspam\\tor\\tham,\\tyou\\tjust\\tneed\\tone\\tneuron\\tin\\tthe\\toutput\\tlayer\\tof\\ta\\tneural\\tnetwork\\n—\\tfor\\texample,\\tindicating\\tthe\\tprobability\\tthat\\tthe\\temail\\tis\\tspam.\\tYou\\twould\\ttypically\\tuse\\tthe\\tlogistic\\nactivation\\tfunction\\tin\\tthe\\toutput\\tlayer\\twhen\\testimating\\ta\\tprobability.\\tIf\\tinstead\\tyou\\twant\\tto\\ttackle\\nMNIST,\\tyou\\tneed\\t10\\tneurons\\tin\\tthe\\toutput\\tlayer,\\tand\\tyou\\tmust\\treplace\\tthe\\tlogistic\\tfunction\\twith\\tthe\\nsoftmax\\tactivation\\tfunction,\\twhich\\tcan\\thandle\\tmultiple\\tclasses,\\toutputting\\tone\\tprobability\\tper\\tclass.\\nNow,\\tif\\tyou\\twant\\tyour\\tneural\\tnetwork\\tto\\tpredict\\thousing\\tprices\\tlike\\tin\\t\\nChapter\\t2\\n,\\tthen\\tyou\\tneed\\tone', 'output\\tneuron,\\tusing\\tno\\tactivation\\tfunction\\tat\\tall\\tin\\tthe\\toutput\\tlayer.\\n4\\n7\\n.\\t\\nBackpropagation\\tis\\ta\\ttechnique\\tused\\tto\\ttrain\\tartificial\\tneural\\tnetworks.\\tIt\\tfirst\\tcomputes\\tthe\\tgradients\\nof\\tthe\\tcost\\tfunction\\twith\\tregards\\tto\\tevery\\tmodel\\tparameter\\t(all\\tthe\\tweights\\tand\\tbiases),\\tand\\tthen\\tit\\nperforms\\ta\\tGradient\\tDescent\\tstep\\tusing\\tthese\\tgradients.\\tThis\\tbackpropagation\\tstep\\tis\\ttypically\\nperformed\\tthousands\\tor\\tmillions\\tof\\ttimes,\\tusing\\tmany\\ttraining\\tbatches,\\tuntil\\tthe\\tmodel\\tparameters\\nconverge\\tto\\tvalues\\tthat\\t(hopefully)\\tminimize\\tthe\\tcost\\tfunction.\\tTo\\tcompute\\tthe\\tgradients,\\nbackpropagation\\tuses\\treverse-mode\\tautodiff\\t(although\\tit\\twasn’t\\tcalled\\tthat\\twhen\\tbackpropagation\\nwas\\tinvented,\\tand\\tit\\thas\\tbeen\\treinvented\\tseveral\\ttimes).\\tReverse-mode\\tautodiff\\tperforms\\ta\\tforward\\npass\\tthrough\\ta\\tcomputation\\tgraph,\\tcomputing\\tevery\\tnode’s\\tvalue\\tfor\\tthe\\tcurrent\\ttraining\\tbatch,\\tand\\nthen\\tit\\tperforms\\ta\\treverse\\tpass,\\tcomputing\\tall\\tthe\\tgradients\\tat\\tonce\\t(see\\t\\nAppendix\\tD\\n\\tfor\\tmore', 'for\\tmore\\ndetails).\\tSo\\twhat’s\\tthe\\tdifference?\\tWell,\\tbackpropagation\\trefers\\tto\\tthe\\twhole\\tprocess\\tof\\ttraining\\tan\\nartificial\\tneural\\tnetwork\\tusing\\tmultiple\\tbackpropagation\\tsteps,\\teach\\tof\\twhich\\tcomputes\\tgradients\\nand\\tuses\\tthem\\tto\\tperform\\ta\\tGradient\\tDescent\\tstep.\\tIn\\tcontrast,\\treverse-mode\\tautodiff\\tis\\ta\\tsimply\\ta\\ntechnique\\tto\\tcompute\\tgradients\\tefficiently,\\tand\\tit\\thappens\\tto\\tbe\\tused\\tby\\tbackpropagation.\\n8\\n.\\t\\nHere\\tis\\ta\\tlist\\tof\\tall\\tthe\\thyperparameters\\tyou\\tcan\\ttweak\\tin\\ta\\tbasic\\tMLP:\\tthe\\tnumber\\tof\\thidden\\tlayers,\\nthe\\tnumber\\tof\\tneurons\\tin\\teach\\thidden\\tlayer,\\tand\\tthe\\tactivation\\tfunction\\tused\\tin\\teach\\thidden\\tlayer\\tand\\nin\\tthe\\toutput\\tlayer.\\n5\\n\\tIn\\tgeneral,\\tthe\\tReLU\\tactivation\\tfunction\\t(or\\tone\\tof\\tits\\tvariants;\\tsee\\t\\nChapter\\t11\\n)\\nis\\ta\\tgood\\tdefault\\tfor\\tthe\\thidden\\tlayers.\\tFor\\tthe\\toutput\\tlayer,\\tin\\tgeneral\\tyou\\twill\\twant\\tthe\\tlogistic\\nactivation\\tfunction\\tfor\\tbinary\\tclassification,\\tthe\\tsoftmax\\tactivation\\tfunction\\tfor\\tmulticlass\\nclassification,\\tor\\tno\\tactivation\\tfunction\\tfor\\tregression.', 'If\\tthe\\tMLP\\toverfits\\tthe\\ttraining\\tdata,\\tyou\\tcan\\ttry\\treducing\\tthe\\tnumber\\tof\\thidden\\tlayers\\tand\\treducing\\nthe\\tnumber\\tof\\tneurons\\tper\\thidden\\tlayer.\\n9\\n.\\t\\nSee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\t\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t11\\n:\\tTraining\\tDeep\\tNeural\\tNets\\n1\\n.\\t\\nNo,\\tall\\tweights\\tshould\\tbe\\tsampled\\tindependently;\\tthey\\tshould\\tnot\\tall\\thave\\tthe\\tsame\\tinitial\\tvalue.\\nOne\\timportant\\tgoal\\tof\\tsampling\\tweights\\trandomly\\tis\\tto\\tbreak\\tsymmetries:\\tif\\tall\\tthe\\tweights\\thave\\tthe\\nsame\\tinitial\\tvalue,\\teven\\tif\\tthat\\tvalue\\tis\\tnot\\tzero,\\tthen\\tsymmetry\\tis\\tnot\\tbroken\\t(i.e.,\\tall\\tneurons\\tin\\ta\\ngiven\\tlayer\\tare\\tequivalent),\\tand\\tbackpropagation\\twill\\tbe\\tunable\\tto\\tbreak\\tit.\\tConcretely,\\tthis\\tmeans\\nthat\\tall\\tthe\\tneurons\\tin\\tany\\tgiven\\tlayer\\twill\\talways\\thave\\tthe\\tsame\\tweights.\\tIt’s\\tlike\\thaving\\tjust\\tone\\nneuron\\tper\\tlayer,\\tand\\tmuch\\tslower.\\tIt\\tis\\tvirtually\\timpossible\\tfor\\tsuch\\ta\\tconfiguration\\tto\\tconverge\\tto\\na\\tgood\\tsolution.\\n2\\n.\\t\\nIt\\tis\\tperfectly\\tfine\\tto\\tinitialize\\tthe\\tbias\\tterms\\tto\\tzero.\\tSome\\tpeople\\tlike\\tto\\tinitialize\\tthem\\tjust\\tlike\\nweights,\\tand\\tthat’s\\tokay\\ttoo;\\tit\\tdoes\\tnot\\tmake\\tmuch\\tdifference.\\n3\\n.\\t\\nA\\tfew\\tadvantages\\tof\\tthe\\tELU\\tfunction\\tover\\tthe\\tReLU\\tfunction\\tare:\\nIt\\tcan\\ttake\\ton\\tnegative\\tvalues,\\tso\\tthe\\taverage\\toutput\\tof\\tthe\\tneurons\\tin\\tany\\tgiven\\tlayer\\tis', 'typically\\tcloser\\tto\\t0\\tthan\\twhen\\tusing\\tthe\\tReLU\\tactivation\\tfunction\\t(which\\tnever\\toutputs\\nnegative\\tvalues).\\tThis\\thelps\\talleviate\\tthe\\tvanishing\\tgradients\\tproblem.\\nIt\\talways\\thas\\ta\\tnonzero\\tderivative,\\twhich\\tavoids\\tthe\\tdying\\tunits\\tissue\\tthat\\tcan\\taffect\\tReLU\\nunits.\\nIt\\tis\\tsmooth\\teverywhere,\\twhereas\\tthe\\tReLU’s\\tslope\\tabruptly\\tjumps\\tfrom\\t0\\tto\\t1\\tat\\t\\nz\\n\\t=\\t0.\\tSuch\\tan\\nabrupt\\tchange\\tcan\\tslow\\tdown\\tGradient\\tDescent\\tbecause\\tit\\twill\\tbounce\\taround\\t\\nz\\n\\t=\\t0.\\n4\\n.\\t\\nThe\\tELU\\tactivation\\tfunction\\tis\\ta\\tgood\\tdefault.\\tIf\\tyou\\tneed\\tthe\\tneural\\tnetwork\\tto\\tbe\\tas\\tfast\\tas\\npossible,\\tyou\\tcan\\tuse\\tone\\tof\\tthe\\tleaky\\tReLU\\tvariants\\tinstead\\t(e.g.,\\ta\\tsimple\\tleaky\\tReLU\\tusing\\tthe\\ndefault\\thyperparameter\\tvalue).\\tThe\\tsimplicity\\tof\\tthe\\tReLU\\tactivation\\tfunction\\tmakes\\tit\\tmany\\npeople’s\\tpreferred\\toption,\\tdespite\\tthe\\tfact\\tthat\\tthey\\tare\\tgenerally\\toutperformed\\tby\\tthe\\tELU\\tand\\tleaky\\nReLU.\\tHowever,\\tthe\\tReLU\\tactivation\\tfunction’s\\tcapability\\tof\\toutputting\\tprecisely\\tzero\\tcan\\tbe\\tuseful\\nin\\tsome\\tcases\\t(e.g.,\\tsee\\t\\nChapter\\t15', ').\\tThe\\thyperbolic\\ttangent\\t(tanh)\\tcan\\tbe\\tuseful\\tin\\tthe\\toutput\\tlayer\\tif\\nyou\\tneed\\tto\\toutput\\ta\\tnumber\\tbetween\\t–1\\tand\\t1,\\tbut\\tnowadays\\tit\\tis\\tnot\\tused\\tmuch\\tin\\thidden\\tlayers.\\nThe\\tlogistic\\tactivation\\tfunction\\tis\\talso\\tuseful\\tin\\tthe\\toutput\\tlayer\\twhen\\tyou\\tneed\\tto\\testimate\\ta\\nprobability\\t(e.g.,\\tfor\\tbinary\\tclassification),\\tbut\\tit\\tis\\talso\\trarely\\tused\\tin\\thidden\\tlayers\\t(there\\tare\\nexceptions\\t—\\tfor\\texample,\\tfor\\tthe\\tcoding\\tlayer\\tof\\tvariational\\tautoencoders;\\tsee\\t\\nChapter\\t15\\n).\\nFinally,\\tthe\\tsoftmax\\tactivation\\tfunction\\tis\\tuseful\\tin\\tthe\\toutput\\tlayer\\tto\\toutput\\tprobabilities\\tfor\\nmutually\\texclusive\\tclasses,\\tbut\\tother\\tthan\\tthat\\tit\\tis\\trarely\\t(if\\tever)\\tused\\tin\\thidden\\tlayers.\\n5\\n.\\t\\nIf\\tyou\\tset\\tthe\\t\\nmomentum\\n\\thyperparameter\\ttoo\\tclose\\tto\\t1\\t(e.g.,\\t0.99999)\\twhen\\tusing\\ta\\nMomentumOptimizer\\n,\\tthen\\tthe\\talgorithm\\twill\\tlikely\\tpick\\tup\\ta\\tlot\\tof\\tspeed,\\thopefully\\troughly\\ttoward\\nthe\\tglobal\\tminimum,\\tbut\\tthen\\tit\\twill\\tshoot\\tright\\tpast\\tthe\\tminimum,\\tdue\\tto\\tits\\t\\nmomentum.\\tThen\\tit\\twill', 'slow\\tdown\\tand\\tcome\\tback,\\taccelerate\\tagain,\\tovershoot\\tagain,\\tand\\tso\\ton.\\tIt\\tmay\\toscillate\\tthis\\tway\\nmany\\ttimes\\tbefore\\tconverging,\\tso\\toverall\\tit\\twill\\ttake\\tmuch\\tlonger\\tto\\tconverge\\tthan\\twith\\ta\\tsmaller\\nmomentum\\n\\tvalue.\\n6\\n.\\t\\nOne\\tway\\tto\\tproduce\\ta\\tsparse\\tmodel\\t(i.e.,\\twith\\tmost\\tweights\\tequal\\tto\\tzero)\\tis\\tto\\ttrain\\tthe\\tmodel\\nnormally,\\tthen\\tzero\\tout\\ttiny\\tweights.\\tFor\\tmore\\tsparsity,\\tyou\\tcan\\tapply\\tℓ\\n1\\n\\tregularization\\tduring', 'training,\\twhich\\tpushes\\tthe\\toptimizer\\ttoward\\tsparsity.\\tA\\tthird\\toption\\tis\\tto\\tcombine\\tℓ\\n1\\n\\tregularization\\nwith\\t\\ndual\\taveraging\\n,\\tusing\\tTensorFlow’s\\t\\nFTRLOptimizer\\n\\tclass.\\n7\\n.\\t\\nYes,\\tdropout\\tdoes\\tslow\\tdown\\ttraining,\\tin\\tgeneral\\troughly\\tby\\ta\\tfactor\\tof\\ttwo.\\tHowever,\\tit\\thas\\tno\\nimpact\\ton\\tinference\\tsince\\tit\\tis\\tonly\\tturned\\ton\\tduring\\ttraining.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t12\\n:\\tDistributing\\tTensorFlow\\tAcross\\tDevices\\tand\\tServers\\n1\\n.\\t\\nWhen\\ta\\tTensorFlow\\tprocess\\tstarts,\\tit\\tgrabs\\tall\\tthe\\tavailable\\tmemory\\ton\\tall\\tGPU\\tdevices\\tthat\\tare\\nvisible\\tto\\tit,\\tso\\tif\\tyou\\tget\\ta\\t\\nCUDA_ERROR_OUT_OF_MEMORY\\n\\twhen\\tstarting\\tyour\\tTensorFlow\\tprogram,\\nit\\tprobably\\tmeans\\tthat\\tother\\tprocesses\\tare\\trunning\\tthat\\thave\\talready\\tgrabbed\\tall\\tthe\\tmemory\\ton\\tat\\nleast\\tone\\tvisible\\tGPU\\tdevice\\t(most\\tlikely\\tit\\tis\\tanother\\tTensorFlow\\tprocess).\\tTo\\tfix\\tthis\\tproblem,\\ta\\ntrivial\\tsolution\\tis\\tto\\tstop\\tthe\\tother\\tprocesses\\tand\\ttry\\tagain.\\tHowever,\\tif\\tyou\\tneed\\tall\\tprocesses\\tto\\trun\\nsimultaneously,\\ta\\tsimple\\toption\\tis\\tto\\tdedicate\\tdifferent\\tdevices\\tto\\teach\\tprocess,\\tby\\tsetting\\tthe\\nCUDA_VISIBLE_DEVICES\\n\\tenvironment\\tvariable\\tappropriately\\tfor\\teach\\tdevice.\\tAnother\\toption\\tis\\tto\\nconfigure\\t\\nTensorFlow\\tto\\tgrab\\tonly\\tpart\\tof\\tthe\\tGPU\\tmemory,\\tinstead\\tof\\tall\\tof\\tit,\\tby\\tcreating\\ta\\nConfigProto\\n,\\tsetting\\tits\\t\\ngpu_options.per_process_gpu_memory_fraction\\n\\tto\\tthe\\tproportion\\tof', 'the\\ttotal\\tmemory\\tthat\\tit\\tshould\\tgrab\\t(e.g.,\\t0.4),\\tand\\tusing\\tthis\\t\\nConfigProto\\n\\twhen\\topening\\ta\\tsession.\\nThe\\tlast\\toption\\tis\\tto\\ttell\\tTensorFlow\\tto\\tgrab\\tmemory\\tonly\\twhen\\tit\\tneeds\\tit\\tby\\tsetting\\tthe\\ngpu_options.allow_growth\\n\\tto\\t\\nTrue\\n.\\tHowever,\\tthis\\tlast\\toption\\tis\\tusually\\tnot\\trecommended\\nbecause\\tany\\tmemory\\tthat\\tTensorFlow\\tgrabs\\tis\\tnever\\treleased,\\tand\\tit\\tis\\tharder\\tto\\tguarantee\\ta\\nrepeatable\\tbehavior\\t(there\\tmay\\tbe\\trace\\tconditions\\tdepending\\ton\\twhich\\tprocesses\\tstart\\tfirst,\\thow\\nmuch\\tmemory\\tthey\\tneed\\tduring\\ttraining,\\tand\\tso\\ton).\\n2\\n.\\t\\nBy\\tpinning\\tan\\toperation\\ton\\ta\\tdevice,\\tyou\\tare\\ttelling\\tTensorFlow\\tthat\\tthis\\tis\\twhere\\tyou\\twould\\tlike\\nthis\\toperation\\tto\\tbe\\tplaced.\\tHowever,\\tsome\\tconstraints\\tmay\\tprevent\\tTensorFlow\\tfrom\\thonoring\\tyour\\nrequest.\\tFor\\texample,\\tthe\\toperation\\tmay\\thave\\tno\\timplementation\\t(called\\ta\\t\\nkernel\\n)\\tfor\\tthat\\tparticular\\ntype\\tof\\tdevice.\\tIn\\tthis\\tcase,\\tTensorFlow\\twill\\traise\\tan\\texception\\tby\\tdefault,\\tbut\\tyou\\tcan\\tconfigure\\tit\\nto\\tfall\\tback\\tto\\tthe\\tCPU\\tinstead\\t(this\\tis\\tcalled\\t\\nsoft\\tplacement', ').\\tAnother\\texample\\tis\\tan\\toperation\\tthat\\ncan\\tmodify\\ta\\tvariable;\\tthis\\toperation\\tand\\tthe\\tvariable\\tneed\\tto\\tbe\\tcollocated.\\tSo\\tthe\\tdifference\\nbetween\\tpinning\\tan\\toperation\\tand\\tplacing\\tan\\toperation\\tis\\tthat\\tpinning\\tis\\twhat\\tyou\\task\\tTensorFlow\\n(“Please\\tplace\\tthis\\toperation\\ton\\tGPU\\t#1”)\\twhile\\tplacement\\tis\\twhat\\tTensorFlow\\tactually\\tends\\tup\\ndoing\\t(“Sorry,\\tfalling\\tback\\tto\\tthe\\tCPU”).\\n3\\n.\\t\\nIf\\tyou\\tare\\trunning\\ton\\ta\\tGPU-enabled\\tTensorFlow\\tinstallation,\\tand\\tyou\\tjust\\tuse\\tthe\\tdefault\\tplacement,\\nthen\\tif\\tall\\toperations\\thave\\ta\\tGPU\\tkernel\\t(i.e.,\\ta\\tGPU\\timplementation),\\tyes,\\tthey\\twill\\tall\\tbe\\tplaced\\ton\\nthe\\tfirst\\tGPU.\\tHowever,\\tif\\tone\\tor\\tmore\\toperations\\tdo\\tnot\\thave\\ta\\tGPU\\tkernel,\\tthen\\tby\\tdefault\\nTensorFlow\\twill\\traise\\tan\\texception.\\tIf\\tyou\\tconfigure\\tTensorFlow\\tto\\tfall\\tback\\tto\\tthe\\tCPU\\tinstead\\n(soft\\tplacement),\\tthen\\tall\\toperations\\twill\\tbe\\tplaced\\ton\\tthe\\tfirst\\tGPU\\texcept\\tthe\\tones\\twithout\\ta\\tGPU\\nkernel\\tand\\tall\\tthe\\toperations\\tthat\\tmust\\tbe\\tcollocated\\twith\\tthem\\t(see\\tthe\\tanswer\\tto\\tthe\\tprevious\\nexercise).\\n4\\n.', '4\\n.\\t\\nYes,\\tif\\tyou\\tpin\\ta\\tvariable\\tto\\t\\n\"/gpu:0\"\\n,\\tit\\tcan\\tbe\\tused\\tby\\toperations\\tplaced\\ton\\t\\n/gpu:1\\n.\\tTensorFlow\\nwill\\tautomatically\\ttake\\tcare\\tof\\tadding\\tthe\\tappropriate\\toperations\\tto\\ttransfer\\tthe\\tvariable’s\\tvalue\\nacross\\tdevices.\\tThe\\tsame\\tgoes\\tfor\\tdevices\\tlocated\\ton\\tdifferent\\tservers\\t(as\\tlong\\tas\\tthey\\tare\\tpart\\tof\\nthe\\tsame\\tcluster).\\n5\\n.\\t\\nYes,\\ttwo\\toperations\\tplaced\\ton\\tthe\\tsame\\tdevice\\tcan\\trun\\tin\\tparallel:\\tTensorFlow\\tautomatically\\ttakes\\ncare\\tof\\trunning\\toperations\\tin\\tparallel\\t(on\\tdifferent\\tCPU\\tcores\\tor\\tdifferent\\tGPU\\tthreads),\\tas\\tlong\\tas\\nno\\toperation\\tdepends\\ton\\tanother\\toperation’s\\toutput.\\tMoreover,\\tyou\\tcan\\tstart\\tmultiple\\tsessions\\tin\\nparallel\\tthreads\\t(or\\tprocesses),\\tand\\tevaluate\\toperations\\tin\\teach\\tthread.\\tSince\\tsessions\\tare\\nindependent,\\tTensorFlow\\twill\\tbe\\table\\tto\\tevaluate\\tany\\toperation\\tfrom\\tone\\tsession\\tin\\tparallel\\twith', 'any\\toperation\\tfrom\\tanother\\tsession.\\n6\\n.\\t\\nControl\\tdependencies\\tare\\tused\\twhen\\tyou\\twant\\tto\\tpostpone\\tthe\\tevaluation\\tof\\tan\\toperation\\tX\\tuntil\\nafter\\tsome\\tother\\toperations\\tare\\trun,\\teven\\tthough\\tthese\\toperations\\tare\\tnot\\trequired\\tto\\tcompute\\tX.\\nThis\\tis\\tuseful\\tin\\tparticular\\twhen\\tX\\twould\\toccupy\\ta\\tlot\\tof\\tmemory\\tand\\tyou\\tonly\\tneed\\tit\\tlater\\tin\\tthe\\ncomputation\\tgraph,\\tor\\tif\\tX\\tuses\\tup\\ta\\tlot\\tof\\tI/O\\t(for\\texample,\\tit\\trequires\\ta\\tlarge\\tvariable\\tvalue\\nlocated\\ton\\ta\\tdifferent\\tdevice\\tor\\tserver)\\tand\\tyou\\tdon’t\\twant\\tit\\tto\\trun\\tat\\tthe\\tsame\\ttime\\tas\\tother\\tI/O-\\nhungry\\toperations,\\tto\\tavoid\\tsaturating\\tthe\\tbandwidth.\\n7\\n.\\t\\nYou’re\\tin\\tluck!\\tIn\\tdistributed\\tTensorFlow,\\tthe\\tvariable\\tvalues\\tlive\\tin\\tcontainers\\tmanaged\\tby\\tthe\\ncluster,\\tso\\teven\\tif\\tyou\\tclose\\tthe\\tsession\\tand\\texit\\tthe\\tclient\\tprogram,\\tthe\\tmodel\\tparameters\\tare\\tstill\\nalive\\tand\\twell\\ton\\tthe\\tcluster.\\tYou\\tsimply\\tneed\\tto\\topen\\ta\\tnew\\tsession\\tto\\tthe\\tcluster\\tand\\tsave\\tthe\\nmodel\\t(make\\tsure\\tyou\\tdon’t\\tcall\\tthe\\tvariable\\tinitializers\\tor\\trestore\\ta\\tprevious\\tmodel,\\tas\\tthis\\twould', 'destroy\\tyour\\tprecious\\tnew\\tmodel!).\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t13\\n:\\tConvolutional\\tNeural\\tNetworks\\n1\\n.\\t\\nThese\\tare\\tthe\\tmain\\tadvantages\\tof\\ta\\tCNN\\tover\\ta\\tfully\\tconnected\\tDNN\\tfor\\timage\\tclassification:\\nBecause\\tconsecutive\\tlayers\\tare\\tonly\\tpartially\\tconnected\\tand\\tbecause\\tit\\theavily\\treuses\\tits\\nweights,\\ta\\tCNN\\thas\\tmany\\tfewer\\tparameters\\tthan\\ta\\tfully\\tconnected\\tDNN,\\twhich\\tmakes\\tit\\tmuch\\nfaster\\tto\\ttrain,\\treduces\\tthe\\trisk\\tof\\toverfitting,\\tand\\trequires\\tmuch\\tless\\ttraining\\tdata.\\nWhen\\ta\\tCNN\\thas\\tlearned\\ta\\tkernel\\tthat\\tcan\\tdetect\\ta\\tparticular\\tfeature,\\tit\\tcan\\tdetect\\tthat\\tfeature\\nanywhere\\ton\\tthe\\timage.\\tIn\\tcontrast,\\twhen\\ta\\tDNN\\tlearns\\ta\\tfeature\\tin\\tone\\tlocation,\\tit\\tcan\\tdetect\\tit\\nonly\\tin\\tthat\\tparticular\\tlocation.\\tSince\\timages\\ttypically\\thave\\tvery\\trepetitive\\tfeatures,\\tCNNs\\tare\\nable\\tto\\tgeneralize\\tmuch\\tbetter\\tthan\\tDNNs\\tfor\\timage\\tprocessing\\ttasks\\tsuch\\tas\\tclassification,\\nusing\\tfewer\\ttraining\\texamples.\\nFinally,\\ta\\tDNN\\thas\\tno\\tprior\\tknowledge\\tof\\thow\\tpixels\\tare\\torganized;\\tit\\tdoes\\tnot\\tknow\\tthat\\nnearby\\tpixels\\tare\\tclose.\\tA\\tCNN’s\\tarchitecture\\tembeds\\tthis\\tprior\\tknowledge.\\tLower\\tlayers', 'typically\\tidentify\\tfeatures\\tin\\tsmall\\tareas\\tof\\tthe\\timages,\\twhile\\thigher\\tlayers\\tcombine\\tthe\\tlower-\\nlevel\\tfeatures\\tinto\\tlarger\\tfeatures.\\tThis\\tworks\\twell\\twith\\tmost\\tnatural\\timages,\\tgiving\\tCNNs\\ta\\ndecisive\\thead\\tstart\\tcompared\\tto\\tDNNs.\\n2\\n.\\t\\nLet’s\\tcompute\\thow\\tmany\\tparameters\\tthe\\tCNN\\thas.\\tSince\\tits\\tfirst\\tconvolutional\\tlayer\\thas\\t3\\t×\\t3\\nkernels,\\tand\\tthe\\tinput\\thas\\tthree\\tchannels\\t(red,\\tgreen,\\tand\\tblue),\\tthen\\teach\\tfeature\\tmap\\thas\\t3\\t×\\t3\\t×\\t3\\nweights,\\tplus\\ta\\tbias\\tterm.\\tThat’s\\t28\\tparameters\\tper\\tfeature\\tmap.\\tSince\\tthis\\tfirst\\tconvolutional\\tlayer\\nhas\\t100\\tfeature\\tmaps,\\tit\\thas\\ta\\ttotal\\tof\\t2,800\\tparameters.\\tThe\\tsecond\\tconvolutional\\tlayer\\thas\\t3\\t×\\t3\\nkernels,\\tand\\tits\\tinput\\tis\\tthe\\tset\\tof\\t100\\tfeature\\tmaps\\tof\\tthe\\tprevious\\tlayer,\\tso\\teach\\tfeature\\tmap\\thas\\t3\\t×\\n3\\t×\\t100\\t=\\t900\\tweights,\\tplus\\ta\\tbias\\tterm.\\tSince\\tit\\thas\\t200\\tfeature\\tmaps,\\tthis\\tlayer\\thas\\t901\\t×\\t200\\t=\\n180,200\\tparameters.\\tFinally,\\tthe\\tthird\\tand\\tlast\\tconvolutional\\tlayer\\talso\\thas\\t3\\t×\\t3\\tkernels,\\tand\\tits', 'input\\tis\\tthe\\tset\\tof\\t200\\tfeature\\tmaps\\tof\\tthe\\tprevious\\tlayers,\\tso\\teach\\tfeature\\tmap\\thas\\t3\\t×\\t3\\t×\\t200\\t=\\n1,800\\tweights,\\tplus\\ta\\tbias\\tterm.\\tSince\\tit\\thas\\t400\\tfeature\\tmaps,\\tthis\\tlayer\\thas\\ta\\ttotal\\tof\\t1,801\\t×\\t400\\t=\\n720,400\\tparameters.\\tAll\\tin\\tall,\\tthe\\tCNN\\thas\\t2,800\\t+\\t180,200\\t+\\t720,400\\t=\\t\\n903,400\\tparameters.\\t\\nNow\\tlet’s\\tcompute\\thow\\tmuch\\tRAM\\tthis\\tneural\\tnetwork\\twill\\trequire\\t(at\\tleast)\\twhen\\tmaking\\ta\\nprediction\\tfor\\ta\\tsingle\\tinstance.\\tFirst\\tlet’s\\tcompute\\tthe\\tfeature\\tmap\\tsize\\tfor\\teach\\tlayer.\\tSince\\twe\\tare\\nusing\\ta\\tstride\\tof\\t2\\tand\\tSAME\\tpadding,\\tthe\\thorizontal\\tand\\tvertical\\tsize\\tof\\tthe\\tfeature\\tmaps\\tare\\ndivided\\tby\\t2\\tat\\teach\\tlayer\\t(rounding\\tup\\tif\\tnecessary),\\tso\\tas\\tthe\\tinput\\tchannels\\tare\\t200\\t×\\t300\\tpixels,\\nthe\\tfirst\\tlayer’s\\tfeature\\tmaps\\tare\\t100\\t×\\t150,\\tthe\\tsecond\\tlayer’s\\tfeature\\tmaps\\tare\\t50\\t×\\t75,\\tand\\tthe\\nthird\\tlayer’s\\tfeature\\tmaps\\tare\\t25\\t×\\t38.\\tSince\\t32\\tbits\\tis\\t4\\tbytes\\tand\\tthe\\tfirst\\tconvolutional\\tlayer\\thas\\n100\\tfeature\\tmaps,\\tthis\\tfirst\\tlayer\\ttakes\\tup\\t4\\tx\\t100\\t×\\t150\\t×\\t100\\t=\\t6\\tmillion\\tbytes\\t(about\\t5.7\\tMB,', 'considering\\tthat\\t1\\tMB\\t=\\t1,024\\tKB\\tand\\t1\\tKB\\t=\\t1,024\\tbytes).\\tThe\\tsecond\\tlayer\\ttakes\\tup\\t4\\t×\\t50\\t×\\t75\\n×\\t200\\t=\\t3\\tmillion\\tbytes\\t(about\\t2.9\\tMB).\\tFinally,\\tthe\\tthird\\tlayer\\ttakes\\tup\\t4\\t×\\t25\\t×\\t38\\t×\\t400\\t=\\n1,520,000\\tbytes\\t(about\\t1.4\\tMB).\\tHowever,\\tonce\\ta\\tlayer\\thas\\tbeen\\tcomputed,\\tthe\\tmemory\\toccupied\\nby\\tthe\\tprevious\\tlayer\\tcan\\tbe\\treleased,\\tso\\tif\\teverything\\tis\\twell\\toptimized,\\tonly\\t6\\t+\\t9\\t=\\t15\\tmillion\\nbytes\\t(about\\t14.3\\tMB)\\tof\\tRAM\\twill\\tbe\\trequired\\t(when\\tthe\\tsecond\\tlayer\\thas\\tjust\\tbeen\\tcomputed,\\tbut\\nthe\\tmemory\\toccupied\\tby\\tthe\\tfirst\\tlayer\\tis\\tnot\\treleased\\tyet).\\tBut\\twait,\\tyou\\talso\\tneed\\tto\\tadd\\tthe\\nmemory\\toccupied\\tby\\tthe\\tCNN’s\\tparameters.\\tWe\\tcomputed\\tearlier\\tthat\\tit\\thas\\t903,400\\tparameters,\\neach\\tusing\\tup\\t4\\tbytes,\\tso\\tthis\\tadds\\t3,613,600\\tbytes\\t(about\\t3.4\\tMB).\\tThe\\ttotal\\tRAM\\trequired\\tis\\t(at\\nleast)\\t18,613,600\\tbytes\\t(about\\t17.8\\tMB).', 'Lastly,\\tlet’s\\tcompute\\tthe\\tminimum\\tamount\\tof\\tRAM\\trequired\\twhen\\ttraining\\tthe\\tCNN\\ton\\ta\\tmini-batch\\nof\\t50\\timages.\\tDuring\\ttraining\\tTensorFlow\\tuses\\tbackpropagation,\\twhich\\trequires\\tkeeping\\tall\\tvalues\\ncomputed\\tduring\\tthe\\tforward\\tpass\\tuntil\\tthe\\treverse\\tpass\\tbegins.\\tSo\\twe\\tmust\\tcompute\\tthe\\ttotal\\tRAM\\nrequired\\tby\\tall\\tlayers\\tfor\\ta\\tsingle\\tinstance\\tand\\tmultiply\\tthat\\tby\\t50!\\tAt\\tthat\\tpoint\\tlet’s\\tstart\\tcounting\\tin\\nmegabytes\\trather\\tthan\\tbytes.\\tWe\\tcomputed\\tbefore\\tthat\\tthe\\tthree\\tlayers\\trequire\\trespectively\\t5.7,\\t2.9,\\nand\\t1.4\\tMB\\tfor\\teach\\tinstance.\\tThat’s\\ta\\ttotal\\tof\\t10.0\\tMB\\tper\\tinstance.\\tSo\\tfor\\t50\\tinstances\\tthe\\ttotal\\nRAM\\tis\\t500\\tMB.\\tAdd\\tto\\tthat\\tthe\\tRAM\\trequired\\tby\\tthe\\tinput\\timages,\\twhich\\tis\\t50\\t×\\t4\\t×\\t200\\t×\\t300\\t×\\n3\\t=\\t36\\tmillion\\tbytes\\t(about\\t34.3\\tMB),\\tplus\\tthe\\tRAM\\trequired\\tfor\\tthe\\tmodel\\tparameters,\\twhich\\tis\\nabout\\t3.4\\tMB\\t(computed\\tearlier),\\tplus\\tsome\\tRAM\\tfor\\tthe\\tgradients\\t(we\\twill\\tneglect\\tthem\\tsince\\tthey\\ncan\\tbe\\treleased\\tgradually\\tas\\tbackpropagation\\tgoes\\tdown\\tthe\\tlayers\\tduring\\tthe\\treverse\\tpass).\\tWe\\tare', 'up\\tto\\ta\\ttotal\\tof\\troughly\\t500.0\\t+\\t34.3\\t+\\t3.4\\t=\\t537.7\\tMB.\\tAnd\\tthat’s\\treally\\tan\\toptimistic\\tbare\\nminimum.\\n3\\n.\\t\\nIf\\tyour\\tGPU\\truns\\tout\\tof\\tmemory\\twhile\\ttraining\\ta\\tCNN,\\there\\tare\\tfive\\tthings\\tyou\\tcould\\ttry\\tto\\tsolve\\tthe\\nproblem\\t(other\\tthan\\tpurchasing\\ta\\tGPU\\twith\\tmore\\tRAM):\\nReduce\\tthe\\tmini-batch\\tsize.\\nReduce\\tdimensionality\\tusing\\ta\\tlarger\\tstride\\tin\\tone\\tor\\tmore\\tlayers.\\nRemove\\tone\\tor\\tmore\\tlayers.\\nUse\\t16-bit\\tfloats\\tinstead\\tof\\t32-bit\\tfloats.\\nDistribute\\tthe\\tCNN\\tacross\\tmultiple\\tdevices.\\n4\\n.\\t\\nA\\tmax\\tpooling\\tlayer\\thas\\tno\\tparameters\\tat\\tall,\\twhereas\\ta\\tconvolutional\\tlayer\\thas\\tquite\\ta\\tfew\\t(see\\tthe\\nprevious\\tquestions).\\n5\\n.\\t\\nA\\t\\nlocal\\tresponse\\tnormalization\\n\\tlayer\\tmakes\\tthe\\tneurons\\tthat\\tmost\\tstrongly\\tactivate\\tinhibit\\tneurons\\nat\\tthe\\tsame\\tlocation\\tbut\\tin\\tneighboring\\tfeature\\tmaps,\\twhich\\tencourages\\tdifferent\\tfeature\\tmaps\\tto\\nspecialize\\tand\\tpushes\\tthem\\tapart,\\tforcing\\tthem\\tto\\texplore\\ta\\twider\\trange\\tof\\tfeatures.\\tIt\\tis\\ttypically', 'used\\tin\\tthe\\tlower\\tlayers\\tto\\thave\\ta\\tlarger\\tpool\\tof\\tlow-level\\tfeatures\\tthat\\tthe\\tupper\\tlayers\\tcan\\tbuild\\nupon.\\n6\\n.\\t\\nThe\\tmain\\tinnovations\\tin\\tAlexNet\\tcompared\\tto\\tLeNet-5\\tare\\t(1)\\tit\\tis\\tmuch\\tlarger\\tand\\tdeeper,\\tand\\t(2)\\nit\\tstacks\\tconvolutional\\tlayers\\tdirectly\\ton\\ttop\\tof\\teach\\tother,\\tinstead\\tof\\tstacking\\ta\\tpooling\\tlayer\\ton\\ttop\\nof\\teach\\tconvolutional\\tlayer.\\tThe\\tmain\\tinnovation\\tin\\tGoogLeNet\\tis\\tthe\\tintroduction\\tof\\t\\ninception\\nmodules\\n,\\twhich\\tmake\\tit\\tpossible\\tto\\thave\\ta\\tmuch\\tdeeper\\tnet\\tthan\\tprevious\\tCNN\\tarchitectures,\\twith\\nfewer\\tparameters.\\tFinally,\\tResNet’s\\tmain\\tinnovation\\tis\\tthe\\tintroduction\\tof\\tskip\\tconnections,\\twhich\\nmake\\tit\\tpossible\\tto\\tgo\\twell\\tbeyond\\t100\\tlayers.\\tArguably,\\tits\\tsimplicity\\tand\\tconsistency\\tare\\talso\\nrather\\tinnovative.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t7,\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t14\\n:\\tRecurrent\\tNeural\\tNetworks\\n1\\n.\\t\\nHere\\tare\\ta\\tfew\\tRNN\\tapplications:\\nFor\\ta\\tsequence-to-sequence\\tRNN:\\tpredicting\\tthe\\tweather\\t(or\\tany\\tother\\ttime\\tseries),\\tmachine\\ntranslation\\t(using\\tan\\tencoder–decoder\\tarchitecture),\\tvideo\\tcaptioning,\\tspeech\\tto\\ttext,\\tmusic\\ngeneration\\t(or\\tother\\tsequence\\tgeneration),\\tidentifying\\tthe\\tchords\\tof\\ta\\tsong.\\nFor\\ta\\tsequence-to-vector\\tRNN:\\tclassifying\\tmusic\\tsamples\\tby\\tmusic\\tgenre,\\tanalyzing\\tthe\\nsentiment\\tof\\ta\\tbook\\treview,\\tpredicting\\twhat\\tword\\tan\\taphasic\\tpatient\\tis\\tthinking\\tof\\tbased\\ton\\nreadings\\tfrom\\tbrain\\timplants,\\tpredicting\\tthe\\tprobability\\tthat\\ta\\tuser\\twill\\twant\\tto\\twatch\\ta\\tmovie\\nbased\\ton\\ther\\twatch\\thistory\\t(this\\tis\\tone\\tof\\tmany\\tpossible\\timplementations\\tof\\t\\ncollaborative\\nfiltering\\n).\\nFor\\ta\\tvector-to-sequence\\tRNN:\\timage\\tcaptioning,\\tcreating\\ta\\tmusic\\tplaylist\\tbased\\ton\\tan\\nembedding\\tof\\tthe\\tcurrent\\tartist,\\tgenerating\\ta\\tmelody\\tbased\\ton\\ta\\tset\\tof\\tparameters,\\tlocating\\npedestrians\\tin\\ta\\tpicture\\t(e.g.,\\ta\\tvideo\\tframe\\tfrom\\ta\\tself-driving\\tcar’s\\tcamera).\\n2\\n.', '2\\n.\\t\\nIn\\tgeneral,\\tif\\tyou\\ttranslate\\ta\\tsentence\\tone\\tword\\tat\\ta\\ttime,\\tthe\\tresult\\twill\\tbe\\tterrible.\\tFor\\texample,\\tthe\\nFrench\\tsentence\\t“Je\\tvous\\ten\\tprie”\\tmeans\\t“You\\tare\\twelcome,”\\tbut\\tif\\tyou\\ttranslate\\tit\\tone\\tword\\tat\\ta\\ntime,\\tyou\\tget\\t“I\\tyou\\tin\\tpray.”\\tHuh?\\tIt\\tis\\tmuch\\tbetter\\tto\\tread\\tthe\\twhole\\tsentence\\tfirst\\tand\\tthen\\ttranslate\\nit.\\tA\\tplain\\tsequence-to-sequence\\tRNN\\twould\\tstart\\ttranslating\\ta\\tsentence\\timmediately\\tafter\\treading\\nthe\\tfirst\\tword,\\twhile\\tan\\tencoder–decoder\\tRNN\\twill\\tfirst\\tread\\tthe\\twhole\\tsentence\\tand\\tthen\\ttranslate\\nit.\\tThat\\tsaid,\\tone\\tcould\\timagine\\ta\\tplain\\tsequence-to-sequence\\tRNN\\tthat\\twould\\toutput\\tsilence\\nwhenever\\tit\\tis\\tunsure\\tabout\\twhat\\tto\\tsay\\tnext\\t(just\\tlike\\thuman\\ttranslators\\tdo\\twhen\\tthey\\tmust\\ttranslate\\na\\tlive\\tbroadcast).\\n3\\n.\\t\\nTo\\tclassify\\tvideos\\tbased\\ton\\tthe\\tvisual\\tcontent,\\tone\\tpossible\\tarchitecture\\tcould\\tbe\\tto\\ttake\\t(say)\\tone\\nframe\\tper\\tsecond,\\tthen\\trun\\teach\\tframe\\tthrough\\ta\\tconvolutional\\tneural\\tnetwork,\\tfeed\\tthe\\toutput\\tof\\tthe', 'CNN\\tto\\ta\\tsequence-to-vector\\tRNN,\\tand\\tfinally\\trun\\tits\\toutput\\tthrough\\ta\\tsoftmax\\tlayer,\\tgiving\\tyou\\tall\\nthe\\tclass\\tprobabilities.\\tFor\\ttraining\\tyou\\twould\\tjust\\tuse\\tcross\\tentropy\\tas\\tthe\\tcost\\tfunction.\\tIf\\tyou\\nwanted\\tto\\tuse\\tthe\\taudio\\tfor\\tclassification\\tas\\twell,\\tyou\\tcould\\tconvert\\tevery\\tsecond\\tof\\taudio\\tto\\ta\\nspectrograph,\\tfeed\\tthis\\tspectrograph\\tto\\ta\\tCNN,\\tand\\tfeed\\tthe\\toutput\\tof\\tthis\\tCNN\\tto\\tthe\\tRNN\\t(along\\nwith\\tthe\\tcorresponding\\toutput\\tof\\tthe\\tother\\tCNN).\\n4\\n.\\t\\nBuilding\\tan\\tRNN\\tusing\\t\\ndynamic_rnn()\\n\\trather\\tthan\\t\\nstatic_rnn()\\n\\toffers\\tseveral\\t\\nadvantages:\\nIt\\tis\\tbased\\ton\\ta\\t\\nwhile_loop()\\n\\toperation\\tthat\\tis\\table\\tto\\tswap\\tthe\\tGPU’s\\tmemory\\tto\\tthe\\tCPU’s\\nmemory\\tduring\\tbackpropagation,\\tavoiding\\tout-of-memory\\terrors.\\nIt\\tis\\targuably\\teasier\\tto\\tuse,\\tas\\tit\\tcan\\tdirectly\\ttake\\ta\\tsingle\\ttensor\\tas\\tinput\\tand\\toutput\\t(covering\\nall\\ttime\\tsteps),\\trather\\tthan\\ta\\tlist\\tof\\ttensors\\t(one\\tper\\ttime\\tstep).\\tNo\\tneed\\tto\\t\\nstack,\\tunstack,\\tor\\ntranspose.\\nIt\\tgenerates\\ta\\tsmaller\\tgraph,\\teasier\\tto\\tvisualize\\tin\\tTensorBoard.\\n5\\n.', '5\\n.\\t\\nTo\\thandle\\tvariable\\tlength\\tinput\\tsequences,\\tthe\\tsimplest\\toption\\tis\\tto\\tset\\tthe\\t\\nsequence_length\\nparameter\\twhen\\tcalling\\tthe\\t\\nstatic_rnn()\\n\\tor\\t\\ndynamic_rnn()\\n\\tfunctions.\\tAnother\\toption\\tis\\tto\\tpad', 'the\\tsmaller\\tinputs\\t(e.g.,\\twith\\tzeros)\\tto\\tmake\\tthem\\tthe\\tsame\\tsize\\tas\\tthe\\tlargest\\tinput\\t(this\\tmay\\tbe\\nfaster\\tthan\\tthe\\tfirst\\toption\\tif\\tthe\\tinput\\tsequences\\tall\\thave\\tvery\\tsimilar\\tlengths).\\tTo\\thandle\\tvariable-\\nlength\\toutput\\tsequences,\\tif\\tyou\\tknow\\tin\\tadvance\\tthe\\tlength\\tof\\teach\\toutput\\tsequence,\\tyou\\tcan\\tuse\\tthe\\nsequence_length\\n\\tparameter\\t(for\\texample,\\tconsider\\ta\\tsequence-to-sequence\\tRNN\\tthat\\tlabels\\tevery\\nframe\\tin\\ta\\tvideo\\twith\\ta\\tviolence\\tscore:\\tthe\\toutput\\tsequence\\twill\\tbe\\texactly\\tthe\\tsame\\tlength\\tas\\tthe\\ninput\\tsequence).\\tIf\\tyou\\tdon’t\\tknow\\tin\\tadvance\\tthe\\tlength\\tof\\tthe\\toutput\\tsequence,\\tyou\\tcan\\tuse\\tthe\\npadding\\ttrick:\\talways\\toutput\\tthe\\tsame\\tsize\\tsequence,\\tbut\\tignore\\tany\\toutputs\\tthat\\tcome\\tafter\\tthe\\tend-\\nof-sequence\\ttoken\\t(by\\tignoring\\tthem\\twhen\\tcomputing\\tthe\\tcost\\tfunction).\\n6\\n.\\t\\nTo\\tdistribute\\ttraining\\tand\\texecution\\tof\\ta\\tdeep\\tRNN\\tacross\\tmultiple\\tGPUs,\\ta\\tcommon\\ttechnique\\tis\\nsimply\\tto\\tplace\\teach\\tlayer\\ton\\ta\\tdifferent\\tGPU\\t(see\\t\\nChapter\\t12\\n).', ').\\nFor\\tthe\\tsolutions\\tto\\texercises\\t7,\\t8,\\tand\\t9,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t15\\n:\\tAutoencoders\\n1\\n.\\t\\nHere\\tare\\tsome\\tof\\tthe\\tmain\\ttasks\\tthat\\tautoencoders\\tare\\tused\\tfor:\\nFeature\\textraction\\nUnsupervised\\tpretraining\\nDimensionality\\treduction\\nGenerative\\tmodels\\nAnomaly\\tdetection\\t(an\\tautoencoder\\tis\\tgenerally\\tbad\\tat\\treconstructing\\toutliers)\\n2\\n.\\t\\nIf\\tyou\\twant\\tto\\ttrain\\ta\\tclassifier\\tand\\tyou\\thave\\tplenty\\tof\\tunlabeled\\ttraining\\tdata,\\tbut\\tonly\\ta\\tfew\\nthousand\\tlabeled\\tinstances,\\tthen\\tyou\\tcould\\tfirst\\ttrain\\ta\\tdeep\\tautoencoder\\ton\\tthe\\tfull\\tdataset\\t(labeled\\n+\\tunlabeled),\\tthen\\treuse\\tits\\tlower\\thalf\\tfor\\tthe\\tclassifier\\t(i.e.,\\treuse\\tthe\\tlayers\\tup\\tto\\tthe\\tcodings\\tlayer,\\nincluded)\\tand\\ttrain\\tthe\\tclassifier\\tusing\\tthe\\tlabeled\\tdata.\\tIf\\tyou\\thave\\tlittle\\tlabeled\\tdata,\\tyou\\tprobably\\nwant\\tto\\tfreeze\\tthe\\treused\\tlayers\\twhen\\ttraining\\tthe\\tclassifier.\\n3\\n.\\t\\nThe\\tfact\\tthat\\tan\\tautoencoder\\tperfectly\\treconstructs\\tits\\tinputs\\tdoes\\tnot\\tnecessarily\\tmean\\tthat\\tit\\tis\\ta\\ngood\\tautoencoder;\\tperhaps\\tit\\tis\\tsimply\\tan\\tovercomplete\\tautoencoder\\tthat\\tlearned\\tto\\tcopy\\tits\\tinputs', 'to\\tthe\\tcodings\\tlayer\\tand\\tthen\\tto\\tthe\\toutputs.\\tIn\\tfact,\\teven\\tif\\tthe\\tcodings\\tlayer\\tcontained\\ta\\tsingle\\nneuron,\\tit\\twould\\tbe\\tpossible\\tfor\\ta\\tvery\\tdeep\\tautoencoder\\tto\\tlearn\\tto\\tmap\\teach\\ttraining\\tinstance\\tto\\ta\\ndifferent\\tcoding\\t(e.g.,\\tthe\\tfirst\\tinstance\\tcould\\tbe\\tmapped\\tto\\t0.001,\\tthe\\tsecond\\tto\\t0.002,\\tthe\\tthird\\tto\\n0.003,\\tand\\tso\\ton),\\tand\\tit\\tcould\\tlearn\\t“by\\theart”\\tto\\treconstruct\\tthe\\tright\\ttraining\\tinstance\\tfor\\teach\\ncoding.\\tIt\\twould\\tperfectly\\treconstruct\\tits\\tinputs\\twithout\\treally\\tlearning\\tany\\tuseful\\tpattern\\tin\\tthe\\tdata.\\nIn\\tpractice\\tsuch\\ta\\tmapping\\tis\\tunlikely\\tto\\thappen,\\tbut\\tit\\tillustrates\\tthe\\tfact\\tthat\\tperfect\\treconstructions\\nare\\tnot\\ta\\tguarantee\\tthat\\tthe\\tautoencoder\\tlearned\\tanything\\tuseful.\\tHowever,\\tif\\tit\\tproduces\\tvery\\tbad\\nreconstructions,\\tthen\\tit\\tis\\talmost\\tguaranteed\\tto\\tbe\\ta\\tbad\\tautoencoder.\\tTo\\tevaluate\\tthe\\tperformance\\tof\\nan\\tautoencoder,\\tone\\toption\\tis\\tto\\tmeasure\\tthe\\treconstruction\\tloss\\t(e.g.,\\tcompute\\tthe\\tMSE,\\tthe\\tmean', 'square\\tof\\tthe\\toutputs\\tminus\\tthe\\tinputs).\\tAgain,\\ta\\thigh\\treconstruction\\tloss\\tis\\ta\\tgood\\tsign\\tthat\\tthe\\nautoencoder\\tis\\tbad,\\tbut\\ta\\tlow\\treconstruction\\tloss\\tis\\tnot\\ta\\tguarantee\\tthat\\tit\\tis\\tgood.\\tYou\\tshould\\talso\\nevaluate\\tthe\\tautoencoder\\taccording\\tto\\twhat\\tit\\twill\\tbe\\tused\\tfor.\\tFor\\texample,\\tif\\tyou\\tare\\tusing\\tit\\tfor\\nunsupervised\\tpretraining\\tof\\ta\\tclassifier,\\tthen\\tyou\\tshould\\talso\\tevaluate\\tthe\\tclassifier’s\\tperformance.\\n4\\n.\\t\\nAn\\tundercomplete\\tautoencoder\\tis\\tone\\twhose\\tcodings\\tlayer\\tis\\tsmaller\\tthan\\tthe\\tinput\\tand\\toutput\\nlayers.\\tIf\\tit\\tis\\tlarger,\\tthen\\tit\\tis\\tan\\tovercomplete\\tautoencoder.\\tThe\\tmain\\trisk\\tof\\tan\\texcessively\\nundercomplete\\tautoencoder\\tis\\tthat\\tit\\tmay\\tfail\\tto\\treconstruct\\tthe\\tinputs.\\tThe\\tmain\\trisk\\tof\\tan\\novercomplete\\tautoencoder\\tis\\tthat\\tit\\tmay\\tjust\\tcopy\\tthe\\tinputs\\tto\\tthe\\toutputs,\\twithout\\tlearning\\tany\\nuseful\\tfeature.\\n5\\n.\\t\\nTo\\ttie\\tthe\\tweights\\tof\\tan\\tencoder\\tlayer\\tand\\tits\\tcorresponding\\tdecoder\\tlayer,\\tyou\\tsimply\\tmake\\tthe\\ndecoder\\tweights\\tequal\\tto\\tthe\\ttranspose\\tof\\tthe\\tencoder\\tweights.\\tThis\\treduces\\tthe\\tnumber\\tof', 'parameters\\tin\\tthe\\tmodel\\tby\\thalf,\\toften\\tmaking\\ttraining\\tconverge\\tfaster\\twith\\tless\\ttraining\\tdata,\\tand\\nreducing\\tthe\\trisk\\tof\\toverfitting\\tthe\\ttraining\\tset.', '6\\n.\\t\\nTo\\tvisualize\\tthe\\tfeatures\\tlearned\\tby\\tthe\\tlower\\tlayer\\tof\\ta\\tstacked\\tautoencoder,\\ta\\tcommon\\ttechnique\\tis\\nsimply\\tto\\tplot\\tthe\\tweights\\tof\\teach\\tneuron,\\tby\\treshaping\\teach\\tweight\\tvector\\tto\\tthe\\tsize\\tof\\tan\\tinput\\nimage\\t(e.g.,\\tfor\\tMNIST,\\treshaping\\ta\\tweight\\tvector\\tof\\tshape\\t\\n[784]\\n\\tto\\t\\n[28,\\t28]\\n).\\tTo\\tvisualize\\tthe\\nfeatures\\tlearned\\tby\\thigher\\tlayers,\\tone\\ttechnique\\tis\\tto\\tdisplay\\tthe\\ttraining\\tinstances\\tthat\\tmost\\tactivate\\neach\\tneuron.\\n7\\n.\\t\\nA\\tgenerative\\tmodel\\tis\\ta\\tmodel\\tcapable\\tof\\trandomly\\tgenerating\\toutputs\\tthat\\tresemble\\tthe\\ttraining\\ninstances.\\tFor\\texample,\\tonce\\ttrained\\tsuccessfully\\ton\\tthe\\tMNIST\\tdataset,\\ta\\tgenerative\\tmodel\\tcan\\tbe\\nused\\tto\\trandomly\\tgenerate\\trealistic\\timages\\tof\\tdigits.\\tThe\\toutput\\tdistribution\\tis\\ttypically\\tsimilar\\tto\\nthe\\ttraining\\tdata.\\tFor\\texample,\\tsince\\tMNIST\\tcontains\\tmany\\timages\\tof\\teach\\tdigit,\\tthe\\tgenerative\\nmodel\\twould\\toutput\\troughly\\tthe\\tsame\\tnumber\\tof\\timages\\tof\\teach\\tdigit.\\tSome\\tgenerative\\tmodels\\tcan', 'be\\tparametrized\\t—\\tfor\\texample,\\tto\\tgenerate\\tonly\\tsome\\tkinds\\tof\\toutputs.\\tAn\\texample\\tof\\ta\\tgenerative\\nautoencoder\\tis\\tthe\\tvariational\\tautoencoder.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.', 'Chapter\\t16\\n:\\tReinforcement\\tLearning\\n1\\n.\\t\\nReinforcement\\tLearning\\tis\\tan\\tarea\\tof\\tMachine\\tLearning\\taimed\\tat\\tcreating\\tagents\\tcapable\\tof\\ttaking\\nactions\\tin\\tan\\tenvironment\\tin\\ta\\tway\\tthat\\tmaximizes\\trewards\\tover\\ttime.\\tThere\\tare\\tmany\\tdifferences\\nbetween\\tRL\\tand\\tregular\\tsupervised\\tand\\tunsupervised\\tlearning.\\tHere\\tare\\ta\\tfew:\\nIn\\tsupervised\\tand\\tunsupervised\\tlearning,\\tthe\\tgoal\\tis\\tgenerally\\tto\\tfind\\tpatterns\\tin\\tthe\\tdata.\\tIn\\nReinforcement\\tLearning,\\tthe\\tgoal\\tis\\tto\\tfind\\ta\\tgood\\tpolicy.\\nUnlike\\tin\\tsupervised\\tlearning,\\tthe\\tagent\\tis\\tnot\\texplicitly\\tgiven\\tthe\\t“right”\\tanswer.\\tIt\\tmust\\tlearn\\nby\\ttrial\\tand\\terror.\\nUnlike\\tin\\tunsupervised\\tlearning,\\tthere\\tis\\ta\\tform\\tof\\tsupervision,\\tthrough\\trewards.\\tWe\\tdo\\tnot\\ttell\\nthe\\tagent\\thow\\tto\\tperform\\tthe\\ttask,\\tbut\\twe\\tdo\\ttell\\tit\\twhen\\tit\\tis\\tmaking\\tprogress\\tor\\twhen\\tit\\tis\\nfailing.\\nA\\tReinforcement\\tLearning\\tagent\\tneeds\\tto\\tfind\\tthe\\tright\\tbalance\\tbetween\\texploring\\tthe\\nenvironment,\\tlooking\\tfor\\tnew\\tways\\tof\\tgetting\\trewards,\\tand\\texploiting\\tsources\\tof\\trewards\\tthat\\tit', 'already\\tknows.\\tIn\\tcontrast,\\tsupervised\\tand\\tunsupervised\\tlearning\\tsystems\\tgenerally\\tdon’t\\tneed\\nto\\tworry\\tabout\\texploration;\\tthey\\tjust\\tfeed\\ton\\tthe\\ttraining\\tdata\\tthey\\tare\\tgiven.\\nIn\\tsupervised\\tand\\tunsupervised\\tlearning,\\ttraining\\tinstances\\tare\\ttypically\\tindependent\\t(in\\tfact,\\nthey\\tare\\tgenerally\\tshuffled).\\tIn\\tReinforcement\\tLearning,\\tconsecutive\\tobservations\\tare\\tgenerally\\nnot\\n\\tindependent.\\tAn\\tagent\\tmay\\tremain\\tin\\tthe\\tsame\\tregion\\tof\\tthe\\tenvironment\\tfor\\ta\\twhile\\tbefore\\nit\\tmoves\\ton,\\tso\\tconsecutive\\tobservations\\twill\\tbe\\tvery\\tcorrelated.\\tIn\\tsome\\tcases\\ta\\treplay\\nmemory\\tis\\tused\\tto\\tensure\\tthat\\tthe\\ttraining\\talgorithm\\tgets\\tfairly\\tindependent\\tobservations.\\n2\\n.\\t\\nHere\\tare\\ta\\tfew\\tpossible\\tapplications\\tof\\tReinforcement\\tLearning,\\tother\\tthan\\tthose\\tmentioned\\tin\\nChapter\\t16\\n:\\nMusic\\tpersonalization\\nThe\\tenvironment\\tis\\ta\\tuser’s\\tpersonalized\\tweb\\tradio.\\tThe\\tagent\\tis\\tthe\\tsoftware\\tdeciding\\twhat\\nsong\\tto\\tplay\\tnext\\tfor\\tthat\\tuser.\\tIts\\tpossible\\tactions\\tare\\tto\\tplay\\tany\\tsong\\tin\\tthe\\tcatalog\\t(it\\tmust\\ttry', 'to\\tchoose\\ta\\tsong\\tthe\\tuser\\twill\\tenjoy)\\tor\\tto\\tplay\\tan\\tadvertisement\\t(it\\tmust\\ttry\\tto\\tchoose\\tan\\tad\\nthat\\tthe\\tuser\\twill\\tbe\\tinterested\\tin).\\tIt\\tgets\\ta\\tsmall\\treward\\tevery\\ttime\\tthe\\tuser\\tlistens\\tto\\ta\\tsong,\\ta\\nlarger\\treward\\tevery\\ttime\\tthe\\tuser\\tlistens\\tto\\tan\\tad,\\ta\\tnegative\\treward\\twhen\\tthe\\tuser\\tskips\\ta\\tsong\\nor\\tan\\tad,\\tand\\ta\\tvery\\tnegative\\treward\\tif\\tthe\\tuser\\tleaves.\\nMarketing\\nThe\\tenvironment\\tis\\tyour\\tcompany’s\\tmarketing\\tdepartment.\\tThe\\tagent\\tis\\tthe\\tsoftware\\tthat\\tdefines\\nwhich\\tcustomers\\ta\\tmailing\\tcampaign\\tshould\\tbe\\tsent\\tto,\\tgiven\\ttheir\\tprofile\\tand\\tpurchase\\thistory\\n(for\\teach\\tcustomer\\tit\\thas\\ttwo\\tpossible\\tactions:\\tsend\\tor\\tdon’t\\tsend).\\tIt\\tgets\\ta\\tnegative\\treward\\nfor\\tthe\\tcost\\tof\\tthe\\tmailing\\tcampaign,\\tand\\ta\\tpositive\\treward\\tfor\\testimated\\trevenue\\tgenerated\\nfrom\\tthis\\tcampaign.\\nProduct\\tdelivery', 'Let\\tthe\\tagent\\tcontrol\\ta\\tfleet\\tof\\tdelivery\\ttrucks,\\tdeciding\\twhat\\tthey\\tshould\\tpick\\tup\\tat\\tthe\\tdepots,\\nwhere\\tthey\\tshould\\tgo,\\twhat\\tthey\\tshould\\tdrop\\toff,\\tand\\tso\\ton.\\tThey\\twould\\tget\\tpositive\\trewards\\nfor\\teach\\tproduct\\tdelivered\\ton\\ttime,\\tand\\tnegative\\trewards\\tfor\\tlate\\tdeliveries.\\n3\\n.\\t\\nWhen\\testimating\\tthe\\tvalue\\tof\\tan\\taction,\\tReinforcement\\tLearning\\talgorithms\\ttypically\\tsum\\tall\\tthe\\nrewards\\tthat\\tthis\\taction\\tled\\tto,\\tgiving\\tmore\\tweight\\tto\\timmediate\\trewards,\\tand\\tless\\tweight\\tto\\tlater\\nrewards\\t(considering\\tthat\\tan\\taction\\thas\\tmore\\tinfluence\\ton\\tthe\\tnear\\tfuture\\tthan\\ton\\tthe\\tdistant\\tfuture).\\nTo\\tmodel\\tthis,\\ta\\tdiscount\\trate\\tis\\ttypically\\tapplied\\tat\\teach\\ttime\\tstep.\\tFor\\texample,\\twith\\ta\\tdiscount\\nrate\\tof\\t0.9,\\ta\\treward\\tof\\t100\\tthat\\tis\\treceived\\ttwo\\ttime\\tsteps\\tlater\\tis\\tcounted\\tas\\tonly\\t0.9\\n2\\n\\t×\\t100\\t=\\t81\\nwhen\\tyou\\tare\\testimating\\tthe\\tvalue\\tof\\tthe\\taction.\\tYou\\tcan\\tthink\\tof\\tthe\\tdiscount\\trate\\tas\\ta\\tmeasure\\tof\\nhow\\tmuch\\tthe\\tfuture\\tis\\tvalued\\trelative\\tto\\tthe\\tpresent:\\tif\\tit\\tis\\tvery\\tclose\\tto\\t1,\\tthen\\tthe\\tfuture\\tis\\tvalued', 'almost\\tas\\tmuch\\tas\\tthe\\tpresent.\\tIf\\tit\\tis\\tclose\\tto\\t0,\\tthen\\tonly\\timmediate\\trewards\\tmatter.\\tOf\\tcourse,\\tthis\\nimpacts\\tthe\\toptimal\\tpolicy\\ttremendously:\\tif\\tyou\\tvalue\\tthe\\tfuture,\\tyou\\tmay\\tbe\\twilling\\tto\\tput\\tup\\twith\\ta\\nlot\\tof\\timmediate\\tpain\\tfor\\tthe\\tprospect\\tof\\teventual\\trewards,\\twhile\\tif\\tyou\\tdon’t\\tvalue\\tthe\\tfuture,\\tyou\\nwill\\tjust\\tgrab\\tany\\timmediate\\treward\\tyou\\tcan\\tfind,\\tnever\\tinvesting\\tin\\tthe\\tfuture.\\n4\\n.\\t\\nTo\\tmeasure\\tthe\\tperformance\\tof\\ta\\tReinforcement\\tLearning\\tagent,\\tyou\\tcan\\tsimply\\tsum\\tup\\tthe\\trewards\\nit\\tgets.\\tIn\\ta\\tsimulated\\tenvironment,\\tyou\\tcan\\trun\\tmany\\tepisodes\\tand\\tlook\\tat\\tthe\\ttotal\\trewards\\tit\\tgets\\ton\\naverage\\t(and\\tpossibly\\tlook\\tat\\tthe\\tmin,\\tmax,\\tstandard\\tdeviation,\\tand\\tso\\ton).\\n5\\n.\\t\\nThe\\tcredit\\tassignment\\tproblem\\tis\\tthe\\tfact\\tthat\\twhen\\ta\\tReinforcement\\tLearning\\tagent\\treceives\\ta\\nreward,\\tit\\thas\\tno\\tdirect\\tway\\tof\\tknowing\\twhich\\tof\\tits\\tprevious\\tactions\\tcontributed\\tto\\tthis\\treward.\\tIt\\ntypically\\toccurs\\twhen\\tthere\\tis\\ta\\tlarge\\tdelay\\tbetween\\tan\\taction\\tand\\tthe\\tresulting\\trewards\\t(e.g.,\\tduring\\na\\tgame\\tof\\tAtari’s\\t\\nPong', 'Pong\\n,\\tthere\\tmay\\tbe\\ta\\tfew\\tdozen\\ttime\\tsteps\\tbetween\\tthe\\tmoment\\tthe\\tagent\\thits\\tthe\\nball\\tand\\tthe\\tmoment\\tit\\twins\\tthe\\tpoint).\\tOne\\tway\\tto\\talleviate\\tit\\tis\\tto\\tprovide\\tthe\\tagent\\twith\\tshorter-\\nterm\\trewards,\\twhen\\tpossible.\\tThis\\tusually\\trequires\\tprior\\tknowledge\\tabout\\tthe\\ttask.\\tFor\\texample,\\tif\\nwe\\twant\\tto\\tbuild\\tan\\tagent\\tthat\\twill\\tlearn\\tto\\tplay\\tchess,\\tinstead\\tof\\tgiving\\tit\\ta\\treward\\tonly\\twhen\\tit\\nwins\\tthe\\tgame,\\twe\\tcould\\tgive\\tit\\ta\\treward\\tevery\\ttime\\tit\\tcaptures\\tone\\tof\\tthe\\topponent’s\\tpieces.\\n6\\n.\\t\\nAn\\tagent\\tcan\\toften\\tremain\\tin\\tthe\\tsame\\tregion\\tof\\tits\\tenvironment\\tfor\\ta\\twhile,\\tso\\tall\\tof\\tits\\texperiences\\nwill\\tbe\\tvery\\tsimilar\\tfor\\tthat\\tperiod\\tof\\ttime.\\tThis\\tcan\\tintroduce\\tsome\\tbias\\tin\\tthe\\tlearning\\talgorithm.\\tIt\\nmay\\ttune\\tits\\tpolicy\\tfor\\tthis\\tregion\\tof\\tthe\\tenvironment,\\tbut\\tit\\twill\\tnot\\tperform\\twell\\tas\\tsoon\\tas\\tit\\nmoves\\tout\\tof\\tthis\\tregion.\\tTo\\tsolve\\tthis\\tproblem,\\tyou\\tcan\\tuse\\ta\\treplay\\tmemory;\\tinstead\\tof\\tusing\\tonly\\nthe\\tmost\\timmediate\\texperiences\\tfor\\tlearning,\\tthe\\tagent\\twill\\tlearn\\tbased\\ton\\ta\\tbuffer\\tof\\tits\\tpast', 'experiences,\\trecent\\tand\\tnot\\tso\\trecent\\t(perhaps\\tthis\\tis\\twhy\\twe\\tdream\\tat\\tnight:\\tto\\treplay\\tour\\nexperiences\\tof\\tthe\\tday\\tand\\tbetter\\tlearn\\tfrom\\tthem?).\\n7\\n.\\t\\nAn\\toff-policy\\tRL\\talgorithm\\tlearns\\tthe\\tvalue\\tof\\tthe\\toptimal\\tpolicy\\t(i.e.,\\tthe\\tsum\\tof\\tdiscounted\\nrewards\\tthat\\tcan\\tbe\\texpected\\tfor\\teach\\tstate\\tif\\tthe\\tagent\\tacts\\toptimally),\\tindependently\\tof\\thow\\tthe\\nagent\\tactually\\tacts.\\tQ-Learning\\tis\\ta\\tgood\\texample\\tof\\tsuch\\tan\\talgorithm.\\tIn\\tcontrast,\\tan\\ton-policy\\nalgorithm\\tlearns\\tthe\\tvalue\\tof\\tthe\\tpolicy\\tthat\\tthe\\tagent\\tactually\\texecutes,\\tincluding\\tboth\\texploration\\nand\\texploitation.\\nFor\\tthe\\tsolutions\\tto\\texercises\\t8,\\t9,\\tand\\t10,\\tplease\\tsee\\tthe\\tJupyter\\tnotebooks\\tavailable\\tat\\nhttps://github.com/ageron/handson-ml\\n.\\nIf\\tyou\\tdraw\\ta\\tstraight\\tline\\tbetween\\tany\\ttwo\\tpoints\\ton\\tthe\\tcurve,\\tthe\\tline\\tnever\\tcrosses\\tthe\\tcurve.\\n1', 'Moreover,\\tthe\\tNormal\\tEquation\\trequires\\tcomputing\\tthe\\tinverse\\tof\\ta\\tmatrix,\\tbut\\tthat\\tmatrix\\tis\\tnot\\talways\\tinvertible.\\tIn\\tcontrast,\\tthe\\tmatrix\\nfor\\tRidge\\tRegression\\tis\\talways\\tinvertible.\\nlog\\n2\\n\\tis\\tthe\\tbinary\\tlog,\\tlog\\n2\\n(\\nm\\n)\\t=\\tlog(\\nm\\n)\\t/\\tlog(2).\\nWhen\\tthe\\tvalues\\tto\\tpredict\\tcan\\tvary\\tby\\tmany\\torders\\tof\\tmagnitude,\\tthen\\tyou\\tmay\\twant\\tto\\tpredict\\tthe\\tlogarithm\\tof\\tthe\\ttarget\\tvalue\\trather\\nthan\\tthe\\ttarget\\tvalue\\tdirectly.\\tSimply\\tcomputing\\tthe\\texponential\\tof\\tthe\\tneural\\tnetwork’s\\toutput\\twill\\tgive\\tyou\\tthe\\testimated\\tvalue\\t(since\\nexp(log\\t\\nv\\n)\\t=\\t\\nv\\n).\\nIn\\t\\nChapter\\t11\\n\\twe\\tdiscuss\\tmany\\ttechniques\\tthat\\tintroduce\\tadditional\\thyperparameters:\\ttype\\tof\\tweight\\tinitialization,\\tactivation\\tfunction\\nhyperparameters\\t(e.g.,\\tamount\\tof\\tleak\\tin\\tleaky\\tReLU),\\tGradient\\tClipping\\tthreshold,\\ttype\\tof\\toptimizer\\tand\\tits\\thyperparameters\\t\\n(e.g.,\\tthe\\nmomentum\\thyperparameter\\twhen\\tusing\\ta\\t\\nMomentumOptimizer\\n),\\ttype\\tof\\tregularization\\tfor\\teach\\tlayer,\\tand\\tthe\\tregularization\\nhyperparameters\\t(e.g.,\\tdropout\\trate\\twhen\\tusing\\tdropout)\\tand\\tso\\ton.\\n2\\n3', '2\\n3\\n4\\n5', 'Appendix\\tB.\\t\\nMachine\\tLearning\\tProject\\nChecklist\\nThis\\t\\nchecklist\\tcan\\tguide\\tyou\\tthrough\\tyour\\tMachine\\tLearning\\tprojects.\\tThere\\tare\\teight\\tmain\\tsteps:\\n1\\n.\\t\\nFrame\\tthe\\tproblem\\tand\\tlook\\tat\\tthe\\tbig\\tpicture.\\n2\\n.\\t\\nGet\\tthe\\tdata.\\n3\\n.\\t\\nExplore\\tthe\\tdata\\tto\\tgain\\tinsights.\\n4\\n.\\t\\nPrepare\\tthe\\tdata\\tto\\tbetter\\texpose\\tthe\\tunderlying\\tdata\\tpatterns\\tto\\tMachine\\tLearning\\talgorithms.\\n5\\n.\\t\\nExplore\\tmany\\tdifferent\\tmodels\\tand\\tshort-list\\tthe\\tbest\\tones.\\n6\\n.\\t\\nFine-tune\\tyour\\tmodels\\tand\\tcombine\\tthem\\tinto\\ta\\tgreat\\tsolution.\\n7\\n.\\t\\nPresent\\tyour\\tsolution.\\n8\\n.\\t\\nLaunch,\\tmonitor,\\tand\\tmaintain\\tyour\\tsystem.\\nObviously,\\tyou\\tshould\\tfeel\\tfree\\tto\\tadapt\\tthis\\tchecklist\\tto\\tyour\\tneeds.', 'Frame\\tthe\\tProblem\\tand\\tLook\\tat\\tthe\\tBig\\tPicture\\n1\\n.\\t\\nDefine\\tthe\\tobjective\\tin\\tbusiness\\tterms.\\n2\\n.\\t\\nHow\\twill\\tyour\\tsolution\\tbe\\tused?\\n3\\n.\\t\\nWhat\\tare\\tthe\\tcurrent\\tsolutions/workarounds\\t(if\\tany)?\\n4\\n.\\t\\nHow\\tshould\\tyou\\tframe\\tthis\\tproblem\\t(supervised/unsupervised,\\tonline/offline,\\tetc.)?\\n5\\n.\\t\\nHow\\tshould\\tperformance\\tbe\\tmeasured?\\n6\\n.\\t\\nIs\\tthe\\tperformance\\tmeasure\\taligned\\twith\\tthe\\tbusiness\\tobjective?\\n7\\n.\\t\\nWhat\\twould\\tbe\\tthe\\tminimum\\tperformance\\tneeded\\tto\\treach\\tthe\\tbusiness\\tobjective?\\n8\\n.\\t\\nWhat\\tare\\tcomparable\\tproblems?\\tCan\\tyou\\treuse\\texperience\\tor\\ttools?\\n9\\n.\\t\\nIs\\thuman\\texpertise\\tavailable?\\n10\\n.\\t\\nHow\\twould\\tyou\\tsolve\\tthe\\tproblem\\tmanually?\\n11\\n.\\t\\nList\\tthe\\tassumptions\\tyou\\t(or\\tothers)\\thave\\tmade\\tso\\tfar.\\n12\\n.\\t\\nVerify\\tassumptions\\tif\\tpossible.', 'Get\\tthe\\tData\\nNote:\\tautomate\\tas\\tmuch\\tas\\tpossible\\tso\\tyou\\tcan\\teasily\\tget\\tfresh\\tdata.\\n1\\n.\\t\\nList\\tthe\\tdata\\tyou\\tneed\\tand\\thow\\tmuch\\tyou\\tneed.\\n2\\n.\\t\\nFind\\tand\\tdocument\\twhere\\tyou\\tcan\\tget\\tthat\\tdata.\\n3\\n.\\t\\nCheck\\thow\\tmuch\\tspace\\tit\\twill\\ttake.\\n4\\n.\\t\\nCheck\\tlegal\\tobligations,\\tand\\tget\\tauthorization\\tif\\tnecessary.\\n5\\n.\\t\\nGet\\taccess\\tauthorizations.\\n6\\n.\\t\\nCreate\\ta\\tworkspace\\t(with\\tenough\\tstorage\\tspace).\\n7\\n.\\t\\nGet\\tthe\\tdata.\\n8\\n.\\t\\nConvert\\tthe\\tdata\\tto\\ta\\tformat\\tyou\\tcan\\teasily\\tmanipulate\\t(without\\tchanging\\tthe\\tdata\\titself).\\n9\\n.\\t\\nEnsure\\tsensitive\\tinformation\\tis\\tdeleted\\tor\\tprotected\\t(e.g.,\\tanonymized).\\n10\\n.\\t\\nCheck\\tthe\\tsize\\tand\\ttype\\tof\\tdata\\t(time\\tseries,\\tsample,\\tgeographical,\\tetc.).\\n11\\n.\\t\\nSample\\ta\\ttest\\tset,\\tput\\tit\\taside,\\tand\\tnever\\tlook\\tat\\tit\\t(no\\tdata\\tsnooping!).', 'Explore\\tthe\\tData\\nNote:\\ttry\\tto\\tget\\tinsights\\tfrom\\ta\\tfield\\texpert\\tfor\\tthese\\tsteps.\\n1\\n.\\t\\nCreate\\ta\\tcopy\\tof\\tthe\\tdata\\tfor\\texploration\\t(sampling\\tit\\tdown\\tto\\ta\\tmanageable\\tsize\\tif\\tnecessary).\\n2\\n.\\t\\nCreate\\ta\\tJupyter\\tnotebook\\tto\\tkeep\\ta\\trecord\\tof\\tyour\\tdata\\texploration.\\n3\\n.\\t\\nStudy\\teach\\tattribute\\tand\\tits\\tcharacteristics:\\nName\\nType\\t(categorical,\\tint/float,\\tbounded/unbounded,\\ttext,\\tstructured,\\tetc.)\\n%\\tof\\tmissing\\tvalues\\nNoisiness\\tand\\ttype\\tof\\tnoise\\t(stochastic,\\toutliers,\\trounding\\terrors,\\tetc.)\\nPossibly\\tuseful\\tfor\\tthe\\ttask?\\nType\\tof\\tdistribution\\t(Gaussian,\\tuniform,\\tlogarithmic,\\tetc.)\\n4\\n.\\t\\nFor\\tsupervised\\tlearning\\ttasks,\\tidentify\\tthe\\ttarget\\tattribute(s).\\n5\\n.\\t\\nVisualize\\tthe\\tdata.\\n6\\n.\\t\\nStudy\\tthe\\tcorrelations\\tbetween\\tattributes.\\n7\\n.\\t\\nStudy\\thow\\tyou\\twould\\tsolve\\tthe\\tproblem\\tmanually.\\n8\\n.\\t\\nIdentify\\tthe\\tpromising\\ttransformations\\tyou\\tmay\\twant\\tto\\tapply.\\n9\\n.\\t\\nIdentify\\textra\\tdata\\tthat\\twould\\tbe\\tuseful\\t(go\\tback\\tto\\t\\n“Get\\tthe\\tData”\\n).\\n10\\n.\\t\\nDocument\\twhat\\tyou\\thave\\tlearned.', 'Prepare\\tthe\\tData\\nNotes:\\nWork\\ton\\tcopies\\tof\\tthe\\tdata\\t(keep\\tthe\\toriginal\\tdataset\\tintact).\\nWrite\\tfunctions\\tfor\\tall\\tdata\\ttransformations\\tyou\\tapply,\\tfor\\tfive\\treasons:\\nSo\\tyou\\tcan\\teasily\\tprepare\\tthe\\tdata\\tthe\\tnext\\ttime\\tyou\\tget\\ta\\tfresh\\tdataset\\nSo\\tyou\\tcan\\tapply\\tthese\\ttransformations\\tin\\tfuture\\tprojects\\nTo\\tclean\\tand\\tprepare\\tthe\\ttest\\tset\\nTo\\tclean\\tand\\tprepare\\tnew\\tdata\\tinstances\\tonce\\tyour\\tsolution\\tis\\tlive\\nTo\\tmake\\tit\\teasy\\tto\\ttreat\\tyour\\tpreparation\\tchoices\\tas\\thyperparameters\\n1\\n.\\t\\nData\\tcleaning:\\nFix\\tor\\tremove\\toutliers\\t(optional).\\nFill\\tin\\tmissing\\tvalues\\t(e.g.,\\twith\\tzero,\\tmean,\\tmedian…)\\tor\\tdrop\\ttheir\\trows\\t(or\\tcolumns).\\n2\\n.\\t\\nFeature\\tselection\\n\\t(optional):\\nDrop\\tthe\\tattributes\\tthat\\tprovide\\tno\\tuseful\\tinformation\\tfor\\tthe\\ttask.\\n3\\n.\\t\\nFeature\\tengineering,\\twhere\\tappropriate:\\nDiscretize\\tcontinuous\\tfeatures.\\nDecompose\\tfeatures\\t(e.g.,\\tcategorical,\\tdate/time,\\tetc.).\\nAdd\\tpromising\\ttransformations\\tof\\tfeatures\\t(e.g.,\\tlog(x),\\tsqrt(x),\\tx^2,\\tetc.).\\nAggregate\\tfeatures\\tinto\\tpromising\\tnew\\tfeatures.\\n4\\n.', '4\\n.\\t\\nFeature\\tscaling:\\tstandardize\\tor\\tnormalize\\tfeatures.', 'Short-List\\tPromising\\tModels\\nNotes:\\nIf\\tthe\\tdata\\tis\\thuge,\\tyou\\tmay\\twant\\tto\\tsample\\tsmaller\\ttraining\\tsets\\tso\\tyou\\tcan\\ttrain\\tmany\\tdifferent\\nmodels\\tin\\ta\\treasonable\\ttime\\t(be\\taware\\tthat\\tthis\\tpenalizes\\tcomplex\\tmodels\\tsuch\\tas\\tlarge\\tneural\\tnets\\nor\\tRandom\\tForests).\\nOnce\\tagain,\\ttry\\tto\\tautomate\\tthese\\tsteps\\tas\\tmuch\\tas\\tpossible.\\n1\\n.\\t\\nTrain\\tmany\\tquick\\tand\\tdirty\\tmodels\\tfrom\\tdifferent\\tcategories\\t(e.g.,\\tlinear,\\tnaive\\tBayes,\\tSVM,\\nRandom\\tForests,\\tneural\\tnet,\\tetc.)\\tusing\\tstandard\\tparameters.\\n2\\n.\\t\\nMeasure\\tand\\tcompare\\ttheir\\tperformance.\\nFor\\teach\\tmodel,\\tuse\\t\\nN\\n-fold\\tcross-validation\\tand\\tcompute\\tthe\\tmean\\tand\\tstandard\\tdeviation\\tof\\nthe\\tperformance\\tmeasure\\ton\\tthe\\t\\nN\\n\\tfolds.\\n3\\n.\\t\\nAnalyze\\tthe\\tmost\\tsignificant\\tvariables\\tfor\\teach\\talgorithm.\\n4\\n.\\t\\nAnalyze\\tthe\\ttypes\\tof\\terrors\\tthe\\tmodels\\tmake.\\nWhat\\tdata\\twould\\ta\\thuman\\thave\\tused\\tto\\tavoid\\tthese\\terrors?\\n5\\n.\\t\\nHave\\ta\\tquick\\tround\\tof\\tfeature\\tselection\\tand\\tengineering.\\n6\\n.\\t\\nHave\\tone\\tor\\ttwo\\tmore\\tquick\\titerations\\tof\\tthe\\tfive\\tprevious\\tsteps.\\n7\\n.', '7\\n.\\t\\nShort-list\\tthe\\ttop\\tthree\\tto\\tfive\\tmost\\tpromising\\tmodels,\\tpreferring\\tmodels\\tthat\\tmake\\tdifferent\\ttypes\\tof\\nerrors.', 'Fine-Tune\\tthe\\tSystem\\nNotes:\\nYou\\twill\\twant\\tto\\tuse\\tas\\tmuch\\tdata\\tas\\tpossible\\tfor\\tthis\\tstep,\\tespecially\\tas\\tyou\\tmove\\ttoward\\tthe\\tend\\nof\\tfine-tuning.\\nAs\\talways\\tautomate\\twhat\\tyou\\tcan.\\n1\\n.\\t\\nFine-tune\\tthe\\thyperparameters\\tusing\\tcross-validation.\\nTreat\\tyour\\tdata\\ttransformation\\tchoices\\tas\\thyperparameters,\\tespecially\\twhen\\tyou\\tare\\tnot\\tsure\\nabout\\tthem\\t(e.g.,\\tshould\\tI\\treplace\\tmissing\\tvalues\\twith\\tzero\\tor\\twith\\tthe\\tmedian\\tvalue?\\tOr\\tjust\\ndrop\\tthe\\trows?).\\nUnless\\tthere\\tare\\tvery\\tfew\\thyperparameter\\tvalues\\tto\\texplore,\\tprefer\\trandom\\tsearch\\tover\\tgrid\\nsearch.\\tIf\\ttraining\\tis\\tvery\\tlong,\\tyou\\tmay\\tprefer\\ta\\tBayesian\\toptimization\\tapproach\\t(e.g.,\\tusing\\nGaussian\\tprocess\\tpriors,\\t\\nas\\tdescribed\\tby\\tJasper\\tSnoek,\\tHugo\\tLarochelle,\\tand\\tRyan\\tAdams\\n).\\n1\\n2\\n.\\t\\nTry\\tEnsemble\\tmethods.\\tCombining\\tyour\\tbest\\tmodels\\twill\\toften\\tperform\\tbetter\\tthan\\trunning\\tthem\\nindividually.\\n3\\n.\\t\\nOnce\\tyou\\tare\\tconfident\\tabout\\tyour\\tfinal\\tmodel,\\tmeasure\\tits\\tperformance\\ton\\tthe\\ttest\\tset\\tto\\testimate\\nthe\\tgeneralization\\terror.\\nWARNING', 'WARNING\\nDon’t\\ttweak\\tyour\\tmodel\\tafter\\tmeasuring\\tthe\\tgeneralization\\terror:\\tyou\\twould\\tjust\\tstart\\toverfitting\\tthe\\ttest\\tset.', 'Present\\tYour\\tSolution\\n1\\n.\\t\\nDocument\\twhat\\tyou\\thave\\tdone.\\n2\\n.\\t\\nCreate\\ta\\tnice\\tpresentation.\\nMake\\tsure\\tyou\\thighlight\\tthe\\tbig\\tpicture\\tfirst.\\n3\\n.\\t\\nExplain\\twhy\\tyour\\tsolution\\tachieves\\tthe\\tbusiness\\tobjective.\\n4\\n.\\t\\nDon’t\\tforget\\tto\\tpresent\\tinteresting\\tpoints\\tyou\\tnoticed\\talong\\tthe\\tway.\\nDescribe\\twhat\\tworked\\tand\\twhat\\tdid\\tnot.\\nList\\tyour\\tassumptions\\tand\\tyour\\tsystem’s\\tlimitations.\\n5\\n.\\t\\nEnsure\\tyour\\tkey\\tfindings\\tare\\tcommunicated\\tthrough\\tbeautiful\\tvisualizations\\tor\\teasy-to-remember\\nstatements\\t(e.g.,\\t“the\\tmedian\\tincome\\tis\\tthe\\tnumber-one\\tpredictor\\tof\\thousing\\tprices”).', 'Launch!\\n1\\n.\\t\\nGet\\tyour\\tsolution\\tready\\tfor\\tproduction\\t(plug\\tinto\\tproduction\\tdata\\tinputs,\\twrite\\tunit\\ttests,\\tetc.).\\n2\\n.\\t\\nWrite\\tmonitoring\\tcode\\tto\\tcheck\\tyour\\tsystem’s\\tlive\\tperformance\\tat\\tregular\\tintervals\\tand\\ttrigger\\talerts\\nwhen\\tit\\tdrops.\\nBeware\\tof\\tslow\\tdegradation\\ttoo:\\tmodels\\ttend\\tto\\t“rot”\\tas\\tdata\\tevolves.\\nMeasuring\\tperformance\\tmay\\trequire\\ta\\thuman\\tpipeline\\t(e.g.,\\tvia\\ta\\tcrowdsourcing\\tservice).\\nAlso\\tmonitor\\tyour\\tinputs’\\tquality\\t(e.g.,\\ta\\tmalfunctioning\\tsensor\\tsending\\trandom\\tvalues,\\tor\\nanother\\tteam’s\\toutput\\tbecoming\\tstale).\\tThis\\tis\\tparticularly\\timportant\\tfor\\tonline\\tlearning\\nsystems.\\n3\\n.\\t\\nRetrain\\tyour\\tmodels\\ton\\ta\\tregular\\tbasis\\ton\\tfresh\\tdata\\t\\n(automate\\tas\\tmuch\\tas\\tpossible).\\n“Practical\\tBayesian\\tOptimization\\tof\\tMachine\\tLearning\\tAlgorithms,”\\tJ.\\tSnoek,\\tH.\\tLarochelle,\\tR.\\tAdams\\t(2012).\\n1', 'Appendix\\tC.\\t\\nSVM\\tDual\\tProblem\\nTo\\tunderstand\\t\\nduality\\n,\\t\\nyou\\tfirst\\tneed\\tto\\tunderstand\\tthe\\t\\nLagrange\\tmultipliers\\n\\tmethod.\\tThe\\tgeneral\\tidea\\tis\\nto\\ttransform\\ta\\tconstrained\\toptimization\\t\\nobjective\\tinto\\tan\\tunconstrained\\tone,\\tby\\tmoving\\tthe\\tconstraints\\ninto\\tthe\\tobjective\\tfunction.\\tLet’s\\tlook\\tat\\ta\\tsimple\\texample.\\tSuppose\\tyou\\twant\\tto\\tfind\\tthe\\tvalues\\tof\\t\\nx\\n\\tand\\t\\ny\\nthat\\tminimize\\tthe\\tfunction\\t\\nf\\n(\\nx\\n,\\ny\\n)\\t=\\t\\nx\\n2\\n\\t+\\t2\\ny\\n,\\tsubject\\tto\\tan\\t\\nequality\\tconstraint\\n:\\t3\\nx\\n\\t+\\t2\\ny\\n\\t+\\t1\\t=\\t0.\\tUsing\\tthe\\nLagrange\\tmultipliers\\tmethod,\\twe\\tstart\\tby\\tdefining\\ta\\tnew\\tfunction\\tcalled\\tthe\\t\\nLagrangian\\n\\t(or\\t\\nLagrange\\nfunction\\n):\\t\\ng\\n(\\nx\\n,\\t\\ny\\n,\\t\\nα\\n)\\t=\\t\\nf\\n(\\nx\\n,\\t\\ny\\n)\\t–\\t\\nα\\n(3\\nx\\n\\t+\\t2\\ny\\n\\t+\\t1).\\tEach\\tconstraint\\t(in\\tthis\\tcase\\tjust\\tone)\\tis\\tsubtracted\\tfrom\\nthe\\toriginal\\tobjective,\\tmultiplied\\tby\\ta\\tnew\\tvariable\\tcalled\\ta\\t\\nLagrange\\tmultiplier.\\nJoseph-Louis\\tLagrange\\tshowed\\tthat\\tif\\t\\n\\tis\\ta\\tsolution\\tto\\tthe\\tconstrained\\toptimization\\tproblem,\\tthen\\nthere\\tmust\\texist\\tan\\t\\n\\tsuch\\tthat\\t\\n\\tis\\t\\na\\t\\nstationary\\tpoint\\n\\tof\\tthe\\tLagrangian\\t(a\\tstationary\\tpoint\\tis\\ta', 'point\\twhere\\tall\\tpartial\\tderivatives\\tare\\tequal\\tto\\tzero).\\tIn\\tother\\twords,\\twe\\tcan\\tcompute\\tthe\\tpartial\\nderivatives\\tof\\t\\ng\\n(\\nx\\n,\\t\\ny\\n,\\t\\nα\\n)\\twith\\tregards\\tto\\t\\nx\\n,\\t\\ny\\n,\\tand\\t\\nα\\n;\\twe\\tcan\\tfind\\tthe\\tpoints\\twhere\\tthese\\tderivatives\\tare\\tall\\nequal\\tto\\tzero;\\tand\\tthe\\tsolutions\\tto\\tthe\\tconstrained\\toptimization\\tproblem\\t(if\\tthey\\texist)\\tmust\\tbe\\tamong\\nthese\\tstationary\\tpoints.\\nIn\\tthis\\texample\\tthe\\tpartial\\tderivatives\\tare:\\t\\nWhen\\tall\\tthese\\tpartial\\tderivatives\\tare\\tequal\\tto\\t0,\\twe\\tfind\\tthat\\t\\n,\\tfrom\\twhich\\twe\\tcan\\teasily\\tfind\\tthat\\t\\n,\\t\\n,\\tand\\t\\n.\\tThis\\tis\\tthe\\tonly\\tstationary\\tpoint,\\tand\\tas\\tit\\trespects\\tthe\\tconstraint,\\tit\\tmust\\tbe\\tthe\\nsolution\\tto\\tthe\\tconstrained\\toptimization\\tproblem.\\nHowever,\\tthis\\tmethod\\tapplies\\tonly\\tto\\t\\nequality\\tconstraints.\\tFortunately,\\tunder\\tsome\\tregularity\\tconditions\\n(which\\tare\\trespected\\tby\\tthe\\tSVM\\tobjectives),\\tthis\\tmethod\\tcan\\tbe\\tgeneralized\\tto\\t\\ninequality\\tconstraints\\n\\t\\nas\\nwell\\t(e.g.,\\t3\\nx\\n\\t+\\t2\\ny\\n\\t+\\t1\\t≥\\t0).\\t\\nThe\\t\\ngeneralized\\tLagrangian\\n\\tfor\\tthe\\thard\\tmargin\\tproblem\\tis\\tgiven\\tby\\nEquation\\tC-1\\n,\\twhere\\tthe\\t\\nα\\n(\\ni\\n)', 'α\\n(\\ni\\n)\\n\\tvariables\\tare\\tcalled\\t\\nthe\\t\\nKarush–Kuhn–Tucker\\n\\t(KKT)\\tmultipliers,\\tand\\tthey\\nmust\\tbe\\tgreater\\tor\\tequal\\tto\\tzero.\\nEquation\\tC-1.\\t\\nGeneralized\\tLagrangian\\tfor\\tthe\\thard\\tmargin\\tproblem\\nJust\\tlike\\twith\\tthe\\tLagrange\\tmultipliers\\tmethod,\\tyou\\tcan\\tcompute\\tthe\\tpartial\\tderivatives\\tand\\tlocate\\tthe\\nstationary\\tpoints.\\tIf\\tthere\\tis\\ta\\tsolution,\\tit\\twill\\tnecessarily\\tbe\\tamong\\tthe\\tstationary\\tpoints\\t\\n\\tthat\\nrespect\\tthe\\t\\nKKT\\tconditions\\n:\\nRespect\\tthe\\tproblem’s\\tconstraints:\\t\\n,', 'Verify\\t\\n,\\nEither\\t\\n\\tor\\tthe\\ti\\nth\\n\\tconstraint\\tmust\\tbe\\tan\\t\\nactive\\tconstraint\\n,\\t\\nmeaning\\tit\\tmust\\thold\\tby\\tequality:\\t\\n.\\tThis\\tcondition\\tis\\tcalled\\tthe\\t\\ncomplementary\\tslackness\\n\\t\\ncondition.\\tIt\\timplies\\nthat\\teither\\t\\n\\tor\\tthe\\ti\\nth\\n\\tinstance\\tlies\\ton\\tthe\\tboundary\\t(it\\tis\\ta\\tsupport\\tvector).\\nNote\\tthat\\tthe\\tKKT\\tconditions\\tare\\tnecessary\\tconditions\\tfor\\ta\\tstationary\\tpoint\\tto\\tbe\\ta\\tsolution\\tof\\tthe\\nconstrained\\toptimization\\tproblem.\\tUnder\\tsome\\tconditions,\\tthey\\tare\\talso\\tsufficient\\tconditions.\\tLuckily,\\tthe\\nSVM\\toptimization\\tproblem\\thappens\\tto\\tmeet\\tthese\\tconditions,\\tso\\tany\\tstationary\\tpoint\\tthat\\tmeets\\tthe\\tKKT\\nconditions\\tis\\tguaranteed\\tto\\tbe\\ta\\tsolution\\tto\\tthe\\tconstrained\\toptimization\\tproblem.\\nWe\\tcan\\tcompute\\tthe\\tpartial\\tderivatives\\tof\\tthe\\tgeneralized\\tLagrangian\\twith\\tregards\\tto\\t\\nw\\n\\tand\\t\\nb\\n\\twith\\nEquation\\tC-2\\n.\\nEquation\\tC-2.\\t\\nPartial\\tderivatives\\tof\\tthe\\tgeneralized\\tLagrangian\\nWhen\\tthese\\tpartial\\tderivatives\\tare\\tequal\\tto\\t0,\\twe\\thave\\t\\nEquation\\tC-3\\n.\\nEquation\\tC-3.\\t\\nProperties\\tof\\tthe\\tstationary\\tpoints\\nIf', 'If\\t\\nwe\\tplug\\tthese\\tresults\\tinto\\tthe\\tdefinition\\tof\\tthe\\t\\ngeneralized\\tLagrangian,\\tsome\\tterms\\tdisappear\\tand\\twe', 'find\\t\\nEquation\\tC-4\\n.\\nEquation\\tC-4.\\t\\nDual\\tform\\tof\\tthe\\tSVM\\tproblem\\nThe\\tgoal\\tis\\tnow\\tto\\tfind\\tthe\\tvector\\t\\n\\tthat\\tminimizes\\tthis\\tfunction,\\twith\\t\\n\\tfor\\tall\\tinstances.\\tThis\\nconstrained\\toptimization\\tproblem\\tis\\tthe\\tdual\\tproblem\\twe\\twere\\tlooking\\tfor.\\nOnce\\tyou\\tfind\\tthe\\toptimal\\t\\n,\\tyou\\tcan\\tcompute\\t\\n\\tusing\\tthe\\tfirst\\tline\\tof\\t\\nEquation\\tC-3\\n.\\tTo\\tcompute\\t\\n,\\tyou\\ncan\\tuse\\tthe\\tfact\\tthat\\ta\\tsupport\\tvector\\tverifies\\t\\nt\\n(\\ni\\n)\\n(\\nw\\nT\\n\\t·\\t\\nx\\n(\\ni\\n)\\n\\t+\\t\\nb\\n)\\t=\\t1,\\tso\\tif\\tthe\\tk\\nth\\n\\tinstance\\tis\\ta\\tsupport\\tvector\\n(i.e.,\\t\\nα\\nk\\n\\t>\\t0),\\tyou\\tcan\\tuse\\tit\\tto\\tcompute\\t\\n.\\tHowever,\\tit\\tis\\toften\\tprefered\\tto\\tcompute\\nthe\\taverage\\tover\\tall\\tsupport\\tvectors\\tto\\tget\\ta\\tmore\\tstable\\tand\\tprecise\\tvalue,\\t\\nas\\tin\\t\\nEquation\\tC-5\\n.\\nEquation\\tC-5.\\t\\nBias\\tterm\\testimation\\tusing\\tthe\\tdual\\tform', 'Appendix\\tD.\\t\\nAutodiff\\nThis\\t\\nappendix\\texplains\\thow\\tTensorFlow’s\\tautodiff\\tfeature\\tworks,\\tand\\thow\\tit\\tcompares\\tto\\tother\\tsolutions.\\nSuppose\\tyou\\tdefine\\ta\\tfunction\\t\\nf\\n(\\nx\\n,\\ny\\n)\\t=\\t\\nx\\n2\\ny\\n\\t+\\t\\ny\\n\\t+\\t2,\\tand\\tyou\\tneed\\tits\\tpartial\\tderivatives\\t\\n\\tand\\t\\n,\\ntypically\\tto\\tperform\\tGradient\\tDescent\\t(or\\tsome\\tother\\toptimization\\talgorithm).\\tYour\\tmain\\toptions\\tare\\nmanual\\tdifferentiation,\\tsymbolic\\tdifferentiation,\\tnumerical\\tdifferentiation,\\tforward-mode\\tautodiff,\\tand\\nfinally\\treverse-mode\\tautodiff.\\tTensorFlow\\timplements\\tthis\\tlast\\toption.\\tLet’s\\tgo\\tthrough\\teach\\tof\\tthese\\noptions.', 'Manual\\tDifferentiation\\nThe\\t\\nfirst\\tapproach\\tis\\tto\\tpick\\tup\\ta\\tpencil\\tand\\ta\\tpiece\\tof\\tpaper\\tand\\tuse\\tyour\\tcalculus\\tknowledge\\tto\\tderive\\nthe\\tpartial\\tderivatives\\tmanually.\\tFor\\tthe\\tfunction\\t\\nf\\n(\\nx\\n,\\ny\\n)\\tjust\\tdefined,\\tit\\tis\\tnot\\ttoo\\thard;\\tyou\\tjust\\tneed\\tto\\tuse\\nfive\\trules:\\nThe\\tderivative\\tof\\ta\\tconstant\\tis\\t0.\\nThe\\tderivative\\tof\\t\\nλx\\n\\tis\\t\\nλ\\n\\t(where\\t\\nλ\\n\\tis\\ta\\tconstant).\\nThe\\tderivative\\tof\\t\\nx\\nλ\\n\\tis\\t\\nλx\\nλ\\n\\t–\\t1\\n,\\tso\\tthe\\tderivative\\tof\\t\\nx\\n2\\n\\tis\\t2\\nx\\n.\\nThe\\tderivative\\tof\\ta\\tsum\\tof\\tfunctions\\tis\\tthe\\tsum\\tof\\tthese\\tfunctions’\\tderivatives.\\nThe\\tderivative\\tof\\t\\nλ\\n\\ttimes\\ta\\tfunction\\tis\\t\\nλ\\n\\ttimes\\tits\\tderivative.\\nFrom\\tthese\\trules,\\tyou\\tcan\\tderive\\t\\nEquation\\tD-1\\n:\\nEquation\\tD-1.\\t\\nPartial\\tderivatives\\tof\\t\\nf\\n(\\nx\\n,\\ny\\n)\\nThis\\tapproach\\tcan\\tbecome\\tvery\\ttedious\\tfor\\tmore\\tcomplex\\tfunctions,\\tand\\tyou\\trun\\tthe\\trisk\\tof\\tmaking\\nmistakes.\\tThe\\tgood\\tnews\\tis\\tthat\\tderiving\\tthe\\tmathematical\\tequations\\tfor\\tthe\\tpartial\\tderivatives\\tlike\\twe\\njust\\tdid\\tcan\\tbe\\tautomated,\\tthrough\\ta\\tprocess\\tcalled\\t\\nsymbolic\\tdifferentiation\\n.', 'Symbolic\\tDifferentiation\\nFigure\\tD-1\\n\\tshows\\t\\nhow\\tsymbolic\\tdifferentiation\\tworks\\ton\\tan\\teven\\tsimpler\\tfunction,\\t\\ng\\n(\\nx\\n,\\ny\\n)\\t=\\t5\\t+\\t\\nxy\\n.\\tThe\\ngraph\\tfor\\tthat\\tfunction\\tis\\trepresented\\ton\\tthe\\tleft.\\tAfter\\tsymbolic\\tdifferentiation,\\twe\\tget\\tthe\\tgraph\\ton\\tthe\\nright,\\twhich\\trepresents\\tthe\\tpartial\\tderivative\\t\\n\\t(we\\tcould\\tsimilarly\\tobtain\\nthe\\tpartial\\tderivative\\twith\\tregards\\tto\\t\\ny\\n).\\nFigure\\tD-1.\\t\\nSymbolic\\tdifferentiation\\nThe\\talgorithm\\tstarts\\tby\\tgetting\\tthe\\tpartial\\tderivative\\tof\\tthe\\tleaf\\tnodes.\\tThe\\tconstant\\tnode\\t(5)\\treturns\\tthe\\nconstant\\t0,\\tsince\\tthe\\tderivative\\tof\\ta\\tconstant\\tis\\talways\\t0.\\tThe\\tvariable\\t\\nx\\n\\treturns\\tthe\\tconstant\\t1\\tsince\\t\\n,\\tand\\tthe\\tvariable\\t\\ny\\n\\treturns\\tthe\\tconstant\\t0\\tsince\\t\\n\\t(if\\twe\\twere\\tlooking\\tfor\\tthe\\tpartial\\nderivative\\twith\\tregards\\tto\\t\\ny\\n,\\tit\\twould\\tbe\\tthe\\treverse).\\nNow\\twe\\thave\\tall\\twe\\tneed\\tto\\tmove\\tup\\tthe\\tgraph\\tto\\tthe\\tmultiplication\\tnode\\tin\\tfunction\\t\\ng\\n.\\tCalculus\\ttells\\tus\\nthat\\tthe\\tderivative\\tof\\tthe\\tproduct\\tof\\ttwo\\tfunctions\\t\\nu\\n\\tand\\t\\nv\\n\\tis\\t\\n.\\tWe\\tcan', '.\\tWe\\tcan\\ntherefore\\tconstruct\\ta\\tlarge\\tpart\\tof\\tthe\\tgraph\\ton\\tthe\\tright,\\trepresenting\\t0\\t×\\t\\nx\\n\\t+\\t\\ny\\n\\t×\\t1.\\nFinally,\\twe\\tcan\\tgo\\tup\\tto\\tthe\\taddition\\tnode\\tin\\tfunction\\t\\ng\\n.\\tAs\\tmentioned,\\tthe\\tderivative\\tof\\ta\\tsum\\tof\\nfunctions\\tis\\tthe\\tsum\\tof\\tthese\\tfunctions’\\tderivatives.\\tSo\\twe\\tjust\\tneed\\tto\\tcreate\\tan\\taddition\\tnode\\tand\\nconnect\\tit\\tto\\tthe\\tparts\\tof\\tthe\\tgraph\\twe\\thave\\talready\\tcomputed.\\tWe\\tget\\tthe\\tcorrect\\tpartial\\tderivative:\\t\\n.\\nHowever,\\tit\\tcan\\tbe\\tsimplified\\t(a\\tlot).\\tA\\tfew\\ttrivial\\t\\npruning\\tsteps\\tcan\\tbe\\tapplied\\tto\\tthis\\tgraph\\tto\\tget\\trid\\tof\\nall\\tunnecessary\\toperations,\\tand\\twe\\tget\\ta\\tmuch\\tsmaller\\tgraph\\twith\\tjust\\tone\\tnode:\\t\\n.\\nIn\\tthis\\tcase,\\tsimplification\\tis\\tfairly\\teasy,\\tbut\\tfor\\ta\\tmore\\tcomplex\\tfunction,\\tsymbolic\\tdifferentiation\\tcan\\nproduce\\ta\\thuge\\tgraph\\tthat\\tmay\\tbe\\ttough\\tto\\tsimplify\\tand\\tlead\\tto\\tsuboptimal\\tperformance.\\tMost\\timportantly,\\nsymbolic\\tdifferentiation\\tcannot\\tdeal\\twith\\tfunctions\\tdefined\\twith\\tarbitrary\\tcode\\t—\\tfor\\texample,\\tthe', 'following\\tfunction\\t\\ndiscussed\\tin\\t\\nChapter\\t9\\n:\\ndef\\n\\t\\nmy_func\\n(\\na\\n,\\n\\t\\nb\\n):\\n\\t\\t\\t\\t\\nz\\n\\t\\n=\\n\\t\\n0\\n\\t\\t\\t\\t\\nfor\\n\\t\\ni\\n\\t\\nin\\n\\t\\nrange\\n(\\n100\\n):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\nz\\n\\t\\n=\\n\\t\\na\\n\\t\\n*\\n\\t\\nnp\\n.\\ncos\\n(\\nz\\n\\t\\n+\\n\\t\\ni\\n)\\n\\t\\n+\\n\\t\\nz\\n\\t\\n*\\n\\t\\nnp\\n.\\nsin\\n(\\nb\\n\\t\\n-\\n\\t\\ni\\n)\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nz', 'Numerical\\tDifferentiation\\nThe\\t\\nsimplest\\tsolution\\tis\\tto\\tcompute\\tan\\tapproximation\\tof\\tthe\\tderivatives,\\tnumerically.\\tRecall\\tthat\\tthe\\nderivative\\t\\nh\\n′(\\nx\\n0\\n)\\tof\\ta\\tfunction\\t\\nh\\n(\\nx\\n)\\tat\\ta\\tpoint\\t\\nx\\n0\\n\\tis\\tthe\\tslope\\tof\\tthe\\tfunction\\tat\\tthat\\tpoint,\\tor\\tmore\\tprecisely\\nEquation\\tD-2\\n.\\nEquation\\tD-2.\\t\\nDerivative\\tof\\ta\\tfunction\\t\\nh\\n(\\nx\\n)\\tat\\tpoint\\t\\nx\\n0\\nSo\\tif\\twe\\twant\\tto\\tcalculate\\tthe\\tpartial\\tderivative\\tof\\t\\nf\\n(\\nx\\n,\\ny\\n)\\twith\\tregards\\tto\\t\\nx\\n,\\tat\\t\\nx\\n\\t=\\t3\\tand\\t\\ny\\n\\t=\\t4,\\twe\\tcan\\nsimply\\tcompute\\t\\nf\\n(3\\t+\\t\\nϵ\\n,\\t4)\\t–\\t\\nf\\n(3,\\t4)\\tand\\tdivide\\tthe\\tresult\\tby\\t\\nϵ\\n,\\tusing\\ta\\tvery\\tsmall\\tvalue\\tfor\\t\\nϵ\\n.\\tThat’s\\nexactly\\twhat\\tthe\\tfollowing\\tcode\\tdoes:\\ndef\\n\\t\\nf\\n(\\nx\\n,\\n\\t\\ny\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\nx\\n**\\n2\\n*\\ny\\n\\t\\n+\\n\\t\\ny\\n\\t\\n+\\n\\t\\n2\\ndef\\n\\t\\nderivative\\n(\\nf\\n,\\n\\t\\nx\\n,\\n\\t\\ny\\n,\\n\\t\\nx_eps\\n,\\n\\t\\ny_eps\\n):\\n\\t\\t\\t\\t\\nreturn\\n\\t\\n(\\nf\\n(\\nx\\n\\t\\n+\\n\\t\\nx_eps\\n,\\n\\t\\ny\\n\\t\\n+\\n\\t\\ny_eps\\n)\\n\\t\\n-\\n\\t\\nf\\n(\\nx\\n,\\n\\t\\ny\\n))\\n\\t\\n/\\n\\t\\n(\\nx_eps\\n\\t\\n+\\n\\t\\ny_eps\\n)\\ndf_dx\\n\\t\\n=\\n\\t\\nderivative\\n(\\nf\\n,\\n\\t\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n0.00001\\n,\\n\\t\\n0\\n)\\ndf_dy\\n\\t\\n=\\n\\t\\nderivative\\n(\\nf\\n,\\n\\t\\n3\\n,\\n\\t\\n4\\n,\\n\\t\\n0\\n,\\n\\t\\n0.00001\\n)', '0.00001\\n)\\nUnfortunately,\\tthe\\tresult\\tis\\timprecise\\t(and\\tit\\tgets\\tworse\\tfor\\tmore\\tcomplex\\tfunctions).\\tThe\\tcorrect\\tresults\\nare\\trespectively\\t24\\tand\\t10,\\tbut\\tinstead\\twe\\tget:\\n>>>\\t\\nprint\\n(\\ndf_dx\\n)\\n24.000039999805264\\n>>>\\t\\nprint\\n(\\ndf_dy\\n)\\n10.000000000331966\\nNotice\\tthat\\tto\\tcompute\\tboth\\tpartial\\tderivatives,\\twe\\thave\\tto\\tcall\\t\\nf()\\n\\tat\\tleast\\tthree\\ttimes\\t(we\\tcalled\\tit\\tfour\\ntimes\\tin\\tthe\\tpreceding\\tcode,\\tbut\\tit\\tcould\\tbe\\toptimized).\\tIf\\tthere\\twere\\t1,000\\tparameters,\\twe\\twould\\tneed\\tto\\ncall\\t\\nf()\\n\\tat\\tleast\\t1,001\\ttimes.\\tWhen\\tyou\\tare\\tdealing\\twith\\tlarge\\tneural\\tnetworks,\\tthis\\tmakes\\tnumerical\\ndifferentiation\\tway\\ttoo\\tinefficient.\\nHowever,\\tnumerical\\tdifferentiation\\tis\\tso\\tsimple\\tto\\timplement\\tthat\\tit\\tis\\ta\\tgreat\\ttool\\tto\\tcheck\\tthat\\tthe\\tother\\nmethods\\tare\\timplemented\\tcorrectly.\\tFor\\texample,\\tif\\tit\\tdisagrees\\twith\\tyour\\tmanually\\tderived\\tfunction,\\tthen\\nyour\\tfunction\\tprobably\\tcontains\\ta\\tmistake.', 'Forward-Mode\\tAutodiff\\nForward-mode\\tautodiff\\n\\tis\\t\\nneither\\tnumerical\\tdifferentiation\\tnor\\tsymbolic\\tdifferentiation,\\tbut\\tin\\tsome\\nways\\tit\\tis\\ttheir\\tlove\\tchild.\\tIt\\trelies\\ton\\t\\ndual\\tnumbers\\n,\\t\\nwhich\\tare\\t(weird\\tbut\\tfascinating)\\tnumbers\\tof\\tthe\\nform\\t\\na\\n\\t+\\t\\nbϵ\\n\\twhere\\t\\na\\n\\tand\\t\\nb\\n\\tare\\treal\\tnumbers\\tand\\t\\nϵ\\n\\tis\\tan\\tinfinitesimal\\tnumber\\tsuch\\tthat\\t\\nϵ\\n2\\n\\t=\\t0\\t(but\\t\\nϵ\\n\\t≠\\t0).\\nYou\\tcan\\tthink\\tof\\tthe\\tdual\\tnumber\\t42\\t+\\t24\\nϵ\\n\\tas\\tsomething\\takin\\tto\\t42.0000000024\\twith\\tan\\tinfinite\\tnumber\\nof\\t0s\\t(but\\tof\\tcourse\\tthis\\tis\\tsimplified\\tjust\\tto\\tgive\\tyou\\tsome\\tidea\\tof\\twhat\\tdual\\tnumbers\\tare).\\tA\\tdual\\nnumber\\tis\\trepresented\\tin\\tmemory\\tas\\ta\\tpair\\tof\\tfloats.\\tFor\\texample,\\t42\\t+\\t24\\nϵ\\n\\tis\\trepresented\\tby\\tthe\\tpair\\n(42.0,\\t24.0).\\nDual\\tnumbers\\tcan\\tbe\\tadded,\\tmultiplied,\\tand\\tso\\ton,\\tas\\tshown\\tin\\t\\nEquation\\tD-3\\n.\\nEquation\\tD-3.\\t\\nA\\tfew\\toperations\\twith\\tdual\\tnumbers\\nMost\\timportantly,\\tit\\tcan\\tbe\\tshown\\tthat\\t\\nh\\n(\\na\\n\\t+\\t\\nbϵ\\n)\\t=\\t\\nh\\n(\\na\\n)\\t+\\t\\nb\\n\\t×\\t\\nh\\n′(\\na\\n)\\nϵ\\n,\\tso\\tcomputing\\t\\nh\\n(\\na\\n\\t+\\t\\nϵ\\n)\\tgives\\tyou\\tboth\\nh\\n(\\na\\n)\\tand\\tthe\\tderivative\\t\\nh\\n′(\\na\\n)\\tin\\tjust\\tone\\tshot.', 'Figure\\tD-2\\n\\tshows\\thow\\tforward-mode\\tautodiff\\tcomputes\\tthe\\npartial\\tderivative\\tof\\t\\nf\\n(\\nx\\n,\\ny\\n)\\twith\\tregards\\tto\\t\\nx\\n\\tat\\t\\nx\\n\\t=\\t3\\tand\\t\\ny\\n\\t=\\t4.\\tAll\\twe\\tneed\\tto\\tdo\\tis\\tcompute\\t\\nf\\n(3\\t+\\t\\nϵ\\n,\\t4);\\nthis\\twill\\toutput\\ta\\tdual\\tnumber\\twhose\\tfirst\\tcomponent\\tis\\tequal\\tto\\t\\nf\\n(3,\\t4)\\tand\\twhose\\tsecond\\tcomponent\\tis\\nequal\\tto\\t\\n.', 'Figure\\tD-2.\\t\\nForward-mode\\tautodiff\\nTo\\tcompute\\t\\n\\twe\\twould\\thave\\tto\\tgo\\tthrough\\tthe\\tgraph\\tagain,\\tbut\\tthis\\ttime\\twith\\t\\nx\\n\\t=\\t3\\tand\\t\\ny\\n\\t=\\t4\\t+\\t\\nϵ\\n.\\nSo\\tforward-mode\\tautodiff\\tis\\tmuch\\tmore\\taccurate\\tthan\\tnumerical\\tdifferentiation,\\tbut\\tit\\tsuffers\\tfrom\\tthe\\nsame\\tmajor\\tflaw:\\tif\\tthere\\twere\\t1,000\\tparameters,\\tit\\twould\\trequire\\t1,000\\tpasses\\tthrough\\tthe\\tgraph\\tto\\ncompute\\tall\\tthe\\tpartial\\tderivatives.\\tThis\\tis\\twhere\\treverse-mode\\tautodiff\\tshines:\\tit\\tcan\\tcompute\\tall\\tof\\nthem\\tin\\tjust\\ttwo\\tpasses\\t\\nthrough\\tthe\\tgraph.', 'Reverse-Mode\\tAutodiff\\nReverse-mode\\tautodiff\\t\\nis\\tthe\\tsolution\\timplemented\\tby\\tTensorFlow.\\tIt\\tfirst\\tgoes\\tthrough\\tthe\\tgraph\\tin\\tthe\\nforward\\tdirection\\t(i.e.,\\tfrom\\tthe\\tinputs\\tto\\tthe\\toutput)\\tto\\tcompute\\tthe\\tvalue\\tof\\teach\\tnode.\\tThen\\tit\\tdoes\\ta\\nsecond\\tpass,\\tthis\\ttime\\tin\\tthe\\treverse\\tdirection\\t(i.e.,\\tfrom\\tthe\\toutput\\tto\\tthe\\tinputs)\\tto\\tcompute\\tall\\tthe\\tpartial\\nderivatives.\\t\\nFigure\\tD-3\\n\\trepresents\\tthe\\tsecond\\tpass.\\tDuring\\tthe\\tfirst\\tpass,\\tall\\tthe\\tnode\\tvalues\\twere\\ncomputed,\\tstarting\\tfrom\\t\\nx\\n\\t=\\t3\\tand\\t\\ny\\n\\t=\\t4.\\tYou\\tcan\\tsee\\tthose\\tvalues\\tat\\tthe\\tbottom\\tright\\tof\\teach\\tnode\\t(e.g.,\\t\\nx\\n×\\t\\nx\\n\\t=\\t9).\\tThe\\tnodes\\tare\\tlabeled\\t\\nn\\n1\\n\\tto\\t\\nn\\n7\\n\\tfor\\tclarity.\\tThe\\toutput\\tnode\\tis\\t\\nn\\n7\\n:\\t\\nf\\n(3,4)\\t=\\t\\nn\\n7\\n\\t=\\t42.\\nFigure\\tD-3.\\t\\nReverse-mode\\tautodiff\\nThe\\tidea\\tis\\tto\\tgradually\\tgo\\tdown\\tthe\\tgraph,\\tcomputing\\tthe\\tpartial\\tderivative\\tof\\t\\nf\\n(\\nx\\n,\\ny\\n)\\twith\\tregards\\tto\\teach\\nconsecutive\\tnode,\\tuntil\\twe\\treach\\tthe\\tvariable\\tnodes.\\tFor\\tthis,\\treverse-mode\\tautodiff\\trelies\\theavily\\ton\\tthe\\nchain\\trule\\n,\\tshown\\tin\\t\\nEquation\\tD-4\\n.\\nEquation\\tD-4.\\t\\nChain\\trule\\nSince\\t\\nn', 'Since\\t\\nn\\n7\\n\\tis\\tthe\\toutput\\tnode,\\t\\nf\\n\\t=\\t\\nn\\n7\\n\\tso\\ttrivially\\t\\n.\\nLet’s\\tcontinue\\tdown\\tthe\\tgraph\\tto\\t\\nn\\n5\\n:\\thow\\tmuch\\tdoes\\t\\nf\\n\\tvary\\twhen\\t\\nn\\n5\\n\\tvaries?\\tThe\\tanswer\\tis\\t\\n.', 'We\\talready\\tknow\\tthat\\t\\n,\\tso\\tall\\twe\\tneed\\tis\\t\\n.\\tSince\\t\\nn\\n7\\n\\tsimply\\tperforms\\tthe\\tsum\\t\\nn\\n5\\n\\t+\\t\\nn\\n6\\n,\\twe\\tfind\\nthat\\t\\n,\\tso\\t\\n.\\nNow\\twe\\tcan\\tproceed\\tto\\tnode\\t\\nn\\n4\\n:\\thow\\tmuch\\tdoes\\t\\nf\\n\\tvary\\twhen\\t\\nn\\n4\\n\\tvaries?\\tThe\\tanswer\\tis\\t\\n.\\nSince\\t\\nn\\n5\\n\\t=\\t\\nn\\n4\\n\\t×\\t\\nn\\n2\\n,\\twe\\tfind\\tthat\\t\\n,\\tso\\t\\n.\\nThe\\tprocess\\tcontinues\\tuntil\\twe\\treach\\tthe\\tbottom\\tof\\tthe\\tgraph.\\tAt\\tthat\\tpoint\\twe\\twill\\thave\\tcalculated\\tall\\tthe\\npartial\\tderivatives\\tof\\t\\nf\\n(\\nx\\n,\\ny\\n)\\tat\\tthe\\tpoint\\t\\nx\\n\\t=\\t3\\tand\\t\\ny\\n\\t=\\t4.\\tIn\\tthis\\texample,\\twe\\tfind\\t\\n\\tand\\t\\n.\\nSounds\\tabout\\tright!\\nReverse-mode\\tautodiff\\tis\\ta\\tvery\\tpowerful\\tand\\taccurate\\ttechnique,\\tespecially\\twhen\\tthere\\tare\\tmany\\tinputs\\nand\\tfew\\toutputs,\\tsince\\tit\\trequires\\tonly\\tone\\tforward\\tpass\\tplus\\tone\\treverse\\tpass\\tper\\toutput\\tto\\tcompute\\tall\\nthe\\tpartial\\tderivatives\\tfor\\tall\\toutputs\\twith\\tregards\\tto\\tall\\tthe\\tinputs.\\tMost\\timportantly,\\tit\\tcan\\tdeal\\twith\\nfunctions\\tdefined\\tby\\tarbitrary\\tcode.\\tIt\\tcan\\talso\\thandle\\tfunctions\\tthat\\tare\\tnot\\tentirely\\tdifferentiable,\\tas\\tlong\\nas\\tyou\\task\\tit\\tto\\tcompute\\tthe\\tpartial\\tderivatives\\tat\\tpoints\\tthat', 'are\\tdifferentiable.\\nTIP\\nIf\\tyou\\timplement\\ta\\tnew\\ttype\\tof\\toperation\\tin\\tTensorFlow\\tand\\tyou\\twant\\tto\\tmake\\tit\\tcompatible\\twith\\tautodiff,\\tthen\\tyou\\tneed\\tto\\nprovide\\ta\\tfunction\\tthat\\tbuilds\\ta\\tsubgraph\\tto\\tcompute\\tits\\tpartial\\tderivatives\\twith\\tregards\\tto\\tits\\tinputs.\\tFor\\texample,\\tsuppose\\tyou\\nimplement\\ta\\tfunction\\tthat\\tcomputes\\tthe\\tsquare\\tof\\tits\\tinput\\t\\nf\\n(\\nx\\n)\\t=\\t\\nx\\n2\\n.\\tIn\\tthat\\tcase\\tyou\\twould\\tneed\\tto\\tprovide\\tthe\\tcorresponding\\nderivative\\tfunction\\t\\nf\\n′(\\nx\\n)\\t=\\t2\\nx\\n.\\tNote\\tthat\\tthis\\tfunction\\tdoes\\tnot\\tcompute\\ta\\tnumerical\\tresult,\\tbut\\tinstead\\tbuilds\\ta\\tsubgraph\\tthat\\twill\\n(later)\\tcompute\\tthe\\tresult.\\tThis\\tis\\tvery\\tuseful\\tbecause\\tit\\tmeans\\tthat\\tyou\\tcan\\tcompute\\tgradients\\tof\\tgradients\\t(to\\tcompute\\tsecond-\\norder\\tderivatives,\\tor\\teven\\thigher-order\\t\\nderivatives).', 'Appendix\\tE.\\t\\nOther\\tPopular\\tANN\\tArchitectures\\nIn\\tthis\\tappendix\\twe\\twill\\tgive\\ta\\tquick\\toverview\\tof\\ta\\tfew\\thistorically\\timportant\\tneural\\tnetwork\\narchitectures\\tthat\\tare\\tmuch\\tless\\tused\\ttoday\\tthan\\tdeep\\tMulti-Layer\\tPerceptrons\\t(\\nChapter\\t10\\n),\\nconvolutional\\tneural\\tnetworks\\t(\\nChapter\\t13\\n),\\trecurrent\\tneural\\tnetworks\\t(\\nChapter\\t14\\n),\\tor\\tautoencoders\\n(\\nChapter\\t15\\n).\\tThey\\tare\\toften\\tmentioned\\tin\\tthe\\tliterature,\\tand\\tsome\\tare\\tstill\\tused\\tin\\tmany\\tapplications,\\tso\\nit\\tis\\tworth\\tknowing\\tabout\\tthem.\\tMoreover,\\twe\\twill\\tdiscuss\\t\\ndeep\\tbelief\\tnets\\n\\t(DBNs),\\twhich\\twere\\tthe\\nstate\\tof\\tthe\\tart\\tin\\tDeep\\tLearning\\tuntil\\tthe\\tearly\\t2010s.\\tThey\\tare\\tstill\\tthe\\tsubject\\tof\\tvery\\tactive\\tresearch,\\tso\\nthey\\tmay\\twell\\tcome\\tback\\twith\\ta\\tvengeance\\tin\\tthe\\tnear\\tfuture.', 'Hopfield\\tNetworks\\nHopfield\\tnetworks\\n\\twere\\t\\nfirst\\tintroduced\\tby\\tW.\\tA.\\tLittle\\tin\\t1974,\\tthen\\tpopularized\\tby\\tJ.\\tHopfield\\tin\\t1982.\\nThey\\t\\nare\\t\\nassociative\\tmemory\\n\\tnetworks:\\tyou\\tfirst\\tteach\\tthem\\tsome\\tpatterns,\\tand\\tthen\\twhen\\tthey\\tsee\\ta\\tnew\\npattern\\tthey\\t(hopefully)\\toutput\\tthe\\tclosest\\tlearned\\tpattern.\\tThis\\thas\\tmade\\tthem\\tuseful\\tin\\tparticular\\tfor\\ncharacter\\trecognition\\tbefore\\tthey\\twere\\toutperformed\\tby\\tother\\tapproaches.\\tYou\\tfirst\\ttrain\\tthe\\tnetwork\\tby\\nshowing\\tit\\texamples\\tof\\tcharacter\\timages\\t(each\\tbinary\\tpixel\\tmaps\\tto\\tone\\tneuron),\\tand\\tthen\\twhen\\tyou\\tshow\\nit\\ta\\tnew\\tcharacter\\timage,\\tafter\\ta\\tfew\\titerations\\tit\\toutputs\\tthe\\tclosest\\tlearned\\tcharacter.\\nThey\\tare\\tfully\\tconnected\\tgraphs\\t(see\\t\\nFigure\\tE-1\\n);\\tthat\\tis,\\tevery\\tneuron\\tis\\tconnected\\tto\\tevery\\tother\\tneuron.\\nNote\\tthat\\ton\\tthe\\tdiagram\\tthe\\timages\\tare\\t6\\t×\\t6\\tpixels,\\tso\\tthe\\tneural\\tnetwork\\ton\\tthe\\tleft\\tshould\\tcontain\\t36\\nneurons\\t(and\\t648\\tconnections),\\tbut\\tfor\\tvisual\\tclarity\\ta\\tmuch\\tsmaller\\tnetwork\\tis\\trepresented.\\nFigure\\tE-1.\\t\\nHopfield\\tnetwork', 'The\\ttraining\\talgorithm\\tworks\\tby\\tusing\\t\\nHebb’s\\trule:\\tfor\\teach\\ttraining\\timage,\\tthe\\tweight\\tbetween\\ttwo\\nneurons\\tis\\tincreased\\tif\\tthe\\tcorresponding\\tpixels\\tare\\tboth\\ton\\tor\\tboth\\toff,\\tbut\\tdecreased\\tif\\tone\\tpixel\\tis\\ton\\nand\\tthe\\tother\\tis\\toff.\\nTo\\tshow\\ta\\tnew\\timage\\tto\\tthe\\tnetwork,\\tyou\\tjust\\tactivate\\tthe\\tneurons\\tthat\\tcorrespond\\tto\\tactive\\tpixels.\\tThe\\nnetwork\\tthen\\tcomputes\\tthe\\toutput\\tof\\tevery\\tneuron,\\tand\\tthis\\tgives\\tyou\\ta\\tnew\\timage.\\tYou\\tcan\\tthen\\ttake\\tthis\\nnew\\timage\\tand\\trepeat\\tthe\\twhole\\tprocess.\\tAfter\\ta\\twhile,\\tthe\\tnetwork\\treaches\\ta\\tstable\\tstate.\\tGenerally,\\tthis\\ncorresponds\\tto\\tthe\\ttraining\\timage\\tthat\\tmost\\tresembles\\tthe\\tinput\\timage.\\nA\\tso-called\\t\\nenergy\\tfunction\\n\\t\\nis\\tassociated\\twith\\tHopfield\\tnets.\\tAt\\teach\\titeration,\\tthe\\tenergy\\tdecreases,\\tso\\nthe\\tnetwork\\tis\\tguaranteed\\tto\\teventually\\tstabilize\\tto\\ta\\tlow-energy\\tstate.\\tThe\\ttraining\\talgorithm\\ttweaks\\tthe\\nweights\\tin\\ta\\tway\\tthat\\tdecreases\\tthe\\tenergy\\tlevel\\tof\\tthe\\ttraining\\tpatterns,\\tso\\tthe\\tnetwork\\tis\\tlikely\\tto', 'stabilize\\tin\\tone\\tof\\tthese\\tlow-energy\\tconfigurations.\\tUnfortunately,\\tsome\\tpatterns\\tthat\\twere\\tnot\\tin\\tthe\\ntraining\\tset\\talso\\tend\\tup\\twith\\tlow\\tenergy,\\tso\\tthe\\tnetwork\\tsometimes\\tstabilizes\\tin\\ta\\tconfiguration\\tthat\\twas', 'not\\tlearned.\\tThese\\tare\\tc\\nalled\\t\\nspurious\\tpatterns\\n.\\nAnother\\tmajor\\tflaw\\twith\\tHopfield\\tnets\\tis\\tthat\\tthey\\tdon’t\\tscale\\tvery\\twell\\t—\\ttheir\\tmemory\\tcapacity\\tis\\nroughly\\tequal\\tto\\t14%\\tof\\tthe\\tnumber\\tof\\tneurons.\\tFor\\texample,\\tto\\tclassify\\t28\\t×\\t28\\timages,\\tyou\\twould\\tneed\\na\\tHopfield\\tnet\\twith\\t784\\tfully\\tconnected\\tneurons\\tand\\t306,936\\tweights.\\tSuch\\ta\\tnetwork\\twould\\tonly\\tbe\\table\\nto\\tlearn\\tabout\\t110\\tdifferent\\tcharacters\\t(14%\\tof\\t784).\\tThat’s\\ta\\tlot\\tof\\tparameters\\tfor\\tsuch\\ta\\t\\nsmall\\tmemory.', 'Boltzmann\\tMachines\\nBoltzmann\\tmachines\\n\\t\\nwere\\tinvented\\tin\\t1985\\tby\\tGeoffrey\\tHinton\\tand\\tTerrence\\tSejnowski.\\tJust\\tlike\\nHopfield\\tnets,\\tthey\\tare\\tfully\\tconnected\\tANNs,\\tbut\\tthey\\tare\\tbased\\t\\non\\t\\nstochastic\\tneurons\\n:\\tinstead\\tof\\tusing\\na\\tdeterministic\\tstep\\tfunction\\tto\\tdecide\\twhat\\tvalue\\tto\\toutput,\\tthese\\tneurons\\toutput\\t1\\twith\\tsome\\tprobability,\\nand\\t0\\totherwise.\\tThe\\tprobability\\tfunction\\tthat\\tthese\\tANNs\\tuse\\tis\\tbased\\ton\\tthe\\tBoltzmann\\tdistribution\\n(used\\tin\\tstatistical\\tmechanics)\\thence\\ttheir\\tname.\\t\\nEquation\\tE-1\\n\\tgives\\tthe\\tprobability\\tthat\\ta\\tparticular\\nneuron\\twill\\toutput\\ta\\t1.\\nEquation\\tE-1.\\t\\nProbability\\tthat\\tthe\\ti\\nth\\n\\tneuron\\twill\\toutput\\t1\\ns\\nj\\n\\tis\\tthe\\tj\\nth\\n\\tneuron’s\\tstate\\t(0\\tor\\t1).\\nw\\ni\\n,\\nj\\n\\tis\\tthe\\tconnection\\tweight\\tbetween\\tthe\\ti\\nth\\n\\tand\\tj\\nth\\n\\tneurons.\\tNote\\tthat\\t\\nw\\ni\\n,\\ni\\n\\t=\\t0.\\nb\\ni\\n\\tis\\tthe\\ti\\nth\\n\\tneuron’s\\tbias\\tterm.\\tWe\\tcan\\timplement\\tthis\\tterm\\tby\\tadding\\ta\\tbias\\tneuron\\tto\\tthe\\tnetwork.\\nN\\n\\tis\\tthe\\tnumber\\tof\\tneurons\\tin\\tthe\\tnetwork.\\nT\\n\\tis\\ta\\tnumber\\tcalled\\tthe\\tnetwork’s\\t\\ntemperature', ';\\tthe\\thigher\\tthe\\ttemperature,\\tthe\\tmore\\trandom\\tthe\\noutput\\tis\\t(i.e.,\\tthe\\tmore\\tthe\\tprobability\\tapproaches\\t50%).\\nσ\\n\\tis\\tthe\\tlogistic\\tfunction.\\nNeurons\\tin\\tBoltzmann\\tmachines\\tare\\tseparated\\tinto\\ttwo\\tgroups:\\t\\nvisible\\tunits\\n\\tand\\t\\nhidden\\tunits\\n\\t(see\\nFigure\\tE-2\\n).\\tAll\\tneurons\\twork\\tin\\tthe\\tsame\\tstochastic\\tway,\\tbut\\tthe\\tvisible\\tunits\\tare\\tthe\\tones\\tthat\\treceive\\nthe\\tinputs\\tand\\tfrom\\twhich\\toutputs\\tare\\tread.', 'Figure\\tE-2.\\t\\nBoltzmann\\tmachine\\nBecause\\tof\\tits\\tstochastic\\tnature,\\ta\\tBoltzmann\\tmachine\\twill\\tnever\\tstabilize\\tinto\\ta\\tfixed\\tconfiguration,\\tbut\\ninstead\\tit\\twill\\tkeep\\tswitching\\tbetween\\tmany\\tconfigurations.\\tIf\\tit\\tis\\tleft\\trunning\\tfor\\ta\\tsufficiently\\tlong\\ttime,\\nthe\\tprobability\\tof\\tobserving\\ta\\tparticular\\tconfiguration\\twill\\tonly\\tbe\\ta\\tfunction\\tof\\tthe\\tconnection\\tweights\\nand\\tbias\\tterms,\\tnot\\tof\\tthe\\toriginal\\tconfiguration\\t(similarly,\\tafter\\tyou\\tshuffle\\ta\\tdeck\\tof\\tcards\\tfor\\tlong\\nenough,\\tthe\\tconfiguration\\tof\\tthe\\tdeck\\tdoes\\tnot\\tdepend\\ton\\tthe\\tinitial\\tstate).\\tWhen\\tthe\\tnetwork\\treaches\\tthis\\nstate\\twhere\\tthe\\toriginal\\tconfiguration\\tis\\t“forgotten,”\\tit\\tis\\tsaid\\tto\\tbe\\t\\nin\\t\\nthermal\\tequilibrium\\n\\t(although\\tits\\nconfiguration\\tkeeps\\tchanging\\tall\\tthe\\ttime).\\tBy\\tsetting\\tthe\\tnetwork\\tparameters\\tappropriately,\\tletting\\tthe\\nnetwork\\treach\\tthermal\\tequilibrium,\\tand\\tthen\\tobserving\\tits\\tstate,\\twe\\tcan\\tsimulate\\ta\\twide\\trange\\tof\\nprobability\\tdistributions.\\tThis\\tis\\tcalled\\t\\na\\t\\ngenerative\\tmodel\\n.', '.\\nTraining\\ta\\tBoltzmann\\tmachine\\tmeans\\tfinding\\tthe\\tparameters\\tthat\\twill\\tmake\\tthe\\tnetwork\\tapproximate\\tthe\\ntraining\\tset’s\\tprobability\\tdistribution.\\tFor\\texample,\\tif\\tthere\\tare\\tthree\\tvisible\\tneurons\\tand\\tthe\\ttraining\\tset\\ncontains\\t75%\\t(0,\\t1,\\t1)\\ttriplets,\\t10%\\t(0,\\t0,\\t1)\\ttriplets,\\tand\\t15%\\t(1,\\t1,\\t1)\\ttriplets,\\tthen\\tafter\\ttraining\\ta\\nBoltzmann\\tmachine,\\tyou\\tcould\\tuse\\tit\\tto\\tgenerate\\trandom\\tbinary\\ttriplets\\twith\\tabout\\tthe\\tsame\\tprobability\\ndistribution.\\tFor\\texample,\\tabout\\t75%\\tof\\tthe\\ttime\\tit\\twould\\toutput\\tthe\\t(0,\\t1,\\t1)\\ttriplet.\\nSuch\\ta\\tgenerative\\tmodel\\tcan\\tbe\\tused\\tin\\ta\\tvariety\\tof\\tways.\\tFor\\texample,\\tif\\tit\\tis\\ttrained\\ton\\timages,\\tand\\tyou\\nprovide\\tan\\tincomplete\\tor\\tnoisy\\timage\\tto\\tthe\\tnetwork,\\tit\\twill\\tautomatically\\t“repair”\\tthe\\timage\\tin\\ta\\nreasonable\\tway.\\tYou\\tcan\\talso\\tuse\\ta\\tgenerative\\tmodel\\tfor\\tclassification.\\tJust\\tadd\\ta\\tfew\\tvisible\\tneurons\\tto\\nencode\\tthe\\ttraining\\timage’s\\tclass\\t(e.g.,\\tadd\\t10\\tvisible\\tneurons\\tand\\tturn\\ton\\tonly\\tthe\\tfifth\\tneuron\\twhen\\tthe', 'training\\timage\\trepresents\\ta\\t5).\\tThen,\\twhen\\tgiven\\ta\\tnew\\timage,\\tthe\\tnetwork\\twill\\tautomatically\\tturn\\ton\\tthe', 'appropriate\\tvisible\\tneurons,\\tindicating\\tthe\\timage’s\\tclass\\t(e.g.,\\tit\\twill\\tturn\\ton\\tthe\\tfifth\\tvisible\\tneuron\\tif\\tthe\\nimage\\trepresents\\ta\\t5).\\nUnfortunately,\\tthere\\tis\\tno\\tefficient\\ttechnique\\tto\\ttrain\\tBoltzmann\\tmachines.\\tHowever,\\tfairly\\tefficient\\nalgorithms\\thave\\tbeen\\t\\ndeveloped\\tto\\t\\ntrain\\t\\nrestricted\\tBoltzmann\\tmachines\\n\\t(RBM).', 'Restricted\\tBoltzmann\\tMachines\\nAn\\tRBM\\tis\\tsimply\\ta\\tBoltzmann\\tmachine\\tin\\twhich\\tthere\\tare\\tno\\tconnections\\tbetween\\tvisible\\tunits\\tor\\nbetween\\thidden\\tunits,\\tonly\\tbetween\\tvisible\\tand\\thidden\\tunits.\\tFor\\texample,\\t\\nFigure\\tE-3\\n\\trepresents\\tan\\tRBM\\nwith\\tthree\\tvisible\\tunits\\tand\\tfour\\thidden\\tunits.\\nFigure\\tE-3.\\t\\nRestricted\\tBoltzmann\\tmachine\\nA\\tvery\\tefficient\\ttraining\\talgorithm,\\tcalled\\t\\nContrastive\\tDivergence\\n,\\t\\nwas\\t\\nintroduced\\tin\\t2005\\tby\\tMiguel\\tÁ.\\nCarreira-Perpiñán\\tand\\tGeoffrey\\tHinton\\n.\\n1\\n\\tHere\\tis\\thow\\tit\\tworks:\\tfor\\teach\\ttraining\\tinstance\\t\\nx\\n,\\tthe\\talgorithm\\nstarts\\tby\\tfeeding\\tit\\tto\\tthe\\tnetwork\\tby\\tsetting\\tthe\\tstate\\tof\\tthe\\tvisible\\tunits\\tto\\t\\nx\\n1\\n,\\t\\nx\\n2\\n,\\t,\\t\\nx\\nn\\n.\\tThen\\tyou\\tcompute\\nthe\\tstate\\tof\\tthe\\thidden\\tunits\\tby\\tapplying\\tthe\\tstochastic\\tequation\\tdescribed\\tbefore\\t(\\nEquation\\tE-1\\n).\\tThis\\ngives\\tyou\\ta\\thidden\\tvector\\t\\nh\\n\\t(where\\t\\nh\\ni\\n\\tis\\tequal\\tto\\tthe\\tstate\\tof\\tthe\\ti\\nth\\n\\tunit).\\tNext\\tyou\\tcompute\\tthe\\tstate\\tof\\tthe\\nvisible\\tunits,\\tby\\tapplying\\tthe\\tsame\\tstochastic\\tequation.\\tThis\\tgives\\tyou\\ta\\tvector\\t\\n.\\tThen\\tonce\\tagain\\tyou', 'compute\\tthe\\tstate\\tof\\tthe\\thidden\\tunits,\\twhich\\tgives\\tyou\\ta\\tvector\\t\\n.\\tNow\\tyou\\tcan\\tupdate\\teach\\tconnection\\nweight\\tby\\tapplying\\tthe\\trule\\tin\\t\\nEquation\\tE-2\\n.\\nEquation\\tE-2.\\t\\nContrastive\\tdivergence\\tweight\\tupdate\\nThe\\tgreat\\tbenefit\\tof\\tthis\\talgorithm\\tit\\tthat\\tit\\tdoes\\tnot\\trequire\\twaiting\\tfor\\tthe\\tnetwork\\tto\\treach\\tthermal\\nequilibrium:\\tit\\tjust\\tgoes\\tforward,\\tbackward,\\tand\\tforward\\tagain,\\tand\\tthat’s\\tit.\\tThis\\tmakes\\tit\\tincomparably\\nmore\\tefficient\\tthan\\tprevious\\talgorithms,\\tand\\tit\\twas\\ta\\tkey\\tingredient\\tto\\tthe\\tfirst\\tsuccess\\tof\\tDeep\\tLearning\\nbased\\ton\\tmultiple\\tstacked\\tRBMs.', 'Deep\\tBelief\\tNets\\nSeveral\\t\\nlayers\\tof\\tRBMs\\tcan\\tbe\\tstacked;\\tthe\\thidden\\tunits\\tof\\tthe\\tfirst-level\\tRBM\\tserves\\tas\\tthe\\tvisible\\tunits\\nfor\\tthe\\tsecond-layer\\tRBM,\\tand\\tso\\ton.\\tSuch\\tan\\tRBM\\tstack\\tis\\tcalled\\ta\\t\\ndeep\\tbelief\\tnet\\n\\t(DBN).\\nYee-Whye\\tTeh,\\tone\\tof\\tGeoffrey\\tHinton’s\\tstudents,\\tobserved\\tthat\\tit\\twas\\tpossible\\tto\\ttrain\\tDBNs\\tone\\tlayer\\nat\\ta\\ttime\\tusing\\tContrastive\\tDivergence,\\tstarting\\twith\\tthe\\tlower\\tlayers\\tand\\tthen\\tgradually\\tmoving\\tup\\tto\\tthe\\ntop\\tlayers.\\tThis\\tled\\tto\\tthe\\t\\ngroundbreaking\\tarticle\\tthat\\tkickstarted\\tthe\\tDeep\\tLearning\\ttsunami\\tin\\t2006\\n.\\n2\\nJust\\tlike\\tRBMs,\\tDBNs\\tlearn\\tto\\treproduce\\tthe\\tprobability\\tdistribution\\tof\\ttheir\\tinputs,\\twithout\\tany\\nsupervision.\\tHowever,\\tthey\\tare\\tmuch\\tbetter\\tat\\tit,\\tfor\\tthe\\tsame\\treason\\tthat\\tdeep\\tneural\\tnetworks\\tare\\tmore\\npowerful\\tthan\\tshallow\\tones:\\treal-world\\tdata\\tis\\toften\\torganized\\tin\\thierarchical\\tpatterns,\\tand\\tDBNs\\ttake\\nadvantage\\tof\\tthat.\\tTheir\\tlower\\tlayers\\tlearn\\tlow-level\\tfeatures\\tin\\tthe\\tinput\\tdata,\\twhile\\thigher\\tlayers\\tlearn\\nhigh-level\\tfeatures.', 'Just\\tlike\\tRBMs,\\tDBNs\\tare\\tfundamentally\\tunsupervised,\\tbut\\tyou\\tcan\\talso\\ttrain\\tthem\\tin\\ta\\tsupervised\\nmanner\\tby\\tadding\\tsome\\tvisible\\tunits\\tto\\trepresent\\tthe\\tlabels.\\tMoreover,\\tone\\tgreat\\tfeature\\tof\\tDBNs\\tis\\tthat\\nthey\\tcan\\tbe\\ttrained\\tin\\ta\\tsemisupervised\\tfashion.\\t\\nFigure\\tE-4\\n\\trepresents\\tsuch\\ta\\tDBN\\tconfigured\\tfor\\nsemisupervised\\tlearning.\\nFigure\\tE-4.\\t\\nA\\tdeep\\tbelief\\tnetwork\\tconfigured\\tfor\\tsemisupervised\\tlearning\\nFirst,\\tthe\\tRBM\\t1\\tis\\ttrained\\twithout\\tsupervision.\\tIt\\tlearns\\tlow-level\\tfeatures\\tin\\tthe\\ttraining\\tdata.\\tThen\\nRBM\\t2\\tis\\ttrained\\twith\\tRBM\\t1’s\\thidden\\tunits\\tas\\tinputs,\\tagain\\twithout\\tsupervision:\\tit\\tlearns\\thigher-level\\nfeatures\\t(note\\tthat\\tRBM\\t2’s\\thidden\\tunits\\tinclude\\tonly\\tthe\\tthree\\trightmost\\tunits,\\tnot\\tthe\\tlabel\\tunits).\\nSeveral\\tmore\\tRBMs\\tcould\\tbe\\tstacked\\tthis\\tway,\\tbut\\tyou\\tget\\tthe\\tidea.\\tSo\\tfar,\\ttraining\\twas\\t100%\\nunsupervised.\\tLastly,\\tRBM\\t3\\tis\\ttrained\\tusing\\tboth\\tRBM\\t2’s\\thidden\\tunits\\tas\\tinputs,\\tas\\twell\\tas\\textra', 'visible\\tunits\\tused\\tto\\trepresent\\tthe\\ttarget\\tlabels\\t(e.g.,\\ta\\tone-hot\\tvector\\trepresenting\\tthe\\tinstance\\tclass).\\tIt\\nlearns\\tto\\tassociate\\thigh-level\\tfeatures\\twith\\ttraining\\tlabels.\\tThis\\tis\\tthe\\tsupervised\\tstep.', 'At\\tthe\\tend\\tof\\ttraining,\\tif\\tyou\\tfeed\\tRBM\\t1\\ta\\tnew\\tinstance,\\tthe\\tsignal\\twill\\tpropagate\\tup\\tto\\tRBM\\t2,\\tthen\\tup\\nto\\tthe\\ttop\\tof\\tRBM\\t3,\\tand\\tthen\\tback\\tdown\\tto\\tthe\\tlabel\\tunits;\\thopefully,\\tthe\\tappropriate\\tlabel\\twill\\tlight\\tup.\\nThis\\tis\\thow\\ta\\tDBN\\tcan\\tbe\\tused\\tfor\\tclassification.\\nOne\\tgreat\\tbenefit\\tof\\tthis\\tsemisupervised\\tapproach\\tis\\tthat\\tyou\\tdon’t\\tneed\\tmuch\\tlabeled\\ttraining\\tdata.\\tIf\\tthe\\nunsupervised\\tRBMs\\tdo\\ta\\tgood\\tenough\\tjob,\\tthen\\tonly\\ta\\tsmall\\tamount\\tof\\tlabeled\\ttraining\\tinstances\\tper\\nclass\\twill\\tbe\\tnecessary.\\tSimilarly,\\ta\\tbaby\\tlearns\\tto\\trecognize\\tobjects\\twithout\\tsupervision,\\tso\\twhen\\tyou\\npoint\\tto\\ta\\tchair\\tand\\tsay\\t“chair,”\\tthe\\tbaby\\tcan\\tassociate\\tthe\\tword\\t“chair”\\twith\\tthe\\tclass\\tof\\tobjects\\tit\\thas\\nalready\\tlearned\\tto\\trecognize\\ton\\tits\\town.\\tYou\\tdon’t\\tneed\\tto\\tpoint\\tto\\tevery\\tsingle\\tchair\\tand\\tsay\\t“chair”;\\nonly\\ta\\tfew\\texamples\\twill\\tsuffice\\t(just\\tenough\\tso\\tthe\\tbaby\\tcan\\tbe\\tsure\\tthat\\tyou\\tare\\tindeed\\treferring\\tto\\tthe\\nchair,\\tnot\\tto\\tits\\tcolor\\tor\\tone\\tof\\tthe\\tchair’s\\tparts).', 'Quite\\tamazingly,\\tDBNs\\tcan\\talso\\twork\\tin\\treverse.\\tIf\\tyou\\tactivate\\tone\\tof\\tthe\\tlabel\\tunits,\\tthe\\tsignal\\twill\\npropagate\\tup\\tto\\tthe\\thidden\\tunits\\tof\\tRBM\\t3,\\tthen\\tdown\\tto\\tRBM\\t2,\\tand\\tthen\\tRBM\\t1,\\tand\\ta\\tnew\\tinstance\\nwill\\tbe\\toutput\\tby\\tthe\\tvisible\\tunits\\tof\\tRBM\\t1.\\tThis\\tnew\\tinstance\\twill\\tusually\\tlook\\tlike\\t\\na\\tregular\\tinstance\\nof\\tthe\\tclass\\twhose\\tlabel\\tunit\\tyou\\tactivated.\\tThis\\tgenerative\\tcapability\\tof\\tDBNs\\tis\\tquite\\tpowerful.\\tFor\\nexample,\\tit\\thas\\tbeen\\tused\\tto\\tautomatically\\tgenerate\\tcaptions\\tfor\\timages,\\tand\\tvice\\tversa:\\tfirst\\ta\\tDBN\\tis\\ntrained\\t(without\\tsupervision)\\tto\\tlearn\\tfeatures\\tin\\timages,\\tand\\tanother\\tDBN\\tis\\ttrained\\t(again\\twithout\\nsupervision)\\tto\\tlearn\\tfeatures\\tin\\tsets\\tof\\tcaptions\\t(e.g.,\\t“car”\\toften\\tcomes\\twith\\t“automobile”).\\tThen\\tan\\nRBM\\tis\\tstacked\\ton\\ttop\\tof\\tboth\\tDBNs\\tand\\ttrained\\twith\\ta\\tset\\tof\\timages\\talong\\twith\\ttheir\\tcaptions;\\tit\\tlearns\\nto\\tassociate\\thigh-level\\tfeatures\\tin\\timages\\twith\\thigh-level\\tfeatures\\tin\\tcaptions.\\tNext,\\tif\\tyou\\tfeed\\tthe\\timage', 'DBN\\tan\\timage\\tof\\ta\\tcar,\\tthe\\tsignal\\twill\\tpropagate\\tthrough\\tthe\\tnetwork,\\tup\\tto\\tthe\\ttop-level\\tRBM,\\tand\\tback\\ndown\\tto\\tthe\\tbottom\\tof\\tthe\\tcaption\\tDBN,\\tproducing\\ta\\tcaption.\\tDue\\tto\\tthe\\tstochastic\\tnature\\tof\\tRBMs\\tand\\nDBNs,\\tthe\\tcaption\\twill\\tkeep\\tchanging\\trandomly,\\tbut\\tit\\twill\\tgenerally\\tbe\\tappropriate\\tfor\\tthe\\timage.\\tIf\\tyou\\ngenerate\\ta\\tfew\\thundred\\tcaptions,\\tthe\\tmost\\tfrequently\\tgenerated\\tones\\twill\\tlikely\\tbe\\ta\\tgood\\t\\ndescription\\tof\\nthe\\timage.\\n3', 'Self-Organizing\\tMaps\\nSelf-organizing\\tmaps\\n\\t(SOM)\\t\\nare\\tquite\\tdifferent\\tfrom\\tall\\tthe\\tother\\ttypes\\tof\\tneural\\tnetworks\\twe\\thave\\ndiscussed\\tso\\tfar.\\tThey\\tare\\tused\\tto\\tproduce\\ta\\tlow-dimensional\\trepresentation\\tof\\ta\\thigh-dimensional\\ndataset,\\tgenerally\\tfor\\tvisualization,\\tclustering,\\tor\\tclassification.\\tThe\\tneurons\\tare\\tspread\\tacross\\ta\\tmap\\n(typically\\t2D\\tfor\\tvisualization,\\tbut\\tit\\tcan\\tbe\\tany\\tnumber\\tof\\tdimensions\\tyou\\twant),\\tas\\tshown\\tin\\t\\nFigure\\tE-5\\n,\\nand\\teach\\tneuron\\thas\\ta\\tweighted\\tconnection\\tto\\tevery\\tinput\\t(note\\tthat\\tthe\\tdiagram\\tshows\\tjust\\ttwo\\tinputs,\\tbut\\nthere\\tare\\ttypically\\ta\\tvery\\tlarge\\tnumber,\\tsince\\tthe\\twhole\\tpoint\\tof\\tSOMs\\tis\\tto\\treduce\\tdimensionality).\\nFigure\\tE-5.\\t\\nSelf-organizing\\tmaps\\nOnce\\tthe\\tnetwork\\tis\\ttrained,\\tyou\\tcan\\tfeed\\tit\\ta\\tnew\\tinstance\\tand\\tthis\\twill\\tactivate\\tonly\\tone\\tneuron\\t(i.e.,\\nhence\\tone\\tpoint\\ton\\tthe\\tmap):\\tthe\\tneuron\\twhose\\tweight\\tvector\\tis\\tclosest\\tto\\tthe\\tinput\\tvector.\\tIn\\tgeneral,\\ninstances\\tthat\\tare\\tnearby\\tin\\tthe\\toriginal\\tinput\\tspace\\twill\\tactivate\\tneurons\\tthat\\tare\\tnearby\\ton\\tthe\\tmap.\\tThis', 'makes\\tSOMs\\tuseful\\tfor\\tvisualization\\t(in\\tparticular,\\tyou\\tcan\\teasily\\tidentify\\tclusters\\ton\\tthe\\tmap),\\tbut\\talso\\nfor\\tapplications\\tlike\\tspeech\\trecognition.\\tFor\\texample,\\tif\\teach\\tinstance\\trepresents\\tthe\\taudio\\trecording\\tof\\ta\\nperson\\tpronouncing\\ta\\tvowel,\\tthen\\tdifferent\\tpronunciations\\tof\\tthe\\tvowel\\t“a”\\twill\\tactivate\\tneurons\\tin\\tthe\\nsame\\tarea\\tof\\tthe\\tmap,\\twhile\\tinstances\\tof\\tthe\\tvowel\\t“e”\\twill\\tactivate\\tneurons\\tin\\tanother\\tarea,\\tand\\nintermediate\\tsounds\\twill\\tgenerally\\tactivate\\tintermediate\\tneurons\\ton\\tthe\\tmap.', 'NOTE\\nOne\\timportant\\tdifference\\twith\\tthe\\tother\\tdimensionality\\treduction\\ttechniques\\tdiscussed\\tin\\t\\nChapter\\t8\\n\\tis\\tthat\\tall\\tinstances\\tget\\nmapped\\tto\\ta\\tdiscrete\\tnumber\\tof\\tpoints\\tin\\tthe\\tlow-dimensional\\tspace\\t(one\\tpoint\\tper\\tneuron).\\tWhen\\tthere\\tare\\tvery\\tfew\\tneurons,\\nthis\\ttechnique\\tis\\tbetter\\tdescribed\\tas\\tclustering\\trather\\tthan\\tdimensionality\\treduction.\\nThe\\ttraining\\talgorithm\\tis\\tunsupervised.\\tIt\\tworks\\tby\\thaving\\tall\\tthe\\tneurons\\tcompete\\tagainst\\teach\\tother.\\nFirst,\\tall\\tthe\\tweights\\tare\\tinitialized\\trandomly.\\tThen\\ta\\ttraining\\tinstance\\tis\\tpicked\\trandomly\\tand\\tfed\\tto\\tthe\\nnetwork.\\tAll\\tneurons\\tcompute\\tthe\\tdistance\\tbetween\\ttheir\\tweight\\tvector\\tand\\tthe\\tinput\\tvector\\t(this\\tis\\tvery\\ndifferent\\tfrom\\tthe\\tartificial\\tneurons\\twe\\thave\\tseen\\tso\\tfar).\\tThe\\tneuron\\tthat\\tmeasures\\tthe\\tsmallest\\tdistance\\nwins\\tand\\ttweaks\\tits\\tweight\\tvector\\tto\\tbe\\teven\\tslightly\\tcloser\\tto\\tthe\\tinput\\tvector,\\tmaking\\tit\\tmore\\tlikely\\tto\\nwin\\tfuture\\tcompetitions\\tfor\\tother\\tinputs\\tsimilar\\tto\\tthis\\tone.\\tIt\\talso\\trecruits\\tits\\tneighboring\\tneurons,\\tand', 'they\\ttoo\\tupdate\\ttheir\\tweight\\tvector\\tto\\tbe\\tslightly\\tcloser\\tto\\tthe\\tinput\\tvector\\t(but\\tthey\\tdon’t\\tupdate\\ttheir\\nweights\\tas\\tmuch\\tas\\tthe\\twinner\\tneuron).\\tThen\\tthe\\talgorithm\\tpicks\\tanother\\ttraining\\tinstance\\tand\\trepeats\\tthe\\nprocess,\\tagain\\tand\\tagain.\\tThis\\t\\nalgorithm\\ttends\\tto\\tmake\\tnearby\\tneurons\\tgradually\\tspecialize\\tin\\tsimilar\\ninputs.\\n4\\n“On\\tContrastive\\tDivergence\\tLearning,”\\tM.\\tÁ.\\tCarreira-Perpiñán\\tand\\tG.\\tHinton\\t(2005).\\n“A\\tFast\\tLearning\\tAlgorithm\\tfor\\tDeep\\tBelief\\tNets,”\\tG.\\tHinton,\\tS.\\tOsindero,\\tY.\\tTeh\\t(2006).\\nSee\\tthis\\tvideo\\tby\\tGeoffrey\\tHinton\\tfor\\tmore\\tdetails\\tand\\ta\\tdemo:\\t\\nhttp://goo.gl/7Z5QiS\\n.\\nYou\\tcan\\timagine\\ta\\tclass\\tof\\tyoung\\tchildren\\twith\\troughly\\tsimilar\\tskills.\\tOne\\tchild\\thappens\\tto\\tbe\\tslightly\\tbetter\\tat\\tbasketball.\\tThis\\tmotivates\\nher\\tto\\tpractice\\tmore,\\tespecially\\twith\\ther\\tfriends.\\tAfter\\ta\\twhile,\\tthis\\tgroup\\tof\\tfriends\\tgets\\tso\\tgood\\tat\\tbasketball\\tthat\\tother\\tkids\\tcannot', 'compete.\\tBut\\tthat’s\\tokay,\\tbecause\\tthe\\tother\\tkids\\tspecialize\\tin\\tother\\ttopics.\\tAfter\\ta\\twhile,\\tthe\\tclass\\tis\\tfull\\tof\\tlittle\\tspecialized\\tgroups.\\n1\\n2\\n3\\n4', 'Index\\nSymbols\\n__call__()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\nε-greedy\\tpolicy\\n,\\t\\nExploration\\tPolicies\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nε-insensitive\\n,\\t\\nSVM\\tRegression\\nχ\\t2\\ttest\\n\\t(\\nsee\\n\\tchi\\tsquare\\ttest)\\nℓ\\t0\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nℓ\\t1\\tand\\tℓ\\t2\\tregularization\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n-\\nℓ1\\tand\\tℓ2\\tRegularization\\nℓ\\t1\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n,\\t\\nLasso\\tRegression\\n,\\t\\nDecision\\tBoundaries\\n,\\t\\nAdam\\nOptimization\\n,\\t\\nAvoiding\\tOverfitting\\tThrough\\tRegularization\\nℓ\\t2\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n,\\t\\nRidge\\tRegression\\n-\\nLasso\\tRegression\\n,\\t\\nDecision\\nBoundaries\\n,\\t\\nSoftmax\\tRegression\\n,\\t\\nAvoiding\\tOverfitting\\tThrough\\tRegularization\\n,\\t\\nMax-Norm\\nRegularization\\nℓ\\tk\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nℓ\\t∞\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nA\\naccuracy\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\n-\\nMeasuring\\nAccuracy\\tUsing\\tCross-Validation\\nactions,\\tevaluating\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\n-\\nEvaluating\\tActions:', 'The\\tCredit\\tAssignment\\tProblem\\nactivation\\tfunctions\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n-\\nMulti-Layer\\tPerceptron\\nand\\tBackpropagation\\nactive\\tconstraints\\n,\\t\\nSVM\\tDual\\tProblem\\nactors\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nactual\\tclass\\n,\\t\\nConfusion\\tMatrix', 'AdaBoost\\n,\\t\\nAdaBoost\\n-\\nAdaBoost\\nAdagrad\\n,\\t\\nAdaGrad\\n-\\nAdaGrad\\nAdam\\toptimization\\n,\\t\\nAdam\\tOptimization\\n-\\nAdam\\tOptimization\\n,\\t\\nAdam\\tOptimization\\nadaptive\\tlearning\\trate\\n,\\t\\nAdaGrad\\nadaptive\\tmoment\\toptimization\\n,\\t\\nAdam\\tOptimization\\nagents\\n,\\t\\nLearning\\tto\\tOptimize\\tRewards\\nAlexNet\\tarchitecture\\n,\\t\\nAlexNet\\n-\\nAlexNet\\nalgorithms\\npreparing\\tdata\\tfor\\n,\\t\\nPrepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\tAlgorithms\\n-\\nSelect\\tand\\nTrain\\ta\\tModel\\nAlphaGo\\n,\\t\\nReinforcement\\tLearning\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n,\\t\\nReinforcement\\nLearning\\n,\\t\\nPolicy\\tGradients\\nAnaconda\\n,\\t\\nCreate\\tthe\\tWorkspace\\nanomaly\\tdetection\\n,\\t\\nUnsupervised\\tlearning\\nApple’s\\tSiri\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\napply_gradients()\\n,\\t\\nGradient\\tClipping\\n,\\t\\nPolicy\\tGradients\\narea\\tunder\\tthe\\tcurve\\t(AUC)\\n,\\t\\nThe\\tROC\\tCurve\\narray_split()\\n,\\t\\nIncremental\\tPCA\\nartificial\\tneural\\tnetworks\\t(ANNs)\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n-\\nExercises\\nBoltzmann\\tMachines\\n,\\t\\nBoltzmann\\tMachines\\n-\\nBoltzmann\\tMachines\\ndeep\\tbelief\\tnetworks\\t(DBNs)\\n,', ',\\t\\nDeep\\tBelief\\tNets\\n-\\nDeep\\tBelief\\tNets\\nevolution\\tof\\n,\\t\\nFrom\\tBiological\\tto\\tArtificial\\tNeurons\\nHopfield\\tNetworks\\n,\\t\\nHopfield\\tNetworks\\n-\\nHopfield\\tNetworks\\nhyperparameter\\tfine-tuning\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\n-\\nActivation', 'Functions\\noverview\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n-\\nFrom\\tBiological\\tto\\tArtificial\\nNeurons\\nPerceptrons\\n,\\t\\nThe\\tPerceptron\\n-\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nself-organizing\\tmaps\\n,\\t\\nSelf-Organizing\\tMaps\\n-\\nSelf-Organizing\\tMaps\\ntraining\\ta\\tDNN\\twith\\tTensorFlow\\n,\\t\\nTraining\\ta\\tDNN\\tUsing\\tPlain\\tTensorFlow\\n-\\nUsing\\tthe\\nNeural\\tNetwork\\nartificial\\tneuron\\n,\\t\\nLogical\\tComputations\\twith\\tNeurons\\n(\\nsee\\talso\\n\\tartificial\\tneural\\tnetwork\\t(ANN))\\nassign()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\nassociation\\trule\\tlearning\\n,\\t\\nUnsupervised\\tlearning\\nassociative\\tmemory\\tnetworks\\n,\\t\\nHopfield\\tNetworks\\nassumptions,\\tchecking\\n,\\t\\nCheck\\tthe\\tAssumptions\\nasynchronous\\tupdates\\n,\\t\\nAsynchronous\\tupdates\\n-\\nAsynchronous\\tupdates\\nasynchrous\\tcommunication\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\n-\\nPaddingFifoQueue\\natrous_conv2d()\\n,\\t\\nResNet\\nattention\\tmechanism\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nattributes\\n,\\t\\nSupervised\\tlearning\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\n-', '-\\nTake\\ta\\tQuick\\tLook\\nat\\tthe\\tData\\tStructure\\n(\\nsee\\talso\\n\\tdata\\tstructure)\\ncombinations\\tof\\n,\\t\\nExperimenting\\twith\\tAttribute\\tCombinations\\n-\\nExperimenting\\twith\\nAttribute\\tCombinations\\npreprocessed\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\ntarget\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure', 'autodiff\\n,\\t\\nUsing\\tautodiff\\n-\\nUsing\\tautodiff\\n,\\t\\nAutodiff\\n-\\nReverse-Mode\\tAutodiff\\nforward-mode\\n,\\t\\nForward-Mode\\tAutodiff\\n-\\nForward-Mode\\tAutodiff\\nmanual\\tdifferentiation\\n,\\t\\nManual\\tDifferentiation\\nnumerical\\tdifferentiation\\n,\\t\\nNumerical\\tDifferentiation\\nreverse-mode\\n,\\t\\nReverse-Mode\\tAutodiff\\n-\\nReverse-Mode\\tAutodiff\\nsymbolic\\tdifferentiation\\n,\\t\\nSymbolic\\tDifferentiation\\n-\\nNumerical\\tDifferentiation\\nautoencoders\\n,\\t\\nAutoencoders\\n-\\nExercises\\nadversarial\\n,\\t\\nOther\\tAutoencoders\\ncontractive\\n,\\t\\nOther\\tAutoencoders\\ndenoising\\n,\\t\\nDenoising\\tAutoencoders\\n-\\nTensorFlow\\tImplementation\\nefficient\\tdata\\trepresentations\\n,\\t\\nEfficient\\tData\\tRepresentations\\ngenerative\\tstochastic\\tnetwork\\t(GSN)\\n,\\t\\nOther\\tAutoencoders\\novercomplete\\n,\\t\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\nPCA\\twith\\tundercomplete\\tlinear\\tautoencoder\\n,\\t\\nPerforming\\tPCA\\twith\\tan\\tUndercomplete\\nLinear\\tAutoencoder\\nreconstructions\\n,\\t\\nEfficient\\tData\\tRepresentations\\nsparse\\n,\\t\\nSparse\\tAutoencoders\\n-\\nTensorFlow\\tImplementation\\nstacked\\n,\\t\\nStacked\\tAutoencoders\\n-', '-\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\nstacked\\tconvolutional\\n,\\t\\nOther\\tAutoencoders\\nundercomplete\\n,\\t\\nEfficient\\tData\\tRepresentations\\nvariational\\n,\\t\\nVariational\\tAutoencoders\\n-\\nGenerating\\tDigits\\nvisualizing\\tfeatures\\n,\\t\\nVisualizing\\tFeatures\\n-\\nVisualizing\\tFeatures\\nwinner-take-all\\t(WTA)\\n,\\t\\nOther\\tAutoencoders\\nautomatic\\tdifferentiating\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow', 'autonomous\\tdriving\\tsystems\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\nAverage\\tAbsolute\\tDeviation\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\naverage\\tpooling\\tlayer\\n,\\t\\nPooling\\tLayer\\navg_pool()\\n,\\t\\nPooling\\tLayer\\nB\\nbackpropagation\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n-\\nMulti-Layer\\tPerceptron\\tand\\nBackpropagation\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n,\\t\\nUnsupervised\\tPretraining\\n,\\nVisualizing\\tFeatures\\nbackpropagation\\tthrough\\ttime\\t(BPTT)\\n,\\t\\nTraining\\tRNNs\\nbagging\\tand\\tpasting\\n,\\t\\nBagging\\tand\\tPasting\\n-\\nOut-of-Bag\\tEvaluation\\nout-of-bag\\tevaluation\\n,\\t\\nOut-of-Bag\\tEvaluation\\n-\\nOut-of-Bag\\tEvaluation\\nin\\tScikit-Learn\\n,\\t\\nBagging\\tand\\tPasting\\tin\\tScikit-Learn\\n-\\nBagging\\tand\\tPasting\\tin\\tScikit-\\nLearn\\nbandwidth\\tsaturation\\n,\\t\\nBandwidth\\tsaturation\\n-\\nBandwidth\\tsaturation\\nBasicLSTMCell\\n,\\t\\nLSTM\\tCell\\nBasicRNNCell\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\n-\\nDistributing\\ta\\tDeep\\tRNN\\nAcross\\tMultiple\\tGPUs\\nBatch\\tGradient\\tDescent\\n,\\t\\nBatch\\tGradient\\tDescent\\n-\\nBatch\\tGradient\\tDescent\\n,\\t\\nLasso\\tRegression\\nbatch\\tlearning\\n,\\t\\nBatch\\tlearning\\n-', '-\\nBatch\\tlearning\\nBatch\\tNormalization\\n,\\t\\nBatch\\tNormalization\\n-\\nImplementing\\tBatch\\tNormalization\\twith\\nTensorFlow\\n,\\t\\nResNet\\noperation\\tsummary\\n,\\t\\nBatch\\tNormalization\\nwith\\tTensorFlow\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n-\\nImplementing\\nBatch\\tNormalization\\twith\\tTensorFlow\\nbatch()\\n,\\t\\nOther\\tconvenience\\tfunctions\\nbatch_join()\\n,\\t\\nOther\\tconvenience\\tfunctions', 'batch_normalization()\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n-\\nImplementing\\nBatch\\tNormalization\\twith\\tTensorFlow\\nBellman\\tOptimality\\tEquation\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nbetween-graph\\treplication\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\nbias\\tneurons\\n,\\t\\nThe\\tPerceptron\\nbias\\tterm\\n,\\t\\nLinear\\tRegression\\nbias/variance\\ttradeoff\\n,\\t\\nLearning\\tCurves\\nbiases\\n,\\t\\nConstruction\\tPhase\\nbinary\\tclassifiers\\n,\\t\\nTraining\\ta\\tBinary\\tClassifier\\n,\\t\\nLogistic\\tRegression\\nbiological\\tneurons\\n,\\t\\nFrom\\tBiological\\tto\\tArtificial\\tNeurons\\n-\\nBiological\\tNeurons\\nblack\\tbox\\tmodels\\n,\\t\\nMaking\\tPredictions\\nblending\\n,\\t\\nStacking\\n-\\nExercises\\nBoltzmann\\tMachines\\n,\\t\\nBoltzmann\\tMachines\\n-\\nBoltzmann\\tMachines\\n(\\nsee\\talso\\n\\trestricted\\tBoltzman\\tmachines\\t(RBMs))\\nboosting\\n,\\t\\nBoosting\\n-\\nGradient\\tBoosting\\nAdaBoost\\n,\\t\\nAdaBoost\\n-\\nAdaBoost\\nGradient\\tBoosting\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\nbootstrap\\taggregation\\n\\t(\\nsee\\n\\tbagging)\\nbootstrapping\\n,\\t\\nGrid\\tSearch\\n,\\t\\nBagging\\tand\\tPasting\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\n,\\t\\nLearning\\tto', 'Play\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nbottleneck\\tlayers\\n,\\t\\nGoogLeNet\\nbrew\\n,\\t\\nStacking\\nC\\nCaffe\\tmodel\\tzoo\\n,\\t\\nModel\\tZoos', 'CART\\t(Classification\\tand\\tRegression\\tTree)\\talgorithm\\n,\\t\\nMaking\\tPredictions\\n-\\nThe\\tCART\\tTraining\\nAlgorithm\\n,\\t\\nRegression\\ncategorical\\tattributes\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\n-\\nHandling\\tText\\tand\\nCategorical\\tAttributes\\ncell\\twrapper\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\nchi\\tsquare\\ttest\\n,\\t\\nRegularization\\tHyperparameters\\nclassification\\tversus\\tregression\\n,\\t\\nSupervised\\tlearning\\n,\\t\\nMultioutput\\tClassification\\nclassifiers\\nbinary\\n,\\t\\nTraining\\ta\\tBinary\\tClassifier\\nerror\\tanalysis\\n,\\t\\nError\\tAnalysis\\n-\\nError\\tAnalysis\\nevaluating\\n,\\t\\nMulticlass\\tClassification\\nMNIST\\tdataset\\n,\\t\\nMNIST\\n-\\nMNIST\\nmulticlass\\n,\\t\\nMulticlass\\tClassification\\n-\\nMulticlass\\tClassification\\nmultilabel\\n,\\t\\nMultilabel\\tClassification\\n-\\nMultilabel\\tClassification\\nmultioutput\\n,\\t\\nMultioutput\\tClassification\\n-\\nMultioutput\\tClassification\\nperformance\\tmeasures\\n,\\t\\nPerformance\\tMeasures\\n-\\nThe\\tROC\\tCurve\\nprecision\\tof\\n,\\t\\nConfusion\\tMatrix\\nvoting\\n,\\t\\nVoting\\tClassifiers\\n-\\nVoting\\tClassifiers\\nclip_by_value()\\n,\\t\\nGradient\\tClipping\\nclosed-form\\tequation\\n,', ',\\t\\nTraining\\tModels\\n,\\t\\nRidge\\tRegression\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\ncluster\\tspecification\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\nclustering\\talgorithms\\n,\\t\\nUnsupervised\\tlearning\\nclusters\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\ncoding\\tspace\\n,\\t\\nVariational\\tAutoencoders', 'codings\\n,\\t\\nAutoencoders\\ncomplementary\\tslackness\\tcondition\\n,\\t\\nSVM\\tDual\\tProblem\\ncomponents_\\n,\\t\\nUsing\\tScikit-Learn\\ncomputational\\tcomplexity\\n,\\t\\nComputational\\tComplexity\\n,\\t\\nComputational\\tComplexity\\n,\\nComputational\\tComplexity\\ncompute_gradients()\\n,\\t\\nGradient\\tClipping\\n,\\t\\nPolicy\\tGradients\\nconcat()\\n,\\t\\nGoogLeNet\\nconfig.gpu_options\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM\\nConfigProto\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM\\nconfusion\\tmatrix\\n,\\t\\nConfusion\\tMatrix\\n-\\nConfusion\\tMatrix\\n,\\t\\nError\\tAnalysis\\n-\\nError\\tAnalysis\\nconnectionism\\n,\\t\\nThe\\tPerceptron\\nconstrained\\toptimization\\n,\\t\\nTraining\\tObjective\\n,\\t\\nSVM\\tDual\\tProblem\\nContrastive\\tDivergence\\n,\\t\\nRestricted\\tBoltzmann\\tMachines\\ncontrol\\tdependencies\\n,\\t\\nControl\\tDependencies\\nconv1d()\\n,\\t\\nResNet\\nconv2d_transpose()\\n,\\t\\nResNet\\nconv3d()\\n,\\t\\nResNet\\nconvergence\\trate\\n,\\t\\nBatch\\tGradient\\tDescent\\nconvex\\tfunction\\n,\\t\\nGradient\\tDescent\\nconvolution\\tkernels\\n,\\t\\nFilters\\n,\\t\\nCNN\\tArchitectures\\n,\\t\\nGoogLeNet\\nconvolutional\\tneural\\tnetworks\\t(CNNs)\\n,\\t\\nConvolutional\\tNeural\\tNetworks\\n-\\nExercises\\narchitectures\\n,\\t\\nCNN\\tArchitectures', '-\\nResNet\\nAlexNet\\n,\\t\\nAlexNet\\n-\\nAlexNet\\nGoogleNet\\n,\\t\\nGoogLeNet\\n-\\nGoogLeNet', 'LeNet5\\n,\\t\\nLeNet-5\\n-\\nLeNet-5\\nResNet\\n,\\t\\nResNet\\n-\\nResNet\\nconvolutional\\tlayer\\n,\\t\\nConvolutional\\tLayer\\n-\\nMemory\\tRequirements\\n,\\t\\nGoogLeNet\\n,\\t\\nResNet\\nfeature\\tmaps\\n,\\t\\nStacking\\tMultiple\\tFeature\\tMaps\\n-\\nTensorFlow\\tImplementation\\nfilters\\n,\\t\\nFilters\\nmemory\\trequirement\\n,\\t\\nMemory\\tRequirements\\n-\\nMemory\\tRequirements\\nevolution\\tof\\n,\\t\\nThe\\tArchitecture\\tof\\tthe\\tVisual\\tCortex\\npooling\\tlayer\\n,\\t\\nPooling\\tLayer\\n-\\nPooling\\tLayer\\nTensorFlow\\timplementation\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\tImplementation\\nCoordinator\\tclass\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\ncorrelation\\tcoefficient\\n,\\t\\nLooking\\tfor\\tCorrelations\\n-\\nLooking\\tfor\\tCorrelations\\ncorrelations,\\tfinding\\n,\\t\\nLooking\\tfor\\tCorrelations\\n-\\nLooking\\tfor\\tCorrelations\\ncost\\tfunction\\n,\\t\\nModel-based\\tlearning\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nin\\tAdaBoost\\n,\\t\\nAdaBoost\\nin\\tadagrad\\n,\\t\\nAdaGrad\\nin\\tartificial\\tneural\\tnetworks\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\n,', ',\\nConstruction\\tPhase\\n-\\nConstruction\\tPhase\\nin\\tautodiff\\n,\\t\\nUsing\\tautodiff\\nin\\tbatch\\tnormalization\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\ncross\\tentropy\\n,\\t\\nLeNet-5\\ndeep\\tQ-Learning\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nin\\tElastic\\tNet\\n,\\t\\nElastic\\tNet\\nin\\tGradient\\tDescent\\n,\\t\\nTraining\\tModels\\n,\\t\\nGradient\\tDescent\\n-\\nGradient\\tDescent\\n,\\t\\nBatch', 'Gradient\\tDescent\\n,\\t\\nBatch\\tGradient\\tDescent\\n-\\nStochastic\\tGradient\\tDescent\\n,\\t\\nGradient\\nBoosting\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\nin\\tLogistic\\tRegression\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\n-\\nTraining\\tand\\tCost\\tFunction\\nin\\tPG\\talgorithms\\n,\\t\\nPolicy\\tGradients\\nin\\tvariational\\tautoencoders\\n,\\t\\nVariational\\tAutoencoders\\nin\\tLasso\\t\\nRegression\\n,\\t\\nLasso\\tRegression\\n-\\nLasso\\tRegression\\nin\\tLinear\\tRegression\\n,\\t\\nThe\\tNormal\\tEquation\\n,\\t\\nGradient\\tDescent\\nin\\tMomentum\\toptimization\\n,\\t\\nMomentum\\tOptimization\\n-\\nNesterov\\tAccelerated\\tGradient\\nin\\tpretrained\\tlayers\\treuse\\n,\\t\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\nin\\tridge\\tregression\\n,\\t\\nRidge\\tRegression\\n-\\nRidge\\tRegression\\nin\\tRNNs\\n,\\t\\nTraining\\tRNNs\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\nstale\\tgradients\\tand\\n,\\t\\nAsynchronous\\tupdates\\ncreative\\tsequences\\n,\\t\\nCreative\\tRNN\\ncredit\\tassignment\\tproblem\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\n-\\nEvaluating\\nActions:\\tThe\\tCredit\\tAssignment\\tProblem\\ncritics\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ncross\\tentropy\\n,', ',\\t\\nSoftmax\\tRegression\\n-\\nSoftmax\\tRegression\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\nHigh-Level\\tAPI\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nPolicy\\tGradients\\ncross-validation\\n,\\t\\nTesting\\tand\\tValidating\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n-\\nBetter\\nEvaluation\\tUsing\\tCross-Validation\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\n-\\nMeasuring\\nAccuracy\\tUsing\\tCross-Validation\\nCUDA\\tlibrary\\n,\\t\\nInstallation\\ncuDNN\\tlibrary\\n,\\t\\nInstallation\\ncurse\\tof\\tdimensionality\\n,\\t\\nDimensionality\\tReduction\\n-\\nThe\\tCurse\\tof\\tDimensionality\\n(\\nsee\\talso\\n\\tdimensionality\\treduction)', 'custom\\ttransformers\\n,\\t\\nCustom\\tTransformers\\n-\\nCustom\\tTransformers\\nD\\ndata\\n,\\t\\nTesting\\tand\\tValidating\\n(\\nsee\\talso\\n\\ttest\\tdata;\\ttraining\\tdata)\\ncreating\\tworkspace\\tfor\\n,\\t\\nGet\\tthe\\tData\\n-\\nDownload\\tthe\\tData\\ndownloading\\n,\\t\\nDownload\\tthe\\tData\\n-\\nDownload\\tthe\\tData\\nfinding\\tcorrelations\\tin\\n,\\t\\nLooking\\tfor\\tCorrelations\\n-\\nLooking\\tfor\\tCorrelations\\nmaking\\tassumptions\\tabout\\n,\\t\\nTesting\\tand\\tValidating\\npreparing\\tfor\\tMachine\\tLearning\\talgorithms\\n,\\t\\nPrepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\nAlgorithms\\n-\\nSelect\\tand\\tTrain\\ta\\tModel\\ntest-set\\tcreation\\n,\\t\\nCreate\\ta\\tTest\\tSet\\n-\\nCreate\\ta\\tTest\\tSet\\nworking\\twith\\treal\\tdata\\n,\\t\\nWorking\\twith\\tReal\\tData\\ndata\\taugmentation\\n,\\t\\nData\\tAugmentation\\n-\\nData\\tAugmentation\\ndata\\tcleaning\\n,\\t\\nData\\tCleaning\\n-\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\ndata\\tmining\\n,\\t\\nWhy\\tUse\\tMachine\\tLearning?\\ndata\\tparallelism\\n,\\t\\nData\\tParallelism\\n-\\nTensorFlow\\timplementation\\nasynchronous\\tupdates\\n,\\t\\nAsynchronous\\tupdates\\n-\\nAsynchronous\\tupdates\\nbandwidth\\tsaturation\\n,\\t\\nBandwidth\\tsaturation\\n-\\nBandwidth\\tsaturation', 'synchronous\\tupdates\\n,\\t\\nSynchronous\\tupdates\\nTensorFlow\\timplementation\\n,\\t\\nTensorFlow\\timplementation\\ndata\\tpipeline\\n,\\t\\nFrame\\tthe\\tProblem\\ndata\\tsnooping\\tbias\\n,\\t\\nCreate\\ta\\tTest\\tSet\\ndata\\tstructure\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\n-\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\nStructure', 'data\\tvisualization\\n,\\t\\nVisualizing\\tGeographical\\tData\\n-\\nVisualizing\\tGeographical\\tData\\nDataFrame\\n,\\t\\nData\\tCleaning\\ndataquest\\n,\\t\\nOther\\tResources\\ndecision\\tboundaries\\n,\\t\\nDecision\\tBoundaries\\n-\\nDecision\\tBoundaries\\n,\\t\\nSoftmax\\tRegression\\n,\\t\\nMaking\\nPredictions\\ndecision\\tfunction\\n,\\t\\nPrecision/Recall\\tTradeoff\\n,\\t\\nDecision\\tFunction\\tand\\tPredictions\\n-\\nDecision\\nFunction\\tand\\tPredictions\\nDecision\\tStumps\\n,\\t\\nAdaBoost\\ndecision\\tthreshold\\n,\\t\\nPrecision/Recall\\tTradeoff\\nDecision\\tTrees\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n-\\nBetter\\tEvaluation\\tUsing\\tCross-\\nValidation\\n,\\t\\nDecision\\tTrees\\n-\\nExercises\\n,\\t\\nEnsemble\\tLearning\\tand\\tRandom\\tForests\\nbinary\\ttrees\\n,\\t\\nMaking\\tPredictions\\nclass\\tprobability\\testimates\\n,\\t\\nEstimating\\tClass\\tProbabilities\\ncomputational\\tcomplexity\\n,\\t\\nComputational\\tComplexity\\ndecision\\tboundaries\\n,\\t\\nMaking\\tPredictions\\nGINI\\timpurity\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\ninstability\\twith\\n,\\t\\nInstability\\n-\\nInstability\\nnumbers\\tof\\tchildren\\n,\\t\\nMaking\\tPredictions\\npredictions\\n,\\t\\nMaking\\tPredictions\\n-', '-\\nEstimating\\tClass\\tProbabilities\\nRandom\\tForests\\n\\t(\\nsee\\n\\tRandom\\tForests)\\nregression\\ttasks\\n,\\t\\nRegression\\n-\\nRegression\\nregularization\\thyperparameters\\n,\\t\\nRegularization\\tHyperparameters\\n-\\nRegularization\\nHyperparameters\\ntraining\\tand\\tvisualizing\\n,\\t\\nTraining\\tand\\tVisualizing\\ta\\tDecision\\tTree\\n-\\nMaking\\tPredictions\\ndecoder\\n,\\t\\nEfficient\\tData\\tRepresentations', 'deconvolutional\\tlayer\\n,\\t\\nResNet\\ndeep\\tautoencoders\\n\\t(\\nsee\\n\\tstacked\\tautoencoders)\\ndeep\\tbelief\\tnetworks\\t(DBNs)\\n,\\t\\nSemisupervised\\tlearning\\n,\\t\\nDeep\\tBelief\\tNets\\n-\\nDeep\\tBelief\\tNets\\nDeep\\tLearning\\n,\\t\\nReinforcement\\tLearning\\n(\\nsee\\talso\\n\\tReinforcement\\tLearning;\\tTensorFlow)\\nabout\\n,\\t\\nThe\\tMachine\\tLearning\\tTsunami\\n,\\t\\nRoadmap\\nlibraries\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\n-\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\ndeep\\tneural\\tnetworks\\t(DNNs)\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n,\\t\\nTraining\\tDeep\\nNeural\\tNets\\n-\\nExercises\\n(\\nsee\\talso\\n\\tMulti-Layer\\tPerceptrons\\t(MLP))\\nfaster\\toptimizers\\tfor\\n,\\t\\nFaster\\tOptimizers\\n-\\nLearning\\tRate\\tScheduling\\nregularization\\n,\\t\\nAvoiding\\tOverfitting\\tThrough\\tRegularization\\n-\\nData\\tAugmentation\\nreusing\\tpretrained\\tlayers\\n,\\t\\nReusing\\tPretrained\\tLayers\\n-\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\ntraining\\tguidelines\\toverview\\n,\\t\\nPractical\\tGuidelines\\ntraining\\twith\\tTensorFlow\\n,\\t\\nTraining\\ta\\tDNN\\tUsing\\tPlain\\tTensorFlow\\n-\\nUsing\\tthe\\tNeural\\nNetwork\\ntraining\\twith\\tTF.Learn\\n,', ',\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nunstable\\tgradients\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\nvanishing\\tand\\texploding\\tgradients\\n,\\t\\nTraining\\tDeep\\tNeural\\tNets\\n-\\nGradient\\tClipping\\nDeep\\tQ-Learning\\n,\\t\\nApproximate\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-\\nLearning\\nMs.\\tPac\\tMan\\texample\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ndeep\\tQ-network\\n,\\t\\nApproximate\\tQ-Learning\\ndeep\\tRNNs\\n,\\t\\nDeep\\tRNNs\\n-\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps', 'applying\\tdropout\\n,\\t\\nApplying\\tDropout\\ndistributing\\tacross\\tmultiple\\tGPUs\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\nlong\\tsequence\\tdifficulties\\n,\\t\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps\\ntruncated\\tbackpropagation\\tthrough\\ttime\\n,\\t\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\nSteps\\nDeepMind\\n,\\t\\nReinforcement\\tLearning\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n,\\nReinforcement\\tLearning\\n,\\t\\nApproximate\\tQ-Learning\\ndegrees\\tof\\tfreedom\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n,\\t\\nLearning\\tCurves\\ndenoising\\tautoencoders\\n,\\t\\nDenoising\\tAutoencoders\\n-\\nTensorFlow\\tImplementation\\ndense()\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nTying\\tWeights\\ndepth\\tconcat\\tlayer\\n,\\t\\nGoogLeNet\\ndepth\\tradius\\n,\\t\\nAlexNet\\ndepthwise_conv2d()\\n,\\t\\nResNet\\ndequeue()\\n,\\t\\nQueues\\tof\\ttuples\\ndequeue_many()\\n,\\t\\nQueues\\tof\\ttuples\\n,\\t\\nPaddingFifoQueue\\ndequeue_up_to()\\n,\\t\\nClosing\\ta\\tqueue\\n-\\nPaddingFifoQueue\\ndequeuing\\tdata\\n,\\t\\nDequeuing\\tdata\\ndescribe()\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\ndevice\\tblocks\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers', 'device()\\n,\\t\\nSimple\\tplacement\\ndimensionality\\treduction\\n,\\t\\nUnsupervised\\tlearning\\n,\\t\\nDimensionality\\tReduction\\n-\\nExercises\\n,\\nAutoencoders\\napproaches\\tto\\nManifold\\tLearning\\n,\\t\\nManifold\\tLearning', 'projection\\n,\\t\\nProjection\\n-\\nProjection\\nchoosing\\tthe\\tright\\tnumber\\tof\\tdimensions\\n,\\t\\nChoosing\\tthe\\tRight\\tNumber\\tof\\tDimensions\\ncurse\\tof\\tdimensionality\\n,\\t\\nDimensionality\\tReduction\\n-\\nThe\\tCurse\\tof\\tDimensionality\\nand\\tdata\\tvisualization\\n,\\t\\nDimensionality\\tReduction\\nIsomap\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\nLLE\\t(Locally\\tLinear\\tEmbedding)\\n,\\t\\nLLE\\n-\\nLLE\\nMultidimensional\\tScaling\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\n-\\nOther\\nDimensionality\\tReduction\\tTechniques\\nPCA\\t(Principal\\tComponent\\tAnalysis)\\n,\\t\\nPCA\\n-\\nRandomized\\tPCA\\nt-Distributed\\tStochastic\\tNeighbor\\tEmbedding\\t(t-SNE)\\n,\\t\\nOther\\tDimensionality\\tReduction\\nTechniques\\ndiscount\\trate\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\ndistributed\\tcomputing\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\ndistributed\\tsessions\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\n-\\nSharing\\tState\\nAcross\\tSessions\\tUsing\\tResource\\tContainers\\nDNNClassifier\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\ndrop()\\n,', 'drop()\\n,\\t\\nPrepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\tAlgorithms\\ndropconnect\\n,\\t\\nDropout\\ndropna()\\n,\\t\\nData\\tCleaning\\ndropout\\n,\\t\\nNumber\\tof\\tNeurons\\tper\\tHidden\\tLayer\\n,\\t\\nApplying\\tDropout\\ndropout\\trate\\n,\\t\\nDropout\\ndropout()\\n,\\t\\nDropout\\nDropoutWrapper\\n,\\t\\nApplying\\tDropout\\nDRY\\t(Don’t\\tRepeat\\tYourself)\\n,\\t\\nModularity', 'Dual\\tAveraging\\n,\\t\\nAdam\\tOptimization\\ndual\\tnumbers\\n,\\t\\nForward-Mode\\tAutodiff\\ndual\\tproblem\\n,\\t\\nThe\\tDual\\tProblem\\nduality\\n,\\t\\nSVM\\tDual\\tProblem\\ndying\\tReLUs\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\ndynamic\\tplacements\\n,\\t\\nDynamic\\tplacement\\tfunction\\ndynamic\\tplacer\\n,\\t\\nPlacing\\tOperations\\ton\\tDevices\\nDynamic\\tProgramming\\n,\\t\\nMarkov\\tDecision\\tProcesses\\ndynamic\\tunrolling\\tthrough\\ttime\\n,\\t\\nDynamic\\tUnrolling\\tThrough\\tTime\\ndynamic_rnn()\\n,\\t\\nDynamic\\tUnrolling\\tThrough\\tTime\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\nGPUs\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nE\\nearly\\tstopping\\n,\\t\\nEarly\\tStopping\\n-\\nEarly\\tStopping\\n,\\t\\nGradient\\tBoosting\\n,\\t\\nNumber\\tof\\tNeurons\\tper\\nHidden\\tLayer\\n,\\t\\nEarly\\tStopping\\nElastic\\tNet\\n,\\t\\nElastic\\tNet\\nembedded\\tdevice\\tblocks\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nEmbedded\\tReber\\tgrammars\\n,\\t\\nExercises\\nembeddings\\n,\\t\\nWord\\tEmbeddings\\n-\\nWord\\tEmbeddings\\nembedding_lookup()\\n,\\t\\nWord\\tEmbeddings\\nencoder\\n,\\t\\nEfficient\\tData\\tRepresentations\\nEncoder–Decoder\\n,\\t\\nInput\\tand\\tOutput\\tSequences', 'end-of-sequence\\t(EOS)\\ttoken\\n,\\t\\nHandling\\tVariable-Length\\tOutput\\tSequences\\nenergy\\tfunctions\\n,\\t\\nHopfield\\tNetworks\\nenqueuing\\tdata\\n,\\t\\nEnqueuing\\tdata', 'Ensemble\\tLearning\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n,\\t\\nEnsemble\\tMethods\\n,\\t\\nEnsemble\\nLearning\\tand\\tRandom\\tForests\\n-\\nExercises\\nbagging\\tand\\tpasting\\n,\\t\\nBagging\\tand\\tPasting\\n-\\nOut-of-Bag\\tEvaluation\\nboosting\\n,\\t\\nBoosting\\n-\\nGradient\\tBoosting\\nin-graph\\tversus\\tbetween-graph\\treplication\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\nReplication\\n-\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\nRandom\\tForests\\n,\\t\\nRandom\\tForests\\n-\\nFeature\\tImportance\\n(\\nsee\\talso\\n\\tRandom\\tForests)\\nrandom\\tpatches\\tand\\trandom\\tsubspaces\\n,\\t\\nRandom\\tPatches\\tand\\tRandom\\tSubspaces\\nstacking\\n,\\t\\nStacking\\n-\\nStacking\\nentropy\\timpurity\\tmeasure\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\nenvironments,\\tin\\treinforcement\\tlearning\\n,\\t\\nLearning\\tto\\tOptimize\\tRewards\\n-\\nEvaluating\\tActions:\\nThe\\tCredit\\tAssignment\\tProblem\\n,\\t\\nExploration\\tPolicies\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\nDeep\\tQ-Learning\\nepisodes\\t(in\\tRL)\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\nProblem\\n-\\nPolicy\\tGradients\\n,\\t\\nPolicy\\tGradients\\n-\\nPolicy\\tGradients\\n,', ',\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\nUsing\\tDeep\\tQ-Learning\\nepochs\\n,\\t\\nStochastic\\tGradient\\tDescent\\nε-insensitive\\n,\\t\\nSVM\\tRegression\\nequality\\tcontraints\\n,\\t\\nSVM\\tDual\\tProblem\\nerror\\tanalysis\\n,\\t\\nError\\tAnalysis\\n-\\nError\\tAnalysis\\nestimators\\n,\\t\\nData\\tCleaning\\nEuclidian\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\neval()\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\nevaluating\\tmodels\\n,\\t\\nTesting\\tand\\tValidating\\n-\\nTesting\\tand\\tValidating\\nexplained\\tvariance\\n,\\t\\nChoosing\\tthe\\tRight\\tNumber\\tof\\tDimensions', 'explained\\tvariance\\tratio\\n,\\t\\nExplained\\tVariance\\tRatio\\nexploding\\tgradients\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n(\\nsee\\talso\\n\\tgradients,\\tvanishing\\tand\\texploding)\\nexploration\\tpolicies\\n,\\t\\nExploration\\tPolicies\\nexponential\\tdecay\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\nexponential\\tlinear\\tunit\\t(ELU)\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\n-\\nNonsaturating\\tActivation\\nFunctions\\nexponential\\tscheduling\\n,\\t\\nLearning\\tRate\\tScheduling\\nExtra-Trees\\n,\\t\\nExtra-Trees\\nF\\nF-1\\tscore\\n,\\t\\nPrecision\\tand\\tRecall\\n-\\nPrecision\\tand\\tRecall\\nface-recognition\\n,\\t\\nMultilabel\\tClassification\\nfake\\tX\\tserver\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\nfalse\\tpositive\\trate\\t(FPR)\\n,\\t\\nThe\\tROC\\tCurve\\n-\\nThe\\tROC\\tCurve\\nfan-in\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\nfan-out\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\nfeature\\tdetection\\n,\\t\\nAutoencoders\\nfeature\\tengineering\\n,\\t\\nIrrelevant\\tFeatures\\nfeature\\textraction\\n,\\t\\nUnsupervised\\tlearning\\nfeature\\timportance\\n,\\t\\nFeature\\tImportance\\n-\\nFeature\\tImportance', 'feature\\tmaps\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\n,\\t\\nFilters\\n-\\nTensorFlow\\nImplementation\\n,\\t\\nResNet\\nfeature\\tscaling\\n,\\t\\nFeature\\tScaling\\nfeature\\tselection\\n,\\t\\nIrrelevant\\tFeatures\\n,\\t\\nGrid\\tSearch\\n,\\t\\nLasso\\tRegression\\n,\\t\\nFeature\\tImportance\\n,', 'Prepare\\tthe\\tData\\nfeature\\tspace\\n,\\t\\nKernel\\tPCA\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nfeature\\tvector\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n,\\t\\nLinear\\tRegression\\n,\\t\\nUnder\\tthe\\tHood\\n,\\nImplementing\\tGradient\\tDescent\\nfeatures\\n,\\t\\nSupervised\\tlearning\\nFeatureUnion\\n,\\t\\nTransformation\\tPipelines\\nfeedforward\\tneural\\tnetwork\\t(FNN)\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nfeed_dict\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\nFIFOQueue\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\n,\\t\\nRandomShuffleQueue\\nfillna()\\n,\\t\\nData\\tCleaning\\nfirst-in\\tfirst-out\\t(FIFO)\\tqueues\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\nfirst-order\\tpartial\\tderivatives\\t(Jacobians)\\n,\\t\\nAdam\\tOptimization\\nfit()\\n,\\t\\nData\\tCleaning\\n,\\t\\nTransformation\\tPipelines\\n,\\t\\nIncremental\\tPCA\\nfitness\\tfunction\\n,\\t\\nModel-based\\tlearning\\nfit_inverse_transform=\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nfit_transform()\\n,\\t\\nData\\tCleaning\\n,\\t\\nTransformation\\tPipelines\\nfolds\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n,\\t\\nMNIST\\n,', 'MNIST\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-\\nValidation\\n-\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nFollow\\tThe\\tRegularized\\tLeader\\t(FTRL)\\n,\\t\\nAdam\\tOptimization\\nforget\\tgate\\n,\\t\\nLSTM\\tCell\\nforward-mode\\tautodiff\\n,\\t\\nForward-Mode\\tAutodiff\\n-\\nForward-Mode\\tAutodiff\\nframing\\ta\\tproblem\\n,\\t\\nFrame\\tthe\\tProblem\\n-\\nFrame\\tthe\\tProblem\\nfrozen\\tlayers\\n,\\t\\nFreezing\\tthe\\tLower\\tLayers\\n-\\nCaching\\tthe\\tFrozen\\tLayers', 'functools.partial()\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n,\\t\\nTensorFlow\\nImplementation\\n,\\t\\nVariational\\tAutoencoders\\nG\\ngame\\tplay\\n\\t(\\nsee\\n\\treinforcement\\tlearning)\\ngamma\\tvalue\\n,\\t\\nGaussian\\tRBF\\tKernel\\ngate\\tcontrollers\\n,\\t\\nLSTM\\tCell\\nGaussian\\tdistribution\\n,\\t\\nVariational\\tAutoencoders\\n,\\t\\nGenerating\\tDigits\\nGaussian\\tRBF\\n,\\t\\nAdding\\tSimilarity\\tFeatures\\nGaussian\\tRBF\\tkernel\\n,\\t\\nGaussian\\tRBF\\tKernel\\n-\\nGaussian\\tRBF\\tKernel\\n,\\t\\nKernelized\\tSVM\\ngeneralization\\terror\\n,\\t\\nTesting\\tand\\tValidating\\ngeneralized\\tLagrangian\\n,\\t\\nSVM\\tDual\\tProblem\\n-\\nSVM\\tDual\\tProblem\\ngenerative\\tautoencoders\\n,\\t\\nVariational\\tAutoencoders\\ngenerative\\tmodels\\n,\\t\\nAutoencoders\\n,\\t\\nBoltzmann\\tMachines\\ngenetic\\talgorithms\\n,\\t\\nPolicy\\tSearch\\ngeodesic\\tdistance\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\nget_variable()\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\nGINI\\timpurity\\n,\\t\\nMaking\\tPredictions\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\nglobal\\taverage\\tpooling\\n,\\t\\nGoogLeNet\\nglobal_step\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning', 'global_variables_initializer()\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\nGlorot\\tinitialization\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n-\\nXavier\\tand\\tHe\\tInitialization\\nGoogle\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nGoogle\\tImages\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks', 'Google\\tPhotos\\n,\\t\\nSemisupervised\\tlearning\\nGoogleNet\\tarchitecture\\n,\\t\\nGoogLeNet\\n-\\nGoogLeNet\\ngpu_options.per_process_gpu_memory_fraction\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM\\ngradient\\tascent\\n,\\t\\nPolicy\\tSearch\\nGradient\\tBoosted\\tRegression\\tTrees\\t(GBRT)\\n,\\t\\nGradient\\tBoosting\\nGradient\\tBoosting\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\nGradient\\tDescent\\t(GD)\\n,\\t\\nTraining\\tModels\\n,\\t\\nGradient\\tDescent\\n-\\nMini-batch\\tGradient\\tDescent\\n,\\nOnline\\tSVMs\\n,\\t\\nTraining\\tDeep\\tNeural\\tNets\\n,\\t\\nMomentum\\tOptimization\\n,\\t\\nAdaGrad\\nalgorithm\\tcomparisons\\n,\\t\\nMini-batch\\tGradient\\tDescent\\n-\\nMini-batch\\tGradient\\tDescent\\nautomatically\\tcomputing\\tgradients\\n,\\t\\nUsing\\tautodiff\\n-\\nUsing\\tautodiff\\nBatch\\tGD\\n,\\t\\nBatch\\tGradient\\tDescent\\n-\\nBatch\\tGradient\\tDescent\\n,\\t\\nLasso\\tRegression\\ndefining\\n,\\t\\nGradient\\tDescent\\nlocal\\tminimum\\tversus\\tglobal\\tminimum\\n,\\t\\nGradient\\tDescent\\nmanually\\tcomputing\\tgradients\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\nMini-batch\\tGD\\n,\\t\\nMini-batch\\tGradient\\tDescent\\n-\\nMini-batch\\tGradient\\tDescent\\n,\\t\\nFeeding\\nData\\tto\\tthe\\tTraining\\tAlgorithm\\n-', '-\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\noptimizer\\n,\\t\\nUsing\\tan\\tOptimizer\\nStochastic\\tGD\\n,\\t\\nStochastic\\tGradient\\tDescent\\n-\\nStochastic\\tGradient\\tDescent\\n,\\t\\nSoft\\tMargin\\nClassification\\nwith\\tTensorFlow\\n,\\t\\nImplementing\\tGradient\\tDescent\\n-\\nUsing\\tan\\tOptimizer\\nGradient\\tTree\\tBoosting\\n,\\t\\nGradient\\tBoosting\\nGradientDescentOptimizer\\n,\\t\\nConstruction\\tPhase\\ngradients()\\n,\\t\\nUsing\\tautodiff\\ngradients,\\tvanishing\\tand\\texploding\\n,\\t\\nTraining\\tDeep\\tNeural\\tNets\\n-\\nGradient\\tClipping\\n,\\t\\nThe\\nDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps', 'Batch\\tNormalization\\n,\\t\\nBatch\\tNormalization\\n-\\nImplementing\\tBatch\\tNormalization\\twith\\nTensorFlow\\nGlorot\\tand\\tHe\\tinitialization\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n-\\nXavier\\tand\\tHe\\nInitialization\\ngradient\\tclipping\\n,\\t\\nGradient\\tClipping\\nnonsaturating\\tactivation\\tfunctions\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\n-\\nNonsaturating\\nActivation\\tFunctions\\ngraphviz\\n,\\t\\nTraining\\tand\\tVisualizing\\ta\\tDecision\\tTree\\ngreedy\\talgorithm\\n,\\t\\nThe\\tCART\\tTraining\\tAlgorithm\\ngrid\\tsearch\\n,\\t\\nFine-Tune\\tYour\\tModel\\n-\\nGrid\\tSearch\\n,\\t\\nPolynomial\\tKernel\\ngroup()\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nGRU\\t(Gated\\tRecurrent\\tUnit)\\tcell\\n,\\t\\nGRU\\tCell\\n-\\nGRU\\tCell\\nH\\nhailstone\\tsequence\\n,\\t\\nEfficient\\tData\\tRepresentations\\nhard\\tmargin\\tclassification\\n,\\t\\nSoft\\tMargin\\tClassification\\n-\\nSoft\\tMargin\\tClassification\\nhard\\tvoting\\tclassifiers\\n,\\t\\nVoting\\tClassifiers\\n-\\nVoting\\tClassifiers\\nharmonic\\tmean\\n,\\t\\nPrecision\\tand\\tRecall\\nHe\\tinitialization\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n-\\nXavier\\tand\\tHe\\tInitialization\\nHeaviside\\tstep\\tfunction\\n,', \",\\t\\nThe\\tPerceptron\\nHebb's\\trule\\n,\\t\\nThe\\tPerceptron\\n,\\t\\nHopfield\\tNetworks\\nHebbian\\tlearning\\n,\\t\\nThe\\tPerceptron\\nhidden\\tlayers\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nhierarchical\\tclustering\\n,\\t\\nUnsupervised\\tlearning\\nhinge\\tloss\\tfunction\\n,\\t\\nOnline\\tSVMs\", 'histograms\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\n-\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\nStructure\\nhold-out\\tsets\\n,\\t\\nStacking\\n(\\nsee\\talso\\n\\tblenders)\\nHopfield\\tNetworks\\n,\\t\\nHopfield\\tNetworks\\n-\\nHopfield\\tNetworks\\nhyperbolic\\ttangent\\t(htan\\tactivation\\tfunction)\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n,\\nActivation\\tFunctions\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\n,\\nRecurrent\\tNeurons\\nhyperparameters\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n,\\t\\nCustom\\tTransformers\\n,\\t\\nGrid\\tSearch\\n-\\nGrid\\nSearch\\n,\\t\\nEvaluate\\tYour\\tSystem\\ton\\tthe\\tTest\\tSet\\n,\\t\\nGradient\\tDescent\\n,\\t\\nPolynomial\\tKernel\\n,\\nComputational\\tComplexity\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\n(\\nsee\\talso\\n\\tneural\\tnetwork\\thyperparameters)\\nhyperplane\\n,\\t\\nDecision\\tFunction\\tand\\tPredictions\\n,\\t\\nManifold\\tLearning\\n-\\nPCA\\n,\\t\\nProjecting\\tDown\\tto\\td\\nDimensions\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\nhypothesis\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nmanifold\\n,\\t\\nManifold\\tLearning\\nhypothesis\\tboosting\\n\\t(\\nsee\\n\\tboosting)\\nhypothesis\\tfunction\\n,', ',\\t\\nLinear\\tRegression\\nhypothesis,\\tnull\\n,\\t\\nRegularization\\tHyperparameters\\nI\\nidentity\\tmatrix\\n,\\t\\nRidge\\tRegression\\n,\\t\\nQuadratic\\tProgramming\\nILSVRC\\t\\nImageNet\\t\\nchallenge\\n,\\t\\nCNN\\tArchitectures\\nimage\\tclassification\\n,\\t\\nCNN\\tArchitectures\\nimpurity\\tmeasures\\n,\\t\\nMaking\\tPredictions\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\nin-graph\\treplication\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\ninception\\tmodules\\n,\\t\\nGoogLeNet', 'Inception-v4\\n,\\t\\nResNet\\nincremental\\tlearning\\n,\\t\\nOnline\\tlearning\\n,\\t\\nIncremental\\tPCA\\ninequality\\tconstraints\\n,\\t\\nSVM\\tDual\\tProblem\\ninference\\n,\\t\\nModel-based\\tlearning\\n,\\t\\nExercises\\n,\\t\\nMemory\\tRequirements\\n,\\t\\nAn\\tEncoder–Decoder\\nNetwork\\tfor\\tMachine\\tTranslation\\ninfo()\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\ninformation\\tgain\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\ninformation\\ttheory\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\ninit\\t\\nnode\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\ninput\\tgate\\n,\\t\\nLSTM\\tCell\\ninput\\tneurons\\n,\\t\\nThe\\tPerceptron\\ninput_keep_prob\\n,\\t\\nApplying\\tDropout\\ninstance-based\\tlearning\\n,\\t\\nInstance-based\\tlearning\\n,\\t\\nModel-based\\tlearning\\nInteractiveSession\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\nintercept\\tterm\\n,\\t\\nLinear\\tRegression\\nInternal\\tCovariate\\t\\nShift\\t\\nproblem\\n,\\t\\nBatch\\tNormalization\\ninter_op_parallelism_threads\\n,\\t\\nParallel\\tExecution\\nintra_op_parallelism_threads\\n,\\t\\nParallel\\tExecution\\ninverse_transform()\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nin_top_k()\\n,\\t\\nConstruction\\tPhase\\nirreducible\\terror\\n,', ',\\t\\nLearning\\tCurves\\nisolated\\tenvironment\\n,\\t\\nCreate\\tthe\\tWorkspace\\n-\\nCreate\\tthe\\tWorkspace\\nIsomap\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques', 'J\\njobs\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\njoin()\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\nand\\ta\\tQueueRunner\\nJupyter\\n,\\t\\nCreate\\tthe\\tWorkspace\\n,\\t\\nCreate\\tthe\\tWorkspace\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\nStructure\\nK\\nK-fold\\tcross-validation\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n-\\nBetter\\tEvaluation\\tUsing\\nCross-Validation\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nk-Nearest\\tNeighbors\\n,\\t\\nModel-based\\tlearning\\n,\\t\\nMultilabel\\tClassification\\nKarush–Kuhn–Tucker\\t(KKT)\\tconditions\\n,\\t\\nSVM\\tDual\\tProblem\\nkeep\\tprobability\\n,\\t\\nDropout\\nKeras\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nKernel\\tPCA\\t(kPCA)\\n,\\t\\nKernel\\tPCA\\n-\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nkernel\\ttrick\\n,\\t\\nPolynomial\\tKernel\\n,\\t\\nGaussian\\tRBF\\tKernel\\n,\\t\\nThe\\tDual\\tProblem\\n-\\nKernelized\\tSVM\\n,\\nKernel\\tPCA\\nkernelized\\tSVM\\n,\\t\\nKernelized\\tSVM\\n-\\nKernelized\\tSVM\\nkernels\\n,\\t\\nPolynomial\\tKernel\\n-\\nGaussian\\tRBF\\tKernel\\n,\\t\\nOperations\\tand\\tkernels\\nKullback–Leibler\\tdivergence\\n,\\t\\nSoftmax\\tRegression\\n,', ',\\t\\nSparse\\tAutoencoders\\nL\\nl1_l2_regularizer()\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\nLabelBinarizer\\n,\\t\\nTransformation\\tPipelines\\nlabels\\n,\\t\\nSupervised\\tlearning\\n,\\t\\nFrame\\tthe\\tProblem\\nLagrange\\tfunction\\n,\\t\\nSVM\\tDual\\tProblem\\n-\\nSVM\\tDual\\tProblem\\nLagrange\\tmultiplier\\n,\\t\\nSVM\\tDual\\tProblem', 'landmarks\\n,\\t\\nAdding\\tSimilarity\\tFeatures\\n-\\nAdding\\tSimilarity\\tFeatures\\nlarge\\tmargin\\tclassification\\n,\\t\\nLinear\\tSVM\\tClassification\\n-\\nLinear\\tSVM\\tClassification\\nLasso\\tRegression\\n,\\t\\nLasso\\tRegression\\n-\\nLasso\\tRegression\\nlatent\\tloss\\n,\\t\\nVariational\\tAutoencoders\\nlatent\\tspace\\n,\\t\\nVariational\\tAutoencoders\\nlaw\\tof\\tlarge\\tnumbers\\n,\\t\\nVoting\\tClassifiers\\nleaky\\tReLU\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\nlearning\\trate\\n,\\t\\nOnline\\tlearning\\n,\\t\\nGradient\\tDescent\\n,\\t\\nBatch\\tGradient\\tDescent\\n-\\nStochastic\\tGradient\\nDescent\\nlearning\\trate\\tscheduling\\n,\\t\\nStochastic\\tGradient\\tDescent\\n,\\t\\nLearning\\tRate\\tScheduling\\n-\\nLearning\\nRate\\tScheduling\\nLeNet-5\\tarchitecture\\n,\\t\\nThe\\tArchitecture\\tof\\tthe\\tVisual\\tCortex\\n,\\t\\nLeNet-5\\n-\\nLeNet-5\\nLevenshtein\\tdistance\\n,\\t\\nGaussian\\tRBF\\tKernel\\nliblinear\\tlibrary\\n,\\t\\nComputational\\tComplexity\\nlibsvm\\tlibrary\\n,\\t\\nComputational\\tComplexity\\nLinear\\tDiscriminant\\tAnalysis\\t(LDA)\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\nlinear\\tmodels\\nearly\\tstopping\\n,\\t\\nEarly\\tStopping\\n-\\nEarly\\tStopping\\nElastic\\tNet\\n,\\t\\nElastic\\tNet', 'Lasso\\tRegression\\n,\\t\\nLasso\\tRegression\\n-\\nLasso\\tRegression\\nLinear\\tRegression\\n\\t(\\nsee\\n\\tLinear\\tRegression)\\nregression\\n\\t(\\nsee\\n\\tLinear\\tRegression)\\nRidge\\tRegression\\n,\\t\\nRidge\\tRegression\\n-\\nRidge\\tRegression\\n,\\t\\nElastic\\tNet\\nSVM\\n,\\t\\nLinear\\tSVM\\tClassification\\n-\\nSoft\\tMargin\\tClassification', 'Linear\\tRegression\\n,\\t\\nModel-based\\tlearning\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n,\\nTraining\\tModels\\n-\\nMini-batch\\tGradient\\tDescent\\n,\\t\\nElastic\\tNet\\ncomputational\\tcomplexity\\n,\\t\\nComputational\\tComplexity\\nGradient\\tDescent\\tin\\n,\\t\\nGradient\\tDescent\\n-\\nMini-batch\\tGradient\\tDescent\\nlearning\\tcurves\\tin\\n,\\t\\nLearning\\tCurves\\n-\\nLearning\\tCurves\\nNormal\\tEquation\\n,\\t\\nThe\\tNormal\\tEquation\\n-\\nComputational\\tComplexity\\nregularizing\\tmodels\\n\\t(\\nsee\\n\\tregularization)\\nusing\\tStochastic\\tGradient\\tDescent\\t(SGD)\\n,\\t\\nStochastic\\tGradient\\tDescent\\nwith\\tTensorFlow\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n-\\nLinear\\tRegression\\twith\\nTensorFlow\\nlinear\\tSVM\\tclassification\\n,\\t\\nLinear\\tSVM\\tClassification\\n-\\nSoft\\tMargin\\tClassification\\nlinear\\tthreshold\\tunits\\t(LTUs)\\n,\\t\\nThe\\tPerceptron\\nLipschitz\\tcontinuous\\n,\\t\\nGradient\\tDescent\\nLLE\\t(Locally\\tLinear\\tEmbedding)\\n,\\t\\nLLE\\n-\\nLLE\\nload_sample_images()\\n,\\t\\nTensorFlow\\tImplementation\\nlocal\\treceptive\\tfield\\n,\\t\\nThe\\tArchitecture\\tof\\tthe\\tVisual\\tCortex\\nlocal\\tresponse\\tnormalization\\n,\\t\\nAlexNet\\nlocal\\tsessions\\n,', ',\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\nlocation\\tinvariance\\n,\\t\\nPooling\\tLayer\\nlog\\tloss\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\nlogging\\tplacements\\n,\\t\\nLogging\\tplacements\\n-\\nLogging\\tplacements\\nlogistic\\tfunction\\n,\\t\\nEstimating\\tProbabilities\\nLogistic\\tRegression\\n,\\t\\nSupervised\\tlearning\\n,\\t\\nLogistic\\tRegression\\n-\\nSoftmax\\tRegression\\ndecision\\tboundaries\\n,\\t\\nDecision\\tBoundaries\\n-\\nDecision\\tBoundaries', 'estimating\\tprobablities\\n,\\t\\nEstimating\\tProbabilities\\n-\\nEstimating\\tProbabilities\\nSoftmax\\tRegression\\tmodel\\n,\\t\\nSoftmax\\tRegression\\n-\\nSoftmax\\tRegression\\ntraining\\tand\\tcost\\tfunction\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\n-\\nTraining\\tand\\tCost\\tFunction\\nlog_device_placement\\n,\\t\\nLogging\\tplacements\\nLSTM\\t(Long\\tShort-Term\\tMemory)\\tcell\\n,\\t\\nLSTM\\tCell\\n-\\nGRU\\tCell', 'M\\nmachine\\tcontrol\\n\\t(\\nsee\\n\\treinforcement\\tlearning)\\nMachine\\tLearning\\nlarge-scale\\tprojects\\n\\t(\\nsee\\n\\tTensorFlow)\\nnotations\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n-\\nSelect\\ta\\tPerformance\\tMeasure\\nprocess\\texample\\n,\\t\\nEnd-to-End\\tMachine\\tLearning\\tProject\\n-\\nExercises\\nproject\\tchecklist\\n,\\t\\nLook\\tat\\tthe\\tBig\\tPicture\\n,\\t\\nMachine\\tLearning\\tProject\\tChecklist\\n-\\nLaunch!\\nresources\\ton\\n,\\t\\nOther\\tResources\\n-\\nOther\\tResources\\nuses\\tfor\\n,\\t\\nMachine\\tLearning\\tin\\tYour\\tProjects\\n-\\nMachine\\tLearning\\tin\\tYour\\tProjects\\nMachine\\tLearning\\tbasics\\nattributes\\n,\\t\\nSupervised\\tlearning\\nchallenges\\n,\\t\\nMain\\tChallenges\\tof\\tMachine\\tLearning\\n-\\nStepping\\tBack\\nalgorithm\\tproblems\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n-\\nUnderfitting\\tthe\\tTraining\\nData\\ntraining\\tdata\\tproblems\\n,\\t\\nPoor-Quality\\tData\\ndefinition\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\nfeatures\\n,\\t\\nSupervised\\tlearning\\noverview\\n,\\t\\nThe\\tMachine\\tLearning\\tLandscape\\nreasons\\tfor\\tusing\\n,\\t\\nWhy\\tUse\\tMachine\\tLearning?\\n-\\nWhy\\tUse\\tMachine\\tLearning?\\nspam\\tfilter\\texample\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\n-\\nWhy\\tUse\\tMachine\\tLearning?\\nsummary', 'summary\\n,\\t\\nStepping\\tBack\\ntesting\\tand\\tvalidating\\n,\\t\\nTesting\\tand\\tValidating\\n-\\nTesting\\tand\\tValidating\\ntypes\\tof\\tsystems\\n,\\t\\nTypes\\tof\\tMachine\\tLearning\\tSystems\\n-\\nModel-based\\tlearning\\nbatch\\tand\\tonline\\tlearning\\n,\\t\\nBatch\\tand\\tOnline\\tLearning\\n-\\nOnline\\tlearning', 'instance-based\\tversus\\tmodel-based\\tlearning\\n,\\t\\nInstance-Based\\tVersus\\tModel-\\nBased\\tLearning\\n-\\nModel-based\\tlearning\\nsupervised/unsupervised\\tlearning\\n,\\t\\nSupervised/Unsupervised\\tLearning\\n-\\nReinforcement\\tLearning\\nworkflow\\texample\\n,\\t\\nModel-based\\tlearning\\n-\\nModel-based\\tlearning\\nmachine\\ttranslation\\n\\t(\\nsee\\n\\tnatural\\tlanguage\\tprocessing\\t(NLP))\\nmake()\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\nManhattan\\tnorm\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nmanifold\\tassumption/hypothesis\\n,\\t\\nManifold\\tLearning\\nManifold\\tLearning\\n,\\t\\nManifold\\tLearning\\n,\\t\\nLLE\\n(\\nsee\\talso\\n\\tLLE\\t(Locally\\tLinear\\tEmbedding)\\nMapReduce\\n,\\t\\nFrame\\tthe\\tProblem\\nmargin\\tviolations\\n,\\t\\nSoft\\tMargin\\tClassification\\nMarkov\\tchains\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nMarkov\\tdecision\\tprocesses\\n,\\t\\nMarkov\\tDecision\\tProcesses\\n-\\nMarkov\\tDecision\\tProcesses\\nmaster\\tservice\\n,\\t\\nThe\\tMaster\\tand\\tWorker\\tServices\\nMatplotlib\\n,\\t\\nCreate\\tthe\\tWorkspace\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\n,\\t\\nThe\\tROC\\nCurve\\n,\\t\\nError\\tAnalysis\\nmax\\tmargin\\tlearning\\n,\\t\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\nmax\\tpooling\\tlayer', ',\\t\\nPooling\\tLayer\\nmax-norm\\tregularization\\n,\\t\\nMax-Norm\\tRegularization\\n-\\nMax-Norm\\tRegularization\\nmax_norm()\\n,\\t\\nMax-Norm\\tRegularization\\nmax_norm_regularizer()\\n,\\t\\nMax-Norm\\tRegularization\\nmax_pool()\\n,\\t\\nPooling\\tLayer', \"Mean\\tAbsolute\\tError\\t(MAE)\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n-\\nSelect\\ta\\tPerformance\\tMeasure\\nmean\\tcoding\\n,\\t\\nVariational\\tAutoencoders\\nMean\\tSquare\\tError\\t(MSE)\\n,\\t\\nLinear\\tRegression\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nSparse\\nAutoencoders\\nmeasure\\tof\\tsimilarity\\n,\\t\\nInstance-based\\tlearning\\nmemmap\\n,\\t\\nIncremental\\tPCA\\nmemory\\tcells\\n,\\t\\nModel\\tParallelism\\n,\\t\\nMemory\\tCells\\nMercer's\\ttheorem\\n,\\t\\nKernelized\\tSVM\\nmeta\\tlearner\\n\\t(\\nsee\\n\\tblending)\\nmin-max\\tscaling\\n,\\t\\nFeature\\tScaling\\nMini-batch\\tGradient\\tDescent\\n,\\t\\nMini-batch\\tGradient\\tDescent\\n-\\nMini-batch\\tGradient\\tDescent\\n,\\nTraining\\tand\\tCost\\tFunction\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\n-\\nFeeding\\tData\\tto\\tthe\\nTraining\\tAlgorithm\\nmini-batches\\n,\\t\\nOnline\\tlearning\\nminimize()\\n,\\t\\nGradient\\tClipping\\n,\\t\\nFreezing\\tthe\\tLower\\tLayers\\n,\\t\\nPolicy\\tGradients\\n,\\t\\nLearning\\tto\\tPlay\\nMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nmin_after_dequeue\\n,\\t\\nRandomShuffleQueue\\nMNIST\\tdataset\\n,\\t\\nMNIST\\n-\\nMNIST\\nmodel\\tparallelism\\n,\\t\\nModel\\tParallelism\\n-\\nModel\\tParallelism\\nmodel\\tparameters\\n,\\t\\nGradient\\tDescent\\n,\", ',\\t\\nBatch\\tGradient\\tDescent\\n,\\t\\nEarly\\tStopping\\n,\\t\\nUnder\\tthe\\nHood\\n,\\t\\nQuadratic\\tProgramming\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\nConstruction\\tPhase\\n,\\t\\nTraining\\tRNNs\\ndefining\\n,\\t\\nModel-based\\tlearning\\nmodel\\tselection\\n,\\t\\nModel-based\\tlearning\\nmodel\\tzoos\\n,\\t\\nModel\\tZoos', 'model-based\\tlearning\\n,\\t\\nModel-based\\tlearning\\n-\\nModel-based\\tlearning\\nmodels\\nanalyzing\\n,\\t\\nAnalyze\\tthe\\tBest\\tModels\\tand\\tTheir\\tErrors\\n-\\nAnalyze\\tthe\\tBest\\tModels\\tand\\nTheir\\tErrors\\nevaluating\\ton\\ttest\\tset\\n,\\t\\nEvaluate\\tYour\\tSystem\\ton\\tthe\\tTest\\tSet\\n-\\nEvaluate\\tYour\\tSystem\\ton\\nthe\\tTest\\tSet\\nmoments\\n,\\t\\nAdam\\tOptimization\\nMomentum\\toptimization\\n,\\t\\nMomentum\\tOptimization\\n-\\nMomentum\\tOptimization\\nMonte\\tCarlo\\ttree\\tsearch\\n,\\t\\nPolicy\\tGradients\\nMulti-Layer\\tPerceptrons\\t(MLP)\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n,\\t\\nThe\\tPerceptron\\n-\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n,\\t\\nNeural\\tNetwork\\tPolicies\\ntraining\\twith\\tTF.Learn\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nmulticlass\\tclassifiers\\n,\\t\\nMulticlass\\tClassification\\n-\\nMulticlass\\tClassification\\nMultidimensional\\tScaling\\t(MDS)\\n,\\t\\nOther\\tDimensionality\\tReduction\\tTechniques\\nmultilabel\\tclassifiers\\n,\\t\\nMultilabel\\tClassification\\n-\\nMultilabel\\tClassification\\nMultinomial\\tLogistic\\tRegression\\n\\t(\\nsee\\n\\tSoftmax\\tRegression)\\nmultinomial()\\n,\\t\\nNeural\\tNetwork\\tPolicies', 'multioutput\\tclassifiers\\n,\\t\\nMultioutput\\tClassification\\n-\\nMultioutput\\tClassification\\nMultiRNNCell\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\nmultithreaded\\treaders\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\nmultivariate\\tregression\\n,\\t\\nFrame\\tthe\\tProblem\\nN\\nnaive\\tBayes\\tclassifiers\\n,\\t\\nMulticlass\\tClassification\\nname\\tscopes\\n,\\t\\nName\\tScopes', 'natural\\tlanguage\\tprocessing\\t(NLP)\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\n,\\t\\nNatural\\tLanguage\\nProcessing\\n-\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nencoder-decoder\\tnetwork\\tfor\\tmachine\\ttranslation\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\nMachine\\tTranslation\\n-\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nTensorFlow\\ttutorials\\n,\\t\\nNatural\\tLanguage\\tProcessing\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\nMachine\\tTranslation\\nword\\tembeddings\\n,\\t\\nWord\\tEmbeddings\\n-\\nWord\\tEmbeddings\\nNesterov\\tAccelerated\\tGradient\\t(NAG)\\n,\\t\\nNesterov\\tAccelerated\\tGradient\\n-\\nNesterov\\tAccelerated\\nGradient\\nNesterov\\tmomentum\\toptimization\\n,\\t\\nNesterov\\tAccelerated\\tGradient\\n-\\nNesterov\\tAccelerated\\nGradient\\nnetwork\\ttopology\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\nneural\\tnetwork\\thyperparameters\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\n-\\nActivation\\nFunctions\\nactivation\\tfunctions\\n,\\t\\nActivation\\tFunctions\\nneurons\\tper\\thidden\\tlayer\\n,\\t\\nNumber\\tof\\tNeurons\\tper\\tHidden\\tLayer\\nnumber\\tof\\thidden\\tlayers\\n,\\t\\nNumber\\tof\\tHidden\\tLayers\\n-', '-\\nNumber\\tof\\tHidden\\tLayers\\nneural\\tnetwork\\tpolicies\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n-\\nNeural\\tNetwork\\tPolicies\\nneurons\\nbiological\\n,\\t\\nFrom\\tBiological\\tto\\tArtificial\\tNeurons\\n-\\nBiological\\tNeurons\\nlogical\\tcomputations\\twith\\n,\\t\\nLogical\\tComputations\\twith\\tNeurons\\nneuron_layer()\\n,\\t\\nConstruction\\tPhase\\nnext_batch()\\n,\\t\\nExecution\\tPhase\\nNo\\tFree\\tLunch\\ttheorem\\n,\\t\\nTesting\\tand\\tValidating\\nnode\\tedges\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\nnonlinear\\tdimensionality\\treduction\\t(NLDR)\\n,\\t\\nLLE', '(\\nsee\\talso\\n\\tKernel\\tPCA;\\tLLE\\t(Locally\\tLinear\\tEmbedding))\\nnonlinear\\tSVM\\tclassification\\n,\\t\\nNonlinear\\tSVM\\tClassification\\n-\\nComputational\\tComplexity\\ncomputational\\tcomplexity\\n,\\t\\nComputational\\tComplexity\\nGaussian\\tRBF\\tkernel\\n,\\t\\nGaussian\\tRBF\\tKernel\\n-\\nGaussian\\tRBF\\tKernel\\nwith\\tpolynomial\\tfeatures\\n,\\t\\nNonlinear\\tSVM\\tClassification\\n-\\nPolynomial\\tKernel\\npolynomial\\tkernel\\n,\\t\\nPolynomial\\tKernel\\n-\\nPolynomial\\tKernel\\nsimilarity\\tfeatures,\\tadding\\n,\\t\\nAdding\\tSimilarity\\tFeatures\\n-\\nAdding\\tSimilarity\\tFeatures\\nnonparametric\\tmodels\\n,\\t\\nRegularization\\tHyperparameters\\nnonresponse\\tbias\\n,\\t\\nNonrepresentative\\tTraining\\tData\\nnonsaturating\\tactivation\\tfunctions\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\n-\\nNonsaturating\\nActivation\\tFunctions\\nNormal\\tEquation\\n,\\t\\nThe\\tNormal\\tEquation\\n-\\nComputational\\tComplexity\\nnormalization\\n,\\t\\nFeature\\tScaling\\nnormalized\\texponential\\n,\\t\\nSoftmax\\tRegression\\nnorms\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\nnotations\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n-\\nSelect\\ta\\tPerformance\\tMeasure\\nNP-Complete\\tproblems\\n,', ',\\t\\nThe\\tCART\\tTraining\\tAlgorithm\\nnull\\thypothesis\\n,\\t\\nRegularization\\tHyperparameters\\nnumerical\\tdifferentiation\\n,\\t\\nNumerical\\tDifferentiation\\nNumPy\\n,\\t\\nCreate\\tthe\\tWorkspace\\nNumPy\\tarrays\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nNVidia\\tCompute\\tCapability\\n,\\t\\nInstallation\\nnvidia-smi\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM', 'n_components\\n,\\t\\nChoosing\\tthe\\tRight\\tNumber\\tof\\tDimensions\\nO\\nobservation\\tspace\\n,\\t\\nNeural\\tNetwork\\tPolicies\\noff-policy\\talgorithm\\n,\\t\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\noffline\\tlearning\\n,\\t\\nBatch\\tlearning\\none-hot\\tencoding\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\none-versus-all\\t\\n(OvA)\\t\\nstrategy\\n,\\t\\nMulticlass\\tClassification\\n,\\t\\nSoftmax\\tRegression\\n,\\t\\nExercises\\none-versus-one\\t(OvO)\\tstrategy\\n,\\t\\nMulticlass\\tClassification\\nonline\\tlearning\\n,\\t\\nOnline\\tlearning\\n-\\nOnline\\tlearning\\nonline\\tSVMs\\n,\\t\\nOnline\\tSVMs\\n-\\nOnline\\tSVMs\\nOpenAI\\tGym\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\n-\\nIntroduction\\tto\\tOpenAI\\tGym\\noperation_timeout_in_ms\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\nOptical\\tCharacter\\tRecognition\\t(OCR)\\n,\\t\\nThe\\tMachine\\tLearning\\tLandscape\\noptimal\\tstate\\tvalue\\n,\\t\\nMarkov\\tDecision\\tProcesses\\noptimizers\\n,\\t\\nFaster\\tOptimizers\\n-\\nLearning\\tRate\\tScheduling\\nAdaGrad\\n,\\t\\nAdaGrad\\n-\\nAdaGrad\\nAdam\\toptimization\\n,\\t\\nAdam\\tOptimization\\n-\\nAdam\\tOptimization\\n,\\t\\nAdam\\tOptimization\\nGradient\\tDescent\\n\\t(\\nsee\\n\\tGradient\\tDescent\\toptimizer)', 'learning\\trate\\tscheduling\\n,\\t\\nLearning\\tRate\\tScheduling\\n-\\nLearning\\tRate\\tScheduling\\nMomentum\\toptimization\\n,\\t\\nMomentum\\tOptimization\\n-\\nMomentum\\tOptimization\\nNesterov\\tAccelerated\\tGradient\\t(NAG)\\n,\\t\\nNesterov\\tAccelerated\\tGradient\\n-\\nNesterov\\nAccelerated\\tGradient\\nRMSProp\\n,\\t\\nRMSProp\\nout-of-bag\\tevaluation\\n,\\t\\nOut-of-Bag\\tEvaluation\\n-\\nOut-of-Bag\\tEvaluation', 'out-of-core\\tlearning\\n,\\t\\nOnline\\tlearning\\nout-of-memory\\t(OOM)\\terrors\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\nout-of-sample\\terror\\n,\\t\\nTesting\\tand\\tValidating\\nOutOfRangeError\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\treaders\\nusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\noutput\\tgate\\n,\\t\\nLSTM\\tCell\\noutput\\tlayer\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nOutputProjectionWrapper\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n-\\nTraining\\tto\\tPredict\\tTime\\tSeries\\noutput_keep_prob\\n,\\t\\nApplying\\tDropout\\novercomplete\\tautoencoder\\n,\\t\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\noverfitting\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n-\\nOverfitting\\tthe\\tTraining\\tData\\n,\\t\\nCreate\\ta\\tTest\\tSet\\n,\\nSoft\\tMargin\\tClassification\\n,\\t\\nGaussian\\tRBF\\tKernel\\n,\\t\\nRegularization\\tHyperparameters\\n,\\nRegression\\n,\\t\\nNumber\\tof\\tNeurons\\tper\\tHidden\\tLayer\\navoiding\\tthrough\\tregularization\\n,\\t\\nAvoiding\\tOverfitting\\tThrough\\tRegularization\\n-\\nData\\nAugmentation\\nP\\np-value\\n,\\t\\nRegularization\\tHyperparameters\\nPaddingFIFOQueue\\n,\\t\\nPaddingFifoQueue\\nPandas\\n,', 'Pandas\\n,\\t\\nCreate\\tthe\\tWorkspace\\n,\\t\\nDownload\\tthe\\tData\\nscatter_matrix\\n,\\t\\nLooking\\tfor\\tCorrelations\\n-\\nLooking\\tfor\\tCorrelations\\nparallel\\tdistributed\\tcomputing\\n,\\t\\nDistributing\\tTensorFlow\\tAcross\\tDevices\\tand\\tServers\\n-\\nExercises\\ndata\\tparallelism\\n,\\t\\nData\\tParallelism\\n-\\nTensorFlow\\timplementation\\nin-graph\\tversus\\tbetween-graph\\treplication\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\nReplication\\n-\\nModel\\tParallelism\\nmodel\\tparallelism\\n,\\t\\nModel\\tParallelism\\n-\\nModel\\tParallelism\\nmultiple\\tdevices\\tacross\\tmultiple\\tservers\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\n-', 'Other\\tconvenience\\tfunctions\\nasynchronous\\tcommunication\\tusing\\tqueues\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\nTensorFlow\\tQueues\\n-\\nPaddingFifoQueue\\nloading\\ttraining\\tdata\\n,\\t\\nLoading\\tData\\tDirectly\\tfrom\\tthe\\tGraph\\n-\\nOther\\tconvenience\\nfunctions\\nmaster\\tand\\tworker\\tservices\\n,\\t\\nThe\\tMaster\\tand\\tWorker\\tServices\\nopening\\ta\\tsession\\n,\\t\\nOpening\\ta\\tSession\\npinning\\toperations\\tacross\\ttasks\\n,\\t\\nPinning\\tOperations\\tAcross\\tTasks\\nsharding\\tvariables\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nsharing\\tstate\\tacross\\tsessions\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\nContainers\\n-\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\nmultiple\\tdevices\\ton\\ta\\tsingle\\tmachine\\n,\\t\\nMultiple\\tDevices\\ton\\ta\\tSingle\\tMachine\\n-\\nControl\\nDependencies\\ncontrol\\tdependencies\\n,\\t\\nControl\\tDependencies\\ninstallation\\n,\\t\\nInstallation\\n-\\nInstallation\\nmanaging\\tthe\\tGPU\\tRAM\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM\\n-\\nManaging\\tthe\\tGPU\\tRAM\\nparallel\\texecution\\n,\\t\\nParallel\\tExecution\\n-\\nParallel\\tExecution\\nplacing\\toperations\\ton\\tdevices\\n,', ',\\t\\nPlacing\\tOperations\\ton\\tDevices\\n-\\nSoft\\tplacement\\none\\tneural\\tnetwork\\tper\\tdevice\\n,\\t\\nOne\\tNeural\\tNetwork\\tper\\tDevice\\n-\\nOne\\tNeural\\tNetwork\\nper\\tDevice\\nparameter\\tefficiency\\n,\\t\\nNumber\\tof\\tHidden\\tLayers\\nparameter\\tmatrix\\n,\\t\\nSoftmax\\tRegression\\nparameter\\tserver\\t(ps)\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\nparameter\\tspace\\n,\\t\\nGradient\\tDescent\\nparameter\\tvector\\n,\\t\\nLinear\\tRegression\\n,\\t\\nGradient\\tDescent\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\n,\\t\\nSoftmax\\nRegression', \"parametric\\tmodels\\n,\\t\\nRegularization\\tHyperparameters\\npartial\\tderivative\\n,\\t\\nBatch\\tGradient\\tDescent\\npartial_fit()\\n,\\t\\nIncremental\\tPCA\\nPearson's\\tr\\n,\\t\\nLooking\\tfor\\tCorrelations\\npeephole\\tconnections\\n,\\t\\nPeephole\\tConnections\\npenalties\\n\\t(\\nsee\\n\\trewards,\\tin\\tRL)\\npercentiles\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\nPerceptron\\tconvergence\\ttheorem\\n,\\t\\nThe\\tPerceptron\\nPerceptrons\\n,\\t\\nThe\\tPerceptron\\n-\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nversus\\tLogistic\\tRegression\\n,\\t\\nThe\\tPerceptron\\ntraining\\n,\\t\\nThe\\tPerceptron\\n-\\nThe\\tPerceptron\\nperformance\\tmeasures\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n-\\nSelect\\ta\\tPerformance\\tMeasure\\nconfusion\\tmatrix\\n,\\t\\nConfusion\\tMatrix\\n-\\nConfusion\\tMatrix\\ncross-validation\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\n-\\nMeasuring\\tAccuracy\\nUsing\\tCross-Validation\\nprecision\\tand\\trecall\\n,\\t\\nPrecision\\tand\\tRecall\\n-\\nPrecision/Recall\\tTradeoff\\nROC\\t(receiver\\toperating\\tcharacteristic)\\tcurve\\n,\\t\\nThe\\tROC\\tCurve\\n-\\nThe\\tROC\\tCurve\\nperformance\\tscheduling\\n,\\t\\nLearning\\tRate\\tScheduling\\npermutation()\\n,\\t\\nCreate\\ta\\tTest\\tSet\", 'PG\\talgorithms\\n,\\t\\nPolicy\\tGradients\\nphoto-hosting\\tservices\\n,\\t\\nSemisupervised\\tlearning\\npinning\\toperations\\n,\\t\\nPinning\\tOperations\\tAcross\\tTasks\\npip\\n,\\t\\nCreate\\tthe\\tWorkspace\\nPipeline\\tconstructor\\n,\\t\\nTransformation\\tPipelines\\n-\\nSelect\\tand\\tTrain\\ta\\tModel', 'pipelines\\n,\\t\\nFrame\\tthe\\tProblem\\nplaceholder\\tnodes\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\nplacers\\n\\t(\\nsee\\n\\tsimple\\tplacer;\\tdynamic\\tplacer)\\npolicy\\n,\\t\\nPolicy\\tSearch\\npolicy\\tgradients\\n,\\t\\nPolicy\\tSearch\\n\\t(\\nsee\\n\\tPG\\talgorithms)\\npolicy\\tspace\\n,\\t\\nPolicy\\tSearch\\npolynomial\\tfeatures,\\tadding\\n,\\t\\nNonlinear\\tSVM\\tClassification\\n-\\nPolynomial\\tKernel\\npolynomial\\tkernel\\n,\\t\\nPolynomial\\tKernel\\n-\\nPolynomial\\tKernel\\n,\\t\\nKernelized\\tSVM\\nPolynomial\\tRegression\\n,\\t\\nTraining\\tModels\\n,\\t\\nPolynomial\\tRegression\\n-\\nPolynomial\\tRegression\\nlearning\\tcurves\\tin\\n,\\t\\nLearning\\tCurves\\n-\\nLearning\\tCurves\\npooling\\tkernel\\n,\\t\\nPooling\\tLayer\\npooling\\tlayer\\n,\\t\\nPooling\\tLayer\\n-\\nPooling\\tLayer\\npower\\tscheduling\\n,\\t\\nLearning\\tRate\\tScheduling\\nprecision\\n,\\t\\nConfusion\\tMatrix\\nprecision\\tand\\trecall\\n,\\t\\nPrecision\\tand\\tRecall\\n-\\nPrecision/Recall\\tTradeoff\\nF-1\\tscore\\n,\\t\\nPrecision\\tand\\tRecall\\n-\\nPrecision\\tand\\tRecall\\nprecision/recall\\t(PR)\\tcurve\\n,\\t\\nThe\\tROC\\tCurve\\nprecision/recall\\ttradeoff\\n,\\t\\nPrecision/Recall\\tTradeoff\\n-\\nPrecision/Recall\\tTradeoff', 'predetermined\\tpiecewise\\tconstant\\tlearning\\trate\\n,\\t\\nLearning\\tRate\\tScheduling\\npredict()\\n,\\t\\nData\\tCleaning\\npredicted\\tclass\\n,\\t\\nConfusion\\tMatrix\\npredictions\\n,\\t\\nConfusion\\tMatrix\\n-\\nConfusion\\tMatrix\\n,\\t\\nDecision\\tFunction\\tand\\tPredictions\\n-\\nDecision\\nFunction\\tand\\tPredictions\\n,\\t\\nMaking\\tPredictions\\n-\\nEstimating\\tClass\\tProbabilities', 'predictors\\n,\\t\\nSupervised\\tlearning\\n,\\t\\nData\\tCleaning\\npreloading\\ttraining\\tdata\\n,\\t\\nPreload\\tthe\\tdata\\tinto\\ta\\tvariable\\nPReLU\\t(parametric\\tleaky\\tReLU)\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\npreprocessed\\tattributes\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\npretrained\\tlayers\\treuse\\n,\\t\\nReusing\\tPretrained\\tLayers\\n-\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\nauxiliary\\ttask\\n,\\t\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\n-\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\ncaching\\tfrozen\\tlayers\\n,\\t\\nCaching\\tthe\\tFrozen\\tLayers\\nfreezing\\tlower\\tlayers\\n,\\t\\nFreezing\\tthe\\tLower\\tLayers\\nmodel\\tzoos\\n,\\t\\nModel\\tZoos\\nother\\tframeworks\\n,\\t\\nReusing\\tModels\\tfrom\\tOther\\tFrameworks\\nTensorFlow\\tmodel\\n,\\t\\nReusing\\ta\\tTensorFlow\\tModel\\n-\\nReusing\\ta\\tTensorFlow\\tModel\\nunsupervised\\tpretraining\\n,\\t\\nUnsupervised\\tPretraining\\n-\\nUnsupervised\\tPretraining\\nupper\\tlayers\\n,\\t\\nTweaking,\\tDropping,\\tor\\tReplacing\\tthe\\tUpper\\tLayers\\nPretty\\tTensor\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nprimal\\tproblem\\n,\\t\\nThe\\tDual\\tProblem\\nprincipal\\tcomponent\\n,\\t\\nPrincipal\\tComponents\\nPrincipal\\tComponent\\tAnalysis\\t(PCA)\\n,\\t\\nPCA\\n-', ',\\t\\nPCA\\n-\\nRandomized\\tPCA\\nexplained\\tvariance\\tratios\\n,\\t\\nExplained\\tVariance\\tRatio\\nfinding\\tprincipal\\tcomponents\\n,\\t\\nPrincipal\\tComponents\\n-\\nPrincipal\\tComponents\\nfor\\tcompression\\n,\\t\\nPCA\\tfor\\tCompression\\n-\\nIncremental\\tPCA\\nIncremental\\tPCA\\n,\\t\\nIncremental\\tPCA\\n-\\nRandomized\\tPCA\\nKernel\\tPCA\\t(kPCA)\\n,\\t\\nKernel\\tPCA\\n-\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nprojecting\\tdown\\tto\\td\\tdimensions\\n,\\t\\nProjecting\\tDown\\tto\\td\\tDimensions', 'Randomized\\tPCA\\n,\\t\\nRandomized\\tPCA\\nScikit\\tLearn\\tfor\\n,\\t\\nUsing\\tScikit-Learn\\nvariance,\\tpreserving\\n,\\t\\nPreserving\\tthe\\tVariance\\n-\\nPreserving\\tthe\\tVariance\\nprobabilistic\\tautoencoders\\n,\\t\\nVariational\\tAutoencoders\\nprobabilities,\\testimating\\n,\\t\\nEstimating\\tProbabilities\\n-\\nEstimating\\tProbabilities\\n,\\t\\nEstimating\\tClass\\nProbabilities\\nproducer\\tfunctions\\n,\\t\\nOther\\tconvenience\\tfunctions\\nprojection\\n,\\t\\nProjection\\n-\\nProjection\\npropositional\\tlogic\\n,\\t\\nFrom\\tBiological\\tto\\tArtificial\\tNeurons\\npruning\\n,\\t\\nRegularization\\tHyperparameters\\n,\\t\\nSymbolic\\tDifferentiation\\nPython\\nisolated\\tenvironment\\tin\\n,\\t\\nCreate\\tthe\\tWorkspace\\n-\\nCreate\\tthe\\tWorkspace\\nnotebooks\\tin\\n,\\t\\nCreate\\tthe\\tWorkspace\\n-\\nDownload\\tthe\\tData\\npickle\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\npip\\n,\\t\\nCreate\\tthe\\tWorkspace\\nQ\\nQ-Learning\\talgorithm\\n,\\t\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\tMs.\\nPac-Man\\tUsing\\tDeep\\tQ-Learning\\napproximate\\tQ-Learning\\n,\\t\\nApproximate\\tQ-Learning\\ndeep\\tQ-Learning\\n,\\t\\nApproximate\\tQ-Learning\\n-', '-\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\nQ-Learning\\nQ-Value\\tIteration\\tAlgorithm\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nQ-Values\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nQuadratic\\tProgramming\\t(QP)\\tProblems\\n,\\t\\nQuadratic\\tProgramming\\n-\\nQuadratic\\tProgramming\\nquantizing\\n,\\t\\nBandwidth\\tsaturation', 'queries\\tper\\tsecond\\t(QPS)\\n,\\t\\nOne\\tNeural\\tNetwork\\tper\\tDevice\\nQueueRunner\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nMultithreaded\\nreaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\nqueues\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\n-\\nPaddingFifoQueue\\nclosing\\n,\\t\\nClosing\\ta\\tqueue\\ndequeuing\\tdata\\n,\\t\\nDequeuing\\tdata\\nenqueuing\\tdata\\n,\\t\\nEnqueuing\\tdata\\nfirst-in\\tfirst-out\\t\\n(FIFO)\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\nof\\ttuples\\n,\\t\\nQueues\\tof\\ttuples\\nPaddingFIFOQueue\\n,\\t\\nPaddingFifoQueue\\nRandomShuffleQueue\\n,\\t\\nRandomShuffleQueue\\nq_network()\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nR\\nRadial\\tBasis\\tFunction\\t(RBF)\\n,\\t\\nAdding\\tSimilarity\\tFeatures\\nRandom\\tForests\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n-\\nGrid\\tSearch\\n,\\t\\nMulticlass\\nClassification\\n,\\t\\nDecision\\tTrees\\n,\\t\\nInstability\\n,\\t\\nEnsemble\\tLearning\\tand\\tRandom\\tForests\\n,\\t\\nRandom\\nForests\\n-\\nFeature\\tImportance\\nExtra-Trees\\n,\\t\\nExtra-Trees\\nfeature\\timportance\\n,\\t\\nFeature\\tImportance\\n-\\nFeature\\tImportance', 'random\\tinitialization\\n,\\t\\nGradient\\tDescent\\n,\\t\\nBatch\\tGradient\\tDescent\\n,\\t\\nStochastic\\tGradient\\nDescent\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\nRandom\\tPatches\\tand\\tRandom\\tSubspaces\\n,\\t\\nRandom\\tPatches\\tand\\tRandom\\tSubspaces\\nrandomized\\tleaky\\tReLU\\t\\n(RReLU)\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\nRandomized\\tPCA\\n,\\t\\nRandomized\\tPCA\\nrandomized\\tsearch\\n,\\t\\nRandomized\\tSearch\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters', 'RandomShuffleQueue\\n,\\t\\nRandomShuffleQueue\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\ngraph\\nrandom_uniform()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\nreader\\toperations\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\nrecall\\n,\\t\\nConfusion\\tMatrix\\nrecognition\\tnetwork\\n,\\t\\nEfficient\\tData\\tRepresentations\\nreconstruction\\terror\\n,\\t\\nPCA\\tfor\\tCompression\\nreconstruction\\tloss\\n,\\t\\nEfficient\\tData\\tRepresentations\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nVariational\\nAutoencoders\\nreconstruction\\tpre-image\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nreconstructions\\n,\\t\\nEfficient\\tData\\tRepresentations\\nrecurrent\\tneural\\tnetworks\\t(RNNs)\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\n-\\nExercises\\ndeep\\tRNNs\\n,\\t\\nDeep\\tRNNs\\n-\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps\\nexploration\\tpolicies\\n,\\t\\nExploration\\tPolicies\\nGRU\\tcell\\n,\\t\\nGRU\\tCell\\n-\\nGRU\\tCell\\ninput\\tand\\toutput\\tsequences\\n,\\t\\nInput\\tand\\tOutput\\tSequences\\n-\\nInput\\tand\\tOutput\\tSequences\\nLSTM\\tcell\\n,\\t\\nLSTM\\tCell\\n-\\nGRU\\tCell\\nnatural\\tlanguage\\tprocessing\\t(NLP)\\n,\\t\\nNatural\\tLanguage\\tProcessing\\n-', '-\\nAn\\tEncoder–Decoder\\nNetwork\\tfor\\tMachine\\tTranslation\\nin\\tTensorFlow\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n-\\nHandling\\tVariable-Length\\tOutput\\tSequences\\ndynamic\\tunrolling\\tthrough\\ttime\\n,\\t\\nDynamic\\tUnrolling\\tThrough\\tTime\\nstatic\\tunrolling\\tthrough\\ttime\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nStatic\\tUnrolling\\nThrough\\tTime\\nvariable\\tlength\\tinput\\tsequences\\n,\\t\\nHandling\\tVariable\\tLength\\tInput\\tSequences\\nvariable\\tlength\\toutput\\tsequences\\n,\\t\\nHandling\\tVariable-Length\\tOutput\\tSequences', 'training\\n,\\t\\nTraining\\tRNNs\\n-\\nCreative\\tRNN\\nbackpropagation\\tthrough\\ttime\\t(BPTT)\\n,\\t\\nTraining\\tRNNs\\ncreative\\tsequences\\n,\\t\\nCreative\\tRNN\\nsequence\\tclassifiers\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\n-\\nTraining\\ta\\tSequence\\nClassifier\\ntime\\tseries\\tpredictions\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n-\\nTraining\\tto\\tPredict\\tTime\\nSeries\\nrecurrent\\tneurons\\n,\\t\\nRecurrent\\tNeurons\\n-\\nInput\\tand\\tOutput\\tSequences\\nmemory\\tcells\\n,\\t\\nMemory\\tCells\\nreduce_mean()\\n,\\t\\nConstruction\\tPhase\\nreduce_sum()\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\tImplementation\\n,\\t\\nVariational\\nAutoencoders\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nregression\\n,\\t\\nSupervised\\tlearning\\nDecision\\tTrees\\n,\\t\\nRegression\\n-\\nRegression\\nregression\\tmodels\\nlinear\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\nregression\\tversus\\tclassification\\n,\\t\\nMultioutput\\tClassification\\nregularization\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n-\\nOverfitting\\tthe\\tTraining\\tData\\n,\\t\\nTesting\\tand\\nValidating\\n,\\t\\nRegularized\\tLinear\\tModels\\n-\\nEarly\\tStopping\\ndata\\taugmentation\\n,\\t\\nData\\tAugmentation\\n-', '-\\nData\\tAugmentation\\nDecision\\tTrees\\n,\\t\\nRegularization\\tHyperparameters\\n-\\nRegularization\\tHyperparameters\\ndropout\\n,\\t\\nDropout\\n-\\nDropout\\nearly\\tstopping\\n,\\t\\nEarly\\tStopping\\n-\\nEarly\\tStopping\\n,\\t\\nEarly\\tStopping\\nElastic\\tNet\\n,\\t\\nElastic\\tNet\\nLasso\\tRegression\\n,\\t\\nLasso\\tRegression\\n-\\nLasso\\tRegression\\nmax-norm\\n,\\t\\nMax-Norm\\tRegularization\\n-\\nMax-Norm\\tRegularization', 'Ridge\\tRegression\\n,\\t\\nRidge\\tRegression\\n-\\nRidge\\tRegression\\nshrinkage\\n,\\t\\nGradient\\tBoosting\\nℓ\\t1\\tand\\tℓ\\t2\\tregularization\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n-\\nℓ1\\tand\\tℓ2\\tRegularization\\nREINFORCE\\talgorithms\\n,\\t\\nPolicy\\tGradients\\nReinforcement\\tLearning\\t(RL)\\n,\\t\\nReinforcement\\tLearning\\n-\\nReinforcement\\tLearning\\n,\\nReinforcement\\tLearning\\n-\\nThank\\tYou!\\nactions\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\n-\\nEvaluating\\tActions:\\tThe\\nCredit\\tAssignment\\tProblem\\ncredit\\tassignment\\tproblem\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\n-\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\ndiscount\\trate\\n,\\t\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\nexamples\\tof\\n,\\t\\nLearning\\tto\\tOptimize\\tRewards\\nMarkov\\tdecision\\tprocesses\\n,\\t\\nMarkov\\tDecision\\tProcesses\\n-\\nMarkov\\tDecision\\tProcesses\\nneural\\tnetwork\\tpolicies\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n-\\nNeural\\tNetwork\\tPolicies\\nOpenAI\\tgym\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\n-\\nIntroduction\\tto\\tOpenAI\\tGym\\nPG\\talgorithms\\n,\\t\\nPolicy\\tGradients\\n-\\nPolicy\\tGradients\\npolicy\\tsearch\\n,\\t\\nPolicy\\tSearch\\n-', '-\\nPolicy\\tSearch\\nQ-Learning\\talgorithm\\n,\\t\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\nMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nrewards,\\tlearning\\tto\\toptimize\\n,\\t\\nLearning\\tto\\tOptimize\\tRewards\\n-\\nLearning\\tto\\tOptimize\\nRewards\\nTemporal\\tDifference\\t(TD)\\tLearning\\n,\\t\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\n-\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\nReLU\\t(rectified\\t\\nlinear\\t\\nunits)\\n,\\t\\nModularity\\n-\\nModularity\\nReLU\\tactivation\\n,\\t\\nResNet', 'ReLU\\tfunction\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n,\\t\\nActivation\\tFunctions\\n,\\t\\nXavier\\nand\\tHe\\tInitialization\\n-\\nNonsaturating\\tActivation\\tFunctions\\nrelu(z)\\n,\\t\\nConstruction\\tPhase\\nrender()\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\nreplay\\tmemory\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nreplica_device_setter()\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nrequest_stop()\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\nreset()\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\nreset_default_graph()\\n,\\t\\nManaging\\tGraphs\\nreshape()\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\nresidual\\terrors\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\nresidual\\tlearning\\n,\\t\\nResNet\\nresidual\\tnetwork\\t(ResNet)\\n,\\t\\nModel\\tZoos\\n,\\t\\nResNet\\n-\\nResNet\\nresidual\\tunits\\n,\\t\\nResNet\\nResNet\\n,\\t\\nResNet\\n-\\nResNet\\nresource\\tcontainers\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\n-\\nSharing\\tState\\nAcross\\tSessions\\tUsing\\tResource\\tContainers\\nrestore()\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\nrestricted\\tBoltzmann\\tmachines\\t\\n(RBMs)\\n,', '(RBMs)\\n,\\t\\nSemisupervised\\tlearning\\n,\\t\\nUnsupervised\\tPretraining\\n,\\nBoltzmann\\tMachines\\nreuse_variables()\\n,\\t\\nSharing\\tVariables\\nreverse-mode\\tautodiff\\n,\\t\\nReverse-Mode\\tAutodiff\\n-\\nReverse-Mode\\tAutodiff\\nrewards,\\tin\\tRL\\n,\\t\\nLearning\\tto\\tOptimize\\tRewards\\n-\\nLearning\\tto\\tOptimize\\tRewards\\nrgb_array\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym', 'Ridge\\tRegression\\n,\\t\\nRidge\\tRegression\\n-\\nRidge\\tRegression\\n,\\t\\nElastic\\tNet\\nRMSProp\\n,\\t\\nRMSProp\\nROC\\t(receiver\\toperating\\tcharacteristic)\\tcurve\\n,\\t\\nThe\\tROC\\tCurve\\n-\\nThe\\tROC\\tCurve\\nRoot\\tMean\\tSquare\\tError\\t(RMSE)\\n,\\t\\nSelect\\ta\\tPerformance\\tMeasure\\n-\\nSelect\\ta\\tPerformance\\nMeasure\\n,\\t\\nLinear\\tRegression\\nRReLU\\t(randomized\\tleaky\\tReLU)\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\nrun()\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\nReplication\\nS\\nSampled\\tSoftmax\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nsampling\\tbias\\n,\\t\\nNonrepresentative\\tTraining\\tData\\n-\\nPoor-Quality\\tData\\n,\\t\\nCreate\\ta\\tTest\\tSet\\nsampling\\tnoise\\n,\\t\\nNonrepresentative\\tTraining\\tData\\nsave()\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\nSaver\\t\\nnode\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\nScikit\\tFlow\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nScikit-Learn\\n,\\t\\nCreate\\tthe\\tWorkspace\\nabout\\n,\\t\\nObjective\\tand\\tApproach\\nbagging\\tand\\tpasting\\tin\\n,\\t\\nBagging\\tand\\tPasting\\tin\\tScikit-Learn\\n-\\nBagging\\tand\\tPasting\\tin\\nScikit-Learn\\nCART\\talgorithm\\n,', ',\\t\\nMaking\\tPredictions\\n-\\nThe\\tCART\\tTraining\\tAlgorithm\\n,\\t\\nRegression\\ncross-validation\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n-\\nBetter\\tEvaluation\\tUsing\\nCross-Validation\\ndesign\\tprinciples\\n,\\t\\nData\\tCleaning\\n-\\nData\\tCleaning\\nimputer\\n,\\t\\nData\\tCleaning\\n-\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nLinearSVR\\tclass\\n,\\t\\nSVM\\tRegression', 'MinMaxScaler\\n,\\t\\nFeature\\tScaling\\nmin_\\tand\\tmax_\\thyperparameters\\n,\\t\\nRegularization\\tHyperparameters\\nPCA\\timplementation\\n,\\t\\nUsing\\tScikit-Learn\\nPerceptron\\tclass\\n,\\t\\nThe\\tPerceptron\\nPipeline\\tconstructor\\n,\\t\\nTransformation\\tPipelines\\n-\\nSelect\\tand\\tTrain\\ta\\tModel\\n,\\t\\nNonlinear\\nSVM\\tClassification\\nRandomized\\tPCA\\n,\\t\\nRandomized\\tPCA\\nRidge\\tRegression\\twith\\n,\\t\\nRidge\\tRegression\\nSAMME\\n,\\t\\nAdaBoost\\nSGDClassifier\\n,\\t\\nTraining\\ta\\tBinary\\tClassifier\\n,\\t\\nPrecision/Recall\\tTradeoff\\n-\\nPrecision/Recall\\nTradeoff\\n,\\t\\nMulticlass\\tClassification\\nSGDRegressor\\n,\\t\\nStochastic\\tGradient\\tDescent\\nsklearn.base.BaseEstimator\\n,\\t\\nCustom\\tTransformers\\n,\\t\\nTransformation\\tPipelines\\n,\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nsklearn.base.clone()\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\n,\\t\\nEarly\\tStopping\\nsklearn.base.TransformerMixin\\n,\\t\\nCustom\\tTransformers\\n,\\t\\nTransformation\\tPipelines\\nsklearn.datasets.fetch_california_housing()\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\nsklearn.datasets.fetch_mldata()\\n,\\t\\nMNIST\\nsklearn.datasets.load_iris()\\n,', ',\\t\\nDecision\\tBoundaries\\n,\\t\\nSoft\\tMargin\\tClassification\\n,\\t\\nTraining\\nand\\tVisualizing\\ta\\tDecision\\tTree\\n,\\t\\nFeature\\tImportance\\n,\\t\\nThe\\tPerceptron\\nsklearn.datasets.load_sample_images()\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\nImplementation\\nsklearn.datasets.make_moons()\\n,\\t\\nNonlinear\\tSVM\\tClassification\\n,\\t\\nExercises\\nsklearn.decomposition.IncrementalPCA\\n,\\t\\nIncremental\\tPCA\\nsklearn.decomposition.KernelPCA\\n,\\t\\nKernel\\tPCA\\n-\\nSelecting\\ta\\tKernel\\tand\\tTuning', 'Hyperparameters\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nsklearn.decomposition.PCA\\n,\\t\\nUsing\\tScikit-Learn\\nsklearn.ensemble.AdaBoostClassifier\\n,\\t\\nAdaBoost\\nsklearn.ensemble.BaggingClassifier\\n,\\t\\nBagging\\tand\\tPasting\\tin\\tScikit-Learn\\n-\\nRandom\\nForests\\nsklearn.ensemble.GradientBoostingRegressor\\n,\\t\\nGradient\\tBoosting\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\nsklearn.ensemble.RandomForestClassifier\\n,\\t\\nThe\\tROC\\tCurve\\n,\\t\\nMulticlass\\tClassification\\n,\\nVoting\\tClassifiers\\nsklearn.ensemble.RandomForestRegressor\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n,\\nGrid\\tSearch\\n-\\nAnalyze\\tthe\\tBest\\tModels\\tand\\tTheir\\tErrors\\n,\\t\\nRandom\\tForests\\n-\\nExtra-Trees\\n,\\nGradient\\tBoosting\\nsklearn.ensemble.VotingClassifier\\n,\\t\\nVoting\\tClassifiers\\nsklearn.externals.joblib\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\nsklearn.linear_model.ElasticNet\\n,\\t\\nElastic\\tNet\\nsklearn.linear_model.Lasso\\n,\\t\\nLasso\\tRegression\\nsklearn.linear_model.LinearRegression\\n,\\t\\nModel-based\\tlearning\\n-\\nModel-based\\tlearning\\n,\\nData\\tCleaning\\n,', ',\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n,\\t\\nThe\\tNormal\\tEquation\\n,\\nMini-batch\\tGradient\\tDescent\\n,\\t\\nPolynomial\\tRegression\\n,\\t\\nLearning\\tCurves\\n-\\nLearning\\nCurves\\nsklearn.linear_model.LogisticRegression\\n,\\t\\nDecision\\tBoundaries\\n,\\t\\nDecision\\tBoundaries\\n,\\nSoftmax\\tRegression\\n,\\t\\nVoting\\tClassifiers\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\nHyperparameters\\nsklearn.linear_model.Perceptron\\n,\\t\\nThe\\tPerceptron\\nsklearn.linear_model.Ridge\\n,\\t\\nRidge\\tRegression\\nsklearn.linear_model.SGDClassifier\\n,\\t\\nTraining\\ta\\tBinary\\tClassifier\\nsklearn.linear_model.SGDRegressor\\n,\\t\\nStochastic\\tGradient\\tDescent\\n-\\nMini-batch\\tGradient', 'Descent\\n,\\t\\nRidge\\tRegression\\n,\\t\\nLasso\\tRegression\\n-\\nEarly\\tStopping\\nsklearn.manifold.LocallyLinearEmbedding\\n,\\t\\nLLE\\n-\\nLLE\\nsklearn.metrics.accuracy_score()\\n,\\t\\nVoting\\tClassifiers\\n,\\t\\nOut-of-Bag\\tEvaluation\\n,\\t\\nTraining\\nan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nsklearn.metrics.confusion_matrix()\\n,\\t\\nConfusion\\tMatrix\\n,\\t\\nError\\tAnalysis\\nsklearn.metrics.f1_score()\\n,\\t\\nPrecision\\tand\\tRecall\\n,\\t\\nMultilabel\\tClassification\\nsklearn.metrics.mean_squared_error()\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n-\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n,\\t\\nEvaluate\\tYour\\tSystem\\ton\\tthe\\tTest\\tSet\\n,\\nLearning\\tCurves\\n,\\t\\nEarly\\tStopping\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\n,\\t\\nSelecting\\ta\\nKernel\\tand\\tTuning\\tHyperparameters\\nsklearn.metrics.precision_recall_curve()\\n,\\t\\nPrecision/Recall\\tTradeoff\\nsklearn.metrics.precision_score()\\n,\\t\\nPrecision\\tand\\tRecall\\n,\\t\\nPrecision/Recall\\tTradeoff\\nsklearn.metrics.recall_score()\\n,\\t\\nPrecision\\tand\\tRecall\\n,\\t\\nPrecision/Recall\\tTradeoff\\nsklearn.metrics.roc_auc_score()\\n,\\t\\nThe\\tROC\\tCurve\\n-', '-\\nThe\\tROC\\tCurve\\nsklearn.metrics.roc_curve()\\n,\\t\\nThe\\tROC\\tCurve\\n-\\nThe\\tROC\\tCurve\\nsklearn.model_selection.cross_val_predict()\\n,\\t\\nConfusion\\tMatrix\\n,\\t\\nPrecision/Recall\\nTradeoff\\n,\\t\\nThe\\tROC\\tCurve\\n,\\t\\nError\\tAnalysis\\n,\\t\\nMultilabel\\tClassification\\nsklearn.model_selection.cross_val_score()\\n,\\t\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n-\\nBetter\\tEvaluation\\tUsing\\tCross-Validation\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\n-\\nConfusion\\tMatrix\\nsklearn.model_selection.GridSearchCV\\n,\\t\\nGrid\\tSearch\\n-\\nRandomized\\tSearch\\n,\\t\\nExercises\\n,\\nError\\tAnalysis\\n,\\t\\nExercises\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nsklearn.model_selection.StratifiedKFold\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nsklearn.model_selection.StratifiedShuffleSplit\\n,\\t\\nCreate\\ta\\tTest\\tSet\\nsklearn.model_selection.train_test_split()\\n,\\t\\nCreate\\ta\\tTest\\tSet\\n,\\t\\nTraining\\tand\\tEvaluating\\non\\tthe\\tTraining\\tSet\\n,\\t\\nLearning\\tCurves\\n,\\t\\nExercises\\n,\\t\\nGradient\\tBoosting', 'sklearn.multiclass.OneVsOneClassifier\\n,\\t\\nMulticlass\\tClassification\\nsklearn.neighbors.KNeighborsClassifier\\n,\\t\\nMultilabel\\tClassification\\n,\\t\\nExercises\\nsklearn.neighbors.KNeighborsRegressor\\n,\\t\\nModel-based\\tlearning\\nsklearn.pipeline.FeatureUnion\\n,\\t\\nTransformation\\tPipelines\\nsklearn.pipeline.Pipeline\\n,\\t\\nTransformation\\tPipelines\\n,\\t\\nLearning\\tCurves\\n,\\t\\nSoft\\tMargin\\nClassification\\n-\\nNonlinear\\tSVM\\tClassification\\n,\\t\\nSelecting\\ta\\tKernel\\tand\\tTuning\\nHyperparameters\\nsklearn.preprocessing.Imputer\\n,\\t\\nData\\tCleaning\\n,\\t\\nTransformation\\tPipelines\\nsklearn.preprocessing.LabelBinarizer\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\n,\\nTransformation\\tPipelines\\nsklearn.preprocessing.LabelEncoder\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nsklearn.preprocessing.OneHotEncoder\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nsklearn.preprocessing.PolynomialFeatures\\n,\\t\\nPolynomial\\tRegression\\n-\\nPolynomial\\nRegression\\n,\\t\\nLearning\\tCurves\\n,\\t\\nRidge\\tRegression\\n,\\t\\nNonlinear\\tSVM\\tClassification\\nsklearn.preprocessing.StandardScaler\\n,', ',\\t\\nFeature\\tScaling\\n-\\nTransformation\\tPipelines\\n,\\nMulticlass\\tClassification\\n,\\t\\nGradient\\tDescent\\n,\\t\\nRidge\\tRegression\\n,\\t\\nLinear\\tSVM\\nClassification\\n,\\t\\nSoft\\tMargin\\tClassification\\n-\\nPolynomial\\tKernel\\n,\\t\\nGaussian\\tRBF\\tKernel\\n,\\nImplementing\\tGradient\\tDescent\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nsklearn.svm.LinearSVC\\n,\\t\\nSoft\\tMargin\\tClassification\\n-\\nNonlinear\\tSVM\\tClassification\\n,\\nGaussian\\tRBF\\tKernel\\n-\\nComputational\\tComplexity\\n,\\t\\nSVM\\tRegression\\n,\\t\\nExercises\\nsklearn.svm.LinearSVR\\n,\\t\\nSVM\\tRegression\\n-\\nSVM\\tRegression\\nsklearn.svm.SVC\\n,\\t\\nSoft\\tMargin\\tClassification\\n,\\t\\nPolynomial\\tKernel\\n,\\t\\nGaussian\\tRBF\\tKernel\\n-\\nComputational\\tComplexity\\n,\\t\\nSVM\\tRegression\\n,\\t\\nExercises\\n,\\t\\nVoting\\tClassifiers\\nsklearn.svm.SVR\\n,\\t\\nExercises\\n,\\t\\nSVM\\tRegression\\nsklearn.tree.DecisionTreeClassifier\\n,\\t\\nRegularization\\tHyperparameters\\n,\\t\\nExercises\\n,\\nBagging\\tand\\tPasting\\tin\\tScikit-Learn\\n-\\nOut-of-Bag\\tEvaluation\\n,\\t\\nRandom\\tForests\\n,\\nAdaBoost', 'sklearn.tree.DecisionTreeRegressor\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n,\\nDecision\\tTrees\\n,\\t\\nRegression\\n,\\t\\nGradient\\tBoosting\\n-\\nGradient\\tBoosting\\nsklearn.tree.export_graphviz()\\n,\\t\\nTraining\\tand\\tVisualizing\\ta\\tDecision\\tTree\\nStandardScaler\\n,\\t\\nGradient\\tDescent\\n,\\t\\nImplementing\\tGradient\\tDescent\\n,\\t\\nTraining\\tan\\tMLP\\nwith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nSVM\\tclassification\\tclasses\\n,\\t\\nComputational\\tComplexity\\nTF.Learn\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nuser\\tguide\\n,\\t\\nOther\\tResources\\nscore()\\n,\\t\\nData\\tCleaning\\nsearch\\tspace\\n,\\t\\nRandomized\\tSearch\\n,\\t\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\nsecond-order\\tpartial\\tderivatives\\t(Hessians)\\n,\\t\\nAdam\\tOptimization\\nself-organizing\\tmaps\\t(SOMs)\\n,\\t\\nSelf-Organizing\\tMaps\\n-\\nSelf-Organizing\\tMaps\\nsemantic\\thashing\\n,\\t\\nExercises\\nsemisupervised\\tlearning\\n,\\t\\nSemisupervised\\tlearning\\nsensitivity\\n,\\t\\nConfusion\\tMatrix\\n,\\t\\nThe\\tROC\\tCurve\\nsentiment\\tanalysis\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\nseparable_conv2d()\\n,\\t\\nResNet\\nsequences\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\nsequence_length\\n,', \",\\t\\nHandling\\tVariable\\tLength\\tInput\\tSequences\\n-\\nHandling\\tVariable-Length\\tOutput\\nSequences\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nShannon's\\tinformation\\ttheory\\n,\\t\\nGini\\tImpurity\\tor\\tEntropy?\\nshortcut\\tconnections\\n,\\t\\nResNet\\nshow()\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\nshow_graph()\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\", 'shrinkage\\n,\\t\\nGradient\\tBoosting\\nshuffle_batch()\\n,\\t\\nOther\\tconvenience\\tfunctions\\nshuffle_batch_join()\\n,\\t\\nOther\\tconvenience\\tfunctions\\nsigmoid\\tfunction\\n,\\t\\nEstimating\\tProbabilities\\nsigmoid_cross_entropy_with_logits()\\n,\\t\\nTensorFlow\\tImplementation\\nsimilarity\\tfunction\\n,\\t\\nAdding\\tSimilarity\\tFeatures\\n-\\nAdding\\tSimilarity\\tFeatures\\nsimulated\\tannealing\\n,\\t\\nStochastic\\tGradient\\tDescent\\nsimulated\\tenvironments\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\n(\\nsee\\talso\\n\\tOpenAI\\tGym)\\nSingular\\tValue\\tDecomposition\\t(SVD)\\n,\\t\\nPrincipal\\tComponents\\nskewed\\tdatasets\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nskip\\tconnections\\n,\\t\\nData\\tAugmentation\\n,\\t\\nResNet\\nslack\\tvariable\\n,\\t\\nTraining\\tObjective\\nsmoothing\\tterms\\n,\\t\\nBatch\\tNormalization\\n,\\t\\nAdaGrad\\n,\\t\\nAdam\\tOptimization\\n,\\t\\nVariational\\nAutoencoders\\nsoft\\tmargin\\tclassification\\n,\\t\\nSoft\\tMargin\\tClassification\\n-\\nSoft\\tMargin\\tClassification\\nsoft\\tplacements\\n,\\t\\nSoft\\tplacement\\nsoft\\tvoting\\n,\\t\\nVoting\\tClassifiers\\nsoftmax\\tfunction\\n,\\t\\nSoftmax\\tRegression\\n,\\t\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\n,', ',\\t\\nTraining\\nan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nSoftmax\\tRegression\\n,\\t\\nSoftmax\\tRegression\\n-\\nSoftmax\\tRegression\\nsource\\tops\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n,\\t\\nParallel\\tExecution\\nspam\\tfilters\\n,\\t\\nThe\\tMachine\\tLearning\\tLandscape\\n-\\nWhy\\tUse\\tMachine\\tLearning?\\n,\\t\\nSupervised\\nlearning', 'sparse\\tautoencoders\\n,\\t\\nSparse\\tAutoencoders\\n-\\nTensorFlow\\tImplementation\\nsparse\\tmatrix\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nsparse\\tmodels\\n,\\t\\nLasso\\tRegression\\n,\\t\\nAdam\\tOptimization\\nsparse_softmax_cross_entropy_with_logits()\\n,\\t\\nConstruction\\tPhase\\nsparsity\\tloss\\n,\\t\\nSparse\\tAutoencoders\\nspecificity\\n,\\t\\nThe\\tROC\\tCurve\\nspeech\\trecognition\\n,\\t\\nWhy\\tUse\\tMachine\\tLearning?\\nspurious\\tpatterns\\n,\\t\\nHopfield\\tNetworks\\nstack()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\nstacked\\tautoencoders\\n,\\t\\nStacked\\tAutoencoders\\n-\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\nAutoencoders\\nTensorFlow\\timplementation\\n,\\t\\nTensorFlow\\tImplementation\\ntraining\\tone-at-a-time\\n,\\t\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\n-\\nTraining\\tOne\\tAutoencoder\\nat\\ta\\tTime\\ntying\\tweights\\n,\\t\\nTying\\tWeights\\n-\\nTying\\tWeights\\nunsupervised\\tpretraining\\twith\\n,\\t\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\n-\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\nvisualizing\\tthe\\treconstructions\\n,\\t\\nVisualizing\\tthe\\tReconstructions\\n-\\nVisualizing\\tthe\\nReconstructions', 'stacked\\tdenoising\\tautoencoders\\n,\\t\\nVisualizing\\tFeatures\\n,\\t\\nDenoising\\tAutoencoders\\nstacked\\tdenoising\\tencoders\\n,\\t\\nDenoising\\tAutoencoders\\nstacked\\tgeneralization\\n\\t(\\nsee\\n\\tstacking)\\nstacking\\n,\\t\\nStacking\\n-\\nStacking\\nstale\\tgradients\\n,\\t\\nAsynchronous\\tupdates\\nstandard\\tcorrelation\\tcoefficient\\n,\\t\\nLooking\\tfor\\tCorrelations', 'standardization\\n,\\t\\nFeature\\tScaling\\nStandardScaler\\n,\\t\\nTransformation\\tPipelines\\n,\\t\\nImplementing\\tGradient\\tDescent\\n,\\t\\nTraining\\tan\\tMLP\\nwith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nstate-action\\tvalues\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nstates\\t\\ntensor\\n,\\t\\nHandling\\tVariable\\tLength\\tInput\\tSequences\\nstate_is_tuple\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\n,\\t\\nLSTM\\tCell\\nstatic\\tunrolling\\tthrough\\ttime\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nStatic\\tUnrolling\\tThrough\\tTime\\nstatic_rnn()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nStatic\\tUnrolling\\tThrough\\tTime\\n,\\t\\nAn\\tEncoder–\\nDecoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nstationary\\tpoint\\n,\\t\\nSVM\\tDual\\tProblem\\n-\\nSVM\\tDual\\tProblem\\nstatistical\\tmode\\n,\\t\\nBagging\\tand\\tPasting\\nstatistical\\tsignificance\\n,\\t\\nRegularization\\tHyperparameters\\nstemming\\n,\\t\\nExercises\\nstep\\tfunctions\\n,\\t\\nThe\\tPerceptron\\nstep()\\n,\\t\\nIntroduction\\tto\\tOpenAI\\tGym\\nStochastic\\tGradient\\tBoosting\\n,\\t\\nGradient\\tBoosting\\nStochastic\\tGradient\\tDescent\\t(SGD)\\n,\\t\\nStochastic\\tGradient\\tDescent\\n-\\nStochastic\\tGradient\\nDescent\\n,\\t\\nSoft\\tMargin\\tClassification\\n,', ',\\t\\nThe\\tPerceptron\\ntraining\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\nStochastic\\tGradient\\tDescent\\t(SGD)\\tclassifier\\n,\\t\\nTraining\\ta\\tBinary\\tClassifier\\n,\\t\\nRidge\\tRegression\\nstochastic\\tneurons\\n,\\t\\nBoltzmann\\tMachines\\nstochastic\\tpolicy\\n,\\t\\nPolicy\\tSearch\\nstratified\\tsampling\\n,\\t\\nCreate\\ta\\tTest\\tSet\\n-\\nCreate\\ta\\tTest\\tSet\\n,\\t\\nMeasuring\\tAccuracy\\tUsing\\tCross-\\nValidation', 'stride\\n,\\t\\nConvolutional\\tLayer\\nstring\\tkernels\\n,\\t\\nGaussian\\tRBF\\tKernel\\nstring_input_producer()\\n,\\t\\nOther\\tconvenience\\tfunctions\\nstrong\\tlearners\\n,\\t\\nVoting\\tClassifiers\\nsubderivatives\\n,\\t\\nOnline\\tSVMs\\nsubgradient\\tvector\\n,\\t\\nLasso\\tRegression\\nsubsample\\n,\\t\\nGradient\\tBoosting\\n,\\t\\nPooling\\tLayer\\nsupervised\\tlearning\\n,\\t\\nSupervised/Unsupervised\\tLearning\\n-\\nSupervised\\tlearning\\nSupport\\tVector\\tMachines\\t(SVMs)\\n,\\t\\nMulticlass\\tClassification\\n,\\t\\nSupport\\tVector\\tMachines\\n-\\nExercises\\ndecision\\tfunction\\tand\\tpredictions\\n,\\t\\nDecision\\tFunction\\tand\\tPredictions\\n-\\nDecision\\tFunction\\nand\\tPredictions\\ndual\\tproblem\\n,\\t\\nSVM\\tDual\\tProblem\\n-\\nSVM\\tDual\\tProblem\\nkernelized\\tSVM\\n,\\t\\nKernelized\\tSVM\\n-\\nKernelized\\tSVM\\nlinear\\tclassification\\n,\\t\\nLinear\\tSVM\\tClassification\\n-\\nSoft\\tMargin\\tClassification\\nmechanics\\tof\\n,\\t\\nUnder\\tthe\\tHood\\n-\\nOnline\\tSVMs\\nnonlinear\\tclassification\\n,\\t\\nNonlinear\\tSVM\\tClassification\\n-\\nComputational\\tComplexity\\nonline\\tSVMs\\n,\\t\\nOnline\\tSVMs\\n-\\nOnline\\tSVMs\\nQuadratic\\tProgramming\\t(QP)\\tproblems\\n,\\t\\nQuadratic\\tProgramming\\n-\\nQuadratic\\nProgramming', 'SVM\\tregression\\n,\\t\\nSVM\\tRegression\\n-\\nOnline\\tSVMs\\nthe\\tdual\\tproblem\\n,\\t\\nThe\\tDual\\tProblem\\ntraining\\tobjective\\n,\\t\\nTraining\\tObjective\\n-\\nTraining\\tObjective\\nsupport\\tvectors\\n,\\t\\nLinear\\tSVM\\tClassification', 'svd()\\n,\\t\\nPrincipal\\tComponents\\nsymbolic\\tdifferentiation\\n,\\t\\nUsing\\tautodiff\\n,\\t\\nSymbolic\\tDifferentiation\\n-\\nNumerical\\tDifferentiation\\nsynchronous\\tupdates\\n,\\t\\nSynchronous\\tupdates', 'T\\nt-Distributed\\tStochastic\\tNeighbor\\tEmbedding\\t(t-SNE)\\n,\\t\\nOther\\tDimensionality\\tReduction\\nTechniques\\ntail\\theavy\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\ntarget\\tattributes\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\ntarget_weights\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\ntasks\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\nTemporal\\tDifference\\t(TD)\\tLearning\\n,\\t\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\n-\\nTemporal\\nDifference\\tLearning\\tand\\tQ-Learning\\ntensor\\tprocessing\\tunits\\t(TPUs)\\n,\\t\\nInstallation\\nTensorBoard\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nTensorFlow\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\n-\\nExercises\\nabout\\n,\\t\\nObjective\\tand\\tApproach\\nautodiff\\n,\\t\\nUsing\\tautodiff\\n-\\nUsing\\tautodiff\\n,\\t\\nAutodiff\\n-\\nReverse-Mode\\tAutodiff\\nBatch\\tNormalization\\twith\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n-\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\nconstruction\\tphase\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\ncontrol\\tdependencies\\n,\\t\\nControl\\tDependencies\\nconvenience\\tfunctions\\n,', ',\\t\\nOther\\tconvenience\\tfunctions\\nconvolutional\\tlayers\\n,\\t\\nResNet\\nconvolutional\\tneural\\tnetworks\\tand\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\nImplementation\\ndata\\tparallelism\\tand\\n,\\t\\nTensorFlow\\timplementation\\ndenoising\\tautoencoders\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\tImplementation\\ndropout\\twith\\n,\\t\\nDropout', 'dynamic\\tplacer\\n,\\t\\nPlacing\\tOperations\\ton\\tDevices\\nexecution\\tphase\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\nfeeding\\tdata\\tto\\tthe\\ttraining\\talgorithm\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\n-\\nFeeding\\nData\\tto\\tthe\\tTraining\\tAlgorithm\\nGradient\\tDescent\\twith\\n,\\t\\nImplementing\\tGradient\\tDescent\\n-\\nUsing\\tan\\tOptimizer\\ngraphs,\\tmanaging\\n,\\t\\nManaging\\tGraphs\\ninitial\\tgraph\\tcreation\\tand\\tsession\\trun\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\nSession\\n-\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\ninstallation\\n,\\t\\nInstallation\\nl1\\tand\\tl2\\tregularization\\twith\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\nlearning\\tschedules\\tin\\n,\\t\\nLearning\\tRate\\tScheduling\\nLinear\\tRegression\\twith\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n-\\nLinear\\tRegression\\twith\\nTensorFlow\\nmax\\tpooling\\tlayer\\tin\\n,\\t\\nPooling\\tLayer\\nmax-norm\\tregularization\\twith\\n,\\t\\nMax-Norm\\tRegularization\\nmodel\\tzoo\\n,\\t\\nModel\\tZoos\\nmodularity\\n,\\t\\nModularity\\n-\\nModularity\\nMomentum\\toptimization\\tin\\n,\\t\\nMomentum\\tOptimization\\nname\\tscopes\\n,\\t\\nName\\tScopes\\nneural\\tnetwork\\tpolicies\\n,', ',\\t\\nNeural\\tNetwork\\tPolicies\\nNLP\\ttutorials\\n,\\t\\nNatural\\tLanguage\\tProcessing\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\nMachine\\tTranslation\\nnode\\tvalue\\tlifecycle\\n,\\t\\nLifecycle\\tof\\ta\\tNode\\tValue\\noperations\\t(ops)\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow', 'optimizer\\n,\\t\\nUsing\\tan\\tOptimizer\\noverview\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\n-\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\nparallel\\tdistributed\\tcomputing\\n\\t(\\nsee\\n\\tparallel\\tdistributed\\tcomputing\\twith\\tTensorFlow)\\nPython\\tAPI\\nconstruction\\n,\\t\\nConstruction\\tPhase\\n-\\nConstruction\\tPhase\\nexecution\\n,\\t\\nExecution\\tPhase\\nusing\\tthe\\tneural\\tnetwork\\n,\\t\\nUsing\\tthe\\tNeural\\tNetwork\\nqueues\\n\\t(\\nsee\\n\\tqueues)\\nreusing\\tpretrained\\tlayers\\n,\\t\\nReusing\\ta\\tTensorFlow\\tModel\\n-\\nReusing\\ta\\tTensorFlow\\tModel\\nRNNs\\tin\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n-\\nHandling\\tVariable-Length\\tOutput\\tSequences\\n(\\nsee\\talso\\n\\trecurrent\\tneural\\tnetworks\\t(RNNs))\\nsaving\\tand\\trestoring\\tmodels\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\n-\\nSaving\\tand\\tRestoring\\nModels\\nsharing\\tvariables\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\nsimple\\tplacer\\n,\\t\\nPlacing\\tOperations\\ton\\tDevices\\nsparse\\tautoencoders\\twith\\n,\\t\\nTensorFlow\\tImplementation\\nand\\tstacked\\tautoencoders\\n,\\t\\nTensorFlow\\tImplementation\\nTensorBoard\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\n-', '-\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\ntf.abs()\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\ntf.add()\\n,\\t\\nModularity\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\ntf.add_n()\\n,\\t\\nModularity\\n-\\nSharing\\tVariables\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\ntf.add_to_collection()\\n,\\t\\nMax-Norm\\tRegularization\\ntf.assign()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nReusing\\tModels\\tfrom\\tOther', 'Frameworks\\n,\\t\\nMax-Norm\\tRegularization\\n-\\nMax-Norm\\tRegularization\\n,\\t\\nChapter\\t9:\\tUp\\tand\\nRunning\\twith\\tTensorFlow\\ntf.bfloat16\\n,\\t\\nBandwidth\\tsaturation\\ntf.bool\\n,\\t\\nDropout\\ntf.cast()\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\ntf.clip_by_norm()\\n,\\t\\nMax-Norm\\tRegularization\\n-\\nMax-Norm\\tRegularization\\ntf.clip_by_value()\\n,\\t\\nGradient\\tClipping\\ntf.concat()\\n,\\t\\nExercises\\n,\\t\\nGoogLeNet\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n,\\t\\nPolicy\\tGradients\\ntf.ConfigProto\\n,\\t\\nManaging\\tthe\\tGPU\\tRAM\\n,\\t\\nLogging\\tplacements\\n-\\nSoft\\tplacement\\n,\\t\\nIn-\\nGraph\\tVersus\\tBetween-Graph\\tReplication\\n,\\t\\nChapter\\t12:\\tDistributing\\tTensorFlow\\tAcross\\nDevices\\tand\\tServers\\ntf.constant()\\n,\\t\\nLifecycle\\tof\\ta\\tNode\\tValue\\n-\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nSimple\\nplacement\\n-\\nDynamic\\tplacement\\tfunction\\n,\\t\\nControl\\tDependencies\\n,\\t\\nOpening\\ta\\tSession\\n-\\nPinning\\tOperations\\tAcross\\tTasks\\ntf.constant_initializer()\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\ntf.container()\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\n-\\nAsynchronous', 'Communication\\tUsing\\tTensorFlow\\tQueues\\n,\\t\\nTensorFlow\\timplementation\\n-\\nExercises\\n,\\nChapter\\t9:\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\ntf.contrib.layers.l1_regularizer()\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n,\\t\\nMax-Norm\\tRegularization\\ntf.contrib.layers.l2_regularizer()\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n,\\t\\nTensorFlow\\nImplementation\\n-\\nTying\\tWeights\\ntf.contrib.layers.variance_scaling_initializer()\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\n-\\nXavier\\tand\\nHe\\tInitialization\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTying\\nWeights\\n,\\t\\nVariational\\tAutoencoders\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n,\\t\\nPolicy\\tGradients\\n,\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.contrib.learn.DNNClassifier\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\ntf.contrib.learn.infer_real_valued_columns_from_input()\\n,\\t\\nTraining\\tan\\tMLP\\twith\\nTensorFlow’s\\tHigh-Level\\tAPI', 'tf.contrib.rnn.BasicLSTMCell\\n,\\t\\nLSTM\\tCell\\n,\\t\\nPeephole\\tConnections\\ntf.contrib.rnn.BasicRNNCell\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nDynamic\\tUnrolling\\nThrough\\tTime\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n-\\nTraining\\nto\\tPredict\\tTime\\tSeries\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n,\\t\\nDeep\\tRNNs\\n-\\nApplying\\tDropout\\n,\\nLSTM\\tCell\\ntf.contrib.rnn.DropoutWrapper\\n,\\t\\nApplying\\tDropout\\ntf.contrib.rnn.GRUCell\\n,\\t\\nGRU\\tCell\\ntf.contrib.rnn.LSTMCell\\n,\\t\\nPeephole\\tConnections\\ntf.contrib.rnn.MultiRNNCell\\n,\\t\\nDeep\\tRNNs\\n-\\nApplying\\tDropout\\ntf.contrib.rnn.OutputProjectionWrapper\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n-\\nTraining\\tto\\nPredict\\tTime\\tSeries\\ntf.contrib.rnn.RNNCell\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\ntf.contrib.rnn.static_rnn()\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n-\\nHandling\\tVariable\\tLength\\tInput\\nSequences\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\n-\\nExercises\\n,\\nChapter\\t14:\\tRecurrent\\tNeural\\tNetworks\\n-\\nChapter\\t14:\\tRecurrent\\tNeural\\tNetworks\\ntf.contrib.slim\\tmodule\\n,', ',\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\n,\\t\\nExercises\\ntf.contrib.slim.nets\\tmodule\\t(nets)\\n,\\t\\nExercises\\ntf.control_dependencies()\\n,\\t\\nControl\\tDependencies\\ntf.decode_csv()\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\nreaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\ntf.device()\\n,\\t\\nSimple\\tplacement\\n-\\nSoft\\tplacement\\n,\\t\\nPinning\\tOperations\\tAcross\\tTasks\\n-\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\n,\\t\\nDistributing\\ta\\tDeep\\tRNN\\nAcross\\tMultiple\\tGPUs\\n-\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\ntf.exp()\\n,\\t\\nVariational\\tAutoencoders\\n-\\nGenerating\\tDigits\\ntf.FIFOQueue\\n,\\t\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\n,\\t\\nQueues\\tof\\ntuples\\n-\\nRandomShuffleQueue\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner', 'tf.float32\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n,\\t\\nChapter\\t9:\\tUp\\tand\\tRunning\\twith\\nTensorFlow\\ntf.get_collection()\\n,\\t\\nReusing\\ta\\tTensorFlow\\tModel\\n-\\nFreezing\\tthe\\tLower\\tLayers\\n,\\t\\nℓ1\\tand\\tℓ2\\nRegularization\\n,\\t\\nMax-Norm\\tRegularization\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nLearning\\tto\\nPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.get_default_graph()\\n,\\t\\nManaging\\tGraphs\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\nUsing\\tTensorBoard\\ntf.get_default_session()\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\ntf.get_variable()\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\n,\\t\\nReusing\\tModels\\tfrom\\tOther\\nFrameworks\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\ntf.global_variables()\\n,\\t\\nReusing\\ta\\tTensorFlow\\tModel\\ntf.global_variables_initializer()\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\nManually\\tComputing\\tthe\\tGradients\\ntf.gradients()\\n,\\t\\nUsing\\tautodiff\\ntf.Graph\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\t\\nManaging\\tGraphs\\n,\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\n,', ',\\t\\nLoading\\tData\\tDirectly\\nfrom\\tthe\\tGraph\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\ntf.GraphKeys.GLOBAL_VARIABLES\\n,\\t\\nReusing\\ta\\tTensorFlow\\tModel\\n-\\nFreezing\\tthe\\nLower\\tLayers\\ntf.GraphKeys.REGULARIZATION_LOSSES\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n,\\t\\nTensorFlow\\nImplementation\\ntf.group()\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.int32\\n,\\t\\nOperations\\tand\\tkernels\\n-\\nQueues\\tof\\ttuples\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\nfrom\\tthe\\tgraph\\n,\\t\\nHandling\\tVariable\\tLength\\tInput\\tSequences\\n,\\t\\nTraining\\ta\\tSequence\\nClassifier\\n,\\t\\nWord\\tEmbeddings\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.int64\\n,\\t\\nConstruction\\tPhase\\ntf.InteractiveSession\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\ntf.layers.batch_normalization()\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n-', 'Implementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\ntf.layers.dense()\\n,\\t\\nConstruction\\tPhase\\nTF.Learn\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\ntf.log()\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nVariational\\tAutoencoders\\n,\\t\\nNeural\\tNetwork\\nPolicies\\n,\\t\\nPolicy\\tGradients\\ntf.matmul()\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n-\\nManually\\tComputing\\tthe\\tGradients\\n,\\nModularity\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n,\\t\\nTying\\tWeights\\n,\\t\\nTraining\\nOne\\tAutoencoder\\tat\\ta\\tTime\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\tImplementation\\ntf.matrix_inverse()\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\ntf.maximum()\\n,\\t\\nModularity\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\n,\\t\\nNonsaturating\\nActivation\\tFunctions\\ntf.multinomial()\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n,\\t\\nPolicy\\tGradients\\ntf.name_scope()\\n,\\t\\nName\\tScopes\\n,\\t\\nModularity\\n-\\nSharing\\tVariables\\n,\\t\\nConstruction\\tPhase\\n,\\nConstruction\\tPhase\\n-\\nConstruction\\tPhase\\n,\\t\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\n-\\nTraining\\nOne\\tAutoencoder\\tat\\ta\\tTime\\ntf.nn.conv2d()\\n,', ',\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\tImplementation\\ntf.nn.dynamic_rnn()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nDynamic\\tUnrolling\\tThrough\\tTime\\n,\\nTraining\\ta\\tSequence\\tClassifier\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n,\\t\\nTraining\\tto\\tPredict\\nTime\\tSeries\\n,\\t\\nDeep\\tRNNs\\n-\\nApplying\\tDropout\\n,\\t\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\nMachine\\tTranslation\\n-\\nExercises\\n,\\t\\nChapter\\t14:\\tRecurrent\\tNeural\\tNetworks\\n-\\nChapter\\t14:\\nRecurrent\\tNeural\\tNetworks\\ntf.nn.elu()\\n,\\t\\nNonsaturating\\tActivation\\tFunctions\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTying\\nWeights\\n,\\t\\nVariational\\tAutoencoders\\n,\\t\\nNeural\\tNetwork\\tPolicies\\n,\\t\\nPolicy\\tGradients\\ntf.nn.embedding_lookup()\\n,\\t\\nWord\\tEmbeddings\\ntf.nn.in_top_k()\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\ntf.nn.max_pool()\\n,\\t\\nPooling\\tLayer\\n-\\nPooling\\tLayer\\ntf.nn.relu()\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n-\\nTraining\\tto\\tPredict', 'Time\\tSeries\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\nDeep\\tQ-Learning\\ntf.nn.sigmoid_cross_entropy_with_logits()\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nGenerating\\nDigits\\n,\\t\\nPolicy\\tGradients\\n-\\nPolicy\\tGradients\\ntf.nn.sparse_softmax_cross_entropy_with_logits()\\n,\\t\\nConstruction\\tPhase\\n-\\nConstruction\\nPhase\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\ntf.one_hot()\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.PaddingFIFOQueue\\n,\\t\\nPaddingFifoQueue\\ntf.placeholder()\\n,\\t\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\n-\\nFeeding\\tData\\tto\\tthe\\tTraining\\nAlgorithm\\n,\\t\\nChapter\\t9:\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\ntf.placeholder_with_default()\\n,\\t\\nTensorFlow\\tImplementation\\ntf.RandomShuffleQueue\\n,\\t\\nRandomShuffleQueue\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\nthe\\tgraph\\n-\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\treaders\\nusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nOther\\tconvenience\\tfunctions\\ntf.random_normal()\\n,\\t\\nModularity\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n,\\t\\nTensorFlow\\nImplementation', ',\\t\\nVariational\\tAutoencoders\\ntf.random_uniform()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\n,\\nWord\\tEmbeddings\\n,\\t\\nChapter\\t9:\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\ntf.reduce_mean()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nName\\tScopes\\n,\\t\\nConstruction\\nPhase\\n-\\nConstruction\\tPhase\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n,\\t\\nTraining\\ta\\tSequence\\tClassifier\\n-\\nTraining\\ta\\tSequence\\tClassifier\\n,\\t\\nPerforming\\tPCA\\twith\\tan\\tUndercomplete\\tLinear\\nAutoencoder\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\n,\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nTensorFlow\\nImplementation\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.reduce_sum()\\n,\\t\\nℓ1\\tand\\tℓ2\\tRegularization\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTensorFlow\\nImplementation\\n,\\t\\nVariational\\tAutoencoders\\n-\\nGenerating\\tDigits\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\nPac-Man\\tUsing\\tDeep\\tQ-Learning\\n-\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-\\nLearning\\ntf.reset_default_graph()\\n,\\t\\nManaging\\tGraphs', 'tf.reshape()\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\nDeep\\tQ-Learning\\ntf.RunOptions\\n,\\t\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication\\ntf.Session\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\t\\nChapter\\t9:\\tUp\\tand\\nRunning\\twith\\tTensorFlow\\ntf.shape()\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nVariational\\tAutoencoders\\ntf.square()\\n,\\t\\nManually\\tComputing\\tthe\\tGradients\\n,\\t\\nName\\tScopes\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\nSeries\\n,\\t\\nPerforming\\tPCA\\twith\\tan\\tUndercomplete\\tLinear\\tAutoencoder\\n,\\t\\nTensorFlow\\nImplementation\\n,\\t\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\n,\\t\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\nTime\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nVariational\\nAutoencoders\\n-\\nGenerating\\tDigits\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-\\nLearning\\ntf.stack()\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\treaders\\nusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\ntf.string\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,', ',\\t\\nMultithreaded\\treaders\\tusing\\na\\tCoordinator\\tand\\ta\\tQueueRunner\\ntf.summary.FileWriter\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\n-\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\ntf.summary.scalar()\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\ntf.tanh()\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\ntf.TextLineReader\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\n,\\t\\nMultithreaded\\nreaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\ntf.to_float()\\n,\\t\\nPolicy\\tGradients\\n-\\nPolicy\\tGradients\\ntf.train.AdamOptimizer\\n,\\t\\nAdam\\tOptimization\\n,\\t\\nAdam\\tOptimization\\n,\\t\\nTraining\\ta\\tSequence\\nClassifier\\n,\\t\\nTraining\\tto\\tPredict\\tTime\\tSeries\\n,\\t\\nPerforming\\tPCA\\twith\\tan\\tUndercomplete\\nLinear\\tAutoencoder\\n,\\t\\nTensorFlow\\tImplementation\\n-\\nTying\\tWeights\\n,\\t\\nTraining\\tOne\\nAutoencoder\\tat\\ta\\tTime\\n,\\t\\nTensorFlow\\tImplementation\\n,\\t\\nGenerating\\tDigits\\n,\\t\\nPolicy\\nGradients\\n-\\nPolicy\\tGradients\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.train.ClusterSpec\\n,', ',\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers', 'tf.train.Coordinator\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\ntf.train.exponential_decay()\\n,\\t\\nLearning\\tRate\\tScheduling\\ntf.train.GradientDescentOptimizer\\n,\\t\\nUsing\\tan\\tOptimizer\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nGradient\\nClipping\\n,\\t\\nMomentum\\tOptimization\\n,\\t\\nAdam\\tOptimization\\ntf.train.MomentumOptimizer\\n,\\t\\nUsing\\tan\\tOptimizer\\n,\\t\\nMomentum\\tOptimization\\n-\\nNesterov\\nAccelerated\\tGradient\\n,\\t\\nLearning\\tRate\\tScheduling\\n,\\t\\nExercises\\n,\\t\\nTensorFlow\\nimplementation\\n,\\t\\nChapter\\t10:\\tIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\n-\\nChapter\\t11:\\nTraining\\tDeep\\tNeural\\tNets\\ntf.train.QueueRunner\\n,\\t\\nMultithreaded\\treaders\\tusing\\ta\\tCoordinator\\tand\\ta\\tQueueRunner\\n-\\nOther\\tconvenience\\tfunctions\\ntf.train.replica_device_setter()\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\n-\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\ntf.train.RMSPropOptimizer\\n,\\t\\nRMSProp\\ntf.train.Saver\\n,\\t\\nSaving\\tand\\tRestoring\\tModels\\n-\\nSaving\\tand\\tRestoring\\tModels\\n,', ',\\t\\nConstruction\\nPhase\\n,\\t\\nExercises\\n,\\t\\nApplying\\tDropout\\n,\\t\\nPolicy\\tGradients\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\nUsing\\tDeep\\tQ-Learning\\ntf.train.Server\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\ntf.train.start_queue_runners()\\n,\\t\\nOther\\tconvenience\\tfunctions\\ntf.transpose()\\n,\\t\\nLinear\\tRegression\\twith\\tTensorFlow\\n-\\nManually\\tComputing\\tthe\\tGradients\\n,\\nStatic\\tUnrolling\\tThrough\\tTime\\n,\\t\\nTying\\tWeights\\ntf.truncated_normal()\\n,\\t\\nConstruction\\tPhase\\ntf.unstack()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\n-\\nDynamic\\tUnrolling\\tThrough\\tTime\\n,\\t\\nTraining\\nto\\tPredict\\tTime\\tSeries\\n,\\t\\nChapter\\t14:\\tRecurrent\\tNeural\\tNetworks\\ntf.Variable\\n,\\t\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\n,\\t\\nChapter\\t9:\\tUp\\tand\\nRunning\\twith\\tTensorFlow\\ntf.variable_scope()\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\n,\\t\\nReusing\\tModels\\tfrom\\tOther\\nFrameworks\\n,\\t\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\n,\\t\\nTraining\\ta', 'Sequence\\tClassifier\\n,\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.zeros()\\n,\\t\\nConstruction\\tPhase\\n,\\t\\nBasic\\tRNNs\\tin\\tTensorFlow\\n,\\t\\nTying\\tWeights\\ntruncated\\tbackpropagation\\tthrough\\ttime\\n,\\t\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\nSteps\\nvisualizing\\tgraph\\tand\\ttraining\\tcurves\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\nTensorBoard\\n-\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\nTensorFlow\\tServing\\n,\\t\\nOne\\tNeural\\tNetwork\\tper\\tDevice\\ntensorflow.contrib\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\ntest\\tset\\n,\\t\\nTesting\\tand\\tValidating\\n,\\t\\nCreate\\ta\\tTest\\tSet\\n-\\nCreate\\ta\\tTest\\tSet\\n,\\t\\nMNIST\\ntesting\\tand\\tvalidating\\n,\\t\\nTesting\\tand\\tValidating\\n-\\nTesting\\tand\\tValidating\\ntext\\tattributes\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\n-\\nHandling\\tText\\tand\\tCategorical\\nAttributes\\nTextLineReader\\n,\\t\\nReading\\tthe\\ttraining\\tdata\\tdirectly\\tfrom\\tthe\\tgraph\\nTF-slim\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\ntf.layers.conv1d()\\n,\\t\\nResNet\\ntf.layers.conv2d()\\n,', ',\\t\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\ntf.layers.conv2d_transpose()\\n,\\t\\nResNet\\ntf.layers.conv3d()\\n,\\t\\nResNet\\ntf.layers.dense()\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\nTensorFlow\\ntf.layers.separable_conv2d()\\n,\\t\\nResNet\\nTF.Learn\\n,\\t\\nUp\\tand\\tRunning\\twith\\tTensorFlow\\n,\\t\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\nAPI\\ntf.nn.atrous_conv2d()\\n,\\t\\nResNet\\ntf.nn.depthwise_conv2d()\\n,\\t\\nResNet', 'thermal\\tequilibrium\\n,\\t\\nBoltzmann\\tMachines\\nthread\\tpools\\t(inter-op/intra-op,\\tin\\tTensorFlow\\n,\\t\\nParallel\\tExecution\\nthreshold\\t\\nvariable\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\nTikhonov\\tregularization\\n,\\t\\nRidge\\tRegression\\ntime\\tseries\\tdata\\n,\\t\\nRecurrent\\tNeural\\tNetworks\\ntoarray()\\n,\\t\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\ntolerance\\thyperparameter\\n,\\t\\nComputational\\tComplexity\\ntraining\\n,\\t\\nImplementing\\tBatch\\tNormalization\\twith\\tTensorFlow\\n-\\nImplementing\\tBatch\\nNormalization\\twith\\tTensorFlow\\n,\\t\\nApplying\\tDropout\\ntraining\\tdata\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\ninsufficient\\tquantities\\n,\\t\\nInsufficient\\tQuantity\\tof\\tTraining\\tData\\nirrelevant\\tfeatures\\n,\\t\\nIrrelevant\\tFeatures\\nloading\\n,\\t\\nLoading\\tData\\tDirectly\\tfrom\\tthe\\tGraph\\n-\\nOther\\tconvenience\\tfunctions\\nnonrepresentative\\n,\\t\\nNonrepresentative\\tTraining\\tData\\noverfitting\\n,\\t\\nOverfitting\\tthe\\tTraining\\tData\\n-\\nOverfitting\\tthe\\tTraining\\tData\\npoor\\tquality\\n,\\t\\nPoor-Quality\\tData\\nunderfitting\\n,\\t\\nUnderfitting\\tthe\\tTraining\\tData\\ntraining\\tinstance\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?', 'training\\tmodels\\n,\\t\\nModel-based\\tlearning\\n,\\t\\nTraining\\tModels\\n-\\nExercises\\nlearning\\tcurves\\tin\\n,\\t\\nLearning\\tCurves\\n-\\nLearning\\tCurves\\nLinear\\tRegression\\n,\\t\\nTraining\\tModels\\n,\\t\\nLinear\\tRegression\\n-\\nMini-batch\\tGradient\\tDescent\\nLogistic\\tRegression\\n,\\t\\nLogistic\\tRegression\\n-\\nSoftmax\\tRegression\\noverview\\n,\\t\\nTraining\\tModels\\n-\\nTraining\\tModels\\nPolynomial\\tRegression\\n,\\t\\nTraining\\tModels\\n,\\t\\nPolynomial\\tRegression\\n-\\nPolynomial\\tRegression', 'training\\tobjectives\\n,\\t\\nTraining\\tObjective\\n-\\nTraining\\tObjective\\ntraining\\tset\\n,\\t\\nWhat\\tIs\\tMachine\\tLearning?\\n,\\t\\nTesting\\tand\\tValidating\\n,\\t\\nDiscover\\tand\\tVisualize\\tthe\\nData\\tto\\tGain\\tInsights\\n,\\t\\nPrepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\tAlgorithms\\n,\\t\\nTraining\\tand\\nEvaluating\\ton\\tthe\\tTraining\\tSet\\n-\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\ncost\\tfunction\\tof\\n,\\t\\nTraining\\tand\\tCost\\tFunction\\n-\\nTraining\\tand\\tCost\\tFunction\\nshuffling\\n,\\t\\nMNIST\\ntransfer\\tlearning\\n,\\t\\nReusing\\tPretrained\\tLayers\\n-\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\n(\\nsee\\talso\\n\\tpretrained\\tlayers\\treuse)\\ntransform()\\n,\\t\\nData\\tCleaning\\n,\\t\\nTransformation\\tPipelines\\ntransformation\\tpipelines\\n,\\t\\nTransformation\\tPipelines\\n-\\nSelect\\tand\\tTrain\\ta\\tModel\\ntransformers\\n,\\t\\nData\\tCleaning\\ntransformers,\\tcustom\\n,\\t\\nCustom\\tTransformers\\n-\\nCustom\\tTransformers\\ntranspose()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\ntrue\\tnegative\\trate\\t(TNR)\\n,\\t\\nThe\\tROC\\tCurve\\ntrue\\tpositive\\trate\\t(TPR)\\n,\\t\\nConfusion\\tMatrix\\n,\\t\\nThe\\tROC\\tCurve\\ntruncated\\tbackpropagation\\tthrough\\ttime\\n,', ',\\t\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps\\ntuples\\n,\\t\\nQueues\\tof\\ttuples\\ntying\\tweights\\n,\\t\\nTying\\tWeights\\nU\\nunderfitting\\n,\\t\\nUnderfitting\\tthe\\tTraining\\tData\\n,\\t\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet\\n,\\nGaussian\\tRBF\\tKernel\\nunivariate\\tregression\\n,\\t\\nFrame\\tthe\\tProblem\\nunstack()\\n,\\t\\nStatic\\tUnrolling\\tThrough\\tTime\\nunsupervised\\tlearning\\n,\\t\\nUnsupervised\\tlearning\\n-\\nUnsupervised\\tlearning\\nanomaly\\tdetection\\n,\\t\\nUnsupervised\\tlearning', 'association\\trule\\tlearning\\n,\\t\\nUnsupervised\\tlearning\\n,\\t\\nUnsupervised\\tlearning\\nclustering\\n,\\t\\nUnsupervised\\tlearning\\ndimensionality\\treduction\\talgorithm\\n,\\t\\nUnsupervised\\tlearning\\nvisualization\\talgorithms\\n,\\t\\nUnsupervised\\tlearning\\nunsupervised\\tpretraining\\n,\\t\\nUnsupervised\\tPretraining\\n-\\nUnsupervised\\tPretraining\\n,\\t\\nUnsupervised\\nPretraining\\tUsing\\tStacked\\tAutoencoders\\n-\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\nAutoencoders\\nupsampling\\n,\\t\\nResNet\\nutility\\tfunction\\n,\\t\\nModel-based\\tlearning\\nV\\nvalidation\\tset\\n,\\t\\nTesting\\tand\\tValidating\\nValue\\tIteration\\n,\\t\\nMarkov\\tDecision\\tProcesses\\nvalue_counts()\\n,\\t\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\nvanishing\\tgradients\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n(\\nsee\\talso\\n\\tgradients,\\tvanishing\\tand\\texploding)\\nvariables,\\tsharing\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\nvariable_scope()\\n,\\t\\nSharing\\tVariables\\n-\\nSharing\\tVariables\\nvariance\\nbias/variance\\ttradeoff\\n,\\t\\nLearning\\tCurves\\nvariance\\tpreservation\\n,\\t\\nPreserving\\tthe\\tVariance\\n-\\nPreserving\\tthe\\tVariance', 'variance_scaling_initializer()\\n,\\t\\nXavier\\tand\\tHe\\tInitialization\\nvariational\\tautoencoders\\n,\\t\\nVariational\\tAutoencoders\\n-\\nGenerating\\tDigits\\nVGGNet\\n,\\t\\nResNet\\nvisual\\tcortex\\n,\\t\\nThe\\tArchitecture\\tof\\tthe\\tVisual\\tCortex', 'visualization\\n,\\t\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\n-\\nVisualizing\\tthe\\nGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\nvisualization\\talgorithms\\n,\\t\\nUnsupervised\\tlearning\\n-\\nUnsupervised\\tlearning\\nvoice\\trecognition\\n,\\t\\nConvolutional\\tNeural\\tNetworks\\nvoting\\tclassifiers\\n,\\t\\nVoting\\tClassifiers\\n-\\nVoting\\tClassifiers\\nW\\nwarmup\\tphase\\n,\\t\\nAsynchronous\\tupdates\\nweak\\tlearners\\n,\\t\\nVoting\\tClassifiers\\nweight-tying\\n,\\t\\nTying\\tWeights\\nweights\\n,\\t\\nConstruction\\tPhase\\nfreezing\\n,\\t\\nFreezing\\tthe\\tLower\\tLayers\\nwhile_loop()\\n,\\t\\nDynamic\\tUnrolling\\tThrough\\tTime\\nwhite\\tbox\\tmodels\\n,\\t\\nMaking\\tPredictions\\nworker\\n,\\t\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\nworker\\tservice\\n,\\t\\nThe\\tMaster\\tand\\tWorker\\tServices\\nworker_device\\n,\\t\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nworkspace\\tdirectory\\n,\\t\\nGet\\tthe\\tData\\n-\\nDownload\\tthe\\tData\\nX\\nXavier\\tinitialization\\n,\\t\\nVanishing/Exploding\\tGradients\\tProblems\\n-\\nXavier\\tand\\tHe\\tInitialization\\nY\\nYouTube\\n,\\t\\nIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\nZ\\nzero\\tpadding\\n,', ',\\t\\nConvolutional\\tLayer\\n,\\t\\nTensorFlow\\tImplementation', 'About\\tthe\\tAuthor\\nAurélien\\tGéron\\n\\tis\\ta\\tMachine\\tLearning\\tconsultant.\\tA\\tformer\\tGoogler,\\the\\tled\\tthe\\tYouTube\\tvideo\\nclassification\\tteam\\tfrom\\t2013\\tto\\t2016.\\tHe\\twas\\talso\\ta\\tfounder\\tand\\tCTO\\tof\\tWifirst\\tfrom\\t2002\\tto\\t2012,\\ta\\nleading\\tWireless\\tISP\\tin\\tFrance;\\tand\\ta\\tfounder\\tand\\tCTO\\tof\\tPolyconseil\\tin\\t2001,\\tthe\\tfirm\\tthat\\tnow\\nmanages\\tthe\\telectric\\tcar\\tsharing\\tservice\\tAutolib’.\\nBefore\\tthis\\the\\tworked\\tas\\tan\\tengineer\\tin\\ta\\tvariety\\tof\\tdomains:\\tfinance\\t(JP\\tMorgan\\tand\\tSociété\\tGénérale),\\ndefense\\t(Canada’s\\tDOD),\\tand\\thealthcare\\t(blood\\ttransfusion).\\tHe\\tpublished\\ta\\tfew\\ttechnical\\tbooks\\t(on\\nC++,\\tWiFi,\\tand\\tinternet\\tarchitectures),\\tand\\twas\\ta\\tComputer\\tScience\\tlecturer\\tin\\ta\\tFrench\\tengineering\\nschool.\\nA\\tfew\\tfun\\tfacts:\\the\\ttaught\\this\\tthree\\tchildren\\tto\\tcount\\tin\\tbinary\\twith\\ttheir\\tfingers\\t(up\\tto\\t1023),\\the\\tstudied\\nmicrobiology\\tand\\tevolutionary\\tgenetics\\tbefore\\tgoing\\tinto\\tsoftware\\tengineering,\\tand\\this\\tparachute\\tdidn’t\\nopen\\ton\\tthe\\tsecond\\tjump.', 'Colophon\\nThe\\tanimal\\ton\\tthe\\tcover\\tof\\t\\nHands-On\\tMachine\\tLearning\\twith\\tScikit-Learn\\tand\\tTensorFlow\\n\\tis\\tthe\\tfar\\neastern\\tfire\\tsalamander\\t(\\nSalamandra\\tinfraimmaculata\\n),\\tan\\tamphibian\\tfound\\tin\\tthe\\tMiddle\\tEast.\\tThey\\nhave\\tblack\\tskin\\tfeaturing\\tlarge\\tyellow\\tspots\\ton\\ttheir\\tback\\tand\\thead.\\tThese\\tspots\\tare\\ta\\twarning\\tcoloration\\nmeant\\tto\\tkeep\\tpredators\\tat\\tbay.\\tFull-grown\\tsalamanders\\tcan\\tbe\\tover\\ta\\tfoot\\tin\\tlength.\\nFar\\teastern\\tfire\\tsalamanders\\tlive\\tin\\tsubtropical\\tshrubland\\tand\\tforests\\tnear\\trivers\\tor\\tother\\tfreshwater\\nbodies.\\tThey\\tspend\\tmost\\tof\\ttheir\\tlife\\ton\\tland,\\tbut\\tlay\\ttheir\\teggs\\tin\\tthe\\twater.\\tThey\\tsubsist\\tmostly\\ton\\ta\\tdiet\\nof\\tinsects,\\tworms,\\tand\\tsmall\\tcrustaceans,\\tbut\\toccasionally\\teat\\tother\\tsalamanders.\\tMales\\tof\\tthe\\tspecies\\nhave\\tbeen\\tknown\\tto\\tlive\\tup\\tto\\t23\\tyears,\\twhile\\tfemales\\tcan\\tlive\\tup\\tto\\t21\\tyears.\\nAlthough\\tnot\\tyet\\tendangered,\\tthe\\tfar\\teastern\\tfire\\tsalamander\\tpopulation\\tis\\tin\\tdecline.\\tPrimary\\tthreats\\ninclude\\tdamming\\tof\\trivers\\t(which\\tdisrupts\\tthe\\tsalamander’s\\tbreeding)\\tand\\tpollution.\\tThey\\tare\\talso', 'threatened\\tby\\tthe\\trecent\\tintroduction\\tof\\tpredatory\\tfish,\\tsuch\\tas\\tthe\\tmosquitofish.\\tThese\\tfish\\twere\\tintended\\nto\\tcontrol\\tthe\\tmosquito\\tpopulation,\\tbut\\tthey\\talso\\tfeed\\ton\\tyoung\\tsalamanders.\\nMany\\tof\\tthe\\tanimals\\ton\\tO’Reilly\\tcovers\\tare\\tendangered;\\tall\\tof\\tthem\\tare\\timportant\\tto\\tthe\\tworld.\\tTo\\tlearn\\nmore\\tabout\\thow\\tyou\\tcan\\thelp,\\tgo\\tto\\t\\nanimals.oreilly.com\\n.\\nThe\\tcover\\timage\\tis\\tfrom\\t\\nWood’s\\tIllustrated\\tNatural\\tHistory\\n.\\tThe\\tcover\\tfonts\\tare\\tURW\\tTypewriter\\tand\\nGuardian\\tSans.\\tThe\\ttext\\tfont\\tis\\tAdobe\\tMinion\\tPro;\\tthe\\theading\\tfont\\tis\\tAdobe\\tMyriad\\tCondensed;\\tand\\tthe\\ncode\\tfont\\tis\\tDalton\\tMaag’s\\tUbuntu\\tMono.', 'Preface\\nThe\\tMachine\\tLearning\\tTsunami\\nMachine\\tLearning\\tin\\tYour\\tProjects\\nObjective\\tand\\tApproach\\nPrerequisites\\nRoadmap\\nOther\\tResources\\nConventions\\tUsed\\tin\\tThis\\tBook\\nUsing\\tCode\\tExamples\\nO’Reilly\\tSafari\\nHow\\tto\\tContact\\tUs\\nAcknowledgments\\nI.\\tThe\\tFundamentals\\tof\\tMachine\\tLearning\\n1.\\tThe\\tMachine\\tLearning\\tLandscape\\nWhat\\tIs\\tMachine\\tLearning?\\nWhy\\tUse\\tMachine\\tLearning?\\nTypes\\tof\\tMachine\\tLearning\\tSystems\\nSupervised/Unsupervised\\tLearning\\nBatch\\tand\\tOnline\\tLearning\\nInstance-Based\\tVersus\\tModel-Based\\tLearning\\nMain\\tChallenges\\tof\\tMachine\\tLearning\\nInsufficient\\tQuantity\\tof\\tTraining\\tData\\nNonrepresentative\\tTraining\\tData\\nPoor-Quality\\tData\\nIrrelevant\\tFeatures\\nOverfitting\\tthe\\tTraining\\tData', 'Underfitting\\tthe\\tTraining\\tData\\nStepping\\tBack\\nTesting\\tand\\tValidating\\nExercises\\n2.\\tEnd-to-End\\tMachine\\tLearning\\tProject\\nWorking\\twith\\tReal\\tData\\nLook\\tat\\tthe\\tBig\\tPicture\\nFrame\\tthe\\tProblem\\nSelect\\ta\\tPerformance\\tMeasure\\nCheck\\tthe\\tAssumptions\\nGet\\tthe\\tData\\nCreate\\tthe\\tWorkspace\\nDownload\\tthe\\tData\\nTake\\ta\\tQuick\\tLook\\tat\\tthe\\tData\\tStructure\\nCreate\\ta\\tTest\\tSet\\nDiscover\\tand\\tVisualize\\tthe\\tData\\tto\\tGain\\tInsights\\nVisualizing\\tGeographical\\tData\\nLooking\\tfor\\tCorrelations\\nExperimenting\\twith\\tAttribute\\tCombinations\\nPrepare\\tthe\\tData\\tfor\\tMachine\\tLearning\\tAlgorithms\\nData\\tCleaning\\nHandling\\tText\\tand\\tCategorical\\tAttributes\\nCustom\\tTransformers\\nFeature\\tScaling\\nTransformation\\tPipelines\\nSelect\\tand\\tTrain\\ta\\tModel\\nTraining\\tand\\tEvaluating\\ton\\tthe\\tTraining\\tSet', 'Better\\tEvaluation\\tUsing\\tCross-Validation\\nFine-Tune\\tYour\\tModel\\nGrid\\tSearch\\nRandomized\\tSearch\\nEnsemble\\tMethods\\nAnalyze\\tthe\\tBest\\tModels\\tand\\tTheir\\tErrors\\nEvaluate\\tYour\\tSystem\\ton\\tthe\\tTest\\tSet\\nLaunch,\\tMonitor,\\tand\\tMaintain\\tYour\\tSystem\\nTry\\tIt\\tOut!\\nExercises\\n3.\\tClassification\\nMNIST\\nTraining\\ta\\tBinary\\tClassifier\\nPerformance\\tMeasures\\nMeasuring\\tAccuracy\\tUsing\\tCross-Validation\\nConfusion\\tMatrix\\nPrecision\\tand\\tRecall\\nPrecision/Recall\\tTradeoff\\nThe\\tROC\\tCurve\\nMulticlass\\tClassification\\nError\\tAnalysis\\nMultilabel\\tClassification\\nMultioutput\\tClassification\\nExercises\\n4.\\tTraining\\tModels\\nLinear\\tRegression\\nThe\\tNormal\\tEquation', 'Computational\\tComplexity\\nGradient\\tDescent\\nBatch\\tGradient\\tDescent\\nStochastic\\tGradient\\tDescent\\nMini-batch\\tGradient\\tDescent\\nPolynomial\\tRegression\\nLearning\\tCurves\\nRegularized\\tLinear\\tModels\\nRidge\\tRegression\\nLasso\\tRegression\\nElastic\\tNet\\nEarly\\tStopping\\nLogistic\\tRegression\\nEstimating\\tProbabilities\\nTraining\\tand\\tCost\\tFunction\\nDecision\\tBoundaries\\nSoftmax\\tRegression\\nExercises\\n5.\\tSupport\\tVector\\tMachines\\nLinear\\tSVM\\tClassification\\nSoft\\tMargin\\tClassification\\nNonlinear\\tSVM\\tClassification\\nPolynomial\\tKernel\\nAdding\\tSimilarity\\tFeatures\\nGaussian\\tRBF\\tKernel\\nComputational\\tComplexity\\nSVM\\tRegression', 'Under\\tthe\\tHood\\nDecision\\tFunction\\tand\\tPredictions\\nTraining\\tObjective\\nQuadratic\\tProgramming\\nThe\\tDual\\tProblem\\nKernelized\\tSVM\\nOnline\\tSVMs\\nExercises\\n6.\\tDecision\\tTrees\\nTraining\\tand\\tVisualizing\\ta\\tDecision\\tTree\\nMaking\\tPredictions\\nEstimating\\tClass\\tProbabilities\\nThe\\tCART\\tTraining\\tAlgorithm\\nComputational\\tComplexity\\nGini\\tImpurity\\tor\\tEntropy?\\nRegularization\\tHyperparameters\\nRegression\\nInstability\\nExercises\\n7.\\tEnsemble\\tLearning\\tand\\tRandom\\tForests\\nVoting\\tClassifiers\\nBagging\\tand\\tPasting\\nBagging\\tand\\tPasting\\tin\\tScikit-Learn\\nOut-of-Bag\\tEvaluation\\nRandom\\tPatches\\tand\\tRandom\\tSubspaces\\nRandom\\tForests\\nExtra-Trees', 'Feature\\tImportance\\nBoosting\\nAdaBoost\\nGradient\\tBoosting\\nStacking\\nExercises\\n8.\\tDimensionality\\tReduction\\nThe\\tCurse\\tof\\tDimensionality\\nMain\\tApproaches\\tfor\\tDimensionality\\tReduction\\nProjection\\nManifold\\tLearning\\nPCA\\nPreserving\\tthe\\tVariance\\nPrincipal\\tComponents\\nProjecting\\tDown\\tto\\td\\tDimensions\\nUsing\\tScikit-Learn\\nExplained\\tVariance\\tRatio\\nChoosing\\tthe\\tRight\\tNumber\\tof\\tDimensions\\nPCA\\tfor\\tCompression\\nIncremental\\tPCA\\nRandomized\\tPCA\\nKernel\\tPCA\\nSelecting\\ta\\tKernel\\tand\\tTuning\\tHyperparameters\\nLLE\\nOther\\tDimensionality\\tReduction\\tTechniques\\nExercises\\nII.\\tNeural\\tNetworks\\tand\\tDeep\\tLearning', '9.\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\nInstallation\\nCreating\\tYour\\tFirst\\tGraph\\tand\\tRunning\\tIt\\tin\\ta\\tSession\\nManaging\\tGraphs\\nLifecycle\\tof\\ta\\tNode\\tValue\\nLinear\\tRegression\\twith\\tTensorFlow\\nImplementing\\tGradient\\tDescent\\nManually\\tComputing\\tthe\\tGradients\\nUsing\\tautodiff\\nUsing\\tan\\tOptimizer\\nFeeding\\tData\\tto\\tthe\\tTraining\\tAlgorithm\\nSaving\\tand\\tRestoring\\tModels\\nVisualizing\\tthe\\tGraph\\tand\\tTraining\\tCurves\\tUsing\\tTensorBoard\\nName\\tScopes\\nModularity\\nSharing\\tVariables\\nExercises\\n10.\\tIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\nFrom\\tBiological\\tto\\tArtificial\\tNeurons\\nBiological\\tNeurons\\nLogical\\tComputations\\twith\\tNeurons\\nThe\\tPerceptron\\nMulti-Layer\\tPerceptron\\tand\\tBackpropagation\\nTraining\\tan\\tMLP\\twith\\tTensorFlow’s\\tHigh-Level\\tAPI\\nTraining\\ta\\tDNN\\tUsing\\tPlain\\tTensorFlow\\nConstruction\\tPhase\\nExecution\\tPhase', 'Using\\tthe\\tNeural\\tNetwork\\nFine-Tuning\\tNeural\\tNetwork\\tHyperparameters\\nNumber\\tof\\tHidden\\tLayers\\nNumber\\tof\\tNeurons\\tper\\tHidden\\tLayer\\nActivation\\tFunctions\\nExercises\\n11.\\tTraining\\tDeep\\tNeural\\tNets\\nVanishing/Exploding\\tGradients\\tProblems\\nXavier\\tand\\tHe\\tInitialization\\nNonsaturating\\tActivation\\tFunctions\\nBatch\\tNormalization\\nGradient\\tClipping\\nReusing\\tPretrained\\tLayers\\nReusing\\ta\\tTensorFlow\\tModel\\nReusing\\tModels\\tfrom\\tOther\\tFrameworks\\nFreezing\\tthe\\tLower\\tLayers\\nCaching\\tthe\\tFrozen\\tLayers\\nTweaking,\\tDropping,\\tor\\tReplacing\\tthe\\tUpper\\tLayers\\nModel\\tZoos\\nUnsupervised\\tPretraining\\nPretraining\\ton\\tan\\tAuxiliary\\tTask\\nFaster\\tOptimizers\\nMomentum\\tOptimization\\nNesterov\\tAccelerated\\tGradient\\nAdaGrad\\nRMSProp\\nAdam\\tOptimization', 'Learning\\tRate\\tScheduling\\nAvoiding\\tOverfitting\\tThrough\\tRegularization\\nEarly\\tStopping\\nℓ1\\tand\\tℓ2\\tRegularization\\nDropout\\nMax-Norm\\tRegularization\\nData\\tAugmentation\\nPractical\\tGuidelines\\nExercises\\n12.\\tDistributing\\tTensorFlow\\tAcross\\tDevices\\tand\\tServers\\nMultiple\\tDevices\\ton\\ta\\tSingle\\tMachine\\nInstallation\\nManaging\\tthe\\tGPU\\tRAM\\nPlacing\\tOperations\\ton\\tDevices\\nParallel\\tExecution\\nControl\\tDependencies\\nMultiple\\tDevices\\tAcross\\tMultiple\\tServers\\nOpening\\ta\\tSession\\nThe\\tMaster\\tand\\tWorker\\tServices\\nPinning\\tOperations\\tAcross\\tTasks\\nSharding\\tVariables\\tAcross\\tMultiple\\tParameter\\tServers\\nSharing\\tState\\tAcross\\tSessions\\tUsing\\tResource\\tContainers\\nAsynchronous\\tCommunication\\tUsing\\tTensorFlow\\tQueues\\nLoading\\tData\\tDirectly\\tfrom\\tthe\\tGraph\\nParallelizing\\tNeural\\tNetworks\\ton\\ta\\tTensorFlow\\tCluster\\nOne\\tNeural\\tNetwork\\tper\\tDevice\\nIn-Graph\\tVersus\\tBetween-Graph\\tReplication', 'Model\\tParallelism\\nData\\tParallelism\\nExercises\\n13.\\tConvolutional\\tNeural\\tNetworks\\nThe\\tArchitecture\\tof\\tthe\\tVisual\\tCortex\\nConvolutional\\tLayer\\nFilters\\nStacking\\tMultiple\\tFeature\\tMaps\\nTensorFlow\\tImplementation\\nMemory\\tRequirements\\nPooling\\tLayer\\nCNN\\tArchitectures\\nLeNet-5\\nAlexNet\\nGoogLeNet\\nResNet\\nExercises\\n14.\\tRecurrent\\tNeural\\tNetworks\\nRecurrent\\tNeurons\\nMemory\\tCells\\nInput\\tand\\tOutput\\tSequences\\nBasic\\tRNNs\\tin\\tTensorFlow\\nStatic\\tUnrolling\\tThrough\\tTime\\nDynamic\\tUnrolling\\tThrough\\tTime\\nHandling\\tVariable\\tLength\\tInput\\tSequences\\nHandling\\tVariable-Length\\tOutput\\tSequences\\nTraining\\tRNNs', 'Training\\ta\\tSequence\\tClassifier\\nTraining\\tto\\tPredict\\tTime\\tSeries\\nCreative\\tRNN\\nDeep\\tRNNs\\nDistributing\\ta\\tDeep\\tRNN\\tAcross\\tMultiple\\tGPUs\\nApplying\\tDropout\\nThe\\tDifficulty\\tof\\tTraining\\tover\\tMany\\tTime\\tSteps\\nLSTM\\tCell\\nPeephole\\tConnections\\nGRU\\tCell\\nNatural\\tLanguage\\tProcessing\\nWord\\tEmbeddings\\nAn\\tEncoder–Decoder\\tNetwork\\tfor\\tMachine\\tTranslation\\nExercises\\n15.\\tAutoencoders\\nEfficient\\tData\\tRepresentations\\nPerforming\\tPCA\\twith\\tan\\tUndercomplete\\tLinear\\tAutoencoder\\nStacked\\tAutoencoders\\nTensorFlow\\tImplementation\\nTying\\tWeights\\nTraining\\tOne\\tAutoencoder\\tat\\ta\\tTime\\nVisualizing\\tthe\\tReconstructions\\nVisualizing\\tFeatures\\nUnsupervised\\tPretraining\\tUsing\\tStacked\\tAutoencoders\\nDenoising\\tAutoencoders\\nTensorFlow\\tImplementation\\nSparse\\tAutoencoders', 'TensorFlow\\tImplementation\\nVariational\\tAutoencoders\\nGenerating\\tDigits\\nOther\\tAutoencoders\\nExercises\\n16.\\tReinforcement\\tLearning\\nLearning\\tto\\tOptimize\\tRewards\\nPolicy\\tSearch\\nIntroduction\\tto\\tOpenAI\\tGym\\nNeural\\tNetwork\\tPolicies\\nEvaluating\\tActions:\\tThe\\tCredit\\tAssignment\\tProblem\\nPolicy\\tGradients\\nMarkov\\tDecision\\tProcesses\\nTemporal\\tDifference\\tLearning\\tand\\tQ-Learning\\nExploration\\tPolicies\\nApproximate\\tQ-Learning\\nLearning\\tto\\tPlay\\tMs.\\tPac-Man\\tUsing\\tDeep\\tQ-Learning\\nExercises\\nThank\\tYou!\\nA.\\tExercise\\tSolutions\\nChapter\\t1:\\tThe\\tMachine\\tLearning\\tLandscape\\nChapter\\t2:\\tEnd-to-End\\tMachine\\tLearning\\tProject\\nChapter\\t3:\\tClassification\\nChapter\\t4:\\tTraining\\tModels\\nChapter\\t5:\\tSupport\\tVector\\tMachines\\nChapter\\t6:\\tDecision\\tTrees', 'Chapter\\t7:\\tEnsemble\\tLearning\\tand\\tRandom\\tForests\\nChapter\\t8:\\tDimensionality\\tReduction\\nChapter\\t9:\\tUp\\tand\\tRunning\\twith\\tTensorFlow\\nChapter\\t10:\\tIntroduction\\tto\\tArtificial\\tNeural\\tNetworks\\nChapter\\t11:\\tTraining\\tDeep\\tNeural\\tNets\\nChapter\\t12:\\tDistributing\\tTensorFlow\\tAcross\\tDevices\\tand\\tServers\\nChapter\\t13:\\tConvolutional\\tNeural\\tNetworks\\nChapter\\t14:\\tRecurrent\\tNeural\\tNetworks\\nChapter\\t15:\\tAutoencoders\\nChapter\\t16:\\tReinforcement\\tLearning\\nB.\\tMachine\\tLearning\\tProject\\tChecklist\\nFrame\\tthe\\tProblem\\tand\\tLook\\tat\\tthe\\tBig\\tPicture\\nGet\\tthe\\tData\\nExplore\\tthe\\tData\\nPrepare\\tthe\\tData\\nShort-List\\tPromising\\tModels\\nFine-Tune\\tthe\\tSystem\\nPresent\\tYour\\tSolution\\nLaunch!\\nC.\\tSVM\\tDual\\tProblem\\nD.\\tAutodiff\\nManual\\tDifferentiation\\nSymbolic\\tDifferentiation\\nNumerical\\tDifferentiation\\nForward-Mode\\tAutodiff\\nReverse-Mode\\tAutodiff', 'E.\\tOther\\tPopular\\tANN\\tArchitectures\\nHopfield\\tNetworks\\nBoltzmann\\tMachines\\nRestricted\\tBoltzmann\\tMachines\\nDeep\\tBelief\\tNets\\nSelf-Organizing\\tMaps\\nIndex']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Retriever:\n",
        "    \"\"\"Sentence embedding based Retrieval Based Augmented generation.\n",
        "        Given database of pdf files, retriever finds num_retrieved_docs relevant documents\"\"\"\n",
        "    def __init__(self, text, num_retrieved_docs=5):\n",
        "        # create a vectorstore database\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "        self.db = FAISS.from_texts(text, embeddings)\n",
        "        self.retriever = self.db.as_retriever(search_kwargs={\"k\": num_retrieved_docs})\n",
        "\n",
        "    def search(self, query):\n",
        "        # retrieve top k similar documents to query\n",
        "        docs = self.retriever.get_relevant_documents(query)\n",
        "        return docs"
      ],
      "metadata": {
        "id": "huP313BPWjcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "66nyJjHr4NkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Assistant:\n",
        "    \"\"\"Gemma 2b based assistant that replies given the retrieved documents\"\"\"\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "        # CPU Enabled uncomment below 👇🏽\n",
        "        # model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
        "        # GPU Enabled use below 👇🏽\n",
        "        self.Gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")\n",
        "\n",
        "    def create_prompt(self, query, retrieved_info):\n",
        "        # instruction to areply to query given the retrived information\n",
        "        prompt = f\"\"\"You need either to explain the concept or answer the question about Machine Learning.\n",
        "        Be detailed, use simple words and examples in your explanations. If required, utilize the relevant information.\n",
        "        If you doesn't know the answer, say that \"Sorry, I don't know the answer\" and add found relevant informations to it.\n",
        "        Question: {query}\n",
        "        Relevant information: {retrieved_info}\n",
        "        Output:\n",
        "        \"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def reply(self, query, retrieved_info):\n",
        "        prompt = self.create_prompt(query, retrieved_info)\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        # Generate text with a focus on factual responses\n",
        "        generated_text = self.Gemma.generate(\n",
        "            **input_ids,\n",
        "            max_length=500, # let answers be not that long\n",
        "            temperature=0.1, # Adjust temperature according to the task, for code generation it can be 0.9\n",
        "            max_new_tokens=500\n",
        "        )\n",
        "        # Decode and return the answer\n",
        "        answer = self.tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "        # answer = self.llm.invoke(prompt)\n",
        "        return answer"
      ],
      "metadata": {
        "id": "moGzPsdYXF_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot = Assistant()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4c6e0974f15e40f98a76df63f0f44b36",
            "b76429f0e8994def883df711e3155333",
            "73891815cad54a16832a8f9a83757722",
            "c04c653037394709841dfd68278b3310",
            "b67a5f0fa6b74e88b818e9ad310eb639",
            "079b743ffb23450a85852c1fef638467",
            "cd9acdff4537409997d6f8cd986d4296",
            "0768abfbc36c4cc3a609e8f36b07629e",
            "21dd17f24f1a49acb1f17c9c50d606b1",
            "7c8f86e5ed0b451bb85ebb74aad0346c",
            "03c003bd3aee4a8da1a3d1ba78c2c0ec"
          ]
        },
        "id": "2YqTTs4Orpd8",
        "outputId": "3f7cfa3c-e4c2-4c09-e81e-d8d903a704ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c6e0974f15e40f98a76df63f0f44b36"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = Retriever(text_data)"
      ],
      "metadata": {
        "id": "i0jVxLRN47nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_reply(query):\n",
        "  related_docs = retriever.search(query)\n",
        "  reply = bot.reply(query, related_docs)\n",
        "  return reply"
      ],
      "metadata": {
        "id": "6bpgJFc263Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply = generate_reply(\"What is supervised learning \")\n",
        "print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohvF0l5b8GGH",
        "outputId": "826ddfd7-7376-4b3c-9b7d-d8c3a79e849e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Both `max_new_tokens` (=500) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You need either to explain the concept or answer the question about Machine Learning.\n",
            "        Be detailed, use simple words and examples in your explanations. If required, utilize the relevant information.\n",
            "        If you doesn't know the answer, say that \"Sorry, I don't know the answer\" and add found relevant informations to it.\n",
            "        Question: What is supervised learning \n",
            "        Relevant information: [Document(page_content='Supervised/Unsupervised\\tLearning\\nMachine\\t\\nLearning\\tsystems\\tcan\\tbe\\tclassified\\taccording\\tto\\tthe\\tamount\\tand\\ttype\\tof\\tsupervision\\tthey\\tget\\nduring\\ttraining.\\tThere\\tare\\tfour\\tmajor\\tcategories:\\tsupervised\\tlearning,\\tunsupervised\\tlearning,\\nsemisupervised\\tlearning,\\tand\\tReinforcement\\tLearning.\\nSupervised\\tlearning\\nIn\\t\\nsupervised\\tlearning\\n,\\tthe\\ttraining\\tdata\\tyou\\tfeed\\tto\\tthe\\talgorithm\\tincludes\\tthe\\tdesired\\tsolutions,\\t\\ncalled\\nlabels\\n\\t(\\nFigure\\t1-5\\n).\\nFigure\\t1-5.\\t\\nA\\tlabeled\\ttraining\\tset\\tfor\\tsupervised\\tlearning\\t(e.g.,\\tspam\\tclassification)\\nA\\ttypical\\tsupervised\\tlearning\\ttask\\t\\nis\\t\\nclassification\\n.\\tThe\\t\\nspam\\tfilter\\tis\\ta\\tgood\\texample\\tof\\tthis:\\tit\\tis\\ttrained\\nwith\\tmany\\texample\\temails\\talong\\twith\\ttheir\\t\\nclass\\n\\t(spam\\tor\\tham),\\tand\\tit\\tmust\\tlearn\\thow\\tto\\tclassify\\tnew\\nemails.\\nAnother\\ttypical\\ttask\\tis\\tto\\tpredict\\ta\\t\\ntarget\\n\\tnumeric\\tvalue,\\tsuch\\tas\\tthe\\tprice\\tof\\ta\\tcar,\\tgiven\\ta\\tset\\tof\\t\\nfeatures\\n(mileage,\\tage,\\tbrand,\\tetc.)\\tcalled\\t\\npredictors\\n.\\t\\nThis\\tsort\\tof\\ttask\\tis\\t\\ncalled\\t\\nregression\\n\\t(\\nFigure\\t1-6\\n).\\n1\\n\\tTo\\ttrain'), Document(page_content='Chapter\\t1\\n:\\tThe\\tMachine\\tLearning\\tLandscape\\n1\\n.\\t\\nMachine\\tLearning\\tis\\tabout\\tbuilding\\tsystems\\tthat\\tcan\\tlearn\\tfrom\\tdata.\\tLearning\\tmeans\\tgetting\\tbetter\\tat\\nsome\\ttask,\\tgiven\\tsome\\tperformance\\tmeasure.\\n2\\n.\\t\\nMachine\\tLearning\\tis\\tgreat\\tfor\\tcomplex\\tproblems\\tfor\\twhich\\twe\\thave\\tno\\talgorithmic\\tsolution,\\tto\\nreplace\\tlong\\tlists\\tof\\thand-tuned\\trules,\\tto\\tbuild\\tsystems\\tthat\\tadapt\\tto\\tfluctuating\\tenvironments,\\tand\\nfinally\\tto\\thelp\\thumans\\tlearn\\t(e.g.,\\tdata\\tmining).\\n3\\n.\\t\\nA\\tlabeled\\ttraining\\tset\\tis\\ta\\ttraining\\tset\\tthat\\tcontains\\tthe\\tdesired\\tsolution\\t(a.k.a.\\ta\\tlabel)\\tfor\\teach\\ninstance.\\n4\\n.\\t\\nThe\\ttwo\\tmost\\tcommon\\tsupervised\\ttasks\\tare\\tregression\\tand\\tclassification.\\n5\\n.\\t\\nCommon\\tunsupervised\\ttasks\\tinclude\\tclustering,\\tvisualization,\\tdimensionality\\treduction,\\tand\\nassociation\\trule\\tlearning.\\n6\\n.\\t\\nReinforcement\\tLearning\\tis\\tlikely\\tto\\tperform\\tbest\\tif\\twe\\twant\\ta\\trobot\\tto\\tlearn\\tto\\twalk\\tin\\tvarious\\nunknown\\tterrains\\tsince\\tthis\\tis\\ttypically\\tthe\\ttype\\tof\\tproblem\\tthat\\tReinforcement\\tLearning\\ttackles.\\tIt'), Document(page_content='Chapter\\t16\\n:\\tReinforcement\\tLearning\\n1\\n.\\t\\nReinforcement\\tLearning\\tis\\tan\\tarea\\tof\\tMachine\\tLearning\\taimed\\tat\\tcreating\\tagents\\tcapable\\tof\\ttaking\\nactions\\tin\\tan\\tenvironment\\tin\\ta\\tway\\tthat\\tmaximizes\\trewards\\tover\\ttime.\\tThere\\tare\\tmany\\tdifferences\\nbetween\\tRL\\tand\\tregular\\tsupervised\\tand\\tunsupervised\\tlearning.\\tHere\\tare\\ta\\tfew:\\nIn\\tsupervised\\tand\\tunsupervised\\tlearning,\\tthe\\tgoal\\tis\\tgenerally\\tto\\tfind\\tpatterns\\tin\\tthe\\tdata.\\tIn\\nReinforcement\\tLearning,\\tthe\\tgoal\\tis\\tto\\tfind\\ta\\tgood\\tpolicy.\\nUnlike\\tin\\tsupervised\\tlearning,\\tthe\\tagent\\tis\\tnot\\texplicitly\\tgiven\\tthe\\t“right”\\tanswer.\\tIt\\tmust\\tlearn\\nby\\ttrial\\tand\\terror.\\nUnlike\\tin\\tunsupervised\\tlearning,\\tthere\\tis\\ta\\tform\\tof\\tsupervision,\\tthrough\\trewards.\\tWe\\tdo\\tnot\\ttell\\nthe\\tagent\\thow\\tto\\tperform\\tthe\\ttask,\\tbut\\twe\\tdo\\ttell\\tit\\twhen\\tit\\tis\\tmaking\\tprogress\\tor\\twhen\\tit\\tis\\nfailing.\\nA\\tReinforcement\\tLearning\\tagent\\tneeds\\tto\\tfind\\tthe\\tright\\tbalance\\tbetween\\texploring\\tthe\\nenvironment,\\tlooking\\tfor\\tnew\\tways\\tof\\tgetting\\trewards,\\tand\\texploiting\\tsources\\tof\\trewards\\tthat\\tit'), Document(page_content='Figure\\t1-6.\\t\\nRegression\\nNote\\tthat\\tsome\\tregression\\talgorithms\\tcan\\tbe\\tused\\tfor\\tclassification\\tas\\twell,\\tand\\tvice\\tversa.\\tFor\\texample,\\nLogistic\\tRegression\\n\\t\\nis\\tcommonly\\tused\\tfor\\tclassification,\\tas\\tit\\tcan\\toutput\\ta\\tvalue\\tthat\\tcorresponds\\tto\\tthe\\nprobability\\tof\\tbelonging\\tto\\ta\\tgiven\\tclass\\t(e.g.,\\t20%\\tchance\\tof\\tbeing\\tspam).\\nHere\\tare\\tsome\\tof\\tthe\\tmost\\timportant\\tsupervised\\tlearning\\talgorithms\\t(covered\\tin\\tthis\\tbook):\\nk-Nearest\\tNeighbors\\nLinear\\tRegression\\nLogistic\\tRegression\\nSupport\\tVector\\tMachines\\t(SVMs)\\nDecision\\tTrees\\tand\\tRandom\\tForests\\nNeural\\tnetworks\\n2\\nUnsupervised\\tlearning\\nIn\\t\\nunsupervised\\tlearning\\n,\\t\\nas\\tyou\\tmight\\tguess,\\tthe\\ttraining\\tdata\\tis\\tunlabeled\\t(\\nFigure\\t1-7\\n).\\tThe\\tsystem\\ttries\\nto\\tlearn\\twithout\\ta\\tteacher.'), Document(page_content='Chapter\\t3.\\t\\nClassification\\nIn\\t\\nChapter\\t1\\n\\twe\\tmentioned\\tthat\\tthe\\tmost\\tcommon\\tsupervised\\tlearning\\ttasks\\tare\\tregression\\t(predicting\\nvalues)\\tand\\tclassification\\t(predicting\\tclasses).\\tIn\\t\\nChapter\\t2\\n\\twe\\texplored\\ta\\tregression\\ttask,\\tpredicting\\nhousing\\tvalues,\\tusing\\tvarious\\talgorithms\\tsuch\\tas\\tLinear\\tRegression,\\tDecision\\tTrees,\\tand\\tRandom\\tForests\\n(which\\twill\\tbe\\texplained\\tin\\tfurther\\tdetail\\tin\\tlater\\tchapters).\\tNow\\twe\\twill\\tturn\\tour\\tattention\\tto\\nclassification\\tsystems.')]\n",
            "        Output:\n",
            "        Sure, here is a detailed explanation of the concept of supervised learning:\n",
            "\n",
            "Supervised learning is a type of machine learning where the algorithm is trained on a dataset that has both input and output data. The algorithm learns to map the input data to the output data, so that it can be used to make new predictions on unseen data.\n",
            "\n",
            "The training process involves the following steps:\n",
            "\n",
            "1. **Data preparation:** The dataset is cleaned and prepared for training. This involves removing any missing values, outliers, and inconsistent data points.\n",
            "2. **Feature engineering:** The input data is transformed into a format that is suitable for the algorithm. This may involve scaling the data, encoding categorical variables, and creating new features.\n",
            "3. **Training the model:** The algorithm is trained on the prepared dataset. This involves running the algorithm through the dataset and adjusting its parameters to minimize the error between the predicted output and the actual output.\n",
            "4. **Evaluation:** Once the model is trained, it is evaluated on a separate dataset. This allows the model to be assessed on its performance and to determine how well it can generalize to unseen data.\n",
            "\n",
            "Supervised learning algorithms can be used for a wide variety of tasks, including:\n",
            "\n",
            "* Classification: Predicting the class of a new data point.\n",
            "* Regression: Predicting a continuous value, such as price or temperature.\n",
            "* Clustering: Grouping data points into similar groups.\n",
            "\n",
            "Supervised learning is a powerful technique that can be used to achieve high accuracy in machine learning tasks. However, it does require a large amount of data to be effective.\n"
          ]
        }
      ]
    }
  ]
}